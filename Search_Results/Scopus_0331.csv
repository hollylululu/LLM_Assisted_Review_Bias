"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","Cited by","DOI","Link","Abstract","Publisher","PubMed ID","Document Type","Publication Stage","Open Access","Source","EID"
"Sudha L.; Aruna K.B.; Sureka V.; Niveditha M.; Prema S.","Sudha, Lakshmanan (57207217074); Aruna, Kari Balakrishnan (57200138702); Sureka, Vijayakumar (57200141626); Niveditha, Mathavan (59459203800); Prema, S. (58906279800)","57207217074; 57200138702; 57200141626; 59459203800; 58906279800","Semantic Image Synthesis from Text: Current Trends and Future Horizons in Text-to-Image Generation","2025","EAI Endorsed Transactions on Internet of Things","11","","","","","","0","10.4108/eetiot.5336","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211156231&doi=10.4108%2feetiot.5336&partnerID=40&md5=35f74590e055df0c8e1aab64e6b9e797","Text-to-image generation, a captivating intersection of natural language processing and computer vision, has undergone a remarkable evolution in recent years. This research paper provides a comprehensive review of the state-of-the-art in text-to-image generation techniques, highlighting key advancements and emerging trends. We begin by surveying the foundational models, with a focus on Generative Adversarial Networks (GANs) and their pivotal role in generating realistic and diverse images from textual descriptions. We delve into the intricacies of training data, model architectures, and evaluation metrics, offering insights into the challenges and opportunities in this field. Furthermore, this paper explores the synergistic relationship between natural language processing and computer vision, showcasing multimodal models like DALL-E and CLIP. These models not only generate images from text but also understand the contextual relationships between textual descriptions and images, opening avenues for content recommendation, search engines, and visual storytelling. The paper discusses applications spanning art, design, e-commerce, healthcare, and education, where text-to-image generation has made significant inroads. We highlight the potential of this technology in automating content creation, aiding in diagnostics, and transforming the fashion and e-commerce industries. However, the journey of text-to-image generation is not without its challenges. We address ethical considerations, emphasizing responsible AI and the mitigation of biases in generated content. We also explore interpretability and model transparency, critical for ensuring trust and accountability. © 2024 L. Sudha et al.","European Alliance for Innovation","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85211156231"
"Gallo R.J.; Baiocchi M.; Savage T.R.; Chen J.H.","Gallo, Robert J. (58285262900); Baiocchi, Michael (36150428700); Savage, Thomas R. (57238513600); Chen, Jonathan H. (57112911000)","58285262900; 36150428700; 57238513600; 57112911000","Establishing best practices in large language model research: an application to repeat prompting","2025","Journal of the American Medical Informatics Association","32","2","","386","390","4","1","10.1093/jamia/ocae294","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216606586&doi=10.1093%2fjamia%2focae294&partnerID=40&md5=ec6f10d921d424c363d8964272a342ff","Objectives: We aimed to demonstrate the importance of establishing best practices in large language model research, using repeat prompting as an illustrative example. Materials and Methods: Using data from a prior study investigating potential model bias in peer review of medical abstracts, we compared methods that ignore correlation in model outputs from repeated prompting with a random effects method that accounts for this correlation. Results: High correlation within groups was found when repeatedly prompting the model, with intraclass correlation coefficient of 0.69. Ignoring the inherent correlation in the data led to over 100-fold inflation of effective sample size. After appropriately accounting for this issue, the authors’ results reverse from a small but highly significant finding to no evidence of model bias. Discussion: The establishment of best practices for LLM research is urgently needed, as demonstrated in this case where accounting for repeat prompting in analyses was critical for accurate study conclusions. © The Author(s) 2024. Published by Oxford University Press on behalf of the American Medical Informatics Association.","Oxford University Press","39656836","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85216606586"
"Young C.C.; Enichen E.; Rao A.; Succi M.D.","Young, Cameron C. (57222156516); Enichen, Elizabeth (59328205600); Rao, Arya (57210925940); Succi, Marc D. (55939445100)","57222156516; 59328205600; 57210925940; 55939445100","Racial, ethnic, and sex bias in large language model opioid recommendations for pain management","2025","Pain","166","3","","511","517","6","4","10.1097/j.pain.0000000000003388","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204024749&doi=10.1097%2fj.pain.0000000000003388&partnerID=40&md5=55dca7caa217086a2bba229ed6ec3d3b","Understanding how large language model (LLM) recommendations vary with patient race/ethnicity provides insight into how LLMs may counter or compound bias in opioid prescription. Forty real-world patient cases were sourced from the MIMIC-IV Note dataset with chief complaints of abdominal pain, back pain, headache, or musculoskeletal pain and amended to include all combinations of race/ethnicity and sex. Large language models were instructed to provide a subjective pain rating and comprehensive pain management recommendation. Univariate analyses were performed to evaluate the association between racial/ethnic group or sex and the specified outcome measures - subjective pain rating, opioid name, order, and dosage recommendations - suggested by 2 LLMs (GPT-4 and Gemini). Four hundred eighty real-world patient cases were provided to each LLM, and responses included pharmacologic and nonpharmacologic interventions. Tramadol was the most recommended weak opioid in 55.4% of cases, while oxycodone was the most frequently recommended strong opioid in 33.2% of cases. Relative to GPT-4, Gemini was more likely to rate a patient's pain as ""severe""(OR: 0.57 95% CI: [0.54, 0.60]; P < 0.001), recommend strong opioids (OR: 2.05 95% CI: [1.59, 2.66]; P < 0.001), and recommend opioids later (OR: 1.41 95% CI: [1.22, 1.62]; P < 0.001). Race/ethnicity and sex did not influence LLM recommendations. This study suggests that LLMs do not preferentially recommend opioid treatment for one group over another. Given that prior research shows race-based disparities in pain perception and treatment by healthcare providers, LLMs may offer physicians a helpful tool to guide their pain management and ensure equitable treatment across patient groups.  © 2024 International Association for the Study of Pain.","Lippincott Williams and Wilkins","39283333","Article","Final","","Scopus","2-s2.0-85204024749"
"Hsieh P.-H.","Hsieh, Pei-Hsun (57935269800)","57935269800","Psychological reactance to vaccine mandates on Twitter: a study of sentiments in the United States","2025","Journal of Public Health Policy","","","","","","","0","10.1057/s41271-025-00554-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217239858&doi=10.1057%2fs41271-025-00554-0&partnerID=40&md5=0f73dc354723d5900e9e9abad22edf6e","This study examines the relationship between vaccine mandates and public sentiment toward vaccines and health officials on Twitter. I analyzed 6.6 million vaccine-related tweets from July 2021 to February 2022 in the United States. Leveraging a large language model, BERT, I identified tweets discussing vaccine mandates even when lacking explicit keywords. Compared to non-mandate tweets, those mentioning mandates exhibit greater negativity, anger, and freedom-related language. Furthermore, increased state-level discussion of mandates correlates with rising levels of negativity and anger toward both vaccines and public health officials. Finally, greater disparity in vaccination progress across counties within a state is associated with increased anger in tweets directed toward both. © The Author(s) 2025.","Palgrave Macmillan","","Article","Article in press","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85217239858"
"Zhu M.; Yang Q.; Gao Z.; Yuan Y.; Liu J.","Zhu, Meilu (57200920263); Yang, Qiushi (57224307944); Gao, Zhifan (55320405200); Yuan, Yixuan (55932867600); Liu, Jun (57218181324)","57200920263; 57224307944; 55320405200; 55932867600; 57218181324","FedBM: Stealing knowledge from pre-trained language models for heterogeneous federated learning","2025","Medical Image Analysis","102","","103524","","","","0","10.1016/j.media.2025.103524","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000465365&doi=10.1016%2fj.media.2025.103524&partnerID=40&md5=3118a378046dbcf04b8b46e3654b824f","Federated learning (FL) has shown great potential in medical image computing since it provides a decentralized learning paradigm that allows multiple clients to train a model collaboratively without privacy leakage. However, current studies have shown that data heterogeneity incurs local learning bias in classifiers and feature extractors of client models during local training, leading to the performance degradation of a federation system. To address these issues, we propose a novel framework called Federated Bias eliMinating (FedBM) to get rid of local learning bias in heterogeneous federated learning (FL), which mainly consists of two modules, i.e., Linguistic Knowledge-based Classifier Construction (LKCC) and Concept-guided Global Distribution Estimation (CGDE). Specifically, LKCC exploits class concepts, prompts and pre-trained language models (PLMs) to obtain concept embeddings. These embeddings are used to estimate the latent concept distribution of each class in the linguistic space. Based on the theoretical derivation, we can rely on these distributions to pre-construct a high-quality classifier for clients to achieve classification optimization, which is frozen to avoid classifier bias during local training. CGDE samples probabilistic concept embeddings from the latent concept distributions to learn a conditional generator to capture the input space of the global model. Three regularization terms are introduced to improve the quality and utility of the generator. The generator is shared by all clients and produces pseudo data to calibrate updates of local feature extractors. Extensive comparison experiments and ablation studies on public datasets demonstrate the superior performance of FedBM over state-of-the-arts and confirm the effectiveness of each module, respectively. The code is available at https://github.com/CUHK-AIM-Group/FedBM. © 2025 Elsevier B.V.","Elsevier B.V.","","Article","Final","","Scopus","2-s2.0-86000465365"
"Arslan B.; Nuhoglu C.; Satici M.O.; Altinbilek E.","Arslan, B. (57832887600); Nuhoglu, C. (59363789200); Satici, M.O. (57217532209); Altinbilek, E. (55560900900)","57832887600; 59363789200; 57217532209; 55560900900","Evaluating LLM-based generative AI tools in emergency triage: A comparative study of ChatGPT Plus, Copilot Pro, and triage nurses","2025","American Journal of Emergency Medicine","89","","","174","181","7","1","10.1016/j.ajem.2024.12.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213016878&doi=10.1016%2fj.ajem.2024.12.024&partnerID=40&md5=5a79c14f7f9f65fdbcc8fea2b38d174d","Background: The number of emergency department (ED) visits has been on steady increase globally. Artificial Intelligence (AI) technologies, including Large Language Model (LLMs)-based generative AI models, have shown promise in improving triage accuracy. This study evaluates the performance of ChatGPT and Copilot in triage at a high-volume urban hospital, hypothesizing that these tools can match trained physicians' accuracy and reduce human bias amidst ED crowding challenges. Methods: This single-center, prospective observational study was conducted in an urban ED over one week. Adult patients were enrolled through random 24-h intervals. Exclusions included minors, trauma cases, and incomplete data. Triage nurses assessed patients while an emergency medicine (EM) physician documented clinical vignettes and assigned emergency severity index (ESI) levels. These vignettes were then introduced to ChatGPT and Copilot for comparison with the triage nurse's decision. Results: The overall triage accuracy was 65.2 % for nurses, 66.5 % for ChatGPT, and 61.8 % for Copilot, with no significant difference (p = 0.000). Moderate agreement was observed between the EM physician and ChatGPT, triage nurses, and Copilot (Cohen's Kappa = 0.537, 0.477, and 0.472, respectively). In recognizing high-acuity patients, ChatGPT and Copilot outperformed triage nurses (87.8 % and 85.7 % versus 32.7 %, respectively). Compared to ChatGPT and Copilot, nurses significantly under-triaged patients (p < 0.05). The analysis of predictive performance for ChatGPT, Copilot, and triage nurses demonstrated varying discrimination abilities across ESI levels, all of which were statistically significant (p < 0.05). ChatGPT and Copilot exhibited consistent accuracy across age, gender, and admission time, whereas triage nurses were more likely to mistriage patients under 45 years old. Conclusion: ChatGPT and Copilot outperform traditional nurse triage in identifying high-acuity patients, but real-time ED capacity data is crucial to prevent overcrowding and ensure high-quality of emergency care. © 2024 Elsevier Inc.","W.B. Saunders","39731895","Article","Final","","Scopus","2-s2.0-85213016878"
"Garcia Valencia O.A.; Thongprayoon C.; Jadlowiec C.C.; Mao S.A.; Leeaphorn N.; Budhiraja P.; Khoury N.; Pham J.H.; Craici I.M.; Gonzalez Suarez M.L.; Cheungpasitporn W.","Garcia Valencia, Oscar A. (57205373508); Thongprayoon, Charat (55512490600); Jadlowiec, Caroline C. (54379938500); Mao, Shennen A. (56096828900); Leeaphorn, Napat (54889052300); Budhiraja, Pooja (16068190900); Khoury, Nadeen (57196050788); Pham, Justin H. (58245302000); Craici, Iasmina M. (8730515900); Gonzalez Suarez, Maria L. (56005758800); Cheungpasitporn, Wisit (47160959700)","57205373508; 55512490600; 54379938500; 56096828900; 54889052300; 16068190900; 57196050788; 58245302000; 8730515900; 56005758800; 47160959700","Advancing health equity: evaluating AI translations of kidney donor information for Spanish speakers","2025","Frontiers in Public Health","13","","1484790","","","","0","10.3389/fpubh.2025.1484790","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217423319&doi=10.3389%2ffpubh.2025.1484790&partnerID=40&md5=4681c5c210eba700b78839c6d0cca744","Background: Health equity and access to essential medical information remain significant challenges, especially for the Spanish-speaking Hispanic population, which faces barriers in accessing living kidney donation opportunities. ChatGPT, an AI language model with sophisticated natural language processing capabilities, has been identified as a promising tool for translating critical health information into Spanish. This study aims to assess ChatGPT’s translation efficacy to ensure the information provided is accurate and culturally relevant. Methods: This study utilized ChatGPT versions 3.5 and 4.0 to translate 27 frequently asked questions (FAQs) from English to Spanish, sourced from Donate Life America’s website. The translated content was reviewed by native Spanish-speaking nephrologists using a standard rubric scale (1–5). The assessment focused on linguistic accuracy and cultural sensitivity, emphasizing retention of the original message, appropriate vocabulary and grammar, and cultural relevance. Results: The mean linguistic accuracy scores were 4.89 ± 0.32 for GPT-3.5 and 5.00 ± 0.00 for GPT-4.0 (p = 0.08). The percentage of excellent-quality translations (score = 5) in linguistic accuracy was 89% for GPT-3.5 and 100% for GPT-4.0 (p = 0.24). The mean cultural sensitivity scores were 4.89 ± 0.32 for both GPT-3.5 and GPT-4.0 (p = 1.00). Similarly, excellent-quality translations in cultural sensitivity were achieved in 89% of cases for both versions (p = 1.00). Conclusion: ChatGPT 4.0 demonstrates strong potential to enhance health equity by improving Spanish-speaking Hispanic patients’ access to LKD information through accurate and culturally sensitive translations. These findings highlight the role of AI in mitigating healthcare disparities and underscore the need for integrating AI-driven tools into healthcare systems. Future efforts should focus on developing accessible platforms and establishing guidelines to maximize AI’s impact on equitable healthcare delivery and patient education. Copyright © 2025 Garcia Valencia, Thongprayoon, Jadlowiec, Mao, Leeaphorn, Budhiraja, Khoury, Pham, Craici, Gonzalez Suarez and Cheungpasitporn.","Frontiers Media SA","39931300","Article","Final","","Scopus","2-s2.0-85217423319"
"Wang C.; Xiao C.; Zhang X.; Zhu Y.; Chen X.; Li Y.; Qi H.","Wang, Chen (57222561625); Xiao, Changqi (59564590400); Zhang, Xuejiao (59564894900); Zhu, Yingying (59564795800); Chen, Xueqing (59564276700); Li, Yilin (59312818500); Qi, Huiying (36705149700)","57222561625; 59564590400; 59564894900; 59564795800; 59564276700; 59312818500; 36705149700","Exploring medical students’ intention to use of ChatGPT from a programming course: a grounded theory study in China","2025","BMC Medical Education","25","1","209","","","","0","10.1186/s12909-025-06807-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218229852&doi=10.1186%2fs12909-025-06807-6&partnerID=40&md5=2ae6a72fcb7b798ea357f3ed4d9f2f1a","Background: In interdisciplinary general education courses, medical students face the daunting challenge of learning programming due to academic pressure, cognitive biases, and differences in thinking patterns. ChatGPT provides an effective way for people to acquire knowledge, improve learning efficiency, and quality. Objective: To explore whether medical students can be assisted in learning programming with the help of ChatGPT, it is necessary to investigate their experience and perception of using ChatGPT, and to study which factors influence their willingness to use ChatGPT. Methods: Drawing on the grounded theory research paradigm, this paper constructs a research model of the influencing factors of ChatGPT usage willingness for medical students in programming courses through the analysis of interview data from 30 undergraduate medical students. It analyzes and discusses the cognition and influencing factors of medical students’ willingness to use ChatGPT in programming learning. Results: The willingness to use ChatGPT in programming learning is divided into three types based on the students’ subjective degree of use: active use, neutral use, and negative use. It is also found that individual factors, technical factors, information factors, and environmental factors are four important dimensions affecting the willingness to use ChatGPT. Conclusions: Based on the analysis of influencing factors, strategies and suggestions such as preventing risks and focusing on ethical education, cultivating critical thinking and establishing a case library, and personalized teaching to enhance core literacy in programming are proposed. © The Author(s) 2025.","BioMed Central Ltd","39923098","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85218229852"
"Al-Qudimat A.R.; Fares Z.E.; Elaarag M.; Osman M.; Al-Zoubi R.M.; Aboumarzouk O.M.","Al-Qudimat, Ahmad R. (57222188028); Fares, Zainab E. (58369434600); Elaarag, Mai (57769382700); Osman, Maha (59659982800); Al-Zoubi, Raed M. (24072809100); Aboumarzouk, Omar M. (29067501200)","57222188028; 58369434600; 57769382700; 59659982800; 24072809100; 29067501200","Advancing Medical Research Through Artificial Intelligence: Progressive and Transformative Strategies: A Literature Review","2025","Health Science Reports","8","2","e70200","","","","0","10.1002/hsr2.70200","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219164836&doi=10.1002%2fhsr2.70200&partnerID=40&md5=41fffd0f18c421839218d589eff8bba4","Background and Aims: Artificial intelligence (AI) has become integral to medical research, impacting various aspects such as data analysis, writing assistance, and publishing. This paper explores the multifaceted influence of AI on the process of writing medical research papers, encompassing data analysis, ethical considerations, writing assistance, and publishing efficiency. Methods: The review was conducted following the PRISMA guidelines; a comprehensive search was performed in Scopus, PubMed, EMBASE, and MEDLINE databases for research publications on artificial intelligence in medical research published up to October 2023. Results: AI facilitates the writing process by generating drafts, offering grammar and style suggestions, and enhancing manuscript quality through advanced models like ChatGPT. Ethical concerns regarding content ownership and potential biases in AI-generated content underscore the need for collaborative efforts among researchers, publishers, and AI creators to establish ethical standards. Moreover, AI significantly influences data analysis in healthcare, optimizing outcomes and patient care, particularly in fields such as obstetrics and gynecology and pharmaceutical research. The application of AI in publishing, ranging from peer review to manuscript quality control and journal matching, underscores its potential to streamline and enhance the entire research and publication process. Overall, while AI presents substantial benefits, ongoing research, and ethical guidelines are essential for its responsible integration into the evolving landscape of medical research and publishing. Conclusion: The integration of AI in medical research has revolutionized efficiency and innovation, impacting data analysis, writing assistance, publishing, and others. While AI tools offer significant benefits, ethical considerations such as biases and content ownership must be addressed. Ongoing research and collaborative efforts are crucial to ensure responsible and transparent AI implementation in the dynamic landscape of medical research and publishing. © 2024 The Authors. Health Science Reports published by Wiley Periodicals LLC.","John Wiley and Sons Inc","","Article","Final","","Scopus","2-s2.0-85219164836"
"Occhipinti J.-A.; Prodan A.; Hynes W.; Buchanan J.; Green R.; Burrow S.; Eyre H.A.; Skinner A.; Hickie I.B.; Heffernan M.; Song Y.J.C.; Ujdur G.; Tanner M.","Occhipinti, Jo-An (57958195900); Prodan, Ante (24469250600); Hynes, William (57191514063); Buchanan, John (8580073900); Green, Roy (7403916331); Burrow, Sharan (58970670600); Eyre, Harris A. (35519384100); Skinner, Adam (57211686695); Hickie, Ian B. (17734678500); Heffernan, Mark (57189591032); Song, Yun Ju Christine (55494043100); Ujdur, Goran (58644006500); Tanner, Marcel (58970213000)","57958195900; 24469250600; 57191514063; 8580073900; 7403916331; 58970670600; 35519384100; 57211686695; 17734678500; 57189591032; 55494043100; 58644006500; 58970213000","Artificial intelligence, recessionary pressures and population health; [Intelligence artificielle, pressions récessionnistes et santé des populations]; [Inteligencia artificial, presiones recesivas y salud de la población]","2025","Bulletin of the World Health Organization","103","2","","155","163","8","2","10.2471/BLT.24.291950","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217357047&doi=10.2471%2fBLT.24.291950&partnerID=40&md5=89828290f7f5bcb2d44cb5a60af3588f","Economic and labour policies have a considerable influence on health and well-being through direct financial impacts, and by shaping social and physical environments. Strong economies are important for public health investment and employment, yet the rapid rise of generative artificial intelligence (AI) has the potential to reshape economies, presenting challenges beyond mere temporary market disruption. Generative AI can perform non-routine cognitive tasks, previously unattainable though traditional automation, creating new efficiencies. While this technology offers opportunities for innovation and productivity, its labour-displacing potential raises serious concerns about economic stability and social equity, both of which are critical to health. Job displacement driven by generative AI could worsen income inequality, shrink middle-class opportunities and reduce consumer demand, triggering recessionary pressures. In this article, we propose the existence of an AI-capital-to-labour ratio threshold beyond which a self-reinforcing cycle of recessionary pressures may emerge, and which market forces alone cannot correct. Traditional responses to such pressures, like fiscal stimulus or monetary easing, may be ineffective in addressing structural disruptions to labour markets caused by generative AI. We call for a proactive global response to harness the benefits of generative AI while mitigating risks. This response should focus on reorienting economic systems towards collective well-being, as emphasized in the World Health Assembly resolution Economics of health for all and the United Nations' Global Digital Compact. Integrated strategies that combine fiscal policy, regulation and social policies are critical to ensuring generative AI advances societal health and equity while avoiding harm from excessive job displacement. © 2025 The authors; licensee World Health Organization.","World Health Organization","39882489","Article","Final","","Scopus","2-s2.0-85217357047"
"Thaya I.M.; Sasirekha S.; Arun N.; Kanyalakshmi G.","Thaya, I.M. (59659216200); Sasirekha, S. (57212600875); Arun, N. (59659032700); Kanyalakshmi, G. (59659778100)","59659216200; 57212600875; 59659032700; 59659778100","An XAI-Driven Support System to Enhance the Detection and Diagnosis of Liver Tumor for Interventional Radiologist","2025","Journal of Information Systems Engineering and Management","10","","","40","46","6","0","10.52783/jisem.v10i11s.1492","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219090147&doi=10.52783%2fjisem.v10i11s.1492&partnerID=40&md5=65620a20672aa969ce3910c0eeb1169a","In healthcare, the use of opaque deep learning models often results in limited transparency, potential bias, and inaccuracies, leading to a lack of trust among healthcare providers and patients. To address these challenges, this work integrates Explainable Artificial Intelligence (XAI) methods to enhance the transparency and interpretability of AI models, particularly in liver tumor segmentation. By employing XAI techniques, such as GradCAM (Gradient-weighted Class Activation Mapping), the proposed approach provides visual explanations that highlight the most critical regions influencing the model's predictions. This study focuses on combining state-of-the-art deep learning models, achieving a high accuracy of 99%, to ensure precise and reliable segmentation of liver tumors. GradCAM further enhances this process by generating heatmaps that explain the AI's decision-making, fostering trust and reliability among medical professionals. Beyond segmentation, the framework extends to decision support systems that offer transparent insights into medical decision-making, predictive analytics for patient outcome forecasting, and natural language processing for analyzing medical data. This approach ultimately empowers interventional medical professionals with accurate, interpretable, and trustworthy AI solutions, transforming how liver tumors are analyzed and segmented. Copyright © 2024 by Author/s and Licensed by JISEM.","IADITI - International Association for Digital Transformation and Technological Innovation","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85219090147"
"Zarfati M.; Soffer S.; Nadkarni G.N.; Klang E.","Zarfati, Mor (59405193100); Soffer, Shelly (57190729426); Nadkarni, Girish N. (36237038500); Klang, Eyal (56080228800)","59405193100; 57190729426; 36237038500; 56080228800","Retrieval-Augmented Generation: Advancing personalized care and research in oncology","2025","European Journal of Cancer","220","","115341","","","","0","10.1016/j.ejca.2025.115341","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000304757&doi=10.1016%2fj.ejca.2025.115341&partnerID=40&md5=8f3c08d786e64acd4d54e7162542e6dc","Retrieval-Augmented Generation (RAG) pairs large language models (LLMs) with recent data to produce more accurate, context-aware outputs. By converting text into numeric embeddings, RAG locates and retrieves relevant “chunks” of data, that along with the query, ground the model's responses in current, specific information. This process helps reduce outdated or fabricated answers. In oncology, RAG has shown particular promise. Studies have demonstrated its ability to improve treatment recommendations by integrating genetic profiles, strengthened clinical trial matching through biomarker analysis, and accelerated drug development by clarifying model-driven insights. Despite its advantages, RAG depends on high-quality data. Biased or incomplete sources can lead to inaccurate outcomes. Careful implementation and human oversight are crucial for ensuring the effectiveness and reliability of RAG in oncology. © 2025 Elsevier Ltd","Elsevier Ltd","","Article","Final","","Scopus","2-s2.0-86000304757"
"Hasan S.S.; Woo J.J.; Cote M.P.; Ramkumar P.N.","Hasan, Sayyida S. (57210264348); Woo, Joshua J. (58307960100); Cote, Mark P. (24831832400); Ramkumar, Prem N. (56157365700)","57210264348; 58307960100; 24831832400; 56157365700","Generative Versus Nongenerative Artificial Intelligence","2025","Arthroscopy - Journal of Arthroscopic and Related Surgery","41","3","","545","546","1","1","10.1016/j.arthro.2024.12.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217026497&doi=10.1016%2fj.arthro.2024.12.001&partnerID=40&md5=b79fbede0183f2e7dbcaec7a987e59c5","Artificial intelligence (AI) is a colossal buzzword, a confusing subject matter, but also an inevitable reality. Generative and nongenerative AI are the 2 core subtypes of AI. Generative AI uses current data to understand patterns and generate new information, and it is especially valuable in producing synthetic medical images, enhancing surgical simulations, and expanding training datasets. Techniques such as generative adversarial networks (GANs), large language models (LLMs), and variational autoencoders (VAEs) allow for the creation of realistic simulations, text, and models that can be used for perioperative communication and planning. Conversely, nongenerative AI is centered on the examination and categorization of pre-existing data to formulate predictions or decisions—the most popular denomination namely machine learning. This approach is instrumental in tasks such as forecasting surgical outcomes, segmenting medical images, and determining patient risk profiles. Models such as convolutional neural networks (CNNs), random forests, and support vector machines (SVMs) are widely used for these purposes, demonstrating high accuracy and reliability in clinical decision making. Although generative AI offers innovative tools for creating new data and simulations, nongenerative AI excels in analyzing existing data to inform patient care. Both approaches have the potential of supporting clinical workflows to automate redundancies and improve efficiencies. However, there are also limitations in the application of AI in orthopaedics, including the potential for bias in models, the challenge of interpreting AI-driven insights, and the ethics of oversight. As the integration of AI in orthopaedics continues to grow, it is essential for practitioners to understand these technologies' capabilities and limitations to harness their full potential and establish appropriate governance. © 2024 Arthroscopy Association of North America","W.B. Saunders","39929595","Article","Final","","Scopus","2-s2.0-85217026497"
"Comeau D.S.; Bitterman D.S.; Celi L.A.","Comeau, Donnella S. (57444552800); Bitterman, Danielle S. (56307969500); Celi, Leo Anthony (16033282700)","57444552800; 56307969500; 16033282700","Preventing unrestricted and unmonitored AI experimentation in healthcare through transparency and accountability","2025","npj Digital Medicine","8","1","42","","","","0","10.1038/s41746-025-01443-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218346992&doi=10.1038%2fs41746-025-01443-2&partnerID=40&md5=b041eb5ac14d4e9d3258cad9c4cecdfc","The integration of large language models (LLMs) into electronic health records offers potential benefits but raises significant ethical, legal, and operational concerns, including unconsented data use, lack of governance, and AI-related malpractice accountability. Sycophancy, feedback loop bias, and data reuse risk amplifying errors without proper oversight. To safeguard patients, especially the vulnerable, clinicians must advocate for patient-centered education, ethical practices, and robust oversight to prevent harm. © The Author(s) 2025.","Nature Research","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85218346992"
"Fleurence R.L.; Bian J.; Wang X.; Xu H.; Dawoud D.; Higashi M.; Chhatwal J.","Fleurence, Rachael L. (10046079400); Bian, Jiang (7103200005); Wang, Xiaoyan (58822222700); Xu, Hua (57215023364); Dawoud, Dalia (26639217600); Higashi, Mitchell (57980968800); Chhatwal, Jagpreet (57203030161)","10046079400; 7103200005; 58822222700; 57215023364; 26639217600; 57980968800; 57203030161","Generative Artificial Intelligence for Health Technology Assessment: Opportunities, Challenges, and Policy Considerations: An ISPOR Working Group Report","2025","Value in Health","28","2","","175","183","8","2","10.1016/j.jval.2024.10.3846","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216324284&doi=10.1016%2fj.jval.2024.10.3846&partnerID=40&md5=a3d80481480398a407018b32db44cd9c","Objectives: To provide an introduction to the uses of generative artificial intelligence (AI) and foundation models, including large language models, in the field of health technology assessment (HTA). Methods: We reviewed applications of generative AI in 3 areas: systematic literature reviews, real-world evidence, and health economic modeling. Results: (1) Literature reviews: generative AI has the potential to assist in automating aspects of systematic literature reviews by proposing search terms, screening abstracts, extracting data, and generating code for meta-analyses; (2) real-world evidence: generative AI can facilitate automating processes and analyze large collections of real-world data, including unstructured clinical notes and imaging; (3) health economic modeling: generative AI can aid in the development of health economic models, from conceptualization to validation. Limitations in the use of foundation models and large language models include challenges surrounding their scientific rigor and reliability, the potential for bias, implications for equity, as well as nontrivial concerns regarding adherence to regulatory and ethical standards, particularly in terms of data privacy and security. Additionally, we survey the current policy landscape and provide suggestions for HTA agencies on responsibly integrating generative AI into their workflows, emphasizing the importance of human oversight and the fast-evolving nature of these tools. Conclusions: Although generative AI technology holds promise with respect to HTA applications, it is still undergoing rapid developments and improvements. Continued careful evaluation of their applications to HTA is required. Both developers and users of research incorporating these tools, should familiarize themselves with their current capabilities and limitations. © 2025","Elsevier Ltd","39536966","Article","Final","","Scopus","2-s2.0-85216324284"
"Haber Y.; Hadar Shoval D.; Levkovich I.; Yinon D.; Gigi K.; Pen O.; Angert T.; Elyoseph Z.","Haber, Yuval (58676609200); Hadar Shoval, Dorit (57191070263); Levkovich, Inbar (56425972800); Yinon, Dror (57217374412); Gigi, Karny (56449227400); Pen, Oori (59563671400); Angert, Tal (58676646400); Elyoseph, Zohar (57213164543)","58676609200; 57191070263; 56425972800; 57217374412; 56449227400; 59563671400; 58676646400; 57213164543","The externalization of internal experiences in psychotherapy through generative artificial intelligence: a theoretical, clinical, and ethical analysis","2025","Frontiers in Digital Health","7","","1512273","","","","0","10.3389/fdgth.2025.1512273","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218207966&doi=10.3389%2ffdgth.2025.1512273&partnerID=40&md5=2aa0069cf2bdcb2c3f55c77857f1171e","Introduction: Externalization techniques are well established in psychotherapy approaches, including narrative therapy and cognitive behavioral therapy. These methods elicit internal experiences such as emotions and make them tangible through external representations. Recent advances in generative artificial intelligence (GenAI), specifically large language models (LLMs), present new possibilities for therapeutic interventions; however, their integration into core psychotherapy practices remains largely unexplored. This study aimed to examine the clinical, ethical, and theoretical implications of integrating GenAI into the therapeutic space through a proof-of-concept (POC) of AI-driven externalization techniques, while emphasizing the essential role of the human therapist. Methods: To this end, we developed two customized GPTs agents: VIVI (visual externalization), which uses DALL-E 3 to create images reflecting patients' internal experiences (e.g., depression or hope), and DIVI (dialogic role-play-based externalization), which simulates conversations with aspects of patients' internal content. These tools were implemented and evaluated through a clinical case study under professional psychological guidance. Results: The integration of VIVI and DIVI demonstrated that GenAI can serve as an “artificial third”, creating a Winnicottian playful space that enhances, rather than supplants, the dyadic therapist-patient relationship. The tools successfully externalized complex internal dynamics, offering new therapeutic avenues, while also revealing challenges such as empathic failures and cultural biases. Discussion: These findings highlight both the promise and the ethical complexities of AI-enhanced therapy, including concerns about data security, representation accuracy, and the balance of clinical authority. To address these challenges, we propose the SAFE-AI protocol, offering clinicians structured guidelines for responsible AI integration in therapy. Future research should systematically evaluate the generalizability, efficacy, and ethical implications of these tools across diverse populations and therapeutic contexts. 2025 Haber, Hadar Shoval, Levkovich, Yinon, Gigi, Pen, Angert and Elyoseph.","Frontiers Media SA","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85218207966"
"Annor E.; Atarere J.; Ubah N.; Jolaoye O.; Kunkle B.; Egbo O.; Martin D.K.","Annor, Eugene (57703107500); Atarere, Joseph (57696108600); Ubah, Nneoma (59355918400); Jolaoye, Oladoyin (59547779500); Kunkle, Bryce (57221932668); Egbo, Olachi (58837705500); Martin, Daniel K. (56937457200)","57703107500; 57696108600; 59355918400; 59547779500; 57221932668; 58837705500; 56937457200","Assessing online chat-based artificial intelligence models for weight loss recommendation appropriateness and bias in the presence of guideline incongruence: Behaviour, Psychology and Sociology","2025","International Journal of Obesity","","","","","","","0","10.1038/s41366-025-01717-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217262182&doi=10.1038%2fs41366-025-01717-5&partnerID=40&md5=ff020f4413f61a48b5c59a1bdba07800","Background and aim: Managing obesity requires a comprehensive approach that involves therapeutic lifestyle changes, medications, or metabolic surgery. Many patients seek health information from online sources and artificial intelligence models like ChatGPT, Google Gemini, and Microsoft Copilot before consulting health professionals. This study aims to evaluate the appropriateness of the responses of Google Gemini and Microsoft Copilot to questions on pharmacologic and surgical management of obesity and assess for bias in their responses to either the ADA or AACE guidelines. Methods: Ten questions were compiled into a set and posed separately to the free editions of Google Gemini and Microsoft Copilot. Recommendations for the questions were extracted from the ADA and the AACE websites, and the responses were graded by reviewers for appropriateness, completeness, and bias to any of the guidelines. Results: All responses from Microsoft Copilot and 8/10 (80%) responses from Google Gemini were appropriate. There were no inappropriate responses. Google Gemini refused to respond to two questions and insisted on consulting a physician. Microsoft Copilot (10/10; 100%) provided a higher proportion of complete responses than Google Gemini (5/10; 50%). Of the eight responses from Google Gemini, none were biased towards any of the guidelines, while two of the responses from Microsoft Copilot were biased. Conclusion: The study highlights the role of Microsoft Copilot and Google Gemini in weight loss management. The differences in their responses may be attributed to the variation in the quality and scope of their training data and design. © The Author(s), under exclusive licence to Springer Nature Limited 2025.","Springer Nature","39871015","Article","Article in press","","Scopus","2-s2.0-85217262182"
"Walker A.; Thorne A.; Das S.; Love J.; Cooper H.L.F.; Livingston M.; Sarker A.","Walker, Andrew (57222393603); Thorne, Annie (57211202634); Das, Sudeshna (58597903700); Love, Jennifer (56655971300); Cooper, Hannah L. F. (8779276900); Livingston, Melvin (36194831500); Sarker, Abeed (36976315000)","57222393603; 57211202634; 58597903700; 56655971300; 8779276900; 36194831500; 36976315000","CARE-SD: classifier-based analysis for recognizing provider stigmatizing and doubt marker labels in electronic health records: model development and validation","2025","Journal of the American Medical Informatics Association","32","2","","365","374","9","0","10.1093/jamia/ocae310","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216606913&doi=10.1093%2fjamia%2focae310&partnerID=40&md5=3bbc2f07c391f8fe79640fabde0ebe39","Objective: To detect and classify features of stigmatizing and biased language in intensive care electronic health records (EHRs) using natural language processing techniques. Materials and Methods: We first created a lexicon and regular expression lists from literature-driven stem words for linguistic features of stigmatizing patient labels, doubt markers, and scare quotes within EHRs. The lexicon was further extended using Word2Vec and GPT 3.5, and refined through human evaluation. These lexicons were used to search for matches across 18 million sentences from the de-identified Medical Information Mart for Intensive Care-III (MIMIC-III) dataset. For each linguistic bias feature, 1000 sentence matches were sampled, labeled by expert clinical and public health annotators, and used to supervised learning classifiers. Results: Lexicon development from expanded literature stem-word lists resulted in a doubt marker lexicon containing 58 expressions, and a stigmatizing labels lexicon containing 127 expressions. Classifiers for doubt markers and stigmatizing labels had the highest performance, with macro F1-scores of 0.84 and 0.79, positive-label recall and precision values ranging from 0.71 to 0.86, and accuracies aligning closely with human annotator agreement (0.87). Discussion: This study demonstrated the feasibility of supervised classifiers in automatically identifying stigmatizing labels and doubt markers in medical text and identified trends in stigmatizing language use in an EHR setting. Additional labeled data may help improve lower scare quote model performance. Conclusions: Classifiers developed in this study showed high model performance and can be applied to identify patterns and target interventions to reduce stigmatizing labels and doubt markers in healthcare systems. © The Author(s) 2024. Published by Oxford University Press on behalf of the American Medical Informatics Association.","Oxford University Press","39724920","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85216606913"
"Heuchan G.N.; Conway R.E.; Tattan-Birch H.; Heggie L.; Llewellyn C.H.","Heuchan, Gabriella N. (57940770500); Conway, Rana E. (57214231762); Tattan-Birch, Harry (57212251852); Heggie, Lisa (58084601500); Llewellyn, Clare H. (24471042200)","57940770500; 57214231762; 57212251852; 58084601500; 24471042200","Social and Economic Patterning in Ultra-Processed Food Intake in Toddlerhood and Middle Childhood: Longitudinal Data From the Gemini Cohort in the United Kingdom","2025","Journal of the Academy of Nutrition and Dietetics","","","","","","","0","10.1016/j.jand.2025.01.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219541846&doi=10.1016%2fj.jand.2025.01.004&partnerID=40&md5=aff80581eb288f401f6bdf11a409c999","Background: Children's consumption of ultra-processed food (UPF) may contribute to inequalities in obesity and wider health. Socioeconomic patterning in younger UK children's UPF intake is unknown. Objective: The aim of this study was to investigate socioeconomic patterning of UK toddlers’ (aged 21 months) and children's (aged 7 years) UPF intake across several household and neighborhood indicators. Design: Secondary analysis of data from a prospective longitudinal cohort study using parent-reported sociodemographic data and 3-day diet diaries. Participants/setting: Participants were children from the UK Gemini study of 4804 twins born in 2007. At ages 21 months and 7 years, 2591 and 592 children, respectively, had at least 2 days of dietary data. Main outcome measures: This study measured percentage energy from UPF at 21 months and 7 years of age, classified using the NOVA system. Statistical analyses performed: Unadjusted linear regression models were run for household socioeconomic position (SEP) composite score; index of multiple deprivation decile; income; occupation level; mother's age; education of mother and partner; and child's ethnicity, sex, and age. Adjusted multivariable linear regression models were adjusted for ethnicity and all SEP indicators except SEP composite score (adjusted 1), in addition to child sex and age (adjusted 2). Missing data were addressed with multiple imputation and inverse probability weighting. CIs and P values were adjusted to account for clustering within families. Results: Children of lower SEP had higher UPF intake across several indicators. Mother's education was the strongest predictor; postgraduate education was associated with 8.64% (95% CI –12.08% to –5.20%; P < .001) and 10.12% (95% CI, –15.68% to –4.56%; P < .001) less energy from UPF at 21 months and 7 years, respectively, compared with no educational qualifications in adjusted model 2. Conclusions: UK children from more disadvantaged backgrounds consumed a greater proportion of their energy from UPF. Mother's education seemed to be the most influential factor. Socioeconomic inequalities, particularly in maternal education, may drive disparities in diet quality and associated health outcomes. Addressing these gaps is essential to reduce childhood obesity and improve long-term health in socioeconomically disadvantaged populations. © 2025 The Authors","Elsevier B.V.","","Article","Article in press","","Scopus","2-s2.0-85219541846"
"Zhou Y.; Di Eugenio B.; Cheng L.","Zhou, Yue (58432975600); Di Eugenio, Barbara (6603126702); Cheng, Lu (57206471993)","58432975600; 6603126702; 57206471993","Unveiling Performance Challenges of Large Language Models in Low-Resource Healthcare: A Demographic Fairness Perspective","2025","Proceedings - International Conference on Computational Linguistics, COLING","Part F206484-1","","","7266","7278","12","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218488336&partnerID=40&md5=2b0608db3ccfd858d095fff916a8781d","This paper studies the performance of large language models (LLMs), particularly regarding demographic fairness, in solving real-world healthcare tasks. We evaluate state-of-the-art LLMs with three prevalent learning frameworks across six diverse healthcare tasks and find significant challenges in applying LLMs to real-world healthcare tasks and persistent fairness issues across demographic groups. We also find that explicitly providing demographic information yields mixed results, while LLM's ability to infer such details raises concerns about biased health predictions. Utilizing LLMs as autonomous agents with access to up-to-date guidelines does not guarantee performance improvement. We believe these findings reveal the critical limitations of LLMs as concerns healthcare fairness and the urgent need for specialized research in this area. WARNING: This paper contains model outputs that may be considered offensive in nature. © 2025 Association for Computational Linguistics.","Association for Computational Linguistics (ACL)","","Conference paper","Final","","Scopus","2-s2.0-85218488336"
"Lee Y.; Chang C.-H.; Yang C.C.","Lee, Yeawon (59090494800); Chang, Chia-Hsuan (57204900221); Yang, Christopher C. (7407740308)","59090494800; 57204900221; 7407740308","Enhancing Patient-Physician Communication: Simulating African American Vernacular English in Medical Diagnostics with Large Language Models","2025","Journal of Healthcare Informatics Research","","","","","","","0","10.1007/s41666-025-00194-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000800870&doi=10.1007%2fs41666-025-00194-9&partnerID=40&md5=52cdfb81ce365f46ce9049eb7101ef97","Effective communication is crucial in reducing health disparities. However, linguistic differences, such as African American Vernacular English (AAVE), can lead to communication gaps between patients and physicians, negatively affecting care and outcomes. This study examines whether large language models (LLMs), specifically GPT-4 and Llama 3.3, can replicate AAVE in simulated clinical dialogues to improve cultural sensitivity. We tested four prompt types—BaseP, DemoP, LingP, and CompP—using United States Medical Licensing Examination (USMLE) case simulations. Statistical analyses on the models’ outputs showed a significant difference among prompt types for both GPT-4 (F(2,70) = 6.218, p = 0.003) and Llama 3.3 (F(2,70) = 12.124, p < 0.001), indicating that including demographic information and/or explicit AAVE cues influences each model’s output. Combining demographic and linguistic cues (CompP) yielded the highest mean AAVE feature counts (e.g., 9.83 for GPT-4 vs. 16.06 for Llama 3.3), although neither model fully captured the diversity of AAVE. Moreover, simply mentioning African American demographics triggers extra informal forms, suggesting built-in stereotypes or biases in both models. Overall, these findings highlight the promise of LLMs for culturally sensitive healthcare communication, while underscoring the need for continued refinement to address stereotypes and more accurately represent diverse linguistic styles. © The Author(s) 2025.","Springer Science and Business Media Deutschland GmbH","","Article","Article in press","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-86000800870"
"Huang X.; Han Y.; Li Y.; Li R.; Wu P.; Zhang K.","Huang, Xiyang (57821550100); Han, Yingjie (8098394400); Li, Yaoxu (58918988600); Li, Runzhi (13409528300); Wu, Pengcheng (58920093900); Zhang, Kunli (55009135800)","57821550100; 8098394400; 58918988600; 13409528300; 58920093900; 55009135800","CmEAA: Cross-modal Enhancement and Alignment Adapter for Radiology Report Generation","2025","Proceedings - International Conference on Computational Linguistics, COLING","Part F206484-1","","","8546","8556","10","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218490368&partnerID=40&md5=452c11c5732b3c109cc1dd33eeea49b1","Automatic radiology report generation is pivotal in reducing the workload of radiologists, while simultaneously improving diagnostic accuracy and operational efficiency. Current methods face significant challenges, including the effective alignment of medical visual features with textual features and the mitigation of data bias. In this paper, we propose a method for radiology report generation that utilizes a Cross-modal Enhancement and Alignment Adapter (CmEAA) to connect a vision encoder with a frozen large language model. Specifically, we introduce two novel modules within CmEAA: Cross-modal Feature Enhancement (CFE) and Neural Mutual Information Aligner (NMIA). CFE extracts observation-related contextual features to enhance the visual features of lesions and abnormal regions in radiology images through a cross-modal enhancement Transformer. NMIA maximizes neural mutual information between visual and textual representations within a low-dimensional alignment embedding space during training and provides potential global alignment visual representations during inference. Additionally, a weights generator is designed to enable the dynamic adaptation of cross-modal enhanced features and vanilla visual features. Experimental results on two prevailing datasets, namely, IU X-Ray and MIMIC-CXR, demonstrate that the proposed model outperforms previous state-of-the-art methods. © 2025 Association for Computational Linguistics.","Association for Computational Linguistics (ACL)","","Conference paper","Final","","Scopus","2-s2.0-85218490368"
"Balt E.; Salmi S.; Bhulai S.; Vrinzen S.; Eikelenboom M.; Gilissen R.; Creemers D.; Popma A.; Mérelle S.","Balt, Elias (57218679539); Salmi, Salim (57221468654); Bhulai, Sandjai (6602990283); Vrinzen, Stefan (59671056400); Eikelenboom, Merijn (6603169584); Gilissen, Renske (57191897529); Creemers, Daan (51565785400); Popma, Arne (12766482700); Mérelle, Saskia (13102849300)","57218679539; 57221468654; 6602990283; 59671056400; 6603169584; 57191897529; 51565785400; 12766482700; 13102849300","Deductively coding psychosocial autopsy interview data using a few-shot learning large language model","2025","Frontiers in Public Health","13","","1512537","","","","0","10.3389/fpubh.2025.1512537","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000102310&doi=10.3389%2ffpubh.2025.1512537&partnerID=40&md5=15cac329627683426efdf93b62bce17a","Background: Psychosocial autopsy is a retrospective study of suicide, aimed to identify emerging themes and psychosocial risk factors. It typically relies heavily on qualitative data from interviews or medical documentation. However, qualitative research has often been scrutinized for being prone to bias and is notoriously time- and cost-intensive. Therefore, the current study aimed to investigate if a Large Language Model (LLM) can be feasibly integrated with qualitative research procedures, by evaluating the performance of the model in deductively coding and coherently summarizing interview data obtained in a psychosocial autopsy. Methods: Data from 38 semi-structured interviews conducted with individuals bereaved by the suicide of a loved one was deductively coded by qualitative researchers and a server-installed LLAMA3 large language model. The model performance was evaluated in three tasks: (1) binary classification of coded segments, (2) independent classification using a sliding window approach, and (3) summarization of coded data. Intercoder agreement scores were calculated using Cohen’s Kappa, and the LLM’s summaries were qualitatively assessed using the Constant Comparative Method. Results: The results showed that the LLM achieved substantial agreement with the researchers for the binary classification (accuracy: 0.84) and the sliding window task (accuracy: 0.67). The performance had large variability across codes. LLM summaries were typically rich enough for subsequent analysis by the researcher, with around 80% of the summaries being rated independently by two researchers as ‘adequate’ or ‘good.’ Emerging themes in the qualitative assessment of the summaries included unsolicited elaboration and hallucination. Conclusion: State-of-the-art LLMs show great potential to support researchers in deductively coding complex interview data, which would alleviate the investment of time and resources. Integrating models with qualitative research procedures can facilitate near real-time monitoring. Based on the findings, we recommend a collaborative model, whereby the LLM’s deductive coding is complemented by review, inductive coding and further interpretation by a researcher. Future research may aim to replicate the findings in different contexts and evaluate models with a larger context size. Copyright © 2025 Balt, Salmi, Bhulai, Vrinzen, Eikelenboom, Gilissen, Creemers, Popma and Mérelle.","Frontiers Media SA","40046117","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-86000102310"
"Huh K.Y.; Song I.; Kim Y.; Park J.; Ryu H.; Koh J.; Yu K.-S.; Kim K.H.; Lee S.","Huh, Ki Young (57211287976); Song, Ildae (57102612900); Kim, Yoonjin (58984985100); Park, Jiyeon (58571503000); Ryu, Hyunwook (59183134700); Koh, JaeEun (59652757200); Yu, Kyung-Sang (7403385744); Kim, Kyung Hwan (56380795800); Lee, SeungHwan (55926041800)","57211287976; 57102612900; 58984985100; 58571503000; 59183134700; 59652757200; 7403385744; 56380795800; 55926041800","Exploration of Using an Open-Source Large Language Model for Analyzing Trial Information: A Case Study of Clinical Trials With Decentralized Elements","2025","Clinical and Translational Science","18","3","e70183","","","","0","10.1111/cts.70183","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219754656&doi=10.1111%2fcts.70183&partnerID=40&md5=be44eeb4a22100865e67fd7d93aaca39","Despite interest in clinical trials with decentralized elements (DCTs), analysis of their trends in trial registries is lacking due to heterogeneous designs and unstandardized terms. We explored Llama 3, an open-source large language model, to efficiently evaluate these trends. Trial data were sourced from Aggregate Analysis of ClinicalTrials.gov, focusing on drug trials conducted between 2018 and 2023. We utilized three Llama 3 models with a different number of parameters: 8b (model 1), fine-tuned 8b (model 2) with curated data, and 70b (model 3). Prompt engineering enabled sophisticated tasks such as classification of DCTs with explanations and extracting decentralized elements. Model performance, evaluated on a 3-month exploratory test dataset, demonstrated that sensitivity could be improved after fine-tuning from 0.0357 to 0.5385. Low positive predictive value in the fine-tuned model 2 could be improved by focusing on trials with DCT-associated expressions from 0.5385 to 0.9167. However, the extraction of decentralized elements was only properly performed by model 3, which had a larger number of parameters. Based on the results, we screened the entire 6-year dataset after applying DCT-associated expressions. After the subsequent application of models 2 and 3, we identified 692 DCTs. We found that a total of 213 trials were classified as phase 2, followed by 162 phase 4 trials, 112 phase 3 trials, and 92 phase 1 trials. In conclusion, our study demonstrated the potential of large language models for analyzing clinical trial information not structured in a machine-readable format. Managing potential biases during model application is crucial. © 2025 The Author(s). Clinical and Translational Science published by Wiley Periodicals LLC on behalf of American Society for Clinical Pharmacology and Therapeutics.","John Wiley and Sons Inc","40025837","Article","Final","","Scopus","2-s2.0-85219754656"
"McBain R.K.; Cantor J.H.; Zhang L.A.; Baker O.; Zhang F.; Halbisen A.; Kofner A.; Breslau J.; Stein B.; Mehrotra A.; Yu H.","McBain, Ryan K. (25321085800); Cantor, Jonathan H. (55607314400); Zhang, Li Ang (59416255300); Baker, Olesya (57027255600); Zhang, Fang (55487100600); Halbisen, Alyssa (58204898300); Kofner, Aaron (36910789600); Breslau, Joshua (16238353700); Stein, Bradley (7201898299); Mehrotra, Ateev (23498035400); Yu, Hao (59674968700)","25321085800; 55607314400; 59416255300; 57027255600; 55487100600; 58204898300; 36910789600; 16238353700; 7201898299; 23498035400; 59674968700","Competency of Large Language Models in Evaluating Appropriate Responses to Suicidal Ideation: Comparative Study","2025","Journal of Medical Internet Research","27","","e67891","","","","0","10.2196/67891","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000305332&doi=10.2196%2f67891&partnerID=40&md5=af2b26f0a0acf2bd140f390ddca3d9e4","Background: With suicide rates in the United States at an all-time high, individuals experiencing suicidal ideation are increasingly turning to large language models (LLMs) for guidance and support. Objective: The objective of this study was to assess the competency of 3 widely used LLMs to distinguish appropriate versus inappropriate responses when engaging individuals who exhibit suicidal ideation. Methods: This observational, cross-sectional study evaluated responses to the revised Suicidal Ideation Response Inventory (SIRI-2) generated by ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro. Data collection and analyses were conducted in July 2024. A common training module for mental health professionals, SIRI-2 provides 24 hypothetical scenarios in which a patient exhibits depressive symptoms and suicidal ideation, followed by two clinician responses. Clinician responses were scored from –3 (highly inappropriate) to +3 (highly appropriate). All 3 LLMs were provided with a standardized set of instructions to rate clinician responses. We compared LLM responses to those of expert suicidologists, conducting linear regression analyses and converting LLM responses to z scores to identify outliers (z score>1.96 or <–1.96; P<0.05). Furthermore, we compared final SIRI-2 scores to those produced by health professionals in prior studies. Results: All 3 LLMs rated responses as more appropriate than ratings provided by expert suicidologists. The item-level mean difference was 0.86 for ChatGPT (95% CI 0.61-1.12; P<.001), 0.61 for Claude (95% CI 0.41-0.81; P<.001), and 0.73 for Gemini (95% CI 0.35-1.11; P<.001). In terms of z scores, 19% (9 of 48) of ChatGPT responses were outliers when compared to expert suicidologists. Similarly, 11% (5 of 48) of Claude responses were outliers compared to expert suicidologists. Additionally, 36% (17 of 48) of Gemini responses were outliers compared to expert suicidologists. ChatGPT produced a final SIRI-2 score of 45.7, roughly equivalent to master’s level counselors in prior studies. Claude produced an SIRI-2 score of 36.7, exceeding prior performance of mental health professionals after suicide intervention skills training. Gemini produced a final SIRI-2 score of 54.5, equivalent to untrained K-12 school staff. Conclusions: Current versions of 3 major LLMs demonstrated an upward bias in their evaluations of appropriate responses to suicidal ideation; however, 2 of the 3 models performed equivalent to or exceeded the performance of mental health professionals. ©Ryan K McBain, Jonathan H Cantor, Li Ang Zhang, Olesya Baker, Fang Zhang, Alyssa Halbisen, Aaron Kofner, Joshua Breslau, Bradley Stein, Ateev Mehrotra, Hao Yu.","JMIR Publications Inc.","40053817","Article","Final","","Scopus","2-s2.0-86000305332"
"Gore R.; Safaee M.M.; Lynch C.J.; Ames C.P.","Gore, Ross (22234235900); Safaee, Michael M. (22942328300); Lynch, Christopher J. (54889878700); Ames, Christopher P. (9841977000)","22234235900; 22942328300; 54889878700; 9841977000","A Spine-Specific Lexicon for the Sentiment Analysis of Interviews with Adult Spinal Deformity Patients Correlates with SF-36, SRS-22, and ODI Scores: A Pilot Study of 25 Patients","2025","Information (Switzerland)","16","2","90","","","","1","10.3390/info16020090","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218439968&doi=10.3390%2finfo16020090&partnerID=40&md5=8be0633b3b027ed5f02be01043981b66","Classic health-related quality of life (HRQOL) metrics are cumbersome, time-intensive, and subject to biases based on the patient’s native language, educational level, and cultural values. Natural language processing (NLP) converts text into quantitative metrics. Sentiment analysis enables subject matter experts to construct domain-specific lexicons that assign a value of either negative (−1) or positive (1) to certain words. The growth of telehealth provides opportunities to apply sentiment analysis to transcripts of adult spinal deformity patients’ visits to derive a novel and less biased HRQOL metric. In this study, we demonstrate the feasibility of constructing a spine-specific lexicon for sentiment analysis to derive an HRQOL metric for adult spinal deformity patients from their preoperative telehealth visit transcripts. We asked each of twenty-five (25) adult patients seven open-ended questions about their spinal conditions, treatment, and quality of life during telehealth visits. We analyzed the Pearson correlation between our sentiment analysis HRQOL metric and established HRQOL metrics (the Scoliosis Research Society-22 questionnaire [SRS-22], 36-Item Short Form Health Survey [SF-36], and Oswestry Disability Index [ODI]). The results show statistically significant correlations (0.43–0.74) between our sentiment analysis metric and the conventional metrics. This provides evidence that applying NLP techniques to patient transcripts can yield an effective HRQOL metric. © 2025 by the authors.","Multidisciplinary Digital Publishing Institute (MDPI)","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85218439968"
"Alqithami S.","Alqithami, Saad (55513756700)","55513756700","Integrating Sentiment Analysis and Reinforcement Learning for Equitable Disaster Response: A Novel Approach","2025","Sustainability (Switzerland)","17","3","1072","","","","0","10.3390/su17031072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217631524&doi=10.3390%2fsu17031072&partnerID=40&md5=c442fb9ca8f182f2912a044be6de69f1","Efficient disaster response requires dynamic and adaptive resource allocation strategies that account for evolving public needs, real-time sentiment, and sustainability concerns. In this study, a sentiment-driven framework is proposed, integrating reinforcement learning, natural language processing, and gamification to optimize the distribution of resources such as water, food, medical aid, shelter, and electricity during disaster scenarios. The model leverages real-time social media data to capture public sentiment, combines it with geospatial and temporal information, and then trains a reinforcement learning agent to maximize both community satisfaction and equitable resource allocation. The model achieved equity scores of up to (Formula presented.) and improved satisfaction metrics by (Formula presented.), which outperforms static allocation baselines. By incorporating a gamified simulation platform, stakeholders can interactively refine policies and address the inherent uncertainties of disaster events. This approach highlights the transformative potential of using advanced artificial intelligence techniques to enhance adaptability, promote sustainability, and foster collaborative decision-making in humanitarian aid efforts. © 2025 by the author.","Multidisciplinary Digital Publishing Institute (MDPI)","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85217631524"
"Gao Y.; Li R.; Croxford E.; Caskey J.; Patterson B.W.; Churpek M.; Miller T.; Dligach D.; Afshar M.","Gao, Yanjun (57382589400); Li, Ruizhe (57216702920); Croxford, Emma (58755364200); Caskey, John (59267952000); Patterson, Brian W. (57203231903); Churpek, Matthew (36705790600); Miller, Timothy (57198615489); Dligach, Dmitriy (24474276200); Afshar, Majid (54954120900)","57382589400; 57216702920; 58755364200; 59267952000; 57203231903; 36705790600; 57198615489; 24474276200; 54954120900","Leveraging Medical Knowledge Graphs Into Large Language Models for Diagnosis Prediction: Design and Application Study","2025","JMIR AI","4","1","e58670","","","","0","10.2196/58670","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219233420&doi=10.2196%2f58670&partnerID=40&md5=7a3c67780b10c63f8e01b1c095049412","Background: Electronic health records (EHRs) and routine documentation practices play a vital role in patients’ daily care, providing a holistic record of health, diagnoses, and treatment. However, complex and verbose EHR narratives can overwhelm health care providers, increasing the risk of diagnostic inaccuracies. While large language models (LLMs) have showcased their potential in diverse language tasks, their application in health care must prioritize the minimization of diagnostic errors and the prevention of patient harm. Integrating knowledge graphs (KGs) into LLMs offers a promising approach because structured knowledge from KGs could enhance LLMs’ diagnostic reasoning by providing contextually relevant medical information. Objective: This study introduces DR.KNOWS (Diagnostic Reasoning Knowledge Graph System), a model that integrates Unified Medical Language System–based KGs with LLMs to improve diagnostic predictions from EHR data by retrieving contextually relevant paths aligned with patient-specific information. Methods: DR.KNOWS combines a stack graph isomorphism network for node embedding with an attention-based path ranker to identify and rank knowledge paths relevant to a patient’s clinical context. We evaluated DR.KNOWS on 2 real-world EHR datasets from different geographic locations, comparing its performance to baseline models, including QuickUMLS and standard LLMs (Text-to-Text Transfer Transformer and ChatGPT). To assess diagnostic reasoning quality, we designed and implemented a human evaluation framework grounded in clinical safety metrics. Results: DR.KNOWS demonstrated notable improvements over baseline models, showing higher accuracy in extracting diagnostic concepts and enhanced diagnostic prediction metrics. Prompt-based fine-tuning of Text-to-Text Transfer Transformer with DR.KNOWS knowledge paths achieved the highest ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation–Longest Common Subsequence) and concept unique identifier F1-scores, highlighting the benefits of KG integration. Human evaluators found the diagnostic rationales of DR.KNOWS to be aligned strongly with correct clinical reasoning, indicating improved abstraction and reasoning. Recognized limitations include potential biases within the KG data, which we addressed by emphasizing case-specific path selection and proposing future bias-mitigation strategies. Conclusions: DR.KNOWS offers a robust approach for enhancing diagnostic accuracy and reasoning by integrating structured KG knowledge into LLM-based clinical workflows. Although further work is required to address KG biases and extend generalizability, DR.KNOWS represents progress toward trustworthy artificial intelligence–driven clinical decision support, with a human evaluation framework focused on diagnostic safety and alignment with clinical standards. © Yanjun Gao, Ruizhe Li, Emma Croxford, John Caskey, Brian W Patterson, Matthew Churpek, Timothy Miller, Dmitriy Dligach, Majid Afshar.","JMIR Publications Inc.","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85219233420"
"Ng M.Y.; Helzer J.; Pfeffer M.A.; Seto T.; Hernandez-Boussard T.","Ng, Madelena Y. (57192119615); Helzer, Jarrod (59392364300); Pfeffer, Michael A. (54972654600); Seto, Tina (55805536100); Hernandez-Boussard, Tina (57119442800)","57192119615; 59392364300; 54972654600; 55805536100; 57119442800","Development of secure infrastructure for advancing generative artificial intelligence research in healthcare at an academic medical center","2025","Journal of the American Medical Informatics Association","32","3","","586","588","2","0","10.1093/jamia/ocaf005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218412343&doi=10.1093%2fjamia%2focaf005&partnerID=40&md5=a3c4e8291b55777b1ab735af91776551","Background: Generative AI, particularly large language models (LLMs), holds great potential for improving patient care and operational efficiency in healthcare. However, the use of LLMs is complicated by regulatory concerns around data security and patient privacy. This study aimed to develop and evaluate a secure infrastructure that allows researchers to safely leverage LLMs in healthcare while ensuring HIPAA compliance and promoting equitable AI. Materials and Methods: We implemented a private Azure OpenAI Studio deployment with secure API-enabled endpoints for researchers. Two use cases were explored, detecting falls from electronic health records (EHR) notes and evaluating bias in mental health prediction using fairness-aware prompts. Results: The framework provided secure, HIPAA-compliant API access to LLMs, allowing researchers to handle sensitive data safely. Both use cases highlighted the secure infrastructure's capacity to protect sensitive patient data while supporting innovation. Discussion and Conclusion: This centralized platform presents a scalable, secure, and HIPAA-compliant solution for healthcare institutions aiming to integrate LLMs into clinical research.  © 2025 The Author(s).","Oxford University Press","39836496","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85218412343"
"De la Iglesia I.; Goenaga I.; Ramirez-Romero J.; Villa-Gonzalez J.M.; Goikoetxea J.; Barrena A.","De la Iglesia, Iker (57386565500); Goenaga, Iakes (36666260600); Ramirez-Romero, Johanna (59012749700); Villa-Gonzalez, Jose Maria (58155535800); Goikoetxea, Josu (57213223776); Barrena, Ander (55866565800)","57386565500; 36666260600; 59012749700; 58155535800; 57213223776; 55866565800","Ranking Over Scoring: Towards Reliable and Robust Automated Evaluation of LLM-Generated Medical Explanatory Arguments","2025","Proceedings - International Conference on Computational Linguistics, COLING","Part F206484-1","","","9456","9471","15","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218502681&partnerID=40&md5=ed063a6dfd79703c5ebd7046cb1a0cef","Evaluating LLM-generated text has become a key challenge, especially in domain-specific contexts like the medical field. This work introduces a novel evaluation methodology for LLM-generated medical explanatory arguments, relying on Proxy Tasks and rankings to closely align results with human evaluation criteria, overcoming the biases typically seen in LLMs used as judges. We demonstrate that the proposed evaluators are robust against adversarial attacks, including the assessment of non-argumentative text. Additionally, the human-crafted arguments needed to train the evaluators are minimized to just one example per Proxy Task. By examining multiple LLM-generated arguments, we establish a methodology for determining whether a Proxy Task is suitable for evaluating LLM-generated medical explanatory arguments, requiring only five examples and two human experts. The Proxy Tasks, LM evaluators, and the code are available for reproducibility. © 2025 Association for Computational Linguistics.","Association for Computational Linguistics (ACL)","","Conference paper","Final","","Scopus","2-s2.0-85218502681"
"Farrow L.; Anderson L.; Zhong M.","Farrow, Luke (56763854800); Anderson, Lesley (10639071400); Zhong, Mingjun (56735373600)","56763854800; 10639071400; 56735373600","Managing class imbalance in the training of a large language model to predict patient selection for total knee arthroplasty: Results from the Artificial intelligence to Revolutionise the patient Care pathway in Hip and knEe aRthroplastY (ARCHERY) project","2025","Knee","54","","","1","8","7","0","10.1016/j.knee.2025.02.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218854087&doi=10.1016%2fj.knee.2025.02.007&partnerID=40&md5=057f436a0220f39efcc9cff0503c2338","Introduction: This study set out to test the efficacy of different techniques used to manage to class imbalance, a type of data bias, in application of a large language model (LLM) to predict patient selection for total knee arthroplasty (TKA). Methods: This study utilised data from the Artificial Intelligence to Revolutionise the Patient Care Pathway in Hip and Knee Arthroplasty (ARCHERY) project (ISRCTN18398037). Data included the pre-operative radiology reports of patients referred to secondary care for knee-related complaints from within the North of Scotland. A clinically based LLM (GatorTron) was trained regarding prediction of selection for TKA. Three methods for managing class imbalance were assessed: a standard model, use of class weighting, and majority class undersampling. Results: A total of 7707 individual knee radiology reports were included (dated from 2015 to 2022). The mean text length was 74 words (range 26–275). Only 910/7707 (11.8%) patients underwent TKA surgery (the designated ‘minority class’). Class weighting technique performed better for minority class discrimination and calibration compared with the other two techniques (Recall 0.61/AUROC 0.73 for class weighting compared with 0.54/0.70 and 0.59/0.72 for the standard model and majority class undersampling, respectively. There was also significant data loss for majority class undersampling when compared with class-weighting. Conclusion: Use of class-weighting appears to provide the optimal method of training a an LLM to perform analytical tasks on free-text clinical information in the face of significant data bias (‘class imbalance’). Such knowledge is an important consideration in the development of high-performance clinical AI models within Trauma and Orthopaedics. © 2025 The Author(s)","Elsevier B.V.","","Article","Final","","Scopus","2-s2.0-85218854087"
"Miao S.; Ji P.; Zhu Y.; Meng H.; Jing M.; Sheng R.; Zhang X.; Ding H.; Guo J.; Gao W.; Yang G.; Liu Y.","Miao, Shumei (56200452600); Ji, Pei (59578817300); Zhu, Yongqian (58299256000); Meng, Haoyu (55819685400); Jing, Mang (57218441457); Sheng, Rongrong (57215429846); Zhang, Xiaoliang (57203620419); Ding, Hailong (58691065000); Guo, Jianjun (59558625900); Gao, Wen (57215386118); Yang, Guanyu (16176923900); Liu, Yun (55899963300)","56200452600; 59578817300; 58299256000; 55819685400; 57218441457; 57215429846; 57203620419; 58691065000; 59558625900; 57215386118; 16176923900; 55899963300","The Construction and Application of a Clinical Decision Support System for Cardiovascular Diseases: Multimodal Data-Driven Development and Validation Study","2025","JMIR Medical Informatics","13","","e63186","","","","0","10.2196/63186","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000186304&doi=10.2196%2f63186&partnerID=40&md5=c2a2d87190316613b36084de256a1dd1","Background: Due to the acceleration of the aging population and the prevalence of unhealthy lifestyles, the incidence of cardiovascular diseases (CVDs) in China continues to grow. However, due to the uneven distribution of medical resources across regions and significant disparities in diagnostic and treatment levels, the diagnosis and management of CVDs face considerable challenges. Objective: The purpose of this study is to build a cardiovascular diagnosis and treatment knowledge base by using new technology, form an auxiliary decision support system, and integrate it into the doctor’s workstation, to improve the assessment rate and treatment standardization rate. This study offers new ideas for the prevention and management of CVDs. Methods: This study designed a clinical decision support system (CDSS) with data, learning, knowledge, and application layers. It integrates multimodal data from hospital laboratory information systems, hospital information systems, electronic medical records, electrocardiography, nursing, and other systems to build a knowledge model. The unstructured data were segmented using natural language processing technology, and medical entity words and entity combination relationships were extracted using IDCNN (iterated dilated convolutional neural network) and TextCNN (text convolutional neural network). The CDSS refers to global CVD assessment indicators to design quality control strategies and an intelligent treatment plan recommendation engine map, establishing a big data analysis platform to achieve multidimensional, visualized data statistics for management decision support. Results: The CDSS system is embedded and interfaced with the physician workstation, triggering in real-time during the clinical diagnosis and treatment process. It establishes a 3-tier assessment control through pop-up windows and screen domination operations. Based on the intelligent diagnostic and treatment reminders of the CDSS, patients are given intervention treatments. The important risk assessment and diagnosis rate indicators significantly improved after the system came into use, and gradually increased within 2 years. The indicators of mandatory control, directly became 100% after the CDSS was online. The CDSS enhanced the standardization of clinical diagnosis and treatment. Conclusions: This study establishes a specialized knowledge base for CVDs, combined with clinical multimodal information, to intelligently assess and stratify cardiovascular patients. It automatically recommends intervention treatments based on assessments and clinical characterizations, proving to be an effective exploration of using a CDSS to build a disease-specific intelligent system. © Shumei Miao, Pei Ji, Yongqian Zhu, Haoyu Meng, Mang Jing, Rongrong Sheng, Xiaoliang Zhang, Hailong Ding, Jianjun Guo, Wen Gao, Guanyu Yang, Yun Liu.","JMIR Publications Inc.","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-86000186304"
"Bucur A.-M.; Moldovan A.-C.; Parvatikar K.; Zampieri M.; Khudabukhsh A.R.; Dinu L.P.","Bucur, Ana-Maria (57220931771); Moldovan, Andreea-Codrina (57429409200); Parvatikar, Krutika (59393707100); Zampieri, Marcos (8948587300); Khudabukhsh, Ashiqur R. (36835705200); Dinu, Liviu P. (6602512834)","57220931771; 57429409200; 59393707100; 8948587300; 36835705200; 6602512834","On the State of NLP Approaches to Modeling Depression in Social Media: A Post-COVID-19 Outlook","2025","IEEE Journal of Biomedical and Health Informatics","","","","","","","0","10.1109/JBHI.2025.3540507","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000485311&doi=10.1109%2fJBHI.2025.3540507&partnerID=40&md5=299c11a5e6e7db5216b0be2965fd90c5","Computational approaches to predicting mental health conditions in social media have been substantially explored in the past years. Multiple reviews have been published on this topic, providing the community with comprehensive accounts of the research in this area. Among all mental health conditions, depression is the most widely studied due to its worldwide prevalence. The COVID-19 global pandemic, starting in early 2020, has had a great impact on mental health worldwide. Harsh measures employed by governments to slow the spread of the virus (e.g., lockdowns) and the subsequent economic downturn experienced in many countries have significantly impacted people's lives and mental health. Studies have shown a substantial increase of above 50% in the rate of depression in the population. In this context, we present a review on natural language processing (NLP) approaches to modeling depression in social media, providing the reader with a post-COVID-19 outlook. This review contributes to the understanding of the impacts of the pandemic on modeling depression in social media. We outline how state-of-the-art approaches and new datasets have been used in the context of the COVID-19 pandemic. Finally, we also discuss ethical issues in collecting and processing mental health data, considering fairness, accountability, and ethics. © 2013 IEEE.","Institute of Electrical and Electronics Engineers Inc.","","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-86000485311"
"Nanor M.A.","Nanor, Michael Ayertey (57195288888)","57195288888","Determinants of households residential mobility decision in Kumasi Ghana","2025","Environment, Development and Sustainability","","","e397","","","","0","10.1007/s10668-025-06046-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218270292&doi=10.1007%2fs10668-025-06046-1&partnerID=40&md5=0e17bf2c03fae6a7995f851f4db2cbb0","Household relocation choices are critical in shaping urban socio-economic landscapes, especially within the framework of the Sustainable Development Goals (SDGs). This study investigates the complex factors influencing household relocation decisions in Kumasi, Ghana, an urban center undergoing rapid growth. The aim is to understand how socio-economic conditions, environmental challenges, and urban policy dynamics interact to shape relocation intentions. Using a qualitative approach, this research use qualitative interviews and natural language processing (NLP) technique to capture the diverse push and pull factors motivating relocation. Findings indicate that push factors, such as inadequate housing, urban congestion, and environmental degradation, diminish quality of life and drive relocation. Conversely, pull factors, including employment opportunities, access to education and healthcare, and enhanced urban amenities, attract households to particular areas within Kumasi. This study situates these relocation dynamics within the SDG framework, highlighting the need for sustainable urban development strategies that address residents' aspirations and challenges. The results emphasize that aligning urban planning with principles of inclusivity, equity, and environmental sustainability can enhance urban resilience and community well-being. The study’s insights are valuable for policymakers, urban planners, and stakeholders seeking to foster urban environments conducive to sustainable development, equitable growth, and social inclusion, ultimately advancing progress toward achieving the SDGs. © The Author(s), under exclusive licence to Springer Nature B.V. 2025.","Springer Science and Business Media B.V.","","Article","Article in press","","Scopus","2-s2.0-85218270292"
"Ben-Zion Z.; Witte K.; Jagadish A.K.; Duek O.; Harpaz-Rotem I.; Khorsandian M.-C.; Burrer A.; Seifritz E.; Homan P.; Schulz E.; Spiller T.R.","Ben-Zion, Ziv (57201300042); Witte, Kristin (57226410887); Jagadish, Akshay K. (57193648204); Duek, Or (49961174700); Harpaz-Rotem, Ilan (6602624108); Khorsandian, Marie-Christine (59670307300); Burrer, Achim (57207832465); Seifritz, Erich (57203074345); Homan, Philipp (37070519500); Schulz, Eric (7202482374); Spiller, Tobias R. (57195731193)","57201300042; 57226410887; 57193648204; 49961174700; 6602624108; 59670307300; 57207832465; 57203074345; 37070519500; 7202482374; 57195731193","Assessing and alleviating state anxiety in large language models","2025","npj Digital Medicine","8","1","132","","","","0","10.1038/s41746-025-01512-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000059115&doi=10.1038%2fs41746-025-01512-6&partnerID=40&md5=118a7dd92bf81db8892b7ee348d08ae9","The use of Large Language Models (LLMs) in mental health highlights the need to understand their responses to emotional content. Previous research shows that emotion-inducing prompts can elevate “anxiety” in LLMs, affecting behavior and amplifying biases. Here, we found that traumatic narratives increased Chat-GPT-4’s reported anxiety while mindfulness-based exercises reduced it, though not to baseline. These findings suggest managing LLMs’ “emotional states” can foster safer and more ethical human-AI interactions. © The Author(s) 2025.","Nature Research","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-86000059115"
"Ramaniharan A.K.; Pednekar A.; Parikh N.A.; Nagaraj U.D.; Manhard M.K.","Ramaniharan, Anandh Kilpattu (56294871900); Pednekar, Amol (8445326100); Parikh, Nehal A. (7006642541); Nagaraj, Usha D. (15119867600); Manhard, Mary Kate (55660510800)","56294871900; 8445326100; 7006642541; 15119867600; 55660510800","A single 1-min brain MRI scan for generating multiple synthetic image contrasts in awake children from quantitative relaxometry maps","2025","Pediatric Radiology","55","2","666020","312","323","11","0","10.1007/s00247-024-06113-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212428061&doi=10.1007%2fs00247-024-06113-1&partnerID=40&md5=7ca3e776bf1e4238a049140507405368","Background: Diagnostically adequate contrast and spatial resolution in brain MRI require prolonged scan times, leading to motion artifacts and image degradation in awake children. Rapid multi-parametric techniques can produce diagnostic images in awake children, which could help to avoid the need for sedation. Objective: To evaluate the utility of a rapid echo-planar imaging (EPI)–based multi-inversion spin and gradient echo (MI-SAGE) technique for generating multi-parametric quantitative brain maps and synthetic contrast images in awake pediatric participants. Materials and methods: In this prospective IRB-approved study, awake research participants 3–10 years old were scanned using MI-SAGE, MOLLI, GRASE, mGRE, and T1-, T2-, T2*-, and FLAIR-weighted sequences. The MI-SAGE T1, T2, and T2* maps and synthetic images were estimated offline. The MI-SAGE parametric values were compared to those from conventional mapping sequences including MOLLI, GRASE, and mGRE, with assessments of repeatability and reproducibility. Synthetic MI-SAGE images and conventional weighted images were reviewed by a neuroradiologist and scored using a 5-point Likert scale. Gray-to-white matter contrast ratios (GWRs) were compared between MI-SAGE synthetic and conventional weighted images. The results were analyzed using the Bland–Altman analysis and intra-class correlation coefficient (ICC). Results: A total of 24 healthy participants aged 3 years to 10 years (mean ± SD, 6.5 ± 1.9; 12 males) completed full imaging exams including the 54-s MI-SAGE acquisition and were included in the analysis. The MI-SAGE T1, T2, and T2* had biases of 32%, -4%, and 23% compared to conventional mapping methods using MOLLI, GRASE, and mGRE, respectively, with moderate to very strong correlations (ICC=0.49–0.99). All MI-SAGE maps exhibited strong to very strong repeatability and reproducibility (ICC=0.80 to 0.99). The synthetic MI-SAGE had average Likert scores of 2.1, 2.1, 2.9, and 2.0 for T1-, T2-, T2*-, and FLAIR-weighted images, respectively, while conventional acquisitions had Likert scores of 3.5, 3.6, 4.6, and 3.8 for T1-, T2-, T2*-, and FLAIR-weighted images, respectively. The MI-SAGE synthetic T1w, T2w, T2*w, and FLAIR GWRs had biases of 17%, 3%, 7%, and 1% compared to the GWR of images from conventional T1w, T2w, T2*w, and FLAIR acquisitions respectively. Conclusion: The derived T1, T2, and T2* maps were correlated with conventional mapping methods and showed strong repeatability and reproducibility. While synthetic MI-SAGE images had greater susceptibility artifacts and lower Likert scores than conventional images, the MI-SAGE technique produced synthetic weighted images with contrasts similar to conventional weighted images and achieved a ten-fold reduction in scan time. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.","Springer Nature","39692886","Article","Final","","Scopus","2-s2.0-85212428061"
"Atmakuru A.; Shahini A.; Chakraborty S.; Seoni S.; Salvi M.; Hafeez-Baig A.; Rashid S.; Tan R.S.; Barua P.D.; Molinari F.; Acharya U.R.","Atmakuru, Anirudh (58653527900); Shahini, Alen (59002287600); Chakraborty, Subrata (56377149900); Seoni, Silvia (57213608081); Salvi, Massimo (57191596088); Hafeez-Baig, Abdul (24461808100); Rashid, Sadaf (59195362200); Tan, Ru San (7201984906); Barua, Prabal Datta (36993665100); Molinari, Filippo (7004289592); Acharya, U Rajendra (7004510847)","58653527900; 59002287600; 56377149900; 57213608081; 57191596088; 24461808100; 59195362200; 7201984906; 36993665100; 7004289592; 7004510847","Artificial intelligence-based suicide prevention and prediction: A systematic review (2019–2023)","2025","Information Fusion","114","","102673","","","","0","10.1016/j.inffus.2024.102673","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204091360&doi=10.1016%2fj.inffus.2024.102673&partnerID=40&md5=274a0486ad187e8316e1763c7580eb24","Suicide is a major global public health concern, and the application of artificial intelligence (AI) methods, such as natural language processing (NLP), machine learning (ML), and deep learning (DL), has shown promise in advancing suicide prediction and prevention efforts. Recent advancements in AI – particularly NLP and DL have opened up new avenues of research in suicide prediction and prevention. While several papers have reviewed specific detection techniques like NLP or DL, there has been no recent study that acts as a one-stop-shop, providing a comprehensive overview of all AI-based studies in this field. In this work, we conduct a systematic literature review to identify relevant studies published between 2019 and 2023, resulting in the inclusion of 156 studies. We provide a comprehensive overview of the current state of research conducted on AI-driven suicide prevention and prediction, focusing on different data types and AI techniques employed. We discuss the benefits and challenges of these approaches and propose future research directions to improve the practical application of AI in suicide research. AI is highly capable of improving the accuracy and efficiency of risk assessment, enabling personalized interventions, and enhancing our understanding of risk and protective factors. Multidisciplinary approaches combining diverse data sources and AI methods can help identify individuals at risk by analyzing social media content, patient histories, and data from mobile devices, enabling timely intervention. However, challenges related to data privacy, algorithmic bias, model interpretability, and real-world implementation must be addressed to realize the full potential of these technologies. Future research should focus on integrating prediction and prevention strategies, harnessing multimodal data, and expanding the scope to include diverse populations. Collaboration across disciplines and stakeholders is essential to ensure that AI-driven suicide prevention and prediction efforts are ethical, culturally sensitive, and person-centered. © 2024 Elsevier B.V.","Elsevier B.V.","","Article","Final","","Scopus","2-s2.0-85204091360"
"Bui N.; Nguyen G.; Nguyen N.; Vo B.; Vo L.; Huynh T.; Tang A.; Tran V.N.; Huynh T.; Nguyen H.Q.; Dinh M.","Bui, Nhat (59227827600); Nguyen, Giang (9043171900); Nguyen, Nguyen (59566164900); Vo, Bao (59565934800); Vo, Luan (59566237200); Huynh, Tom (58404701200); Tang, Arthur (57201649539); Tran, Van Nhiem (57736724900); Huynh, Tuyen (59566007800); Nguyen, Huy Quang (57459546700); Dinh, Minh (56722749600)","59227827600; 9043171900; 59566164900; 59565934800; 59566237200; 58404701200; 57201649539; 57736724900; 59566007800; 57459546700; 56722749600","Fine-tuning large language models for improved health communication in low-resource languages","2025","Computer Methods and Programs in Biomedicine","263","","108655","","","","0","10.1016/j.cmpb.2025.108655","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218354763&doi=10.1016%2fj.cmpb.2025.108655&partnerID=40&md5=1f412394ea4b574ccda2d45df676eaa8","Background: The reported study illustrates a methodology for compiling training datasets to fine-tune Large Language Models (LLMs) for healthcare information in Vietnamese, a low-resource language. The objective is to bridge the gap in medical information accessibility and enhance healthcare communication in developing countries by adapting LLMs to specific linguistic nuances and domain needs. Method: The methodology involves selecting a base model, compiling a domain-specific dataset, and fine-tuning the model with this dataset. Three open-source models were selected. The dataset, comprising approximately 337,000 prompt-response pairs in Vietnamese, was compiled using existing datasets, data crawled from Vietnamese medical online forums, and distilled from Vietnamese medical textbooks. The three models were fine-tuned using the Low-Rank adaptation (LoRA) and Quantized Low-Rank adaptation (QLoRA) techniques. Models’ performances were evaluated using BertScore score, Rouge-L score, and the ""LLM-as-a-Judge"" method. Results: The fine-tuned models showed enhancements in performance over their base versions across evaluation metrics in BertScore score, Rouge-L score and “LLM-as-a-Judge” method, confirming the effectiveness of the fine-tuning process. This study details the process of fine-tuning open-source LLMs for health information inquiries in Vietnamese, demonstrating its potential to improve healthcare communication in low-resource languages. Deploying the fine-tuned LLM on-premise enhances data privacy and security. However, the significant computing power and costs required pose challenges, especially for organizations in developing countries. Conclusion: This case study highlights the unique challenges faced by developing countries using low-resource languages. Initiatives are needed to emphasize efforts to bridge healthcare gaps in underserved areas and contribute to global health equity. © 2025 The Author(s)","Elsevier Ireland Ltd","","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85218354763"
"Bevara R.V.K.; Mannuru N.R.; Karedla S.P.; Lund B.; Xiao T.; Pasem H.; Dronavalli S.C.; Rupeshkumar S.","Bevara, Ravi Varma Kumar (58920425200); Mannuru, Nishith Reddy (58149819400); Karedla, Sai Pranathi (59074465500); Lund, Brady (57202058854); Xiao, Ting (57202463025); Pasem, Harshitha (59655443000); Dronavalli, Sri Chandra (58889980400); Rupeshkumar, Siddhanth (59655710300)","58920425200; 58149819400; 59074465500; 57202058854; 57202463025; 59655443000; 58889980400; 59655710300","Resume2Vec: Transforming Applicant Tracking Systems with Intelligent Resume Embeddings for Precise Candidate Matching","2025","Electronics (Switzerland)","14","4","794","","","","0","10.3390/electronics14040794","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218913856&doi=10.3390%2felectronics14040794&partnerID=40&md5=032e5263e938e81667e52133133e376b","Conventional Applicant Tracking Systems (ATSs) encounter considerable constraints in accurately aligning resumes with job descriptions (JD), especially in handling unstructured data and intricate qualifications. We provide Resume2Vec, an innovative method that utilizes transformer-based deep learning models, including encoders (BERT, RoBERTa, and DistilBERT) and decoders (GPT, Gemini, and Llama), to create embeddings for resumes and job descriptions, employing cosine similarity for evaluation. Our methodology integrates quantitative analysis via embedding-based evaluation with qualitative human assessment across several professional fields. Experimental findings indicate that Resume2Vec outperformed conventional ATS systems, achieving enhancements of up to 15.85% in Normalized Discounted Cumulative Gain (nDCG) and 15.94% in Ranked Biased Overlap (RBO) scores, especially within the mechanical engineering and health and fitness domains. Although conventional the ATS exhibited slightly superior nDCG scores in operations management and software testing, Resume2Vec consistently displayed a more robust alignment with human preferences across the majority of domains, as indicated by the RBO metrics. This research demonstrates that Resume2Vec is a powerful and scalable method for matching resumes to job descriptions, effectively overcoming the shortcomings of traditional systems, while preserving a high alignment with human evaluation criteria. The results indicate considerable promise for transformer-based methodologies in enhancing recruiting technology, facilitating more efficient and precise candidate selection procedures. © 2025 by the authors.","Multidisciplinary Digital Publishing Institute (MDPI)","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85218913856"
"He K.; Mao R.; Lin Q.; Ruan Y.; Lan X.; Feng M.; Cambria E.","He, Kai (57207796786); Mao, Rui (57207859287); Lin, Qika (57204147391); Ruan, Yucheng (57654588000); Lan, Xiang (57218716704); Feng, Mengling (12644999000); Cambria, Erik (56140547500)","57207796786; 57207859287; 57204147391; 57654588000; 57218716704; 12644999000; 56140547500","A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics","2025","Information Fusion","118","","102963","","","","2","10.1016/j.inffus.2025.102963","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216023945&doi=10.1016%2fj.inffus.2025.102963&partnerID=40&md5=9099a43d505ff5c013992a989fc31c14","The utilization of large language models (LLMs) for Healthcare has generated both excitement and concern due to their ability to effectively respond to free-text queries with certain professional knowledge. This survey outlines the capabilities of the currently developed Healthcare LLMs and explicates their development process, to provide an overview of the development road map from traditional Pretrained Language Models (PLMs) to LLMs. Specifically, we first explore the potential of LLMs to enhance the efficiency and effectiveness of various Healthcare applications highlighting both the strengths and limitations. Secondly, we conduct a comparison between the previous PLMs and the latest LLMs, and summarize related Healthcare training data, learning methods, and usage. Finally, the unique concerns associated with deploying LLMs are investigated, particularly regarding fairness, accountability, transparency, and ethics. Besides, we support researchers by compiling a collection of open-source resources1. Summarily, we contend that a significant paradigm shift is underway, transitioning from PLMs to LLMs. This shift encompasses a move from discriminative AI approaches to generative AI approaches, as well as a move from model-centered methodologies to data-centered methodologies. We determine that the biggest obstacle of using LLMs in Healthcare are fairness, accountability, transparency and ethics. © 2025 The Authors","Elsevier B.V.","","Article","Final","","Scopus","2-s2.0-85216023945"
"Cibelli Hibben K.; Smith Z.; Rogers B.; Ryan V.; Scanlon P.; Hoppe T.","Cibelli Hibben, Kristen (57202076834); Smith, Zachary (59126184600); Rogers, Benjamin (59125675400); Ryan, Valerie (57220743580); Scanlon, Paul (57196991623); Hoppe, Travis (15046745000)","57202076834; 59126184600; 59125675400; 57220743580; 57196991623; 15046745000","Semi-Automated Nonresponse Detection for Open-Text Survey Data","2025","Social Science Computer Review","43","1","","166","190","24","0","10.1177/08944393241249720","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193015598&doi=10.1177%2f08944393241249720&partnerID=40&md5=682f74333cac1a61aea27538232b8fa9","Open-ended survey questions can enable researchers to gain insights beyond more commonly used closed-ended question formats by allowing respondents an opportunity to provide information with few constraints and in their own words. Open-ended web probes are also increasingly used to inform the design and evaluation of survey questions. However, open-ended questions are more susceptible to insufficient or irrelevant responses that can be burdensome and time-consuming to identify and remove manually, often resulting in underuse of open-ended questions and, when used, potential inclusion of poor-quality data. To address these challenges, we developed and publicly released the Semi-Automated Nonresponse Detection for Survey text (SANDS), an item nonresponse detection approach based on a Bidirectional Transformer for Language Understanding model, fine-tuned using Simple Contrastive Sentence Embedding and targeted human coding, to categorize open-ended text data as valid or likely nonresponse. This approach is powerful in that it uses natural language processing as opposed to existing nonresponse detection approaches that have relied exclusively on rules or regular expressions or used bag-of-words approaches that tend to perform less well on short pieces of text, typos, or uncommon words, often prevalent in open-text survey data. This paper presents the development of SANDS and a quantitative evaluation of its performance and potential bias using open-text responses from a series of web probes as case studies. Overall, the SANDS model performed well in identifying a dataset of likely valid results to be used for quantitative or qualitative analysis, particularly on health-related data. Developed for generalizable use and accessible to others, the SANDS model can greatly improve the efficiency of identifying inadequate and irrelevant open-text responses, offering expanded opportunities for the use of open-text data to inform question design and improve survey data quality. © The Author(s) 2024.","SAGE Publications Inc.","","Article","Final","","Scopus","2-s2.0-85193015598"
"Fu B.; Hadid A.; Damer N.","Fu, Biying (56963339900); Hadid, Abdenour (55925650500); Damer, Naser (50861109400)","56963339900; 55925650500; 50861109400","Generative AI in the context of assistive technologies: Trends, limitations and future directions","2025","Image and Vision Computing","154","","105347","","","","1","10.1016/j.imavis.2024.105347","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211061272&doi=10.1016%2fj.imavis.2024.105347&partnerID=40&md5=740b8dc473ee604930c53deb53b897eb","With the tremendous successes of Large Language Models (LLMs) like ChatGPT for text generation and Dall-E for high-quality image generation, generative Artificial Intelligence (AI) models have shown a hype in our society. Generative AI seamlessly delved into different aspects of society ranging from economy, education, legislation, computer science, finance, and even healthcare. This article provides a comprehensive survey on the increased and promising use of generative AI in assistive technologies benefiting different parties, ranging from the assistive system developers, medical practitioners, care workforce, to the people who need the care and the comfort. Ethical concerns, biases, lack of transparency, insufficient explainability, and limited trustworthiness are major challenges when using generative AI in assistive technologies, particularly in systems that impact people directly. Key future research directions to address these issues include creating standardized rules, establishing commonly accepted evaluation metrics and benchmarks for explainability and reasoning processes, and making further advancements in understanding and reducing bias and its potential harms. Beyond showing the current trends of applying generative AI in the scope of assistive technologies in four identified key domains, which include care sectors, medical sectors, helping people in need, and co-working, the survey also discusses the current limitations and provides promising future research directions to foster better integration of generative AI in assistive technologies. © 2024 The Authors","Elsevier Ltd","","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85211061272"
"Madanay F.; Bundorf M.K.; Ubel P.A.","Madanay, Farrah (57148389200); Bundorf, M Kate (57203127172); Ubel, Peter A. (35312638200)","57148389200; 57203127172; 35312638200","Physician Gender and Patient Perceptions of Interpersonal and Technical Skills in Online Reviews","2025","JAMA network open","8","2","","e2460018","","","0","10.1001/jamanetworkopen.2024.60018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218842335&doi=10.1001%2fjamanetworkopen.2024.60018&partnerID=40&md5=698031d1fe78b98fd85a480344c0f791","Importance: Prior studies have revealed gender differences in workplace assessments of physicians, but little is known about differences by physician gender in patients' online written reviews. Objective: To analyze whether patients' perceptions of their physicians' interpersonal manner and technical competence differ by physician gender and practicing specialty and are associated with review star ratings. Design, Setting, and Participants: This cross-sectional study sampled written reviews submitted by patients between October 16, 2015, and May 27, 2020, for physicians across the US from a commercial physician rating and review website. Physicians included primary care physicians (PCPs) listed under family medicine, internal medicine, and pediatrics and surgeons listed under general surgery; orthopedic surgery; and cosmetic, plastic, and reconstructive surgery. Hand-coded reviews were used to fine-tune a natural language processing algorithm to classify all reviews for the presence and valence of patients' comments of physicians' interpersonal manner and technical competence. Statistical analyses were performed from July 2022 to December 2024. Exposure: Female or male physician gender. Main Outcomes and Measures: Outcomes included the presence and valence of interpersonal manner and technical competence comments and receipt of high star ratings. Multilevel logistic regressions analyzed differences by female or male physician gender in interpersonal manner and technical competence comments and whether those comments were associated with review star ratings. Results: The analysis included 345 053 written reviews of 167 150 physicians (mean [SD] age, 55.16 [11.40] years); 60 060 physicians (35.9%) were female, and 36 132 (21.6%) were surgeons. Female physicians overall had higher odds than males of receiving any (odds ratio [OR], 1.19; 95% CI, 1.16-1.22) or negative (OR, 1.22; 95% CI, 1.18-1.26) patient comments for their interpersonal manner. Among PCPs, females had higher odds than males of receiving a negative comment for interpersonal manner (OR, 1.22; 95% CI, 1.18-1.27) and, when receiving that negative comment, had disproportionately lower odds of receiving a high star rating (OR, 0.62; 95% CI, 0.53-0.73). Female physicians overall (OR, 1.09; 95% CI, 1.05-1.13) and female PCPs (OR, 1.08; 95% CI, 1.04-1.13) had higher odds than their male counterparts of receiving a negative comment for their technical competence. When receiving a negative comment for technical competence, both female PCPs (OR, 0.60; 95% CI, 0.50-0.73) and female surgeons (OR, 0.67; 95% CI, 0.50-0.89) had disproportionately lower odds of receiving a high star rating compared with their male counterparts. Female PCPs also had lower odds than male PCPs of receiving a high star rating when receiving a positive comment for technical competence (OR, 0.82; 95% CI, 0.70-0.96). Conclusions and Relevance: In this cross-sectional study of online written reviews, female and male physician gender were differently associated with patients' perceptions of their physicians' interpersonal manner and technical competence. The findings suggest that patients harbored negative gender biases about the interpersonal manner of female physicians, especially female PCPs, and also assessed disproportionate penalties related to technical competence for both female PCPs and female surgeons.","","39951262","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85218842335"
"Ji C.; Zhao B.; Wang Z.; Wang Y.; Zhang Y.; Cheng Y.; Feng R.; Zhang X.","Ji, Changkai (58779372000); Zhao, Bowen (58779688600); Wang, Zhuoyao (57845116800); Wang, Yingwen (35216447500); Zhang, Yuejie (9734634900); Cheng, Ying (57221148204); Feng, Rui (56611353400); Zhang, Xiaobo (37092301200)","58779372000; 58779688600; 57845116800; 35216447500; 9734634900; 57221148204; 56611353400; 37092301200","RoBGuard: Enhancing LLMs to Assess Risk of Bias in Clinical Trial Documents","2025","Proceedings - International Conference on Computational Linguistics, COLING","Part F206484-1","","","1258","1277","19","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218505044&partnerID=40&md5=abdf034b008c05ef0227acf2f3486d91","Randomized Controlled Trials (RCTs) are rigorous clinical studies crucial for reliable decision-making, but their credibility can be compromised by bias. The Cochrane Risk of Bias tool (RoB 2) assesses this risk, yet manual assessments are time-consuming and labor-intensive. Previous approaches have employed Large Language Models (LLMs) to automate this process. However, they typically focus on manually crafted prompts and a restricted set of simple questions, limiting their accuracy and generalizability. Inspired by the human bias assessment process, we propose RoBGuard, a novel framework for enhancing LLMs to assess the risk of bias in RCTs. Specifically, RoBGuard integrates medical knowledge-enhanced question reformulation, multimodal document parsing, and multi-expert collaboration to ensure both completeness and accuracy. Additionally, to address the lack of suitable datasets, we introduce two new datasets: RoB-Item and RoB-Domain. Experimental results demonstrate RoBGuard's effectiveness on the RoB-Item dataset, outperforming existing methods. © 2025 Association for Computational Linguistics.","Association for Computational Linguistics (ACL)","","Conference paper","Final","","Scopus","2-s2.0-85218505044"
"Ma Y.; Sun X.; Ma A.","Ma, Yongsheng (59545185600); Sun, Xianhui (39763250800); Ma, Aiqun (59545245300)","59545185600; 39763250800; 59545245300","PSG-Pair: A Psychological Snapshot Guided Model for Improved Query-Response Pairing in Psychological Wellness Communities","2025","IEEE Access","","","","","","","0","10.1109/ACCESS.2025.3545902","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218970548&doi=10.1109%2fACCESS.2025.3545902&partnerID=40&md5=3b31ed53bff053a13935348074f70190","Psychological wellness has become an increasingly significant global health issue, with millions affected worldwide. In response to the growing demand for accessible psychological support, we propose a novel model, Psychological Snapshot Guided Pairing (PSG-Pair), designed to enhance query-response pairing in Community Question Answering for Psychological Wellness (CQA-PW). Unlike traditional methods that focus solely on conceptual-level matching, PSG-Pair integrates role-based psychological snapshots derived from the historical posts of help-seekers and supporters. The model operates in two phases: the initial screening phase, which utilizes a BERT-based retrieval model to filter relevant supportive posts, and the pairing phase, which incorporates psychological snapshots using a stacked attention mechanism to refine conceptual pairings based on the psychological characteristics of users. Extensive experiments conducted on the CLPsych 2022 Shared Task dataset demonstrate that PSG-Pair significantly outperforms traditional single-phase models, enhancing both precision and recall in pairing processes. The inclusion of psychological snapshots allows the model to better handle the complexities of psychological wellness scenarios, thereby improving the overall effectiveness of automated psychological support systems. However, this study has several limitations. Firstly, the dataset used for experiments, although rich, still suffers from data imbalance and noise due to the high proportion of irrelevant negative samples, which could potentially impact the model's performance. Secondly, while the approach demonstrates promising results in the context of psychological wellness, the generalizability of the model to other domains or applications remains uncertain. Further exploration into the adaptability of PSG-Pair to diverse scenarios is required. Additionally, while the current evaluation metrics adequately reflect the retrieval and pairing capabilities, there is a need for the development of more tailored evaluation systems to assess models within the unique context of psychological wellness support. Future work should also investigate how to mitigate biases in user-generated content, as the quality and authenticity of answers in non-factual Q&A platforms can vary significantly, potentially affecting the accuracy of the pairing.  © 2013 IEEE.","Institute of Electrical and Electronics Engineers Inc.","","Article","Article in press","","Scopus","2-s2.0-85218970548"
"Ibrahim S.T.; Li M.; Patel J.; Katapally T.R.","Ibrahim, Sheriff Tolulope (58489382500); Li, Madeline (59194383200); Patel, Jamin (58572285700); Katapally, Tarun Reddy (56188375400)","58489382500; 59194383200; 58572285700; 56188375400","Utilizing natural language processing for precision prevention of mental health disorders among youth: A systematic review","2025","Computers in Biology and Medicine","188","","109859","","","","0","10.1016/j.compbiomed.2025.109859","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218265309&doi=10.1016%2fj.compbiomed.2025.109859&partnerID=40&md5=cd07374e3f810896f10ac222580d780c","Background: The global mental health crisis has created barriers to youth mental healthcare, leaving many disorders unaddressed. Precision prevention, which identifies individual risks, offers the potential for tailored interventions. While natural language processing (NLP) has shown promise in the early detection of mental health disorders, no review has examined its role in youth mental health detection. We hypothesize that NLP can improve early detection and personalized care in mental healthcare among youth. Methodology: After screening 1197 articles from 5 databases, 12 papers were included covering six categories: (1) mental health disorders, (2) data sources, (3) NLP objective for mental health detection, (4) annotation and validation techniques, (5) linguistic markers, and (6) performance and evaluation. Study quality was assessed using Hawker's checklist for disparate study designs. Results: Most studies focused on suicide risk (42 %), depression (25 %), and stress (17 %). Social media (42 %) and interviews (33 %) were the most common data sources, with linguistic inquiry and word count and support vector machines frequently used for analysis. While most studies were exploratory, one implemented a real-time tool for detecting mental health risks. Validation methods, including precision and recall metrics, showed strong predictive performance. Conclusions: This review highlights the potential of NLP in youth mental health detection, addressing challenges such as bias, data quality, and ethical concerns. Future research should refine NLP models using diverse, multimodal datasets, addressing data imbalance, and improving real-time detection. Exploring transformer-based models and ensuring ethical, inclusive data handling will be key to advancing NLP-driven interventions. © 2025","Elsevier Ltd","39986200","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85218265309"
"Ali I.; Vasant Patil Y.; Jangid A.; Rahaman M.A.; Dilip Taru R.; Iftikhar A.","Ali, Imad (59420518700); Vasant Patil, Yogesh (59668283100); Jangid, Ankita (59667372000); Rahaman, Md Atikur (57219245876); Dilip Taru, Rupali (59667974100); Iftikhar, Ambreen (59420518600)","59420518700; 59668283100; 59667372000; 57219245876; 59667974100; 59420518600","Blockchain-Driven Supply Chain Finance for Public Healthcare in India: Enhancing Financial Resilience in Public Health Systems; [Financiamiento de Cadenas de Suministro Impulsado por Blockchain para la Salud Pública en India: Mejorando la Resiliencia Financiera en los Sistemas de Salud Pública]","2025","Salud, Ciencia y Tecnologia","5","","1400","","","","0","10.56294/saludcyt20251400","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219682064&doi=10.56294%2fsaludcyt20251400&partnerID=40&md5=2b7d02241bb09d89c3477016c56eeff4","Introduction: public healthcare systems in India face persistent inefficiencies, including delays in financial workflows, lack of transparency, and fraud, particularly in rural and underserved areas. Blockchain and machine learning (ML) technologies offer transformative potential to address these challenges by enhancing transparency, efficiency, and accountability in healthcare supply chains. Method: a mixed-methods approach was adopted, combining structured surveys, semi-structured interviews, and secondary data analysis. Quantitative data were analysed using techniques such as descriptive statistics, predictive modelling (Random Forest), clustering (K-means), and anomaly detection (Isolation Forest). Qualitative data from stakeholder interviews were analysed using Natural Language Processing (NLP) to identify recurring themes and sentiment trends. Results: the analysis revealed significant inefficiencies and readiness disparities among stakeholders. Blockchain was identified as a critical tool for improving transparency, with readiness levels being the strongest predictor of adoption success. ML demonstrated robust capabilities in fraud detection, with 5 % of transactions flagged as anomalies, and predictive modelling identified key factors influencing readiness. Clustering analysis revealed distinct groups of stakeholders, highlighting the need for tailored interventions to bridge readiness gaps. Sentiment analysis indicated 65 % of stakeholders held positive views on blockchain and ML adoption. Conclusions: blockchain and ML technologies have the potential to transform public healthcare financing by addressing inefficiencies, enhancing transparency, and optimizing resource allocation. However, disparities in stakeholder readiness necessitate targeted capacity-building and phased implementation strategies. These findings provide a roadmap for integrating blockchain and ML into public healthcare systems, fostering financial resilience and improving service delivery in rural and underserved areas. © 2025; Los autores.","AG Editor (Argentina)","","Article","Final","","Scopus","2-s2.0-85219682064"
"Gu B.; Shao V.; Liao Z.; Carducci V.; Brufau S.R.; Yang J.; Desai R.J.","Gu, Bowen (59151332200); Shao, Vivian (59334763000); Liao, Ziqian (58683680100); Carducci, Valentina (59001984400); Brufau, Santiago Romero (55212243000); Yang, Jie (57196225258); Desai, Rishi J. (56157710900)","59151332200; 59334763000; 58683680100; 59001984400; 55212243000; 57196225258; 56157710900","Scalable information extraction from free text electronic health records using large language models","2025","BMC Medical Research Methodology","25","1","23","","","","0","10.1186/s12874-025-02470-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217189812&doi=10.1186%2fs12874-025-02470-z&partnerID=40&md5=c0d1932778de9b79ec173bdf89a33eff","Background: A vast amount of potentially useful information such as description of patient symptoms, family, and social history is recorded as free-text notes in electronic health records (EHRs) but is difficult to reliably extract at scale, limiting their utility in research. This study aims to assess whether an “out of the box” implementation of open-source large language models (LLMs) without any fine-tuning can accurately extract social determinants of health (SDoH) data from free-text clinical notes. Methods: We conducted a cross-sectional study using EHR data from the Mass General Brigham (MGB) system, analyzing free-text notes for SDoH information. We selected a random sample of 200 patients and manually labeled nine SDoH aspects. Eight advanced open-source LLMs were evaluated against a baseline pattern-matching model. Two human reviewers provided the manual labels, achieving 93% inter-annotator agreement. LLM performance was assessed using accuracy metrics for overall, mentioned, and non-mentioned SDoH, and macro F1 scores. Results: LLMs outperformed the baseline pattern-matching approach, particularly for explicitly mentioned SDoH, achieving up to 40% higher Accuracymentioned. openchat_3.5 was the best-performing model, surpassing the baseline in overall accuracy across all nine SDoH aspects. The refined pipeline with prompt engineering reduced hallucinations and improved accuracy. Conclusions: Open-source LLMs are effective and scalable tools for extracting SDoH from unstructured EHRs, surpassing traditional pattern-matching methods. Further refinement and domain-specific training could enhance their utility in clinical research and predictive analytics, improving healthcare outcomes and addressing health disparities. © The Author(s) 2025.","BioMed Central Ltd","39871166","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85217189812"
"Mansoor M.; Ansari K.","Mansoor, Masab (59347634200); Ansari, Kashif (59347234000)","59347634200; 59347234000","Artificial Intelligence-Driven Analysis of Telehealth Effectiveness in Youth Mental Health Services: Insights from SAMHSA Data","2025","Journal of Personalized Medicine","15","2","63","","","","0","10.3390/jpm15020063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218870736&doi=10.3390%2fjpm15020063&partnerID=40&md5=a9bb0aa9f039a849b0d34a76a2f162d2","Background: The rapid adoption of telehealth services for youth mental health care necessitates a comprehensive evaluation of its effectiveness. This study aimed to analyze the impact of telehealth on youth mental health outcomes using artificial intelligence techniques applied to large-scale public health data. Methods: We conducted an AI-driven analysis of data from the National Survey on Drug Use and Health (NSDUH) and other SAMHSA datasets. Machine learning techniques, including random forest models, K-means clustering, and time series analysis, were employed to evaluate telehealth adoption patterns, predictors of effectiveness, and comparative outcomes with traditional in-person care. Natural language processing was used to analyze sentiment in user feedback. Results: Telehealth adoption among youth increased significantly, with usage rising from 2.3 sessions per year in 2019 to 8.7 in 2022. Telehealth showed comparable effectiveness to in-person care for depressive disorders and superior effectiveness for anxiety disorders. Session frequency, age, and prior diagnosis were identified as key predictors of telehealth effectiveness. Four distinct user clusters were identified, with socioeconomic status and home environment strongly associated with positive outcomes. States with favorable reimbursement policies saw a 15% greater increase in youth telehealth utilization and 7% greater improvement in mental health outcomes. Conclusions: Telehealth demonstrates significant potential in improving access to and effectiveness of mental health services for youth. However, addressing technological barriers and socioeconomic disparities is crucial to maximize its benefits. © 2025 by the authors.","Multidisciplinary Digital Publishing Institute (MDPI)","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85218870736"
"Chun J.; Kim J.; Kim H.; Lee G.; Cho S.; Kim C.; Chung Y.; Heo S.","Chun, Jiyong (59585080400); Kim, Jeongsoo (59590081300); Kim, Hyejin (56181018700); Lee, Geumgu (59589250100); Cho, Sanggoo (59585080500); Kim, Changshik (59590081400); Chung, Yeesook (59590908500); Heo, Seoyoon (57226974500)","59585080400; 59590081300; 56181018700; 59589250100; 59585080500; 59590081400; 59590908500; 57226974500","A Comparative Analysis of On-Device AI-Driven, Self-Regulated Learning and Traditional Pedagogy in University Health Sciences Education","2025","Applied Sciences (Switzerland)","15","4","1815","","","","1","10.3390/app15041815","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218445214&doi=10.3390%2fapp15041815&partnerID=40&md5=bfec007e76bb33871575b2a9bff82aec","Generative artificial intelligence (AI) has introduced transformative paradigms into education, facilitating personalized, real-time, and interactive learning experiences. This study investigates the integration of AI-based textbooks within regular college curricula, with a specific focus on their application in ancillary engineering subjects that demand high precision. AI textbooks enable customized learning pathways, enhance student engagement through adaptive content, and provide educators with data-driven insights. Employing a mixed-methods approach, this research compares the academic performance and learning experiences of two groups: the Traditional Learning Group (TLG), which utilized printed materials; and the AI Learning Group (ALG), which employed a generative AI-powered textbook based on LLama 3.1. Over a 15-week semester, data were collected through pre- and post-tests, task evaluations, platform log analyses, and satisfaction surveys. The findings reveal no statistically significant differences between the two groups in quantitative measures such as academic achievement, learning time, and overall satisfaction. However, the qualitative assessments underscore the role of AI-based learning in supporting self-directed education, while also highlighting the critical challenges, including digital equity, algorithmic biases, and data privacy concerns. Furthermore, the study emphasizes the necessity of comprehensive educator training, the establishment of ethical frameworks, and the development of scalable implementation strategies to fully leverage the potential of AI textbooks. Future research should prioritize randomized controlled trials (RCTs) to evaluate the long-term impacts of AI-based educational tools and develop adaptive frameworks that balance technological advancements with the emotional and motivational dimensions of human-centered education. This pioneering research lays the groundwork for advancing generative AI in higher education and fostering an innovative and equitable learning ecosystem. © 2025 by the authors.","Multidisciplinary Digital Publishing Institute (MDPI)","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85218445214"
"Kitapcioglu D.; Aksoy M.E.; Ozkan A.E.; Usseli T.; Cabuk Colak D.; Torun T.","Kitapcioglu, Dilek (55078240400); Aksoy, Mehmet Emin (23970353200); Ozkan, Arun Ekin (59296933400); Usseli, Tuba (58590939700); Cabuk Colak, Dilan (59671403600); Torun, Tugrul (59672073400)","55078240400; 23970353200; 59296933400; 58590939700; 59671403600; 59672073400","Enhancing Immersion in Virtual Reality–Based Advanced Life Support Training: Randomized Controlled Trial","2025","JMIR Serious Games","13","","e68272","","","","0","10.2196/68272","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000168295&doi=10.2196%2f68272&partnerID=40&md5=4a8f9452d9dfd76d11f684a4b650bf6e","Background: Serious game–based training modules are pivotal for simulation-based health care training. With advancements in artificial intelligence (AI) and natural language processing, voice command interfaces offer an intuitive alternative to traditional virtual reality (VR) controllers in VR applications. Objective: This study aims to compare AI-supported voice command interfaces and traditional VR controllers in terms of user performance, exam scores, presence, and confidence in advanced cardiac life support (ACLS) training. Methods: A total of 62 volunteer students from Acibadem Mehmet Ali Aydinlar University Vocational School for Anesthesiology, aged 20-22 years, participated in the study. All the participants completed a pretest consisting of 10 multiple-choice questions about ACLS. Following the pretest, participants were randomly divided into 2 groups: the voice command group (n=31) and the VR controller group (n=31). The voice command group members completed the VR-based ACLS serious game in training mode twice, using an AI-supported voice command as the game interface. The VR controller group members also completed the VR-based ACLS serious game in training mode twice, but they used VR controllers as the game interface. The participants completed a survey to assess their level of presence and confidence during gameplay. Following the survey, participants completed the exam module of the VR-based serious gaming module. At the final stage of the study, participants completed a posttest, which had the same content as the pretest. VR-based exam scores of the voice command and VR controller groups were compared using a 2-tailed, independent-samples t test, and linear regression analysis was conducted to examine the effect of presence and confidence rating. Results: Both groups showed an improvement in performance from pretest to posttest, with no significant difference in the magnitude of improvement between the 2 groups (P=.83). When comparing presence ratings, there was no significant difference between the voice command group (mean 5.18, SD 0.83) and VR controller group (mean 5.42, SD 0.75; P=.25). However, when comparing VR-based exam scores, the VR controller group (mean 80.47, SD 13.12) significantly outperformed the voice command group (mean 66.70, SD 21.65; P=.005), despite both groups having similar time allocations for the exam (voice command group: mean 18.59, SD 5.28 minutes and VR controller group: mean 17.3, SD 4.83 minutes). Confidence levels were similar between the groups (voice command group: mean 3.79, SD 0.77 and VR controller group: mean 3.60, SD 0.72), but the voice command group displayed a significant overconfidence bias (voice command group: mean 0.09, SD 0.24 and VR controller group: mean –0.09, SD 0.18; P=.002). Conclusions: VR-based ACLS training demonstrated effectiveness; however, the use of voice commands did not result in improved performance. Further research should explore ways to optimize AI’s role in education through VR. ©Dilek Kitapcioglu, Mehmet Emin Aksoy, Arun Ekin Ozkan, Tuba Usseli, Dilan Cabuk Colak, Tugrul Torun.","JMIR Publications Inc.","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-86000168295"
"Busch F.; Hoffmann L.; Rueger C.; van Dijk E.H.C.; Kader R.; Ortiz-Prado E.; Makowski M.R.; Saba L.; Hadamitzky M.; Kather J.N.; Truhn D.; Cuocolo R.; Adams L.C.; Bressem K.K.","Busch, Felix (57808692600); Hoffmann, Lena (58635407200); Rueger, Christopher (58783239200); van Dijk, Elon HC (56694140200); Kader, Rawen (57219449805); Ortiz-Prado, Esteban (56037076100); Makowski, Marcus R. (35322730700); Saba, Luca (16234937700); Hadamitzky, Martin (6603321287); Kather, Jakob Nikolas (55550069300); Truhn, Daniel (23010796500); Cuocolo, Renato (55253274100); Adams, Lisa C. (57192991712); Bressem, Keno K. (57197715594)","57808692600; 58635407200; 58783239200; 56694140200; 57219449805; 56037076100; 35322730700; 16234937700; 6603321287; 55550069300; 23010796500; 55253274100; 57192991712; 57197715594","Current applications and challenges in large language models for patient care: a systematic review","2025","Communications Medicine","5","1","26","","","","2","10.1038/s43856-024-00717-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218112324&doi=10.1038%2fs43856-024-00717-2&partnerID=40&md5=7499878030b8128b44eb7990b51fbb55","Background: The introduction of large language models (LLMs) into clinical practice promises to improve patient education and empowerment, thereby personalizing medical care and broadening access to medical knowledge. Despite the popularity of LLMs, there is a significant gap in systematized information on their use in patient care. Therefore, this systematic review aims to synthesize current applications and limitations of LLMs in patient care. Methods: We systematically searched 5 databases for qualitative, quantitative, and mixed methods articles on LLMs in patient care published between 2022 and 2023. From 4349 initial records, 89 studies across 29 medical specialties were included. Quality assessment was performed using the Mixed Methods Appraisal Tool 2018. A data-driven convergent synthesis approach was applied for thematic syntheses of LLM applications and limitations using free line-by-line coding in Dedoose. Results: We show that most studies investigate Generative Pre-trained Transformers (GPT)-3.5 (53.2%, n = 66 of 124 different LLMs examined) and GPT-4 (26.6%, n = 33/124) in answering medical questions, followed by patient information generation, including medical text summarization or translation, and clinical documentation. Our analysis delineates two primary domains of LLM limitations: design and output. Design limitations include 6 second-order and 12 third-order codes, such as lack of medical domain optimization, data transparency, and accessibility issues, while output limitations include 9 second-order and 32 third-order codes, for example, non-reproducibility, non-comprehensiveness, incorrectness, unsafety, and bias. Conclusions: This review systematically maps LLM applications and limitations in patient care, providing a foundational framework and taxonomy for their implementation and evaluation in healthcare settings. © The Author(s) 2025.","Springer Nature","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85218112324"
"Chang C.T.; Farah H.; Gui H.; Rezaei S.J.; Bou-Khalil C.; Park Y.-J.; Swaminathan A.; Omiye J.A.; Kolluri A.; Chaurasia A.; Lozano A.; Heiman A.; Jia A.S.; Kaushal A.; Jia A.; Iacovelli A.; Yang A.; Salles A.; Singhal A.; Narasimhan B.; Belai B.; Jacobson B.H.; Li B.; Poe C.H.; Sanghera C.; Zheng C.; Messer C.; Kettud D.V.; Pandya D.; Kaur D.; Hla D.; Dindoust D.; Moehrle D.; Ross D.; Chou E.; Lin E.; Haredasht F.N.; Cheng G.; Gao I.; Chang J.; Silberg J.; Fries J.A.; Xu J.; Jamison J.; Tamaresis J.S.; Chen J.H.; Lazaro J.; Banda J.M.; Lee J.J.; Matthys K.E.; Steffner K.R.; Tian L.; Pegolotti L.; Srinivasan M.; Manimaran M.; Schwede M.; Zhang M.; Nguyen M.; Fathzadeh M.; Zhao Q.; Bajra R.; Khurana R.; Azam R.; Bartlett R.; Truong S.T.; Fleming S.L.; Raj S.; Behr S.; Onyeka S.; Muppidi S.; Bandali T.; Eulalio T.Y.; Chen W.; Zhou X.; Ding Y.; Cui Y.; Tan Y.; Liu Y.; Shah N.; Daneshjou R.","Chang, Crystal T. (58838418400); Farah, Hodan (58514456800); Gui, Haiwen (57222230278); Rezaei, Shawheen Justin (57217825768); Bou-Khalil, Charbel (57824695500); Park, Ye-Jean (58136813100); Swaminathan, Akshay (57207546640); Omiye, Jesutofunmi A. (57213171062); Kolluri, Akaash (59152336300); Chaurasia, Akash (57204094195); Lozano, Alejandro (57382733600); Heiman, Alice (58024821400); Jia, Allison Sihan (57482922800); Kaushal, Amit (59153017200); Jia, Angela (59152672500); Iacovelli, Angelica (59152447500); Yang, Archer (57923477300); Salles, Arghavan (24067383200); Singhal, Arpita (59152336400); Narasimhan, Balasubramanian (56366977000); Belai, Benjamin (59152791300); Jacobson, Benjamin H. (57219642055); Li, Binglan (57201666615); Poe, Celeste H. (57222640153); Sanghera, Chandan (57207763457); Zheng, Chenming (59152336500); Messer, Conor (59153131600); Kettud, Damien Varid (59152902300); Pandya, Deven (59152672600); Kaur, Dhamanpreet (57216671964); Hla, Diana (57369320000); Dindoust, Diba (57375688600); Moehrle, Dominik (59152672700); Ross, Duncan (59152560000); Chou, Ellaine (59152447700); Lin, Eric (59153017300); Haredasht, Fateme Nateghi (57194035824); Cheng, Ge (59152672800); Gao, Irena (57193345496); Chang, Jacob (59152336700); Silberg, Jake (59153131700); Fries, Jason A. (57190403708); Xu, Jiapeng (59153131800); Jamison, Joe (59152791500); Tamaresis, John S. (6507299425); Chen, Jonathan H. (57112911000); Lazaro, Joshua (59152336800); Banda, Juan M. (35197796500); Lee, Julie J. (59195126200); Matthys, Karen Ebert (59153131900); Steffner, Kirsten R. (56433407300); Tian, Lu (59298747000); Pegolotti, Luca (57203845058); Srinivasan, Malathi (35238575500); Manimaran, Maniragav (59152672900); Schwede, Matthew (37097864200); Zhang, Minghe (59152447800); Nguyen, Minh (57223231354); Fathzadeh, Mohsen (24340940100); Zhao, Qian (57194343179); Bajra, Rika (57219426541); Khurana, Rohit (59152791600); Azam, Ruhana (59152447900); Bartlett, Rush (59153132000); Truong, Sang T. (58022287500); Fleming, Scott L. (57208627380); Raj, Shriti (57193544625); Behr, Solveig (57919003100); Onyeka, Sonia (59003875700); Muppidi, Sri (26667009600); Bandali, Tarek (57219312474); Eulalio, Tiffany Y. (57203618335); Chen, Wenyuan (59152791800); Zhou, Xuanyu (59153132100); Ding, Yanan (59152902600); Cui, Ying (57761753300); Tan, Yuqi (56518750100); Liu, Yutong (59152673000); Shah, Nigam (7401823709); Daneshjou, Roxana (41261080700)","58838418400; 58514456800; 57222230278; 57217825768; 57824695500; 58136813100; 57207546640; 57213171062; 59152336300; 57204094195; 57382733600; 58024821400; 57482922800; 59153017200; 59152672500; 59152447500; 57923477300; 24067383200; 59152336400; 56366977000; 59152791300; 57219642055; 57201666615; 57222640153; 57207763457; 59152336500; 59153131600; 59152902300; 59152672600; 57216671964; 57369320000; 57375688600; 59152672700; 59152560000; 59152447700; 59153017300; 57194035824; 59152672800; 57193345496; 59152336700; 59153131700; 57190403708; 59153131800; 59152791500; 6507299425; 57112911000; 59152336800; 35197796500; 59195126200; 59153131900; 56433407300; 59298747000; 57203845058; 35238575500; 59152672900; 37097864200; 59152447800; 57223231354; 24340940100; 57194343179; 57219426541; 59152791600; 59152447900; 59153132000; 58022287500; 57208627380; 57193544625; 57919003100; 59003875700; 26667009600; 57219312474; 57203618335; 59152791800; 59153132100; 59152902600; 57761753300; 56518750100; 59152673000; 7401823709; 41261080700","Red teaming ChatGPT in medicine to yield real-world insights on model behavior","2025","npj Digital Medicine","8","1","149","","","","0","10.1038/s41746-025-01542-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000356540&doi=10.1038%2fs41746-025-01542-0&partnerID=40&md5=029f8d6d0a6dec926e4a872ce0e28b04","Red teaming, the practice of adversarially exposing unexpected or undesired model behaviors, is critical towards improving equity and accuracy of large language models, but non-model creator-affiliated red teaming is scant in healthcare. We convened teams of clinicians, medical and engineering students, and technical professionals (80 participants total) to stress-test models with real-world clinical cases and categorize inappropriate responses along axes of safety, privacy, hallucinations/accuracy, and bias. Six medically-trained reviewers re-analyzed prompt-response pairs and added qualitative annotations. Of 376 unique prompts (1504 responses), 20.1% were inappropriate (GPT-3.5: 25.8%; GPT-4.0: 16%; GPT-4.0 with Internet: 17.8%). Subsequently, we show the utility of our benchmark by testing GPT-4o, a model released after our event (20.4% inappropriate). 21.5% of responses appropriate with GPT-3.5 were inappropriate in updated models. We share insights for constructing red teaming prompts, and present our benchmark for iterative model assessments. © The Author(s) 2025.","Nature Research","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-86000356540"
"Hua Y.; Beam A.; Chibnik L.B.; Torous J.","Hua, Yining (57787052500); Beam, Andrew (36452687100); Chibnik, Lori B. (6508199497); Torous, John (55816955800)","57787052500; 36452687100; 6508199497; 55816955800","From statistics to deep learning: Using large language models in psychiatric research","2025","International Journal of Methods in Psychiatric Research","34","1","e70007","","","","0","10.1002/mpr.70007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214513954&doi=10.1002%2fmpr.70007&partnerID=40&md5=9663677b2bbf006900bda4720f9429c5","Background: Large Language Models (LLMs) hold promise in enhancing psychiatric research efficiency. However, concerns related to bias, computational demands, data privacy, and the reliability of LLM-generated content pose challenges. Gap: Existing studies primarily focus on the clinical applications of LLMs, with limited exploration of their potentials in broader psychiatric research. Objective: This study adopts a narrative review format to assess the utility of LLMs in psychiatric research, beyond clinical settings, focusing on their effectiveness in literature review, study design, subject selection, statistical modeling, and academic writing. Implication: This study provides a clearer understanding of how LLMs can be effectively integrated in the psychiatric research process, offering guidance on mitigating the associated risks and maximizing their potential benefits. While LLMs hold promise for advancing psychiatric research, careful oversight, rigorous validation, and adherence to ethical standards are crucial to mitigating risks such as bias, data privacy concerns, and reliability issues, thereby ensuring their effective and responsible use in improving psychiatric research. © 2025 The Author(s). International Journal of Methods in Psychiatric Research published by John Wiley & Sons Ltd.","John Wiley and Sons Ltd","39777756","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85214513954"
"Davis V.H.; Qiang J.R.; MacCarthy I.A.; Howse D.; Seshie A.Z.; Kosowan L.; Delahunty-Pike A.; Abaga E.; Cooney J.; Robinson M.; Senior D.; Zsager A.; Aubrey-Bassler K.; Irwin M.; Jackson L.A.; Katz A.; Marshall E.G.; Muhajarine N.; Neudorf C.; Garies S.; Pinto A.D.","Davis, Victoria H. (57223873273); Qiang, Jinfan Rose (58161967600); MacCarthy, Itunuoluwa Adekoya (59680186400); Howse, Dana (36911805300); Seshie, Abigail Zita (59679536800); Kosowan, Leanne (57200563762); Delahunty-Pike, Alannah (56597996100); Abaga, Eunice (58068681600); Cooney, Jane (57202430210); Robinson, Marjeiry (57202428588); Senior, Dorothy (57212175268); Zsager, Alexander (57218136420); Aubrey-Bassler, Kris (23484237400); Irwin, Mandi (58068696300); Jackson, Lois A. (7402099010); Katz, Alan (26644863700); Marshall, Emily Gard (35752100700); Muhajarine, Nazeem (55898437500); Neudorf, Cory (15519916200); Garies, Stephanie (55595426900); Pinto, Andrew D. (7402306537)","57223873273; 58161967600; 59680186400; 36911805300; 59679536800; 57200563762; 56597996100; 58068681600; 57202430210; 57202428588; 57212175268; 57218136420; 23484237400; 58068696300; 7402099010; 26644863700; 35752100700; 55898437500; 15519916200; 55595426900; 7402306537","Perspectives on Using Artificial Intelligence to Derive Social Determinants of Health Data From Medical Records in Canada: Large Multijurisdictional Qualitative Study","2025","Journal of Medical Internet Research","27","","e52244","","","","0","10.2196/52244","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000533508&doi=10.2196%2f52244&partnerID=40&md5=a3e003dd9ee38c0c59cb6d4194716d6e","Background: Data on the social determinants of health could be used to improve care, support quality improvement initiatives, and track progress toward health equity. However, this data collection is not widespread. Artificial intelligence (AI), specifically natural language processing and machine learning, could be used to derive social determinants of health data from electronic medical records. This could reduce the time and resources required to obtain social determinants of health data. Objective: This study aimed to understand perspectives of a diverse sample of Canadians on the use of AI to derive social determinants of health information from electronic medical record data, including benefits and concerns. Methods: Using a qualitative description approach, in-depth interviews were conducted with 195 participants purposefully recruited from Ontario, Newfoundland and Labrador, Manitoba, and Saskatchewan. Transcripts were analyzed using an inductive and deductive content analysis. Results: A total of 4 themes were identified. First, AI was described as the inevitable future, facilitating more efficient, accessible social determinants of health information and use in primary care. Second, participants expressed concerns about potential health care harms and a distrust in AI and public systems. Third, some participants indicated that AI could lead to a loss of the human touch in health care, emphasizing a preference for strong relationships with providers and individualized care. Fourth, participants described the critical importance of consent and the need for strong safeguards to protect patient data and trust. Conclusions: These findings provide important considerations for the use of AI in health care, and particularly when health care administrators and decision makers seek to derive social determinants of health data. © Victoria H Davis, Jinfan Rose Qiang, Itunuoluwa Adekoya MacCarthy, Dana Howse, Abigail Zita Seshie, Leanne Kosowan, Alannah Delahunty-Pike, Eunice Abaga, Jane Cooney, Marjeiry Robinson, Dorothy Senior, Alexander Zsager, Kris Aubrey-Bassler, Mandi Irwin, Lois A Jackson, Alan Katz, Emily Gard Marshall, Nazeem Muhajarine, Cory Neudorf, Stephanie Garies, Andrew D Pinto.","JMIR Publications Inc.","40053728","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-86000533508"
"Qiu Y.; Mintenig S.; Barchiesi M.; Koelmans A.A.","Qiu, Yanning (56594111500); Mintenig, Svenja (56879071400); Barchiesi, Margherita (57220006748); Koelmans, Albert A. (7004747405)","56594111500; 56879071400; 57220006748; 7004747405","Using artificial intelligence tools for data quality evaluation in the context of microplastic human health risk assessments","2025","Environment International","197","","109341","","","","0","10.1016/j.envint.2025.109341","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218261581&doi=10.1016%2fj.envint.2025.109341&partnerID=40&md5=9e0a9d1683edf8ec048a51fad0f47d56","Concerns about the negative impacts of microplastics on human health are increasing in society, while exposure and risk assessments require high-quality, reliable data. Although quality assurance and –control (QA/QC) frameworks exist to evaluate the reliability of data for these purposes, manually assessing studies is too time-consuming and prone to inconsistencies due to semantic ambiguities and evaluator bias. The rapid growth of microplastic studies makes manually screening relevant data practically unfeasible. This study explores the potential of artificial intelligence (AI), specifically large language models (LLMs) such as OpenAI's ChatGPT and Google's Gemini, to streamline and standardize the QA/QC screening of data in microplastics research. We developed specific prompts based on previously published QA/QC criteria for the analysis of microplastics in drinking water and its sources, and used these to instruct AI tools to evaluate 73 studies published between 2011 and 2024. Our approach demonstrated the effectiveness of AI in extracting relevant information, interpreting the reliability of studies, and replicating human assessments. The findings indicate that AI-assisted assessments show promise in improving speed, consistency and applicability in QA/QC tasks, as well as in ranking studies or datasets based on their suitability for exposure and risk assessments. This groundbreaking application of LLMs in the environmental sciences suggests that AI can play a vital role in harmonizing microplastics risk assessments within regulatory frameworks and demonstrates how to meet the demands of an increasingly data-intensive application domain. © 2025 The Authors","Elsevier Ltd","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85218261581"
"Salama N.; Bsharat R.; Alwawi A.; Khlaif Z.N.","Salama, Nisreen (59654208300); Bsharat, Rebhi (55361883500); Alwawi, Abdallah (57216676391); Khlaif, Zuheir N. (36682701800)","59654208300; 55361883500; 57216676391; 36682701800","Knowledge, attitudes, and practices toward AI technology (ChatGPT) among nursing students at Palestinian universities","2025","BMC Nursing","24","1","269","","","","0","10.1186/s12912-025-02913-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000073977&doi=10.1186%2fs12912-025-02913-4&partnerID=40&md5=2834ba9acd065f3327eda0a665367071","Background: AI can improve medical practice, address staff shortages, and enhance diagnostic efficiency. The ChatGPT of Open AI, launched in 2022, uses AI in medical education. However, the long-term impact is uncertain, and integration varies globally, particularly in the Middle East. Aim: To explore the knowledge, practices, and attitudes of nursing students in Palestinian universities regarding AI, specifically the use of ChatGPT. Methodology: A cross-sectional design was used to conduct this study. The study was performed at 8 private and governmental universities in the West Bank, Palestine, from 1st May 2024 to 30 May 2024, and 304 nursing students participated. Results: The study revealed that 84.5% of nursing students at Palestinian universities were aware of AI technology, yet 69.9% lacked formal education or training related to ChatGPT. Despite this gap, 79% supported the integration of AI into nursing curricula and specialized training programs, reflecting strong optimism about its role in education and healthcare. While 58.6% had used AI in their coursework and 68.1% felt comfortable with technology, disparities in proficiency and access remain key barriers to effective AI integration. Major challenges to AI adoption in Palestine include insufficient training, the absence of AI-focused curricula, and financial constraints, underscoring the need for institutional and pedagogical reforms. Concerns about AI’s reliability, costs, and potential diagnostic errors persist, emphasizing the complexities of its integration into nursing education and practice. Conclusion: This study highlights the knowledge, attitudes, and practices of Palestinian nursing students regarding AI and ChatGPT. It reveals that, despite growing awareness, the lack of formal education on AI underscores the need for comprehensive curricula. While students’ express optimism about AI’s potential in healthcare, concerns about its reliability and integration persist. The study also reveals that barriers such as inadequate training, limited curricula, and financial constraints must be addressed to effectively integrate AI into nursing education and prepare students for its expanding role in healthcare. © The Author(s) 2025.","BioMed Central Ltd","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-105000073977"
"Mess S.A.; Mackey A.J.; Yarowsky D.E.","Mess, Sarah A. (12775732300); Mackey, Alison J. (7006250192); Yarowsky, David E. (16410214900)","12775732300; 7006250192; 16410214900","Artificial Intelligence Scribe and Large Language Model Technology in Healthcare Documentation: Advantages, Limitations, and Recommendations","2025","Plastic and Reconstructive Surgery - Global Open","13","1","","e6450","","","0","10.1097/GOX.0000000000006450","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215842201&doi=10.1097%2fGOX.0000000000006450&partnerID=40&md5=d35bba30294394fe4669e98e03dd436b","Summary: Artificial intelligence (AI) scribe applications in the healthcare community are in the early adoption phase and offer unprecedented efficiency for medical documentation. They typically use an application programming interface with a large language model (LLM), for example, generative pretrained transformer 4. They use automatic speech recognition on the physician-patient interaction, generating a full medical note for the encounter, together with a draft follow-up e-mail for the patient and, often, recommendations, all within seconds or minutes. This provides physicians with increased cognitive freedom during medical encounters due to less time needed interfacing with electronic medical records. However, careful proofreading of the AI-generated language by the physician signing the note is essential. Insidious and potentially significant errors of omission, fabrication, or substitution may occur. The neural network algorithms of LLMs have unpredictable sensitivity to user input and inherent variability in their output. LLMs are unconstrained by established medical knowledge or rules. As they gain increasing levels of access to large corpora of medical records, the explosion of discovered knowledge comes with large potential risks, including to patient privacy, and potential bias in algorithms. Medical AI developers should use robust regulatory oversights, adhere to ethical guidelines, correct bias in algorithms, and improve detection and correction of deviations from the intended output.  © 2025 The Authors.","Lippincott Williams and Wilkins","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85215842201"
"Agnihotri A.P.; Nagel I.D.; Artiaga J.C.M.; Guevarra M.C.B.; Sosuan G.M.N.; Kalaw F.G.P.","Agnihotri, Akshay Prashant (59411627100); Nagel, Ines Doris (59383858100); Artiaga, Jose Carlo M. (57222355493); Guevarra, Ma. Carmela B. (57575620100); Sosuan, George Michael N. (57204447352); Kalaw, Fritz Gerald P. (58163571900)","59411627100; 59383858100; 57222355493; 57575620100; 57204447352; 58163571900","Large Language Models in Ophthalmology: A Review of Publications from Top Ophthalmology Journals","2025","Ophthalmology Science","5","3","100681","","","","0","10.1016/j.xops.2024.100681","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219503020&doi=10.1016%2fj.xops.2024.100681&partnerID=40&md5=bb1ad0c59f3b09527774caa624acfde6","Purpose: To review and evaluate the current literature on the application and impact of large language models (LLMs) in the field of ophthalmology, focusing on studies published in high-ranking ophthalmology journals. Design: This is a retrospective review of published articles. Participants: This study did not involve human participation. Methods: Articles published in the first quartile (Q1) of ophthalmology journals on Scimago Journal & Country Rank discussing different LLMs up to June 7, 2024, were reviewed, parsed, and analyzed. Main Outcome Measures: All available articles were parsed and analyzed, which included the article and author characteristics and data regarding the LLM used and its applications, focusing on its use in medical education, clinical assistance, research, and patient education. Results: There were 35 Q1-ranked journals identified, 19 of which contained articles discussing LLMs, with 101 articles eligible for review. One-third were original investigations (32%; 32/101), with an average of 5.3 authors per article. The United States (50.4%; 51/101) was the most represented country, followed by the United Kingdom (25.7%; 26/101) and Canada (16.8%; 17/101). ChatGPT was the most used LLM among the studies, with different versions discussed and compared. Large language model applications were discussed relevant to their implications in medical education, clinical assistance, research, and patient education. Conclusions: The numerous publications on the use of LLM in ophthalmology can provide valuable insights for stakeholders and consumers of these applications. Large language models present significant opportunities for advancement in ophthalmology, particularly in team science, education, clinical assistance, and research. Although LLMs show promise, they also show challenges such as performance inconsistencies, bias, and ethical concerns. The study emphasizes the need for ongoing artificial intelligence improvement, ethical guidelines, and multidisciplinary collaboration. Financial Disclosure(s): The author(s) have no proprietary or commercial interest in any materials discussed in this article. © 2024 American Academy of Ophthalmology","Elsevier Inc.","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85219503020"
"Lee H.","Lee, Hocheol (57211534531)","57211534531","Assessment of Digital Capabilities by 9 Countries in the Alliance for Healthy Cities Using AI: Cross-Sectional Analysis","2025","JMIR Formative Research","9","","e62935","","","","0","10.2196/62935","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217257121&doi=10.2196%2f62935&partnerID=40&md5=29849ea30ebb96129bd7dfe1f8bcd257","Background: The Alma-Ata Declaration of 1978 initiated a global focus on universal health, supported by the World Health Organization (WHO) through healthy cities policies. The concept emerged at the 1984 Toronto “Beyond Health Care” conference, leading to WHO’s first pilot project in Lisbon in 1986. The WHO continues to support regional healthy city networks, emphasizing digital transformation and data-driven health management in the digital era. Objective: This study explored the capabilities of digital healthy cities within the framework of digital transformation, focusing on member countries of the Asian Forum of Healthy Cities. It examined the cities’ preparedness and policy needs for transitioning to digital health. Methods: A cross-sectional survey was conducted of 9 countries—Australia, Cambodia, China, Japan, South Korea, Malaysia, Mongolia, the Philippines, and Vietnam—from August 1 to September 21, 2023. The 6-section SPIRIT (setting approach and sustainability; political commitment, policy, and community participation; information and innovation; resources and research; infrastructure and intersectoral; and training) checklist was modified to assess healthy cities’ digital capabilities. With input from 3 healthy city experts, the checklist was revised for digital capabilities, renaming “healthy city” to “digital healthy city.” The revised tool comprises 8 sections with 33 items. The survey leveraged ChatGPT (version 4.0; OpenAI, Microsoft), accessed via Python (Python Software Foundation) application programming interface. The openai library was installed, and an application programming interface key was entered to use ChatGPT (version 4.0). The “GPT-4 Turbo” model command was applied. A qualitative analysis of the collected data was conducted by 5 healthy city experts through group deep-discussions. Results: The results indicate that these countries should establish networks and committees for sustainable digital healthy cities. Cambodia showed the lowest access to electricity (70%) and significant digital infrastructure disparities. Efforts to sustain digital health initiatives varied, with countries such as Korea focusing on telemedicine, while China aimed to build a comprehensive digital health database, highlighting the need for tailored strategies in promoting digital healthy cities. Life expectancy was the highest in the Republic of Korea and Japan (both 84 y). Access to electricity was the lowest in Cambodia (70%) with the remaining countries having had 95% or higher access. The internet use rate was the highest in Malaysia (97.4%), followed by the Republic of Korea (97.2%), Australia (96.2%), and Japan (82.9%). Conclusions: This study highlights the importance of big data-driven policies and personal information protection systems. Collaborative efforts across sectors for effective implementation of digital healthy cities. The findings suggest that the effectiveness of digital healthy cities is diminished without adequate digital literacy among managers and users, suggesting the need for policies to improve digital literacy. © Hocheol Lee.","JMIR Publications Inc.","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85217257121"
"Andrew K.; Montalbano M.J.","Andrew, Kevlian (58528786000); Montalbano, Michael J. (57190378733)","58528786000; 57190378733","Through a Glass Darkly: Perceptions of Ethnoracial Identity in Artificial Intelligence Generated Medical Vignettes and Images","2025","Medical Science Educator","","","","","","","0","10.1007/s40670-025-02332-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219025934&doi=10.1007%2fs40670-025-02332-9&partnerID=40&md5=ec7b2b7f4dfb2fe0f61c21339ed57526","Purpose: Medical education professionals expect artificial intelligence (AI) systems to be an efficient faculty resource for content creation. However, prior findings suggest that machine learning algorithms may exacerbate negative stereotypes and undermine efforts for diversity, equity, and inclusivity. This investigation explores the potential of OpenAI’s ChatGPT (OCG) and Microsoft’s Bing A.I. Image Creator (MBIC) to perpetuate ethnoracial stereotypes in medical cases. Materials and Methods: A series of medically relevant vignettes and visual representatives were requested from ChatGPT and MBIC for five medical conditions traditionally associated with certain ethnoracial groups: sickle cell anemia, cystic fibrosis, Tay-Sachs disease, beta-thalassemia, and aldehyde dehydrogenase deficiency. Initial prompting, self-prompting, and prompt engineering were iteratively performed to ascertain the extent to which AI outputs for generated vignettes and imagery were mutable or fixed. Results: The ethnoracial identity in the vignettes of the clinical conditions adhered more closely than described in epidemiologic studies. Following prompt engineering and self-prompting, an increase in diversity was seen. On initial prompting, the most common ethnoracial identity depicted was Caucasian. Secondary prompting resulted in less diversity with higher conformation to the traditionally expected ethnoracial identity. Conclusion: The prevalence of dataset bias and AI’s user-dependent learning abilities underscore the importance of human stewardship. The increasing use of AI in generating medical education content, like MCQs, demands vigilant use of such tools to combat the reinforcement of the race-based stereotypes in medicine. © The Author(s) 2025.","Springer","","Article","Article in press","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85219025934"
"Kavak E.E.; Dilli İ.","Kavak, Engin Eren (56178529500); Dilli, İsmail (59557461800)","56178529500; 59557461800","Progression-Free Survival Prediction Performance of ChatGPT: Analysis With Real Life Data in Early and Locally Advanced Prostate Cancer","2025","Prostate","","","","","","","0","10.1002/pros.24871","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217777603&doi=10.1002%2fpros.24871&partnerID=40&md5=1e55279df4396b150c5337d71939d32a","Objective: To evaluate the progression-free survival (PFS) time in patients with early-stage and locally advanced prostate cancer and to compare the estimates provided by ChatGPT with actual survival data. Methods: A retrospective analysis was conducted on patients diagnosed with early-stage/locally advanced prostate cancer. Each patient's estimated PFS times were calculated using an artificial intelligence chatbot. These estimates were generated by considering several factors, including the patient's clinical characteristics, tumor stage, treatment modalities, and biochemical parameters. A statistical comparison was conducted between the predicted PFS and actual PFS times. Results: The AI chatbot tended to overestimate the overall PFS times. A statistically significant discrepancy was observed between the predicted and actual survival times (p < 0.05). A discrepancy of 9.19 months was observed between the PFS predictions made by ChatGPT and the actual PFS. The bias value was 48.57, yet this discrepancy had a negligible impact on clinical practice (Cohen's d = 0.189). Discussion: Artificial intelligence-based models have the potential to play an important role in the prediction of progression in cancers such as prostate cancer, where 5–10-year survival rates can reach 100%. However, this study's findings indicate that the AI model's predictions are not aligned with the actual clinical data. The reliability of the integration of artificial intelligence into clinical decision support systems can be enhanced through the undertaking of comprehensive future studies. Conclusion: The use of artificial intelligence in predictive modeling may prove an effective approach for forecasting the PFS of prostate cancer. It has the potential to supplant the nomograms that are currently in use. Nevertheless, further studies are required to substantiate the accuracy and reliability of these systems. © 2025 Wiley Periodicals LLC.","John Wiley and Sons Inc","","Article","Article in press","","Scopus","2-s2.0-85217777603"
"Sung M.L.; León C.; Reisman J.I.; Gordon K.S.; Kerns R.D.; Li W.; Liu W.; Mitra A.; Yu H.; Becker W.C.","Sung, Minhee L. (57670363600); León, Casey (57189325955); Reisman, Joel I. (23010141700); Gordon, Kirsha S. (23093805100); Kerns, Robert D. (57204832763); Li, Wenjun (55719039700); Liu, Weisong (57204355702); Mitra, Avijit (57223232889); Yu, Hong (35785447400); Becker, William C. (9238646900)","57670363600; 57189325955; 23010141700; 23093805100; 57204832763; 55719039700; 57204355702; 57223232889; 35785447400; 9238646900","Disparities in Receipt of Medications for Opioid Use Disorder Before and During the COVID-19 Pandemic in the US Veterans Health Administration","2025","Substance Use and Addiction Journal","","","","","","","0","10.1177/29767342241293334","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218257694&doi=10.1177%2f29767342241293334&partnerID=40&md5=87c3562082822865a568a7bd615ead41","Background: Populations disproportionately impacted by the opioid epidemic are less likely to receive medications for opioid use disorder (MOUD; OUD). The COVID-19 pandemic exacerbated these disparities. We performed an ecological survey of subpopulations to compare differences in MOUD receipt among Veterans with OUD before versus during the pandemic. Methods: Using 2 cross-sections of 2 time periods of national Veterans Health Administration electronic health record data, we calculated proportions of Veterans with any MOUD receipt by demographics, Elixhauser comorbidity index, and natural language processing (NLP)-derived substance use and social determinants of health in each time period. We evaluated differences in MOUD receipt before and during the pandemic by patient characteristics using Chi-square and Cohen’s h for effect size. Results: Among 62 195 patients with OUD before the pandemic, the proportion prescribed MOUD increased from 46.5% before to 47.5% (P =.0003) during the pandemic. Statistically significant increased receipt of MOUD was observed for patients who were ≥55 years, men, White, with Elixhauser comorbidity indices of 2 and ≥5, and with NLP-derived indicators of substance use. There was a decrease that did not achieve statistical significance in MOUD receipt from before to during the pandemic for patients who were women, Black, Latinx, and food insecure. Conclusions: The proportions of patients with OUD prescribed MOUD increased from before to during the pandemic. However, Veterans who were women, Black, Latinx, and food insecure did not experience these increases. These patients may benefit from interventions such as targeted outreach efforts to improve MOUD engagement to reduce OUD harms. © 2024 by AMERSA, Inc. (Association for Multidisciplinary Education and Research in Substance use and Addiction).","SAGE Publications Inc.","","Article","Article in press","","Scopus","2-s2.0-85218257694"
"Bouguettaya A.; Team V.; Stuart E.M.; Aboujaoude E.","Bouguettaya, Ayoub (57193404175); Team, Victoria (14039871900); Stuart, Elizabeth M. (59368512600); Aboujaoude, Elias (8213643800)","57193404175; 14039871900; 59368512600; 8213643800","AI-driven report-generation tools in mental healthcare: A review of commercial tools","2025","General Hospital Psychiatry","94","","","150","158","8","0","10.1016/j.genhosppsych.2025.02.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000502440&doi=10.1016%2fj.genhosppsych.2025.02.018&partnerID=40&md5=a4dca53ecc61771f3cc47a9266b4435f","Artificial intelligence (AI) systems are increasingly being integrated in clinical care, including for AI-powered note-writing. We aimed to develop and apply a scale for assessing mental health electronic health records (EHRs) that use large language models (LLMs) for note-writing, focusing on their features, security, and ethics. The assessment involved analyzing product information and directly querying vendors about their systems. On their websites, the majority of vendors provided comprehensive information on data protection, privacy measures, multi-platform availability, patient access features, software update history, and Meaningful Use compliance. Most products clearly indicated the LLM's capabilities in creating customized reports or functioning as a co-pilot. However, critical information was often absent, including details on LLM training methodologies, the specific LLM used, bias correction techniques, and methods for evaluating the evidence base. The lack of transparency regarding LLM specifics and bias mitigation strategies raises concerns about the ethical implementation and reliability of these systems in clinical practice. While LLM-enhanced EHRs show promise in alleviating the documentation burden for mental health professionals, there is a pressing need for greater transparency and standardization in reporting LLM-related information. We propose recommendations for the future development and implementation of these systems to ensure they meet the highest standards of security, ethics, and clinical care. © 2025 Elsevier Inc.","Elsevier Inc.","","Article","Final","","Scopus","2-s2.0-86000502440"
"Ellison I.E.; Oslock W.M.; Abdullah A.; Wood L.; Thirumalai M.; English N.; Jones B.A.; Hollis R.; Rubyan M.; Chu D.I.","Ellison, India E. (59500996700); Oslock, Wendelyn M. (57210151941); Abdullah, Abiha (59262869800); Wood, Lauren (55573453400); Thirumalai, Mohanraj (6506604241); English, Nathan (57192099450); Jones, Bayley A. (57202189247); Hollis, Robert (7102238724); Rubyan, Michael (57205180253); Chu, Daniel I. (57204762241)","59500996700; 57210151941; 59262869800; 55573453400; 6506604241; 57192099450; 57202189247; 7102238724; 57205180253; 57204762241","De novo generation of colorectal patient educational materials using large language models: Prompt engineering key to improved readability","2025","Surgery (United States)","180","","109024","","","","0","10.1016/j.surg.2024.109024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214096430&doi=10.1016%2fj.surg.2024.109024&partnerID=40&md5=ed941a5fa681c92e81a5b6735d79639c","Background: Improving patient education has been shown to improve clinical outcomes and reduce disparities, though such efforts can be labor intensive. Large language models may serve as an accessible method to improve patient educational material. The aim of this study was to compare readability between existing educational materials and those generated by large language models. Methods: Baseline colorectal surgery educational materials were gathered from a large academic institution (n = 52). Three prompts were entered into Perplexity and ChatGPT 3.5 for each topic: a Basic prompt that simply requested patient educational information the topic, an Iterative prompt that repeated instruction asking for the information to be more health literate, and a Metric-based prompt that requested a sixth-grade reading level, short sentences, and short words. Flesch-Kincaid Grade Level or Grade Level, Flesch-Kincaid Reading Ease or Ease, and Modified Grade Level scores were calculated for all materials, and unpaired t tests were used to compare mean scores between baseline and documents generated by artificial intelligence platforms. Results: Overall existing materials were longer than materials generated by the large language models across categories and prompts: 863–956 words vs 170–265 (ChatGPT) and 220–313 (Perplexity), all P < .01. Baseline materials did not meet sixth-grade readability guidelines based on grade level (Grade Level 7.0–9.8 and Modified Grade Level 9.6–11.5) or ease of readability (Ease 53.1–65.0). Readability of materials generated by a large language model varied by prompt and platform. Overall, ChatGPT materials were more readable than baseline materials with the Metric-based prompt: Grade Level 5.2 vs 8.1, Modified Grade Level 7.3 vs 10.3, and Ease 70.5 vs 60.4, all P < .01. In contrast, Perplexity-generated materials were significantly less readable except for those generated with the Metric-based prompt, which did not statistically differ. Conclusion: Both existing materials and the majority of educational materials created by large language models did not meet readability recommendations. The exception to this was with ChatGPT materials generated with a Metric-based prompt that consistently improved readability scores from baseline and met recommendations in terms of the average Grade Level score. The variability in performance highlights the importance of the prompt used with large language models. © 2024 Elsevier Inc.","Elsevier Inc.","","Article","Final","","Scopus","2-s2.0-85214096430"
"Shi L.; Zhao T.; Shi S.; Tan T.; Regmi A.; Cai Y.","Shi, Lili (36705257000); Zhao, Tong (57960899000); Shi, Shimiao (58768791400); Tan, Tianyu (57192296573); Regmi, Aksara (58540256800); Cai, Yuyang (36661968700)","36705257000; 57960899000; 58768791400; 57192296573; 58540256800; 36661968700","Top health service concerns: a data mining study of the Shanghai health hotline","2025","Frontiers in Digital Health","7","","1462167","","","","0","10.3389/fdgth.2025.1462167","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000307283&doi=10.3389%2ffdgth.2025.1462167&partnerID=40&md5=74d8972b50ee55b40d0d1ae55a2d9d80","Objective: Our study aims to explore the health service issues of public concern through analyzing the basic characteristics of callers and information from the health hotline in Shanghai. The findings of this study will provide a reference to relevant government departments and assist the government in optimizing the allocation of health resources. Methods: Our research utilized 16,962 original work orders from the 12,320 health hotline, collected since 2015. We applied natural language processing (NLP) to analyze the content of these work orders, facilitating effective text mining and information extraction. Initially, we performed data cleaning to remove irrelevant information and protect caller privacy by anonymizing personal details. This cleaned data was then organized into a structured database for further analysis. Using text mining, we examined various aspects of the calls, including duration, purpose, and topics discussed, to identify patterns and themes that emerged. Results: The calls were categorized into four main groups: complaints, suggestions, inquiries, and requests for assistance. Complaints were the most frequent category, totaling 8,669 (51.11%), followed by help-seeking at 3,335 (19.66%), consultations at 2,727 (16.08%), and comments and suggestions at 1,484 (8.75%). The analysis revealed that men made 6,689 (56.88%), surpassing the 5,071 (43.12%) from women. Additionally, calls from parents numbered 2,126 (56.84%), slightly exceeding the 1,614 (43.16%) from children. The top 10 health service concerns identified in Shanghai included medical staff attitudes, medications, fees, registration, family planning, medical disputes, ambulance services, environmental health, illegal medical practices, and immunization. Conclusions: This study not only identifies critical issues within the Shanghai health service system but also offers actionable insights to inform targeted policy interventions. The high volume of complaints regarding service attitudes and medical expenses underscores the need for stronger policies to improve patient-provider communication and ensure transparency and fairness in healthcare costs. Additionally, the data reveals considerable public concern about the availability and quality of medical services, suggesting that existing policies on resource allocation and service delivery may not adequately meet population needs. The methodologies employed here can be applied to other urban health contexts, providing a valuable framework for improving public health strategies globally. 2025 Shi, Zhao, Shi, Tan, Regmi and Cai.","Frontiers Media SA","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-86000307283"
"Mori T.; Watanabe T.; Kosugi S.","Mori, Takuya (57199154154); Watanabe, Takuya (57976332400); Kosugi, Shinji (57223832265)","57199154154; 57976332400; 57223832265","Exploring ethical considerations in medical research: Harnessing pre-generated transformers for AI-powered ethics discussions","2025","PLoS ONE","20","2 February","e0311148","","","","0","10.1371/journal.pone.0311148","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216941043&doi=10.1371%2fjournal.pone.0311148&partnerID=40&md5=fd57134aefc44fe36c7e1f2c20350941","Introduction In medical research involving human subjects, ethical review is essential to protect individuals. However, concerns have been raised about variations in ethical review opinions and a decline in review quality. Adequately protecting human subjects requires multifaceted opinions from ethics committee members. Despite the need to increase the number of committee members, resources are limited. To address these challenges, we explored the use of a generative pre- learning transformer, an interactive artificial intelligence (AI) tool, to discuss ethical issues in medical research. Methods The generation AI used in the research used ChatGPT3.5, which has learned ethical guidelines from various countries worldwide. We requested the generative AI to provide insights on ethical considerations for virtual research involving individuals. The obtained answers were documented and verified by experts. Results The AI successfully highlighted considerations for informed consent regarding individuals with dementia and mental illness, as well as concerns about invasiveness in research. It also raised points about potential side effects of off-label drug use. However, it could not offer specific measures for psychological considerations or broader ethical issues, providing limited ethical insights. This limitation may be attributed to biased opinions resulting from machine learning optimization, preventing comprehensive identification of certain ethical issues. Conclusion Although the validity of ethical opinions generated by the generative AI requires further examination, our findings suggest that this technology could be employed to prompt reviews and re-evaluate ethical concerns arising in research. © 2025 Mori et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","Public Library of Science","39899559","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85216941043"
"Tornimbene B.; Leiva Rioja Z.B.; Brownstein J.; Dunn A.; Faye S.; Kong J.; Malou N.; Nordon C.; Rader B.; Morgan O.","Tornimbene, Barbara (56010212500); Leiva Rioja, Zoila Beatriz (58427221400); Brownstein, John (8872411400); Dunn, Adam (59681437500); Faye, Sylvain (26643892400); Kong, Jude (56305065700); Malou, Nada (38862365800); Nordon, Clara (57215002238); Rader, Benjamin (57219295170); Morgan, Oliver (8265576400)","56010212500; 58427221400; 8872411400; 59681437500; 26643892400; 56305065700; 38862365800; 57215002238; 57219295170; 8265576400","Harnessing the power of artificial intelligence for disease-surveillance purposes","2025","BMC Proceedings","19","Suppl 4","7","","","","0","10.1186/s12919-025-00320-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000757318&doi=10.1186%2fs12919-025-00320-w&partnerID=40&md5=1bb7cd7d468d68301f7a073724f8e9ae","The COVID-19 pandemic accelerated the development of AI-driven tools to improve public health surveillance and outbreak management. While AI programs have shown promise in disease surveillance, they also present issues such as data privacy, prejudice, and human-AI interactions. This sixth session of the of the WHO Pandemic and Epidemic Intelligence Innovation Forum examines the use of Artificial Intelligence (AI) in public health by collecting the experience of key global health organizations, such the Boston Children's Hospital, the Global South AI for Pandemic & Epidemic Preparedness & Response (AI4PEP) network, Medicines Sans Frontières (MSF), and the University of Sydney. AI's utility in clinical care, particularly in diagnostics, medication discovery, and data processing, has resulted in improvements that may also benefit public health surveillance. However, the use of AI in global health necessitates careful consideration of ethical issues, particularly those involving data use and algorithmic bias. As AI advances, particularly with large language models, public health officials must develop governance frameworks that stress openness, accountability, and fairness. These systems should address worldwide differences in data access and ensure that AI technologies are tailored to specific local needs. Ultimately, AI's ability to improve healthcare efficiency and equity is dependent on multidisciplinary collaboration, community involvement, and inclusive AI designs in ensuring equitable healthcare outcomes to fit the unique demands of global communities. © The Author(s) 2025.","BioMed Central Ltd","","Article","Final","","Scopus","2-s2.0-86000757318"
"Busigó Torres R.; Restrepo M.; Stern B.Z.; Yahuaca B.I.; Buerba R.A.; Garca I.A.; Hernandez V.H.; Navarro R.A.","Busigó Torres, Rodnell (59005193000); Restrepo, Mariana (58825602700); Stern, Brocha Z. (57202367512); Yahuaca, B. Israel (57193847998); Buerba, Rafael A. (38361053100); Garca, Ivan A. (55636320790); Hernandez, Victor H. (56438984700); Navarro, Ronald A. (7202827341)","59005193000; 58825602700; 57202367512; 57193847998; 38361053100; 55636320790; 56438984700; 7202827341","Artificial Intelligence Shows Limited Success in Improving Readability Levels of Spanish-language Orthopaedic Patient Education Materials","2025","Clinical Orthopaedics and Related Research","","","10.1097/CORR.0000000000003413","","","","0","10.1097/CORR.0000000000003413","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217836358&doi=10.1097%2fCORR.0000000000003413&partnerID=40&md5=7cbbae4f12769c5515a6141ab8c9c556","Background The more than 41 million people in the United States who speak Spanish represent one of the fastest-growing US populations. Non-English-speaking patients often face poorer health outcomes because of language barriers that hinder patient education. Orthopaedic education materials have limited availability in Spanish and may be difficult for some patients to read. The American Academy of Orthopaedic Surgeons (AAOS) has translated education materials into Spanish, but their readability levels remain unknown. Additionally, although artificial intelligence (AI) dialogue platforms have been shown to improve readability in English, no studies have specifically evaluated their effectiveness in non-English languages. Questions/purposes (1) What is the readability of AAOS Spanish-language education materials? (2) Can an AI dialogue platform improve the readability of Spanish-language education materials while maintaining their accuracy and usefulness?MethodsAfter excluding COVID-19 articles and inaccessible websites, Spanish-language education materials were extracted from the AAOS OrthoInfo website, and their Fernández-Huerta and Spanish Orthographic Length (SOL) readability grade levels were calculated. Fernández-Huerta focuses on syntactic complexity (sentence and syllable structure) and SOL assesses lexical complexity (word length and frequency). For both, the higher the grade level, the harder it is to read. Education materials with a reading level above the sixth-grade level were inputted into the ChatGPT-4 AI platform to be adapted to a fifth-grade level. Readability metrics of the adaptations were reassessed and compared with the original versions. Secondarily, one of four Spanish-speaking orthopaedic surgeons evaluated each AI-adapted education material for accuracy and usefulness compared with the original version. We used a single review per material, trusting the orthopaedic surgeon's expertise to minimize discrepancies. We included a total of 77 of 82 education materials covering topics like diseases and conditions, treatment, and recovery and staying healthy. Results Before AI adaptations, none of the 77 education materials met the recommended reading level of sixth grade or below according to both readability formulas. The original education materials were written at a seventh- to eighth-grade reading level in 32% of cases (25 of 77). In comparison, after a single attempt at simplification, AI-adapted materials achieved this reading level in 53% of cases (41 of 77; p < 0.001). Only 23% (18) and 16% (12) of the AI adaptations were written at or below the recommended sixth-grade level per the Fernández-Huerta and SOL grade levels, respectively. Of the AI adaptations, 52% (40) were rated as accurate and 56% (43) were rated as useful for patient education by the evaluating orthopaedic surgeons. AI adaptations that were classified as accurate or useful had a higher median (IQR) word count than those that were inaccurate (accurate 255 [216 to 331] versus inaccurate 236 [209 to 256]; p = 0.04) or not useful (useful 257 [216 to 337] versus not useful 233 [209 to 251]; p = 0.01). Conclusion Ongoing attention is needed to improve the readability of Spanish education materials to reduce health disparities. ChatGPT-4 has limited success in improving readability without compromising accuracy and usefulness. We urge AAOS to enhance the readability of these materials and recommend physicians use them as supplemental resources while prioritizing direct patient education for Spanish-speaking individuals. Further research is needed to develop readable and culturally appropriate education materials for non-English-speaking patients that incorporate direct patient feedback. Clinical Relevance This study shows that Spanish-language orthopaedic materials often exceed recommended readability levels, limiting their effectiveness and worsening health disparities. While AI tools like ChatGPT-4 improve readability, they may fall short in accuracy and usefulness. This underscores the need for clearer, culturally appropriate materials and the importance of physicians providing direct education.  © 2025 by the Association of Bone and Joint Surgeons.","Wolters Kluwer Health Inc","","Article","Article in press","","Scopus","2-s2.0-85217836358"
"Placidi G.; Cinque L.; Foresti G.L.; Galassi F.; Mignosi F.; Nappi M.; Polsinelli M.","Placidi, Giuseppe (7006022089); Cinque, Luigi (56213232800); Foresti, Gian Luca (7006427233); Galassi, Francesca (57212874582); Mignosi, Filippo (6701576008); Nappi, Michele (6603906020); Polsinelli, Matteo (57196245149)","7006022089; 56213232800; 7006427233; 57212874582; 6701576008; 6603906020; 57196245149","A Context-Dependent CNN-Based Framework for Multiple Sclerosis Segmentation in MRI","2025","International Journal of Neural Systems","35","3","2550006","","","","0","10.1142/S0129065725500066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214362274&doi=10.1142%2fS0129065725500066&partnerID=40&md5=fb7563b580efe999b34160883f151962","Despite several automated strategies for identi¯cation/segmentation of Multiple Sclerosis (MS) lesions in Magnetic Resonance Imaging (MRI) being developed, they consistently fall short when compared to the performance of human experts. This emphasizes the unique skills and expertise of human professionals in dealing with the uncertainty resulting from the vagueness and variability of MS, the lack of speci¯city of MRI concerning MS, and the inherent instabilities of MRI. Physicians manage this uncertainty in part by relying on their radiological, clinical, and anatomical experience. We have developed an automated framework for identifying and segmenting MS lesions in MRI scans by introducing a novel approach to replicating human diagnosis, a signi¯cant advancement in the ¯eld. This framework has the potential to revolutionize the way MS lesions are identi¯ed and segmented, being based on three main concepts: (1) Modeling the uncertainty; (2) Use of separately trained Convolutional Neural Networks (CNNs) optimized for detecting lesions, also considering their context in the brain, and to ensure spatial continuity; (3) Implementing an ensemble classi¯er to combine information from these CNNs. The proposed framework has been trained, validated, and tested on a single MRI modality, the FLuid-Attenuated Inversion Recovery (FLAIR) of the MSSEG benchmark public data set containing annotated data from seven expert radiologists and one ground truth. The comparison with the ground truth and each of the seven human raters demonstrates that it operates similarly to human raters. At the same time, the proposed model demonstrates more stability, e®ectiveness and robustness to biases than any other state-of-the-art model though using just the FLAIR modality. © The Author(s)","World Scientific","39962837","Article","Final","","Scopus","2-s2.0-85214362274"
"Lee J.K.; Chung T.-M.","Lee, Jun Koo (58874950700); Chung, Tai-Myoung (59269612900)","58874950700; 59269612900","Detecting Bias in Large Language Models: Fine-Tuned KcBERT","2025","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) ","14893 LNCS","","","76","90","14","0","10.1007/978-981-97-8705-0_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219166785&doi=10.1007%2f978-981-97-8705-0_6&partnerID=40&md5=66ff0ad19418ce2838eef7fcefd1f11f","The rapid advancement of large language models (LLMs) has brought their natural language processing capabilities to a level comparable to human performance. These models are now extensively used in various societal domains, including education and healthcare. However, despite their versatility, LLMs can produce subjective and normative language, potentially leading to discriminatory outcomes among social groups, particularly through the dissemination of offensive language online. In this paper, we define such phenomena as societal bias and assess ethnic, gender, and racial biases in a model fine-tuned with Korean comments using the Bidirectional Encoder Representations from Transformers (KcBERT) and Korean Language Open Data (KOLD) through template-based Masked Language Modeling (MLM). To quantitatively evaluate these biases, we use the Language Pattern Bias Score (LPBS) and Contextual Bias Score (CBS) metrics. Our results show that, compared to KcBERT, the fine-tuned model exhibits a reduction in ethnic bias but significant alterations in gender and racial biases. To mitigate these societal biases, we propose two methods: First, a data balancing approach during the pre-training phase that adjusts the uniformity of data by aligning the distribution of specific word occurrences and converting surrounding harmful words into non-harmful alternatives. Second, during the in-training phase, we apply Debiasing Regularization by adjusting dropout and regularization parameters, resulting in decreased training loss. Our work highlights the existence of societal biases in Korean language models and demonstrates the importance of language-dependent characteristics in these models. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.","Springer Science and Business Media Deutschland GmbH","","Conference paper","Final","","Scopus","2-s2.0-85219166785"
"Zohny H.; Allen J.W.; Wilkinson D.; Savulescu J.","Zohny, Hazem (56600819000); Allen, Jemima Winifred (58520288500); Wilkinson, Dominic (59674311200); Savulescu, Julian (59674888400)","56600819000; 58520288500; 59674311200; 59674888400","Which AI doctor would you like to see? Emulating healthcare provider–patient communication models with GPT-4: proof-of-concept and ethical exploration","2025","Journal of Medical Ethics","","","","","","","0","10.1136/jme-2024-110256","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000312342&doi=10.1136%2fjme-2024-110256&partnerID=40&md5=421274fe18c8c52ef64f0fe1fa8d9593","Large language models (LLMs) have demonstrated potential in enhancing various aspects of healthcare, including health provider–patient communication. However, some have raised the concern that such communication may adopt implicit communication norms that deviate from what patients want or need from talking with their healthcare provider. This paper explores the possibility of using LLMs to enable patients to choose their preferred communication style when discussing their medical cases. By providing a proof-of-concept demonstration using ChatGPT-4, we suggest LLMs can emulate different healthcare provider–patient communication approaches (building on Emanuel and Emanuel’s four models: paternalistic, informative, interpretive and deliberative). This allows patients to engage in a communication style that aligns with their individual needs and preferences. We also highlight potential risks associated with using LLMs in healthcare communication, such as reinforcing patients’ biases and the persuasive capabilities of LLMs that may lead to unintended manipulation. © Author(s) (or their employer(s)) 2025.","BMJ Publishing Group","","Article","Article in press","","Scopus","2-s2.0-86000312342"
"McCaffrey P.; Jackups R.; Seheult J.; Zaydman M.A.; Balis U.; Thaker H.M.; Rashidi H.; Gullapalli R.R.","McCaffrey, Peter (35491387300); Jackups, Ronald (9735074900); Seheult, Jansen (55820665300); Zaydman, Mark A. (36095096400); Balis, Ulysses (6603836901); Thaker, Harshwardhan M. (6603730674); Rashidi, Hooman (6506820780); Gullapalli, Rama R. (15765243900)","35491387300; 9735074900; 55820665300; 36095096400; 6603836901; 6603730674; 6506820780; 15765243900","Evaluating Use of Generative Artificial Intelligence in Clinical Pathology Practice Opportunities and the Way Forward","2025","Archives of Pathology and Laboratory Medicine","149","2","","130","141","11","0","10.5858/arpa.2024-0208-RA","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216494599&doi=10.5858%2farpa.2024-0208-RA&partnerID=40&md5=e6ea2b2bc3dab21b3aa35801f74b439e","• Context.—Generative artificial intelligence (GAI) technologies are likely to dramatically impact health care workflows in clinical pathology (CP). Applications in CP include education, data mining, decision support, result summaries, and patient trend assessments. Objective.—To review use cases of GAI in CP, with a particular focus on large language models. Specific examples are provided for the applications of GAI in the subspecialties of clinical chemistry, microbiology, hematopathology, and molecular diagnostics. Additionally, the review addresses potential pitfalls of GAI paradigms. Data Sources.—Current literature on GAI in health care was reviewed broadly. The use case scenarios for each CP subspecialty review common data sources generated in each subspecialty. The potential for utilization of CP data in the GAI context was subsequently assessed, focusing on issues such as future reporting paradigms, impact on quality metrics, and potential for translational research activities. Conclusions.—GAI is a powerful tool with the potential to revolutionize health care for patients and practitioners alike. However, GAI must be implemented with much caution considering various shortcomings of the technology such as biases, hallucinations, practical challenges of implementing GAI in existing CP workflows, and end-user acceptance. Human-in-the-loop models of GAI implementation have the potential to revolutionize CP by delivering deeper, meaningful insights into patient outcomes both at an individual and a population level. © 2025 College of American Pathologists. All rights reserved.","College of American Pathologists","39384182","Article","Final","","Scopus","2-s2.0-85216494599"
"Bejan C.A.; Reed A.M.; Mikula M.; Zhang S.; Xu Y.; Fabbri D.; Embí P.J.; Hsi R.S.","Bejan, Cosmin A. (13007466300); Reed, Amy M. (58260932200); Mikula, Matthew (57212563508); Zhang, Siwei (57219622713); Xu, Yaomin (57213544473); Fabbri, Daniel (51663119900); Embí, Peter J. (14061844500); Hsi, Ryan S. (55255647000)","13007466300; 58260932200; 57212563508; 57219622713; 57213544473; 51663119900; 14061844500; 55255647000","Large language models improve the identification of emergency department visits for symptomatic kidney stones","2025","Scientific Reports","15","1","3503","","","","0","10.1038/s41598-025-86632-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217272463&doi=10.1038%2fs41598-025-86632-5&partnerID=40&md5=3128f83fa59ee3a0124f1afb6afb9a89","Recent advancements of large language models (LLMs) like generative pre-trained transformer 4 (GPT-4) have generated significant interest among the scientific community. Yet, the potential of these models to be utilized in clinical settings remains largely unexplored. In this study, we investigated the abilities of multiple LLMs and traditional machine learning models to analyze emergency department (ED) reports and determine if the corresponding visits were due to symptomatic kidney stones. Leveraging a dataset of manually annotated ED reports, we developed strategies to enhance LLMs including prompt optimization, zero- and few-shot prompting, fine-tuning, and prompt augmentation. Further, we implemented fairness assessment and bias mitigation methods to investigate the potential disparities by LLMs with respect to race and gender. A clinical expert manually assessed the explanations generated by GPT-4 for its predictions to determine if they were sound, factually correct, unrelated to the input prompt, or potentially harmful. The best results were achieved by GPT-4 (macro-F1 = 0.833, 95% confidence interval [CI] 0.826–0.841) and GPT-3.5 (macro-F1 = 0.796, 95% CI 0.796–0.796). Ablation studies revealed that the initial pre-trained GPT-3.5 model benefits from fine-tuning. Adding demographic information and prior disease history to the prompts allows LLMs to make better decisions. Bias assessment found that GPT-4 exhibited no racial or gender disparities, in contrast to GPT-3.5, which failed to effectively model racial diversity. © The Author(s) 2025.","Nature Research","39875475","Article","Final","","Scopus","2-s2.0-85217272463"
"Baeta T.; Rocha A.L.L.; Oliveira J.A.; Couto Da Silva A.P.; Reis Z.S.N.","Baeta, Thais (57219335062); Rocha, Ana Luiza Lunardi (57195466502); Oliveira, Juliana Almeida (59009700300); Couto Da Silva, Ana Paula (59157724100); Reis, Zilma Silveira Nogueira (23029078900)","57219335062; 57195466502; 59009700300; 59157724100; 23029078900","Accuracy of machine learning and traditional statistical models in the prediction of postpartum haemorrhage: A systematic review","2025","BMJ Open","15","3","e094455","","","","0","10.1136/bmjopen-2024-094455","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000451448&doi=10.1136%2fbmjopen-2024-094455&partnerID=40&md5=0371c0d6d89cd6f802a123a3c7bf8a12","Objectives To evaluate whether postpartum haemorrhage (PPH) can be predicted using both machine learning (ML) and traditional statistical models. Design Diagnostic systematic review and meta-analysis of observational and clinical studies, prospectively registered on PROSPERO, performed accordingly to the Preferred Reporting Items for Systematic Reviews and Meta-analysis and Prediction model risk of bias assessment tool for studies developing, validating or updating prediction models, with the use of an independent analysis by a large language model (GPT-4 Open AI). Data sources MEDLINE/PubMed, LILACS-BVS, Cochrane Library, Scopus-Elsevier, Embase-Elsevier and Web of Science. Eligibility criteria for selected studies The literature search was conducted on 4 January 2024 and included observational studies and clinical trials published in the past 10 years that assessed early PPH and PPH prediction and that applied accuracy metrics for outcomes evaluation. We excluded studies that did not define PPH or had exclusive PPH subgroups evaluation. Primary and secondary outcome measures The primary outcome is the accuracy of PPH prediction using both ML and conventional statistical models. A secondary outcome is to describe the strongest risk factors of PPH identified by ML and traditional statistical models. Results Of 551 citations screened, 35 studies were eligible for inclusion. The synthesis gathered 383 648 patients in 24 studies conducted with conventional statistics (CS), 9 studies using ML models and 2 studies using both methods. Multivariate regression was a preferred modelling approach to predict PPH in CS studies, while ML approaches used multiple models and a myriad of features. ML comparison to CS was only performed in two studies, and ML models demonstrated a 95% higher likelihood of PPH prediction compared with CS when applied to the same dataset (OR 1.95, 95% CI 1.88 to 2.01, p<0.001). The I² had a value of 54%, p=0.14, indicating moderate heterogeneity between the studies. Conclusions ML models are promising for predicting PPH. Nevertheless, they often require a large number of predictors, which may limit their applicability or necessitate automation through digital systems. This poses challenges in resource-scarce settings where the majority of PPH complications occur. PROSPERO registration number CRD42024521059.  © Author(s) (or their employer(s)) 2025.","BMJ Publishing Group","40032385","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-86000451448"
"Dubbala K.; Prizak R.; Metzler I.; Rubeis G.","Dubbala, Keerthi (59681779500); Prizak, Roshan (37023349500); Metzler, Ingrid (36497158100); Rubeis, Giovanni (57190741534)","59681779500; 37023349500; 36497158100; 57190741534","Exploring Heart Disease–Related mHealth Apps in India: Systematic Search in App Stores and Metadata Analysis","2025","Journal of Medical Internet Research","27","","e53823","","","","0","10.2196/53823","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000462295&doi=10.2196%2f53823&partnerID=40&md5=03c90422396daddc566b44f4e4b3a1c4","Background: Smartphone mobile health (mHealth) apps have the potential to enhance access to health care services and address health care disparities, especially in low-resource settings. However, when developed without attention to equity and inclusivity, mHealth apps can also exacerbate health disparities. Understanding and creating solutions for the disparities caused by mHealth apps is crucial for achieving health equity. There is a noticeable gap in research that comprehensively assesses the entire spectrum of existing health apps and extensively explores apps for specific health priorities from a health care and public health perspective. In this context, with its vast and diverse population, India presents a unique context for studying the landscape of mHealth apps. Objective: This study aimed to create a comprehensive dataset of mHealth apps available in India with an initial focus on heart disease (HD)–related apps. Methods: We collected individual app data from apps in the “medical” and “health and fitness” categories from the Google Play Store and the Apple App Store in December 2022 and July 2023, respectively. Using natural language processing techniques, we selected HD apps, performed statistical analysis, and applied latent Dirichlet allocation for clustering and topic modeling to categorize the resulting HD apps. Results: We collected 118,555 health apps from the Apple App Store and 108,945 health apps from the Google Play Store. Within these datasets, we found that approximately 1.7% (1990/118,555) of apps on the Apple App Store and 0.5% (548/108,945) on the Google Play Store included support for Indian languages. Using monograms and bigrams related to HD, we identified 1681 HD apps from the Apple App Store and 588 HD apps from the Google Play Store. HD apps make up only a small fraction of the total number of health apps available in India. About 90% (1496/1681 on Apple App Store and 548/588 on Google Play Store) of the HD apps were free of cost. However, more than 70% (1329/1681, 79.1% on Apple App Store and 423/588, 71.9% on Google Play Store) of HD apps had no reviews and rating-scores, indicating low overall use. Conclusions: Our study proposed a robust method for collecting and analyzing metadata from a wide array of mHealth apps available in India through the Apple App Store and Google Play Store. We revealed the limited representation of India’s linguistic diversity within the health and medical app landscape, evident from the negligible presence of Indian-language apps. We observed a scarcity of mHealth apps dedicated to HD, along with a lower level of user engagement, as indicated by reviews and app ratings. While most HD apps are financially accessible, uptake remains a challenge. Further research should focus on app quality assessment and factors influencing user adoption. ©Keerthi Dubbala, Roshan Prizak, Ingrid Metzler, Giovanni Rubeis.","JMIR Publications Inc.","40063078","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-86000462295"
"Xie J.; Zhang Z.; Zeng S.; Hilliard J.; An G.; Tang X.; Jiang L.; Yu Y.; Wan X.; Xu D.","Xie, Jiacheng (58204567200); Zhang, Ziyang (58392141500); Zeng, Shuai (57188946534); Hilliard, Joel (58889824600); An, Guanghui (15768691500); Tang, Xiaoting (58890007300); Jiang, Lei (59041028800); Yu, Yang (59090288800); Wan, Xiufeng (9335365800); Xu, Dong (7404074295)","58204567200; 58392141500; 57188946534; 58889824600; 15768691500; 58890007300; 59041028800; 59090288800; 9335365800; 7404074295","Leveraging Large Language Models for Infectious Disease Surveillance—Using a Web Service for Monitoring COVID-19 Patterns From Self-Reporting Tweets: Content Analysis","2025","Journal of Medical Internet Research","27","","e63190","","","","0","10.2196/63190","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218728760&doi=10.2196%2f63190&partnerID=40&md5=45264307d66fcdc010ca67398d0574dc","Background: The emergence of new SARS-CoV-2 variants, the resulting reinfections, and post–COVID-19 condition continue to impact many people’s lives. Tracking websites like the one at Johns Hopkins University no longer report the daily confirmed cases, posing challenges to accurately determine the true extent of infections. Many COVID-19 cases with mild symptoms are self-assessed at home and reported on social media, which provides an opportunity to monitor and understand the progression and evolving trends of the disease. Objective: We aim to build a publicly available database of COVID-19–related tweets and extracted information about symptoms and recovery cycles from self-reported tweets. We have presented the results of our analysis of infection, reinfection, recovery, and long-term effects of COVID-19 on a visualization website that refreshes data on a weekly basis. Methods: We used Twitter (subsequently rebranded as X) to collect COVID-19–related data, from which 9 native English-speaking annotators annotated a training dataset of COVID-19–positive self-reporters. We then used large language models to identify positive self-reporters from other unannotated tweets. We used the Hibert transform to calculate the lead of the prediction curve ahead of the reported curve. Finally, we presented our findings on symptoms, recovery, reinfections, and long-term effects of COVID-19 on the Covlab website. Results: We collected 7.3 million tweets related to COVID-19 between January 1, 2020, and April 1, 2024, including 262,278 self-reported cases. The predicted number of infection cases by our model is 7.63 days ahead of the official report. In addition to common symptoms, we identified some symptoms that were not included in the list from the US Centers for Disease Control and Prevention, such as lethargy and hallucinations. Repeat infections were commonly occurring, with rates of second and third infections at 7.49% (19,644/262,278) and 1.37% (3593/262,278), respectively, whereas 0.45% (1180/262,278) also reported that they had been infected >5 times. We identified 723 individuals who shared detailed recovery experiences through tweets, indicating a substantially reduction in recovery time over the years. Specifically, the average recovery period decreased from around 30 days in 2020 to approximately 12 days in 2023. In addition, geographic information collected from confirmed individuals indicates that the temporal patterns of confirmed cases in states such as California and Texas closely mirror the overall trajectory observed across the United States. Conclusions: Although with some biases and limitations, self-reported tweet data serves as a valuable complement to clinical data, especially in the postpandemic era dominated by mild cases. Our web-based analytic platform can play a significant role in continuously tracking COVID-19, finding new uncommon symptoms, detecting and monitoring the manifestation of long-term effects, and providing necessary insights to the public and decision-makers. ©Jiacheng Xie, Ziyang Zhang, Shuai Zeng, Joel Hilliard, Guanghui An, Xiaoting Tang, Lei Jiang, Yang Yu, Xiufeng Wan, Dong Xu.","JMIR Publications Inc.","39977859","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85218728760"
"Topaz M.; Davoudi A.; Evans L.; Sridharan S.; Song J.; Chae S.; Barrón Y.; Hobensack M.; Scharp D.; Cato K.; Rossetti S.C.; Kapela P.; Xu Z.; Gupta P.; Zhang Z.; Mcdonald M.V.; Bowles K.H.","Topaz, Maxim (54790231000); Davoudi, Anahita (57216397259); Evans, Lauren (57208026323); Sridharan, Sridevi (55658421300); Song, Jiyoun (57211232468); Chae, Sena (57200543066); Barrón, Yolanda (57200973704); Hobensack, Mollie (57208080605); Scharp, Danielle (57818690400); Cato, Kenrick (35071035400); Rossetti, Sarah Collins (57022911000); Kapela, Piotr (59485595000); Xu, Zidu (57393186000); Gupta, Pallavi (59485192200); Zhang, Zhihong (59485456800); Mcdonald, Margaret V. (7403020077); Bowles, Kathryn H. (7004543385)","54790231000; 57216397259; 57208026323; 55658421300; 57211232468; 57200543066; 57200973704; 57208080605; 57818690400; 35071035400; 57022911000; 59485595000; 57393186000; 59485192200; 59485456800; 7403020077; 7004543385","Building a Time-Series Model to Predict Hospitalization Risks in Home Health Care: Insights Into Development, Accuracy, and Fairness","2025","Journal of the American Medical Directors Association","26","2","105417","","","","0","10.1016/j.jamda.2024.105417","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212946037&doi=10.1016%2fj.jamda.2024.105417&partnerID=40&md5=6b1cbc96231451a8a66d2ffa581c0962","Objectives: Home health care (HHC) serves more than 5 million older adults annually in the United States, aiming to prevent unnecessary hospitalizations and emergency department (ED) visits. Despite efforts, up to 25% of patients in HHC experience these adverse events. The underutilization of clinical notes, aggregated data approaches, and potential demographic biases have limited previous HHC risk prediction models. This study aimed to develop a time-series risk model to predict hospitalizations and ED visits in patients in HHC, examine model performance over various prediction windows, identify top predictive variables and map them to data standards, and assess model fairness across demographic subgroups. Setting and Participants: A total of 27,222 HHC episodes between 2015 and 2017. Methods: The study used health care process modeling of electronic health records, including clinical notes processed with natural language processing techniques and Medicare claims data. A Light Gradient Boosting Machine algorithm was used to develop the risk prediction model, with performance evaluated using 5-fold cross-validation. Model fairness was assessed across gender, race/ethnicity, and socioeconomic subgroups. Results: The model achieved high predictive performance, with an F1 score of 0.84 for a 5-day prediction window. Twenty top predictive variables were identified, including novel indicators such as the length of nurse-patient visits and visit frequency. Eighty-five percent of these variables mapped completely to the US Core Data for Interoperability standard. Fairness assessment revealed performance disparities across demographic and socioeconomic groups, with lower model effectiveness for more historically underserved populations. Conclusions and Implications: This study developed a robust time-series risk model for predicting adverse events in patients in HHC, incorporating diverse data types and demonstrating high predictive accuracy. The findings highlight the importance of considering established and novel risk factors in HHC. Importantly, the observed performance disparities across subgroups emphasize the need for fairness adjustments to ensure equitable risk prediction across all patient populations. © 2024 Post-Acute and Long-Term Care Medical Association","Elsevier Inc.","39689864","Article","Final","","Scopus","2-s2.0-85212946037"
"Rządeczka M.; Sterna A.; Stolińska J.; Kaczyńska P.; Moskalewicz M.","Rządeczka, Marcin (57204195978); Sterna, Anna (57190005593); Stolińska, Julia (59214965800); Kaczyńska, Paulina (57216337061); Moskalewicz, Marcin (37079567600)","57204195978; 57190005593; 59214965800; 57216337061; 37079567600","The Efficacy of Conversational AI in Rectifying the Theory-of-Mind and Autonomy Biases: Comparative Analysis","2025","JMIR Mental Health","12","","e64396","","","","0","10.2196/64396","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219118191&doi=10.2196%2f64396&partnerID=40&md5=1c5d5d2c2bae0431f4c99074d0ff58d2","Background: The increasing deployment of conversational artificial intelligence (AI) in mental health interventions necessitates an evaluation of their efficacy in rectifying cognitive biases and recognizing affect in human-AI interactions. These biases are particularly relevant in mental health contexts as they can exacerbate conditions such as depression and anxiety by reinforcing maladaptive thought patterns or unrealistic expectations in human-AI interactions. Objective: This study aimed to assess the effectiveness of therapeutic chatbots (Wysa and Youper) versus general-purpose language models (GPT-3.5, GPT-4, and Gemini Pro) in identifying and rectifying cognitive biases and recognizing affect in user interactions. Methods: This study used constructed case scenarios simulating typical user-bot interactions to examine how effectively chatbots address selected cognitive biases. The cognitive biases assessed included theory-of-mind biases (anthropomorphism, overtrust, and attribution) and autonomy biases (illusion of control, fundamental attribution error, and just-world hypothesis). Each chatbot response was evaluated based on accuracy, therapeutic quality, and adherence to cognitive behavioral therapy principles using an ordinal scale to ensure consistency in scoring. To enhance reliability, responses underwent a double review process by 2 cognitive scientists, followed by a secondary review by a clinical psychologist specializing in cognitive behavioral therapy, ensuring a robust assessment across interdisciplinary perspectives. Results: This study revealed that general-purpose chatbots outperformed therapeutic chatbots in rectifying cognitive biases, particularly in overtrust bias, fundamental attribution error, and just-world hypothesis. GPT-4 achieved the highest scores across all biases, whereas the therapeutic bot Wysa scored the lowest. Notably, general-purpose bots showed more consistent accuracy and adaptability in recognizing and addressing bias-related cues across different contexts, suggesting a broader flexibility in handling complex cognitive patterns. In addition, in affect recognition tasks, general-purpose chatbots not only excelled but also demonstrated quicker adaptation to subtle emotional nuances, outperforming therapeutic bots in 67% (4/6) of the tested biases. Conclusions: This study shows that, while therapeutic chatbots hold promise for mental health support and cognitive bias intervention, their current capabilities are limited. Addressing cognitive biases in AI-human interactions requires systems that can both rectify and analyze biases as integral to human cognition, promoting precision and simulating empathy. The findings reveal the need for improved simulated emotional intelligence in chatbot design to provide adaptive, personalized responses that reduce overreliance and encourage independent coping skills. Future research should focus on enhancing affective response mechanisms and addressing ethical concerns such as bias mitigation and data privacy to ensure safe, effective AI-based mental health support. ©Marcin Rządeczka, Anna Sterna, Julia Stolińska, Paulina Kaczyńska, Marcin Moskalewicz.","JMIR Publications Inc.","","Article","Final","","Scopus","2-s2.0-85219118191"
"Goh E.; Bunning B.; Khoong E.C.; Gallo R.J.; Milstein A.; Centola D.; Chen J.H.","Goh, Ethan (58297824800); Bunning, Bryan (56485987300); Khoong, Elaine C. (55349940000); Gallo, Robert J. (58285262900); Milstein, Arnold (7005598768); Centola, Damon (8623163000); Chen, Jonathan H. (57112911000)","58297824800; 56485987300; 55349940000; 58285262900; 7005598768; 8623163000; 57112911000","Physician clinical decision modification and bias assessment in a randomized controlled trial of AI assistance","2025","Communications Medicine","5","1","59","","","","0","10.1038/s43856-025-00781-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000080369&doi=10.1038%2fs43856-025-00781-2&partnerID=40&md5=12ea50bc9dc8f2d71165644ade49944c","Background: Artificial intelligence assistance in clinical decision making shows promise, but concerns exist about potential exacerbation of demographic biases in healthcare. This study aims to evaluate how physician clinical decisions and biases are influenced by AI assistance in a chest pain triage scenario. Methods: A randomized, pre post-intervention study was conducted with 50 US-licensed physicians who reviewed standardized chest pain video vignettes featuring either a white male or Black female patient. Participants answered clinical questions about triage, risk assessment, and treatment before and after receiving GPT-4 generated recommendations. Clinical decision accuracy was evaluated against evidence-based guidelines. Results: Here we show that physicians are willing to modify their clinical decisions based on GPT-4 assistance, leading to improved accuracy scores from 47% to 65% in the white male patient group and 63% to 80% in the Black female patient group. The accuracy improvement occurs without introducing or exacerbating demographic biases, with both groups showing similar magnitudes of improvement (18%). A post-study survey indicates that 90% of physicians expect AI tools to play a significant role in future clinical decision making. Conclusions: Physician clinical decision making can be augmented by AI assistance while maintaining equitable care across patient demographics. These findings suggest a path forward for AI clinical decision support that improves medical care without amplifying healthcare disparities. © The Author(s) 2025.","Springer Nature","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-86000080369"
"Aremu T.; Akinwehinmi O.; Nwagu C.; Ahmed S.I.; Orji R.; Amo P.A.D.; Saddik A.E.","Aremu, Toluwani (57833577800); Akinwehinmi, Oluwakemi (58498152300); Nwagu, Chukwuemeka (58261571600); Ahmed, Syed Ishtiaque (35209643600); Orji, Rita (36802416800); Amo, Pedro Arnau Del (59507018200); Saddik, Abdulmotaleb El (57221837385)","57833577800; 58498152300; 58261571600; 35209643600; 36802416800; 59507018200; 57221837385","On the reliability of Large Language Models to misinformed and demographically informed prompts","2025","AI Magazine","46","1","e12208","","","","0","10.1002/aaai.12208","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214496727&doi=10.1002%2faaai.12208&partnerID=40&md5=1022a51fbd11bb175008c447cd09574e","We investigate and observe the behavior and performance of Large Language Model (LLM)-backed chatbots in addressing misinformed prompts and questions with demographic information within the domains of Climate Change and Mental Health. Through a combination of quantitative and qualitative methods, we assess the chatbots' ability to discern the veracity of statements, their adherence to facts, and the presence of bias or misinformation in their responses. Our quantitative analysis using True/False questions reveals that these chatbots can be relied on to give the right answers to these close-ended questions. However, the qualitative insights, gathered from domain experts, shows that there are still concerns regarding privacy, ethical implications, and the necessity for chatbots to direct users to professional services. We conclude that while these chatbots hold significant promise, their deployment in sensitive areas necessitates careful consideration, ethical oversight, and rigorous refinement to ensure they serve as a beneficial augmentation to human expertise rather than an autonomous solution. Dataset and assessment information can be found at https://github.com/tolusophy/Edge-of-Tomorrow. © 2025 The Author(s). AI Magazine published by John Wiley & Sons Ltd on behalf of Association for the Advancement of Artificial Intelligence.","John Wiley and Sons Inc","","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85214496727"
"Kaloudis E.; Kouti V.; Triantafillou F.-M.; Ventouris P.; Pavlidis R.; Bountziouka V.","Kaloudis, Efstathios (35174594800); Kouti, Victoria (59677274000); Triantafillou, Foteini-Maria (59676159000); Ventouris, Patroklos (59676532700); Pavlidis, Rafail (59676532800); Bountziouka, Vasiliki (21833569800)","35174594800; 59677274000; 59676159000; 59676532700; 59676532800; 21833569800","AI-Powered Analysis of Weight Loss Reports from Reddit: Unlocking Social Media’s Potential in Dietary Assessment","2025","Nutrients ","17","5","818","","","","0","10.3390/nu17050818","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000670993&doi=10.3390%2fnu17050818&partnerID=40&md5=957135e10b7fc577af2c56d9b56aabcb","Background/Objectives: The increasing use of social media for sharing health and diet experiences presents new opportunities for nutritional research and dietary assessment. Large language models (LLMs) and artificial intelligence (AI) offer innovative approaches to analyzing self-reported data from online communities. This study explores weight loss experiences associated with the ketogenic diet (KD) using user-generated content from Reddit, aiming to identify trends and potential biases in self-reported outcomes. Methods: A dataset of 35,079 Reddit posts related to KD was collected and processed. Posts mentioning weight loss, diet duration, and additional factors (age, gender, physical activity, health conditions) were identified, yielding 2416 complete cases. Descriptive statistics summarized weight loss distributions and diet adherence patterns, while linear regression models examined factors associated with weight loss. Results: The median reported weight loss was 10.9 kg (IQR: 4.4–22.7 kg). Diet adherence varied with 36.3% of users following KD for up to 30 days and 7.8% for more than a year. Metabolic (27%) and cardiovascular disorders (17%) were the most frequently reported health conditions. Adherence beyond one year was associated with an average weight loss of 28.2 kg (95% CI: 25.5–30.9) compared to up to 30 days. Male gender was associated with an additional weight loss of 5.2 kg (95% CI: 3.8–6.6) compared to females. Conclusions: Findings suggest KD may lead to substantial weight loss based on self-reported online data. This study highlights the value of social media data in nutritional research, uncovering hidden dietary patterns that could inform public health strategies and personalized nutrition plans. © 2025 by the authors.","Multidisciplinary Digital Publishing Institute (MDPI)","40077687","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-86000670993"
"Gonzalez Fiol A.; Mootz A.A.; He Z.; Delgado C.; Ortiz V.; Reale S.C.","Gonzalez Fiol, Antonio (56324559100); Mootz, Allison A. (57205060231); He, Zili (58946719900); Delgado, Carlos (56902310700); Ortiz, Vilma (8692033100); Reale, Sharon C. (57208211077)","56324559100; 57205060231; 58946719900; 56902310700; 8692033100; 57208211077","Accuracy of Spanish and English-generated ChatGPT responses to commonly asked patient questions about labor epidurals: a survey-based study among bilingual obstetric anesthesia experts","2025","International Journal of Obstetric Anesthesia","61","","104290","","","","4","10.1016/j.ijoa.2024.104290","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209741091&doi=10.1016%2fj.ijoa.2024.104290&partnerID=40&md5=1197e1b3f00aa53b9e91a1dbd33c6a29","Background: Large language models (LLMs), of which ChatGPT is the most well known, are now available to patients to seek medical advice in various languages. However, the accuracy of the information utilized to train these models remains unknown. Methods: Ten commonly asked questions regarding labor epidurals were translated from English to Spanish, and all 20 questions were entered into ChatGPT version 3.5. The answers were transcribed. A survey was then sent to 10 bilingual fellowship-trained obstetric anesthesiologists to assess the accuracy of these answers utilizing a 5-point Likert scale. Results: Overall, the accuracy scores for the ChatGPT-generated answers in Spanish were lower than for the English answers with a median score of 34 (IQR 33–36.5) versus 40.5 (IQR 39–44.3), respectively (P value 0.02). Answers to two questions were scored significantly lower: “Do epidurals prolong labor?” (2 (IQR 2–2.5) versus 4 (IQR 4–4.5), P value 0.03) and “Do epidurals increase the risk of needing cesarean delivery?” (3(IQR 2–4) versus 4 (IQR 4–5); P value 0.03). There was a strong agreement that answers to the question “Do epidurals cause autism” were accurate in both Spanish and English. Conclusion: ChatGPT-generated answers in Spanish to ten questions about labor epidurals scored lower for accuracy than answers generated in English, particularly regarding the effect of labor epidurals on labor course and mode of delivery. This disparity in ChatGPT-generated information may extend already-known health inequities among non-English-speaking patients and perpetuate misinformation. © 2024 Elsevier Ltd","Churchill Livingstone","","Article","Final","","Scopus","2-s2.0-85209741091"
"Kunze K.N.; Nwachukwu B.U.; Cote M.P.; Ramkumar P.N.","Kunze, Kyle N. (57202689199); Nwachukwu, Benedict U. (35847555000); Cote, Mark P. (24831832400); Ramkumar, Prem N. (56157365700)","57202689199; 35847555000; 24831832400; 56157365700","Large Language Models Applied to Health Care Tasks May Improve Clinical Efficiency, Value of Care Rendered, Research, and Medical Education","2025","Arthroscopy - Journal of Arthroscopic and Related Surgery","41","3","","547","556","9","1","10.1016/j.arthro.2024.12.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212627188&doi=10.1016%2fj.arthro.2024.12.010&partnerID=40&md5=b23983ee4d041d4ff594352505340637","Large language models (LLMs) are generative artificial intelligence models that create content on the basis of the data on which it was trained. Processing capabilities have evolved from text only to being multimodal including text, images, audio, and video features. In health care settings, LLMs are being applied to several clinically important areas, including patient care and workflow efficiency, communications, hospital operations and data management, medical education, practice management, and health care research. Under the umbrella of patient care, several core use cases of LLMs include simplifying documentation tasks, enhancing patient communication (interactive language and written), conveying medical knowledge, and performing medical triage and diagnosis. However, LLMs warrant scrutiny when applied to health care tasks, as errors may have negative implications for health care outcomes, specifically in the context of perpetuating bias, ethical considerations, and cost-effectiveness. Customized LLMs developed for more narrow purposes may help overcome certain performance limitations, transparency challenges, and biases present in contemporary generalized LLMs by curating training data. Methods of customizing LLMs broadly fall under 4 categories: prompt engineering, retrieval augmented generation, fine-tuning, and agentic augmentation, with each approach conferring different information-retrieval properties for the LLM. Level of Evidence: Level V, expert opinion. © 2024 Arthroscopy Association of North America","W.B. Saunders","39694303","Article","Final","","Scopus","2-s2.0-85212627188"
"Li W.; Hua Y.; Zhou P.; Zhou L.; Xu X.; Yang J.","Li, Wanxin (58300264200); Hua, Yining (57787052500); Zhou, Peilin (57667298900); Zhou, Li (56518549100); Xu, Xin (56424245100); Yang, Jie (57196225258)","58300264200; 57787052500; 57667298900; 56518549100; 56424245100; 57196225258","Characterizing Public Sentiments and Drug Interactions in the COVID-19 Pandemic Using Social Media: Natural Language Processing and Network Analysis","2025","Journal of Medical Internet Research","27","","e63755","","","","0","10.2196/63755","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000152649&doi=10.2196%2f63755&partnerID=40&md5=cabb95e0b65ec03891c67ea2676e6645","Background: While the COVID-19 pandemic has induced massive discussion of available medications on social media, traditional studies focused only on limited aspects, such as public opinions, and endured reporting biases, inefficiency, and long collection times. Objective: Harnessing drug-related data posted on social media in real-time can offer insights into how the pandemic impacts drug use and monitor misinformation. This study aimed to develop a natural language processing (NLP) pipeline tailored for the analysis of social media discourse on COVID-19–related drugs. Methods: This study constructed a full pipeline for COVID-19–related drug tweet analysis, using pretrained language model–based NLP techniques as the backbone. This pipeline is architecturally composed of 4 core modules: named entity recognition and normalization to identify medical entities from relevant tweets and standardize them to uniform medication names for time trend analysis, target sentiment analysis to reveal sentiment polarities associated with the entities, topic modeling to understand underlying themes discussed by the population, and drug network analysis to dig potential adverse drug reactions (ADR) and drug-drug interactions (DDI). The pipeline was deployed to analyze tweets related to the COVID-19 pandemic and drug therapies between February 1, 2020, and April 30, 2022. Results: From a dataset comprising 169,659,956 COVID-19–related tweets from 103,682,686 users, our named entity recognition model identified 2,124,757 relevant tweets sourced from 1,800,372 unique users, and the top 5 most-discussed drugs: ivermectin, hydroxychloroquine, remdesivir, zinc, and vitamin D. Time trend analysis revealed that the public focused mostly on repurposed drugs (ie, hydroxychloroquine and ivermectin), and least on remdesivir, the only officially approved drug among the 5. Sentiment analysis of the top 5 most-discussed drugs revealed that public perception was predominantly shaped by celebrity endorsements, media hot spots, and governmental directives rather than empirical evidence of drug efficacy. Topic analysis obtained 15 general topics of overall drug-related tweets, with “clinical treatment effects of drugs” and “physical symptoms” emerging as the most frequently discussed topics. Co-occurrence matrices and complex network analysis further identified emerging patterns of DDI and ADR that could be critical for public health surveillance like better safeguarding public safety in medicines use. Conclusions: This study shows that an NLP-based pipeline can be a robust tool for large-scale public health monitoring and can offer valuable supplementary data for traditional epidemiological studies concerning DDI and ADR. The framework presented here aspires to serve as a cornerstone for future social media–based public health analytics. ©Wanxin Li, Yining Hua, Peilin Zhou, Li Zhou, Xin Xu, Jie Yang. Originally published in the Journal of Medical Internet Research (https://www.jmir.org), 05.03.2025.","JMIR Publications Inc.","40053730","Article","Final","","Scopus","2-s2.0-86000152649"
"Tierney A.A.; Reed M.E.; Grant R.W.; Doo F.X.; Payán D.D.; Liu V.X.","Tierney, Aaron A. (7005695528); Reed, Mary E. (7401799979); Grant, Richard W. (7402005952); Doo, Florence X. (56450951300); Payán, Denise D. (57189997305); Liu, Vincent X. (35558866700)","7005695528; 7401799979; 7402005952; 56450951300; 57189997305; 35558866700","Health Equity in the Era of Large Language Models","2025","American Journal of Managed Care","31","3","","112","117","5","0","10.37765/ajmc.2025.89695","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000000370&doi=10.37765%2fajmc.2025.89695&partnerID=40&md5=0b431bdaecb59a38aa05788edbeb4e9b","This commentary presents a summary of 8 major regulations and guidelines that have direct implications for the equitable design, implementation, and maintenance of health care–focused large language models (LLMs) deployed in the US. We grouped key equity issues for LLMs into 3 domains: (1) linguistic and cultural bias, (2) accessibility and trust, and (3) oversight and quality control. Solutions shared by these regulations and guidelines are to (1) ensure diverse representation in training data and in teams that develop artificial intelligence (AI) tools, (2) develop techniques to evaluate AI-enabled health care tool performance against real-world data, (3) ensure that AI used in health care is free of discrimination and integrates equity principles, (4) take meaningful steps to ensure access for patients with limited English proficiency, (5) apply AI tools to make workplaces more efficient and reduce administrative burdens, (6) require human oversight of AI tools used in health care delivery, and (7) ensure AI tools are safe, accessible, and beneficial while respecting privacy. There is an opportunity to prevent further embedding of existing disparities and issues in the health care system by enhancing health equity through thoughtfully designed and deployed LLMs. © 2025 Ascend Media. All rights reserved.","Ascend Media","40053403","Review","Final","","Scopus","2-s2.0-105000000370"
"Armitage R.C.","Armitage, Richard C. (57216204894)","57216204894","Implications of Large Language Models for Clinical Practice: Ethical Analysis Through the Principlism Framework","2025","Journal of Evaluation in Clinical Practice","31","1","e14250","","","","0","10.1111/jep.14250","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211134270&doi=10.1111%2fjep.14250&partnerID=40&md5=b0628a68c5ca36e3afaabc866b498d8b","Introduction: The potential applications of large language models (LLMs)—a form of generative artificial intelligence (AI)—in medicine and health care are being increasingly explored by medical practitioners and health care researchers. Methods: This paper considers the ethical implications of LLMs for medical practitioners in their delivery of clinical care through the ethical framework of principlism. Findings: It finds that, regarding beneficence, LLMs can improve patient outcomes through supporting administrative tasks that surround patient care, and by directly informing clinical care. Simultaneously, LLMs can cause patient harm through various mechanisms, meaning non-maleficence would prevent their deployment in the absence of sufficient risk mitigation. Regarding autonomy, medical practitioners must inform patients if their medical care will be influenced by LLMs for their consent to be informed, and alternative care uninfluenced by LLMs must be available for patients who withhold such consent. Finally, regarding justice, LLMs could promote the standardisation of care within individual medical practitioners by mitigating any biases harboured by those practitioners and by protecting against human factors, while also up-skilling existing medical practitioners in low-resource settings to reduce global health disparities. Discussion: Accordingly, this paper finds a strong case for the incorporation of LLMs into clinical practice and, if their risk of patient harm is sufficiently mitigated, this incorporation might be ethically required, at least according to principlism. © 2024 The Author(s). Journal of Evaluation in Clinical Practice published by John Wiley & Sons Ltd.","John Wiley and Sons Inc","39618089","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85211134270"
"Caponio V.C.A.; Musella G.; Pérez-Sayáns M.; Lo Muzio L.; Amaral Mendes R.; López-Pintor R.M.","Caponio, Vito Carlo Alberto (57201483976); Musella, Gennaro (58527394400); Pérez-Sayáns, Mario (23976434900); Lo Muzio, Lorenzo (57205491805); Amaral Mendes, Rui (59677558300); López-Pintor, Rosa María (15056423400)","57201483976; 58527394400; 23976434900; 57205491805; 59677558300; 15056423400","The Need to Improve the Medical Subject Headings (MeSH) and the Excerpta Medica Tree (EMTREE) Thesauri to Perform Systematic Review on Oral Potentially Malignant Disorders","2025","Journal of Oral Pathology and Medicine","","","","","","","0","10.1111/jop.13616","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000577734&doi=10.1111%2fjop.13616&partnerID=40&md5=2935ef42008e93d9bf1fab822d906dfc","Background: Despite recent advancements in the understanding and classification of oral potentially malignant disorders (OPMD), their terminology remains inconsistent and heterogeneous throughout the scientific literature, thus affecting evidence-based decision-making relevant for clinical management of these disorders. Updating this classification represents a necessity to improve the indexing and retrieval of OPMD publications, in particular for systematic reviews and meta-analysis. Methods: Through a critical appraisal of the Medical Subject Headings (MeSH) and Excerpta Medica Tree (EMTREE) thesauri, we assessed gaps in the indexing for OPMD literature and propose improvements for enhanced categorisation and retrieval. Results: The present study identifies inconsistencies and limitations in the classification of these disorders across the major medical databases, which may be summarized in the following findings: a) The MeSH database lacks a dedicated subject heading for “oral potentially malignant disorders”; b) EMTREE indexing is incomplete, with only 5 out of 11 recognised OPMD having corresponding terms; c) Incoherent controlled vocabulary mappings hinder systematic literature retrieval. Conclusion: To ensure accurate evidence synthesis, the authors recommend searching both PubMed and Embase for OPMD studies. Moreover, the use of Embase’s PubMed query translator and Large Language Models, such as ChatGPT, may lead to retrieval biases due to indexing discrepancies, posing challenges for early-career researchers and students. We recommend introducing “oral potentially malignant disorders” as a standardised subject heading. Evidence-based medicine underpins clinical decision support systems, which rely on standardised clinical coding for reliable health information. Enhanced medical ontologies will facilitate structured clinical coding, ensuring interoperability and improving clinical decision support systems. © 2025 The Author(s). Journal of Oral Pathology & Medicine published by John Wiley & Sons Ltd.","John Wiley and Sons Inc","","Article","Article in press","","Scopus","2-s2.0-86000577734"
"Malk N.A.; Diwan S.A.","Malk, Noor Alwan (59674236300); Diwan, Sinan Adnan (57188571965)","59674236300; 57188571965","Exploring the Role of AI in Understanding Human Emotion","2025","Journal of Information Systems Engineering and Management","10","","","64","75","11","0","10.52783/jisem.v10i13s.2004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000346895&doi=10.52783%2fjisem.v10i13s.2004&partnerID=40&md5=1606f447a9ce9f8a5fe43a8b1efa141c","This research will explore the intersection of artificial intelligence (AI) and understanding human emotions, a growing field known as emotional AI. The research aims to provide a comprehensive overview of how AI techniques, such as machine learning, natural language processing (NLP), and facial and voice expression analysis, can be used to analyze human emotions and improve human-machine interactions. The research reviews the theoretical foundations for understanding emotions based on several psychological and neurological models, and also discusses AI applications in the fields of mental health, education, and marketing, where these technologies can improve user experiences by recognizing their emotional responses and tailoring their interactions accordingly. However, this field faces significant challenges, such as privacy and data security, model bias, and accuracy in sentiment analysis. The study highlights the need to develop more transparent and fair systems to ensure the ethical use of this technology. The research suggests that advances in deep learning and multi-dimensional interaction can improve AI’s ability to accurately understand human emotions, opening the door to developing AI systems that are more aware and responsive to human emotions. Copyright © 2024 by Author/s and Licensed by JISEM.","IADITI - International Association for Digital Transformation and Technological Innovation","","Article","Final","","Scopus","2-s2.0-86000346895"
"Gao Y.; Myers S.; Chen S.; Dligach D.; Miller T.; Bitterman D.S.; Chen G.; Mayampurath A.; Churpek M.M., Md,phd; Afshar M.","Gao, Yanjun (57382589400); Myers, Skatje (57220023360); Chen, Shan (58166586200); Dligach, Dmitriy (24474276200); Miller, Timothy (57198615489); Bitterman, Danielle S (56307969500); Chen, Guanhua (56953708700); Mayampurath, Anoop (8605166600); Churpek, Matthew M (36705790600); Afshar, Majid (54954120900)","57382589400; 57220023360; 58166586200; 24474276200; 57198615489; 56307969500; 56953708700; 8605166600; 36705790600; 54954120900","Uncertainty estimation in diagnosis generation from large language models: Next-word probability is not pre-Test probability","2025","JAMIA Open","8","1","ooae154","","","","0","10.1093/jamiaopen/ooae154","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215550622&doi=10.1093%2fjamiaopen%2fooae154&partnerID=40&md5=e2f975cff47a341fae8f778687458bd5","Objective: To evaluate large language models (LLMs) for pre-Test diagnostic probability estimation and compare their uncertainty estimation performance with a traditional machine learning classifier. Materials and Methods: We assessed 2 instruction-Tuned LLMs, Mistral-7B-Instruct and Llama3-70B-chat-hf, on predicting binary outcomes for Sepsis, Arrhythmia, and Congestive Heart Failure (CHF) using electronic health record (EHR) data from 660 patients. Three uncertainty estimation methods-Verbalized Confidence, Token Logits, and LLM Embedding+XGB-were compared against an eXtreme Gradient Boosting (XGB) classifier trained on raw EHR data. Performance metrics included AUROC and Pearson correlation between predicted probabilities. Results: The XGB classifier outperformed the LLM-based methods across all tasks. LLM Embedding+XGB showed the closest performance to the XGB baseline, while Verbalized Confidence and Token Logits underperformed. Discussion: These findings, consistent across multiple models and demographic groups, highlight the limitations of current LLMs in providing reliable pre-Test probability estimations and underscore the need for improved calibration and bias mitigation strategies. Future work should explore hybrid approaches that integrate LLMs with numerical reasoning modules and calibrated embeddings to enhance diagnostic accuracy and ensure fairer predictions across diverse populations. Conclusions: LLMs demonstrate potential but currently fall short in estimating diagnostic probabilities compared to traditional machine learning classifiers trained on structured EHR data. Further improvements are needed for reliable clinical use.  © 2025 The Author(s).","Oxford University Press","","Article","Final","","Scopus","2-s2.0-85215550622"
"Malekzadeh M.; Willberg E.; Torkko J.; Toivonen T.","Malekzadeh, Milad (57263090400); Willberg, Elias (57216621669); Torkko, Jussi (58286363000); Toivonen, Tuuli (7003603971)","57263090400; 57216621669; 58286363000; 7003603971","Urban attractiveness according to ChatGPT: Contrasting AI and human insights","2025","Computers, Environment and Urban Systems","117","","102243","","","","2","10.1016/j.compenvurbsys.2024.102243","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212830252&doi=10.1016%2fj.compenvurbsys.2024.102243&partnerID=40&md5=f24a9cfaeede623ceac962e4844405e1","The attractiveness of urban environments significantly impacts residents' satisfaction with their living spaces and their overall mood, which in turn, affects their health and well-being. Given the resource-intensive nature of gathering evaluations on urban attractiveness through surveys or inquiries from residents, there is a constant quest for automated solutions to streamline this process and support spatial planning. In this study, we applied an off-the-shelf AI model to automate the analysis of urban attractiveness, using over 1800 Google Street View images of Helsinki, Finland. By incorporating the GPT-4 model, we assessed these images through three criteria-based prompts. Simultaneously, 24 participants, categorised into residents and non-residents, were asked to rate the images. To gain insights into the non-transparent decision-making processes of GPT-4, we employed semantic segmentation to explore how the model uses different image features. Our results demonstrated a strong alignment between GPT-4 and participant ratings, although geographic disparities were noted. Specifically, GPT-4 showed a preference for suburban areas with significant greenery, contrasting with participants who found these areas less attractive. Conversely, in the city centre and densely populated urban regions of Helsinki, GPT-4 assigned lower attractiveness scores than participant ratings. The semantic segmentation analysis revealed that GPT-4's ratings were primarily influenced by physical features like vegetation, buildings, and sidewalk. While there was general agreement between AI and human assessments across various locations, GPT-4 struggled to incorporate contextual nuances into its ratings, unlike participants, who considered both context and features of the urban environment. The study suggests that leveraging AI models like GPT-4 allows spatial planners to gather insights into the attractiveness of different areas efficiently. However, caution is necessary, while we used an off-the-shelf model, it is crucial to develop models specifically trained to understand the local context. Although AI models provide valuable insights, human perspectives are essential for a comprehensive understanding of urban attractiveness. © 2024","Elsevier Ltd","","Article","Final","","Scopus","2-s2.0-85212830252"
"Mendel T.; Singh N.; Mann D.M.; Wiesenfeld B.; Nov O.","Mendel, Tamir (57193543905); Singh, Nina (57183430900); Mann, Devin M. (7402056653); Wiesenfeld, Batia (6603613122); Nov, Oded (22981114200)","57193543905; 57183430900; 7402056653; 6603613122; 22981114200","Laypeople’s Use of and Attitudes Toward Large Language Models and Search Engines for Health Queries: Survey Study","2025","Journal of Medical Internet Research","27","","e64290","","","","0","10.2196/64290","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217972812&doi=10.2196%2f64290&partnerID=40&md5=66c00953dc0326e00f35e140ec572919","Background: Laypeople have easy access to health information through large language models (LLMs), such as ChatGPT, and search engines, such as Google. Search engines transformed health information access, and LLMs offer a new avenue for answering laypeople’s questions. Objective: We aimed to compare the frequency of use and attitudes toward LLMs and search engines as well as their comparative relevance, usefulness, ease of use, and trustworthiness in responding to health queries. Methods: We conducted a screening survey to compare the demographics of LLM users and nonusers seeking health information, analyzing results with logistic regression. LLM users from the screening survey were invited to a follow-up survey to report the types of health information they sought. We compared the frequency of use of LLMs and search engines using ANOVA and Tukey post hoc tests. Lastly, paired-sample Wilcoxon tests compared LLMs and search engines on perceived usefulness, ease of use, trustworthiness, feelings, bias, and anthropomorphism. Results: In total, 2002 US participants recruited on Prolific participated in the screening survey about the use of LLMs and search engines. Of them, 52% (n=1045) of the participants were female, with a mean age of 39 (SD 13) years. Participants were 9.7% (n=194) Asian, 12.1% (n=242) Black, 73.3% (n=1467) White, 1.1% (n=22) Hispanic, and 3.8% (n=77) were of other races and ethnicities. Further, 1913 (95.6%) used search engines to look up health queries versus 642 (32.6%) for LLMs. Men had higher odds (odds ratio [OR] 1.63, 95% CI 1.34-1.99; P<.001) of using LLMs for health questions than women. Black (OR 1.90, 95% CI 1.42-2.54; P<.001) and Asian (OR 1.66, 95% CI 1.19-2.30; P<.01) individuals had higher odds than White individuals. Those with excellent perceived health (OR 1.46, 95% CI 1.1-1.93; P=.01) were more likely to use LLMs than those with good health. Higher technical proficiency increased the likelihood of LLM use (OR 1.26, 95% CI 1.14-1.39; P<.001). In a follow-up survey of 281 LLM users for health, most participants used search engines first (n=174, 62%) to answer health questions, but the second most common first source consulted was LLMs (n=39, 14%). LLMs were perceived as less useful (P<.01) and less relevant (P=.07), but elicited fewer negative feelings (P<.001), appeared more human (LLM: n=160, vs search: n=32), and were seen as less biased (P<.001). Trust (P=.56) and ease of use (P=.27) showed no differences. Conclusions: Search engines are the primary source of health information; yet, positive perceptions of LLMs suggest growing use. Future work could explore whether LLM trust and usefulness are enhanced by supplementing answers with external references and limiting persuasive language to curb overreliance. Collaboration with health organizations can help improve the quality of LLMs’ health output. ©Tamir Mendel, Nina Singh, Devin M Mann, Batia Wiesenfeld, Oded Nov.","JMIR Publications Inc.","39946180","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85217972812"
"Carl N.; Haggenmüller S.; Wies C.; Nguyen L.; Winterstein J.T.; Hetz M.J.; Mangold M.H.; Hartung F.O.; Grüne B.; Holland-Letz T.; Michel M.S.; Brinker T.J.; Wessels F.","Carl, Nicolas (58297164900); Haggenmüller, Sarah (57221470647); Wies, Christoph (58118013100); Nguyen, Lisa (59378569900); Winterstein, Jana Theres (57219216879); Hetz, Martin Joachim (58080370200); Mangold, Maurin Helen (59668312500); Hartung, Friedrich Otto (57222744829); Grüne, Britta (57215852994); Holland-Letz, Tim (7801651576); Michel, Maurice Stephan (7401892802); Brinker, Titus Josef (56286969000); Wessels, Frederik (57201922501)","58297164900; 57221470647; 58118013100; 59378569900; 57219216879; 58080370200; 59668312500; 57222744829; 57215852994; 7801651576; 7401892802; 56286969000; 57201922501","Evaluating interactions of patients with large language models for medical information","2025","BJU International","","","","","","","0","10.1111/bju.16676","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219667169&doi=10.1111%2fbju.16676&partnerID=40&md5=78ec2d333c2da68c84f899ebc1b7b139","Objectives: To explore the interaction of real-world patients with a chatbot in a clinical setting, investigating key aspects of medical information provided by large language models (LLMs). Patients and methods: The study enrolled 300 patients seeking urological counselling between February and July 2024. First, participants voluntarily conversed with a Generative Pre-trained Transformer 4 (GPT-4) powered chatbot to ask questions related to their medical situation. In the following survey, patients rated the perceived utility, completeness, and understandability of the information provided during the simulated conversation as well as user-friendliness. Finally, patients were asked which, in their experience, best answered their questions: LLMs, urologists, or search engines. Results: A total of 292 patients completed the study. The majority of patients perceived the chatbot as providing useful, complete, and understandable information, as well as being user-friendly. However, the ability of human urologists to answer medical questions in an understandable way was rated higher than of LLMs. Interestingly, 53% of participants rated the question-answering ability of LLMs higher than search engines. Age was not associated with preferences. Limitations include social desirability and sampling biases. Discussion: This study highlights the potential of LLMs to enhance patient education and communication in clinical settings, with patients valuing their user-friendliness and comprehensiveness for medical information. By addressing preliminary questions, LLMs could potentially relieve time constraints on healthcare providers, enabling medical personnel to focus on complex inquiries and patient care. © 2025 The Author(s). BJU International published by John Wiley & Sons Ltd on behalf of BJU International.","John Wiley and Sons Inc","","Article","Article in press","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85219667169"
"Bedi S.; Liu Y.; Orr-Ewing L.; Dash D.; Koyejo S.; Callahan A.; Fries J.A.; Wornow M.; Swaminathan A.; Lehmann L.S.; Hong H.J.; Kashyap M.; Chaurasia A.R.; Shah N.R.; Singh K.; Tazbaz T.; Milstein A.; Pfeffer M.A.; Shah N.H.","Bedi, Suhana (59152357200); Liu, Yutong (59152673000); Orr-Ewing, Lucy (59152472100); Dash, Dev (57234226200); Koyejo, Sanmi (57846623900); Callahan, Alison (16548985200); Fries, Jason A. (57190403708); Wornow, Michael (57217053730); Swaminathan, Akshay (57207546640); Lehmann, Lisa Soleymani (7005975722); Hong, Hyo Jung (59153038500); Kashyap, Mehr (57216849334); Chaurasia, Akash R. (58142302500); Shah, Nirav R. (9333876600); Singh, Karandeep (56531139000); Tazbaz, Troy (58786032700); Milstein, Arnold (7005598768); Pfeffer, Michael A. (54972654600); Shah, Nigam H. (7401823709)","59152357200; 59152673000; 59152472100; 57234226200; 57846623900; 16548985200; 57190403708; 57217053730; 57207546640; 7005975722; 59153038500; 57216849334; 58142302500; 9333876600; 56531139000; 58786032700; 7005598768; 54972654600; 7401823709","Testing and Evaluation of Health Care Applications of Large Language Models: A Systematic Review","2025","JAMA","333","4","","319","328","9","21","10.1001/jama.2024.21700","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217518288&doi=10.1001%2fjama.2024.21700&partnerID=40&md5=4f44433c336df6cbb401ecc02cc6aea5","Importance: Large language models (LLMs) can assist in various health care activities, but current evaluation approaches may not adequately identify the most useful application areas. Objective: To summarize existing evaluations of LLMs in health care in terms of 5 components: (1) evaluation data type, (2) health care task, (3) natural language processing (NLP) and natural language understanding (NLU) tasks, (4) dimension of evaluation, and (5) medical specialty. Data Sources: A systematic search of PubMed and Web of Science was performed for studies published between January 1, 2022, and February 19, 2024. Study Selection: Studies evaluating 1 or more LLMs in health care. Data Extraction and Synthesis: Three independent reviewers categorized studies via keyword searches based on the data used, the health care tasks, the NLP and NLU tasks, the dimensions of evaluation, and the medical specialty. Results: Of 519 studies reviewed, published between January 1, 2022, and February 19, 2024, only 5% used real patient care data for LLM evaluation. The most common health care tasks were assessing medical knowledge such as answering medical licensing examination questions (44.5%) and making diagnoses (19.5%). Administrative tasks such as assigning billing codes (0.2%) and writing prescriptions (0.2%) were less studied. For NLP and NLU tasks, most studies focused on question answering (84.2%), while tasks such as summarization (8.9%) and conversational dialogue (3.3%) were infrequent. Almost all studies (95.4%) used accuracy as the primary dimension of evaluation; fairness, bias, and toxicity (15.8%), deployment considerations (4.6%), and calibration and uncertainty (1.2%) were infrequently measured. Finally, in terms of medical specialty area, most studies were in generic health care applications (25.6%), internal medicine (16.4%), surgery (11.4%), and ophthalmology (6.9%), with nuclear medicine (0.6%), physical medicine (0.4%), and medical genetics (0.2%) being the least represented. Conclusions and Relevance: Existing evaluations of LLMs mostly focus on accuracy of question answering for medical examinations, without consideration of real patient care data. Dimensions such as fairness, bias, and toxicity and deployment considerations received limited attention. Future evaluations should adopt standardized applications and metrics, use clinical data, and broaden focus to include a wider range of tasks and specialties.","","39405325","Article","Final","","Scopus","2-s2.0-85217518288"
"Theodorou B.; Danek B.; Tummala V.; Kumar S.P.; Malin B.; Sun J.","Theodorou, Brandon (57669740100); Danek, Benjamin (58582942400); Tummala, Venkat (58146770500); Kumar, Shivam Pankaj (59516245000); Malin, Bradley (6602093073); Sun, Jimeng (59338574400)","57669740100; 58582942400; 58146770500; 59516245000; 6602093073; 59338574400","Improving medical machine learning models with generative balancing for equity and excellence","2025","npj Digital Medicine","8","1","100","","","","0","10.1038/s41746-025-01438-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218355805&doi=10.1038%2fs41746-025-01438-z&partnerID=40&md5=8146dba6f6fda3ecf7f6c0d14474b1f9","Applying machine learning to clinical outcome prediction is challenging due to imbalanced datasets and sensitive tasks that contain rare yet critical outcomes and where equitable treatment across diverse patient groups is essential. Despite attempts, biases in predictions persist, driven by disparities in representation and exacerbated by the scarcity of positive labels, perpetuating health inequities. This paper introduces FairPlay, a synthetic data generation approach leveraging large language models, to address these issues. FairPlay enhances algorithmic performance and reduces bias by creating realistic, anonymous synthetic patient data that improves representation and augments dataset patterns while preserving privacy. Through experiments on multiple datasets, we demonstrate that FairPlay boosts mortality prediction performance across diverse subgroups, achieving up to a 21% improvement in F1 Score without requiring additional data or altering downstream training pipelines. Furthermore, FairPlay consistently reduces subgroup performance gaps, as shown by universal improvements in performance and fairness metrics across four experimental setups. © The Author(s) 2025.","Nature Research","","Article","Final","","Scopus","2-s2.0-85218355805"
"Scroggins J.K.; Hulchafo I.I.; Harkins S.; Scharp D.; Moen H.; Davoudi A.; Cato K.; Tadiello M.; Topaz M.; Barcelona V.","Scroggins, Jihye Kim (57856270100); Hulchafo, Ismael I. (59120899100); Harkins, Sarah (58508544600); Scharp, Danielle (57818690400); Moen, Hans (25655389700); Davoudi, Anahita (57216397259); Cato, Kenrick (35071035400); Tadiello, Michele (59143672100); Topaz, Maxim (54790231000); Barcelona, Veronica (6506938957)","57856270100; 59120899100; 58508544600; 57818690400; 25655389700; 57216397259; 35071035400; 59143672100; 54790231000; 6506938957","Identifying stigmatizing and positive/preferred language in obstetric clinical notes using natural language processing","2025","Journal of the American Medical Informatics Association","32","2","","308","317","9","0","10.1093/jamia/ocae290","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216606737&doi=10.1093%2fjamia%2focae290&partnerID=40&md5=df0ea26f6a598fa32cac9761c5602c5b","Objective: To identify stigmatizing language in obstetric clinical notes using natural language processing (NLP). Materials and Methods: We analyzed electronic health records from birth admissions in the Northeast United States in 2017. We annotated 1771 clinical notes to generate the initial gold standard dataset. Annotators labeled for exemplars of 5 stigmatizing and 1 positive/preferred language categories. We used a semantic similarity-based search approach to expand the initial dataset by adding additional exemplars, composing an enhanced dataset. We employed traditional classifiers (Support Vector Machine, Decision Trees, and Random Forest) and a transformer-based model, ClinicalBERT (Bidirectional Encoder Representations from Transformers) and BERT base. Models were trained and validated on initial and enhanced datasets and were tested on enhanced testing dataset. Results: In the initial dataset, we annotated 963 exemplars as stigmatizing or positive/preferred. The most frequently identified category was marginalized language/identities (n = 397, 41%), and the least frequent was questioning patient credibility (n = 51, 5%). After employing a semantic similarity-based search approach, 502 additional exemplars were added, increasing the number of low-frequency categories. All NLP models also showed improved performance, with Decision Trees demonstrating the greatest improvement (21%). ClinicalBERT outperformed other models, with the highest average F1-score of 0.78. Discussion: Clinical BERT seems to most effectively capture the nuanced and context-dependent stigmatizing language found in obstetric clinical notes, demonstrating its potential clinical applications for real-time monitoring and alerts to prevent usages of stigmatizing language use and reduce healthcare bias. Future research should explore stigmatizing language in diverse geographic locations and clinical settings to further contribute to high-quality and equitable perinatal care. Conclusion: ClinicalBERT effectively captures the nuanced stigmatizing language in obstetric clinical notes. Our semantic similarity-based search approach to rapidly extract additional exemplars enhanced the performances while reducing the need for labor-intensive annotation. © The Author(s) 2024. Published by Oxford University Press on behalf of the American Medical Informatics Association.","Oxford University Press","39569431","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85216606737"
"Bai X.; Wang A.; Sucholutsky I.; Griffiths T.L.","Bai, Xuechunzi (57202197909); Wang, Angelina (57219738225); Sucholutsky, Ilia (57200529787); Griffiths, Thomas L. (57222226477)","57202197909; 57219738225; 57200529787; 57222226477","Explicitly unbiased large language models still form biased associations","2025","Proceedings of the National Academy of Sciences of the United States of America","122","8","e2416228122","","","","0","10.1073/pnas.2416228122","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219322052&doi=10.1073%2fpnas.2416228122&partnerID=40&md5=cc700253277e9ecd4e2933ed3756f4f8","Large language models (LLMs) can pass explicit social bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: As LLMs become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make. We address both challenges by introducing two measures: LLM Word Association Test, a prompt-based method for revealing implicit bias; and LLM Relative Decision Test, a strategy to detect subtle discrimination in contextual decisions. Both measures are based on psychological research: LLM Word Association Test adapts the Implicit Association Test, widely used to study the automatic associations between concepts held in human minds; and LLM Relative Decision Test operationalizes psychological results indicating that relative evaluations between two candidates, not absolute evaluations assessing each independently, are more diagnostic of implicit biases. Using these measures, we found pervasive stereotype biases mirroring those in society in 8 value-aligned models across 4 social categories (race, gender, religion, health) in 21 stereotypes (such as race and criminality, race and weapons, gender and science, age and negativity). These prompt-based measures draw from psychology’s long history of research into measuring stereotypes based on purely observable behavior; they expose nuanced biases in proprietary value-aligned LLMs that appear unbiased according to standard benchmarks. Copyright © 2025 the Author(s).","National Academy of Sciences","39977313","Article","Final","","Scopus","2-s2.0-85219322052"
"Tocilă-Mătășel C.; Dudea S.M.; Iana G.","Tocilă-Mătășel, Claudia (58690496500); Dudea, Sorin Marian (6507759016); Iana, Gheorghe (36652749400)","58690496500; 6507759016; 36652749400","Addressing Multi-Center Variability in Radiomic Analysis: A Comparative Study of Image Acquisition Methods Across Two 3T MRI Scanners","2025","Diagnostics","15","4","485","","","","0","10.3390/diagnostics15040485","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218905500&doi=10.3390%2fdiagnostics15040485&partnerID=40&md5=5156f52694968e15e4dfd19e573bc26e","Background: Radiomics has become a valuable tool in medical imaging, but its clinical use is limited by data variability and a lack of reproducibility between centers. This study aims to assess the differences between two scanners and provide guidance on image acquisition methods to reduce variations between images obtained from different centers. Methods: This study utilized medical images obtained in two different imaging centers, with two different 3T MRI scanners. For each scanner, 3D T2 FLAIR sequences were acquired in two forms: the raw and the clinical practice images typically used in diagnostic workflows. The differences between images were analyzed regarding resolution, SNR, CNR, and radiomic features. To facilitate comparison, bias field correction was applied, and the data were standardized to the same scale using Z-score normalization. Descriptive and inferential statistical methods were used to analyze the data. Results: The results show that there are significant differences between centers. Filtering and zero-padding significantly influence the resolution, SNR, CNR values, and radiomics features. Applying Z-score normalization has resolved variations in features sensitive to scale differences, but features reflecting dispersion and extreme values remain significantly different between scanners. Some feature differences may be resolved by analyzing the raw images in both centers. Conclusions: Variations arise due to different acquisition parameters and the differing quality and sensitivity of the equipment. In multi-center studies, acquiring raw images and then applying standardized post-processing methods across all images can enhance the robustness of results. This approach minimizes technical differences, and preserves the integrity of the information, reflecting a more accurate representation of reality and contributing to more reliable and reproducible findings. © 2025 by the authors.","Multidisciplinary Digital Publishing Institute (MDPI)","","Article","Final","","Scopus","2-s2.0-85218905500"
"Huang T.; Socrates V.; Ovchinnikova P.; Faustino I.; Kumar A.; Safranek C.; Chi L.; Wang E.A.; Puglisi L.; Wong A.H.; Wang K.H.; Taylor R.A.","Huang, Thomas (58068715100); Socrates, Vimig (57194574332); Ovchinnikova, Polina (58768220800); Faustino, Isaac (57322715800); Kumar, Anusha (58900651800); Safranek, Conrad (57202209015); Chi, Ling (58068758500); Wang, Emily A. (35774682200); Puglisi, Lisa (57200652169); Wong, Ambrose H. (36460490800); Wang, Karen H. (55389247600); Taylor, R. Andrew (57223661992)","58068715100; 57194574332; 58768220800; 57322715800; 58900651800; 57202209015; 58068758500; 35774682200; 57200652169; 36460490800; 55389247600; 57223661992","Characterizing Emergency Department Care for Patients With Histories of Incarceration","2025","JACEP Open","6","1","100022","","","","0","10.1016/j.acepjo.2024.100022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216857112&doi=10.1016%2fj.acepjo.2024.100022&partnerID=40&md5=c3b3f6d1e42de37cf3ef1e558587edb8","Objectives: Patients with a history of incarceration experience bias from health care team members, barriers to privacy, and a multitude of health care disparities. We aimed to assess care processes delivered in emergency departments (EDs) for people with histories of incarceration. Methods: We utilized a fine-tuned large language model to identify patient incarceration status from 480,374 notes from the ED setting. We compared socio-demographic characteristics, comorbidities, and care processes, including disposition, restraint use, and sedation, between individuals with and without a history of incarceration. We then conducted multivariable logistic regression to assess the independent correlation of incarceration history and management in the ED while adjusting for demographic characteristics, health behaviors, presentation, and past medical history. Results: We found 1734 unique patient encounters with a history of incarceration from a total of 177,987 encounters. Patients with history of incarceration were more likely to be male, Black, Hispanic, or other race/ethnicity, currently unemployed or disabled, and had smoking and substance use histories, compared with those without. This cohort demonstrated higher odds of elopement (OR: 3.59 [95% CI: 2.41–5.12]), leaving against medical advice (OR: 2.39 [95% CI: 1.46–3.67]), and being subjected to sedation (OR: 3.89 [95% CI: 3.19–4.70]) and restraint use (OR: 3.76 [95% CI: 3.06–4.57]). After adjusting for covariates, the association between incarceration and elopement remained significant (adjusted odds ratio: 1.65 [95% CI: 1.08–2.43]), while associations with other dispositions, restraint use, and sedation did not persist. Conclusion: This study identified differences in patient characteristics and care processes in the ED for patients with histories of incarceration and demonstrated the potential of using natural language processing in measuring care processes in populations that are largely hidden, but highly prevalent and subject to discrimination, in the health care system. © 2024 The Author(s)","Elsevier Inc.","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85216857112"
"Nwachukwu B.U.; Varady N.H.; Allen A.A.; Dines J.S.; Altchek D.W.; Williams R.J., III; Kunze K.N.","Nwachukwu, Benedict U. (35847555000); Varady, Nathan H. (57021972500); Allen, Answorth A. (7402802681); Dines, Joshua S. (6701668135); Altchek, David W. (35519144800); Williams, Riley J. (24571844600); Kunze, Kyle N. (57202689199)","35847555000; 57021972500; 7402802681; 6701668135; 35519144800; 24571844600; 57202689199","Currently Available Large Language Models Do Not Provide Musculoskeletal Treatment Recommendations That Are Concordant With Evidence-Based Clinical Practice Guidelines","2025","Arthroscopy - Journal of Arthroscopic and Related Surgery","41","2","","263","275.e6","","8","10.1016/j.arthro.2024.07.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203009740&doi=10.1016%2fj.arthro.2024.07.040&partnerID=40&md5=807f594cae6dfa8a0823d8843ec1f22c","Purpose: To determine whether several leading, commercially available large language models (LLMs) provide treatment recommendations concordant with evidence-based clinical practice guidelines (CPGs) developed by the American Academy of Orthopaedic Surgeons (AAOS). Methods: All CPGs concerning the management of rotator cuff tears (n = 33) and anterior cruciate ligament injuries (n = 15) were extracted from the AAOS. Treatment recommendations from Chat-Generative Pretrained Transformer version 4 (ChatGPT-4), Gemini, Mistral-7B, and Claude-3 were graded by 2 blinded physicians as being concordant, discordant, or indeterminate (i.e., neutral response without definitive recommendation) with respect to AAOS CPGs. The overall concordance between LLM and AAOS recommendations was quantified, and the comparative overall concordance of recommendations among the 4 LLMs was evaluated through the Fisher exact test. Results: Overall, 135 responses (70.3%) were concordant, 43 (22.4%) were indeterminate, and 14 (7.3%) were discordant. Inter-rater reliability for concordance classification was excellent (κ = 0.92). Concordance with AAOS CPGs was most frequently observed with ChatGPT-4 (n = 38, 79.2%) and least frequently observed with Mistral-7B (n = 28, 58.3%). Indeterminate recommendations were most frequently observed with Mistral-7B (n = 17, 35.4%) and least frequently observed with Claude-3 (n = 8, 6.7%). Discordant recommendations were most frequently observed with Gemini (n = 6, 12.5%) and least frequently observed with ChatGPT-4 (n = 1, 2.1%). Overall, no statistically significant difference in concordant recommendations was observed across LLMs (P = .12). Of all recommendations, only 20 (10.4%) were transparent and provided references with full bibliographic details or links to specific peer-reviewed content to support recommendations. Conclusions: Among leading commercially available LLMs, more than 1-in-4 recommendations concerning the evaluation and management of rotator cuff and anterior cruciate ligament injuries do not reflect current evidence-based CPGs. Although ChatGPT-4 showed the highest performance, clinically significant rates of recommendations without concordance or supporting evidence were observed. Only 10% of responses by LLMs were transparent, precluding users from fully interpreting the sources from which recommendations were provided. Clinical Relevance: Although leading LLMs generally provide recommendations concordant with CPGs, a substantial error rate exists, and the proportion of recommendations that do not align with these CPGs suggests that LLMs are not trustworthy clinical support tools at this time. Each off-the-shelf, closed-source LLM has strengths and weaknesses. Future research should evaluate and compare multiple LLMs to avoid bias associated with narrow evaluation of few models as observed in the current literature. © 2024 Arthroscopy Association of North America","W.B. Saunders","39173690","Article","Final","","Scopus","2-s2.0-85203009740"
