TY  - JOUR
AU  - Sudha, L.
AU  - Aruna, K.B.
AU  - Sureka, V.
AU  - Niveditha, M.
AU  - Prema, S.
TI  - Semantic Image Synthesis from Text: Current Trends and Future Horizons in Text-to-Image Generation
PY  - 2025
T2  - EAI Endorsed Transactions on Internet of Things
VL  - 11
DO  - 10.4108/eetiot.5336
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211156231&doi=10.4108%2feetiot.5336&partnerID=40&md5=35f74590e055df0c8e1aab64e6b9e797
AB  - Text-to-image generation, a captivating intersection of natural language processing and computer vision, has undergone a remarkable evolution in recent years. This research paper provides a comprehensive review of the state-of-the-art in text-to-image generation techniques, highlighting key advancements and emerging trends. We begin by surveying the foundational models, with a focus on Generative Adversarial Networks (GANs) and their pivotal role in generating realistic and diverse images from textual descriptions. We delve into the intricacies of training data, model architectures, and evaluation metrics, offering insights into the challenges and opportunities in this field. Furthermore, this paper explores the synergistic relationship between natural language processing and computer vision, showcasing multimodal models like DALL-E and CLIP. These models not only generate images from text but also understand the contextual relationships between textual descriptions and images, opening avenues for content recommendation, search engines, and visual storytelling. The paper discusses applications spanning art, design, e-commerce, healthcare, and education, where text-to-image generation has made significant inroads. We highlight the potential of this technology in automating content creation, aiding in diagnostics, and transforming the fashion and e-commerce industries. However, the journey of text-to-image generation is not without its challenges. We address ethical considerations, emphasizing responsible AI and the mitigation of biases in generated content. We also explore interpretability and model transparency, critical for ensuring trust and accountability. © 2024 L. Sudha et al.
PB  - European Alliance for Innovation
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Gallo, R.J.
AU  - Baiocchi, M.
AU  - Savage, T.R.
AU  - Chen, J.H.
TI  - Establishing best practices in large language model research: an application to repeat prompting
PY  - 2025
T2  - Journal of the American Medical Informatics Association
VL  - 32
IS  - 2
SP  - 386
EP  - 390
DO  - 10.1093/jamia/ocae294
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216606586&doi=10.1093%2fjamia%2focae294&partnerID=40&md5=ec6f10d921d424c363d8964272a342ff
AB  - Objectives: We aimed to demonstrate the importance of establishing best practices in large language model research, using repeat prompting as an illustrative example. Materials and Methods: Using data from a prior study investigating potential model bias in peer review of medical abstracts, we compared methods that ignore correlation in model outputs from repeated prompting with a random effects method that accounts for this correlation. Results: High correlation within groups was found when repeatedly prompting the model, with intraclass correlation coefficient of 0.69. Ignoring the inherent correlation in the data led to over 100-fold inflation of effective sample size. After appropriately accounting for this issue, the authors’ results reverse from a small but highly significant finding to no evidence of model bias. Discussion: The establishment of best practices for LLM research is urgently needed, as demonstrated in this case where accounting for repeat prompting in analyses was critical for accurate study conclusions. © The Author(s) 2024. Published by Oxford University Press on behalf of the American Medical Informatics Association.
PB  - Oxford University Press
C2  - 39656836
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Young, C.C.
AU  - Enichen, E.
AU  - Rao, A.
AU  - Succi, M.D.
TI  - Racial, ethnic, and sex bias in large language model opioid recommendations for pain management
PY  - 2025
T2  - Pain
VL  - 166
IS  - 3
SP  - 511
EP  - 517
DO  - 10.1097/j.pain.0000000000003388
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204024749&doi=10.1097%2fj.pain.0000000000003388&partnerID=40&md5=55dca7caa217086a2bba229ed6ec3d3b
AB  - Understanding how large language model (LLM) recommendations vary with patient race/ethnicity provides insight into how LLMs may counter or compound bias in opioid prescription. Forty real-world patient cases were sourced from the MIMIC-IV Note dataset with chief complaints of abdominal pain, back pain, headache, or musculoskeletal pain and amended to include all combinations of race/ethnicity and sex. Large language models were instructed to provide a subjective pain rating and comprehensive pain management recommendation. Univariate analyses were performed to evaluate the association between racial/ethnic group or sex and the specified outcome measures - subjective pain rating, opioid name, order, and dosage recommendations - suggested by 2 LLMs (GPT-4 and Gemini). Four hundred eighty real-world patient cases were provided to each LLM, and responses included pharmacologic and nonpharmacologic interventions. Tramadol was the most recommended weak opioid in 55.4% of cases, while oxycodone was the most frequently recommended strong opioid in 33.2% of cases. Relative to GPT-4, Gemini was more likely to rate a patient's pain as "severe"(OR: 0.57 95% CI: [0.54, 0.60]; P < 0.001), recommend strong opioids (OR: 2.05 95% CI: [1.59, 2.66]; P < 0.001), and recommend opioids later (OR: 1.41 95% CI: [1.22, 1.62]; P < 0.001). Race/ethnicity and sex did not influence LLM recommendations. This study suggests that LLMs do not preferentially recommend opioid treatment for one group over another. Given that prior research shows race-based disparities in pain perception and treatment by healthcare providers, LLMs may offer physicians a helpful tool to guide their pain management and ensure equitable treatment across patient groups.  © 2024 International Association for the Study of Pain.
PB  - Lippincott Williams and Wilkins
C2  - 39283333
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Hsieh, P.-H.
TI  - Psychological reactance to vaccine mandates on Twitter: a study of sentiments in the United States
PY  - 2025
T2  - Journal of Public Health Policy
DO  - 10.1057/s41271-025-00554-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217239858&doi=10.1057%2fs41271-025-00554-0&partnerID=40&md5=0f73dc354723d5900e9e9abad22edf6e
AB  - This study examines the relationship between vaccine mandates and public sentiment toward vaccines and health officials on Twitter. I analyzed 6.6 million vaccine-related tweets from July 2021 to February 2022 in the United States. Leveraging a large language model, BERT, I identified tweets discussing vaccine mandates even when lacking explicit keywords. Compared to non-mandate tweets, those mentioning mandates exhibit greater negativity, anger, and freedom-related language. Furthermore, increased state-level discussion of mandates correlates with rising levels of negativity and anger toward both vaccines and public health officials. Finally, greater disparity in vaccination progress across counties within a state is associated with increased anger in tweets directed toward both. © The Author(s) 2025.
PB  - Palgrave Macmillan
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Zhu, M.
AU  - Yang, Q.
AU  - Gao, Z.
AU  - Yuan, Y.
AU  - Liu, J.
TI  - FedBM: Stealing knowledge from pre-trained language models for heterogeneous federated learning
PY  - 2025
T2  - Medical Image Analysis
VL  - 102
C7  - 103524
DO  - 10.1016/j.media.2025.103524
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000465365&doi=10.1016%2fj.media.2025.103524&partnerID=40&md5=3118a378046dbcf04b8b46e3654b824f
AB  - Federated learning (FL) has shown great potential in medical image computing since it provides a decentralized learning paradigm that allows multiple clients to train a model collaboratively without privacy leakage. However, current studies have shown that data heterogeneity incurs local learning bias in classifiers and feature extractors of client models during local training, leading to the performance degradation of a federation system. To address these issues, we propose a novel framework called Federated Bias eliMinating (FedBM) to get rid of local learning bias in heterogeneous federated learning (FL), which mainly consists of two modules, i.e., Linguistic Knowledge-based Classifier Construction (LKCC) and Concept-guided Global Distribution Estimation (CGDE). Specifically, LKCC exploits class concepts, prompts and pre-trained language models (PLMs) to obtain concept embeddings. These embeddings are used to estimate the latent concept distribution of each class in the linguistic space. Based on the theoretical derivation, we can rely on these distributions to pre-construct a high-quality classifier for clients to achieve classification optimization, which is frozen to avoid classifier bias during local training. CGDE samples probabilistic concept embeddings from the latent concept distributions to learn a conditional generator to capture the input space of the global model. Three regularization terms are introduced to improve the quality and utility of the generator. The generator is shared by all clients and produces pseudo data to calibrate updates of local feature extractors. Extensive comparison experiments and ablation studies on public datasets demonstrate the superior performance of FedBM over state-of-the-arts and confirm the effectiveness of each module, respectively. The code is available at https://github.com/CUHK-AIM-Group/FedBM. © 2025 Elsevier B.V.
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Arslan, B.
AU  - Nuhoglu, C.
AU  - Satici, M.O.
AU  - Altinbilek, E.
TI  - Evaluating LLM-based generative AI tools in emergency triage: A comparative study of ChatGPT Plus, Copilot Pro, and triage nurses
PY  - 2025
T2  - American Journal of Emergency Medicine
VL  - 89
SP  - 174
EP  - 181
DO  - 10.1016/j.ajem.2024.12.024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213016878&doi=10.1016%2fj.ajem.2024.12.024&partnerID=40&md5=5a79c14f7f9f65fdbcc8fea2b38d174d
AB  - Background: The number of emergency department (ED) visits has been on steady increase globally. Artificial Intelligence (AI) technologies, including Large Language Model (LLMs)-based generative AI models, have shown promise in improving triage accuracy. This study evaluates the performance of ChatGPT and Copilot in triage at a high-volume urban hospital, hypothesizing that these tools can match trained physicians' accuracy and reduce human bias amidst ED crowding challenges. Methods: This single-center, prospective observational study was conducted in an urban ED over one week. Adult patients were enrolled through random 24-h intervals. Exclusions included minors, trauma cases, and incomplete data. Triage nurses assessed patients while an emergency medicine (EM) physician documented clinical vignettes and assigned emergency severity index (ESI) levels. These vignettes were then introduced to ChatGPT and Copilot for comparison with the triage nurse's decision. Results: The overall triage accuracy was 65.2 % for nurses, 66.5 % for ChatGPT, and 61.8 % for Copilot, with no significant difference (p = 0.000). Moderate agreement was observed between the EM physician and ChatGPT, triage nurses, and Copilot (Cohen's Kappa = 0.537, 0.477, and 0.472, respectively). In recognizing high-acuity patients, ChatGPT and Copilot outperformed triage nurses (87.8 % and 85.7 % versus 32.7 %, respectively). Compared to ChatGPT and Copilot, nurses significantly under-triaged patients (p < 0.05). The analysis of predictive performance for ChatGPT, Copilot, and triage nurses demonstrated varying discrimination abilities across ESI levels, all of which were statistically significant (p < 0.05). ChatGPT and Copilot exhibited consistent accuracy across age, gender, and admission time, whereas triage nurses were more likely to mistriage patients under 45 years old. Conclusion: ChatGPT and Copilot outperform traditional nurse triage in identifying high-acuity patients, but real-time ED capacity data is crucial to prevent overcrowding and ensure high-quality of emergency care. © 2024 Elsevier Inc.
PB  - W.B. Saunders
C2  - 39731895
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Garcia Valencia, O.A.
AU  - Thongprayoon, C.
AU  - Jadlowiec, C.C.
AU  - Mao, S.A.
AU  - Leeaphorn, N.
AU  - Budhiraja, P.
AU  - Khoury, N.
AU  - Pham, J.H.
AU  - Craici, I.M.
AU  - Gonzalez Suarez, M.L.
AU  - Cheungpasitporn, W.
TI  - Advancing health equity: evaluating AI translations of kidney donor information for Spanish speakers
PY  - 2025
T2  - Frontiers in Public Health
VL  - 13
C7  - 1484790
DO  - 10.3389/fpubh.2025.1484790
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217423319&doi=10.3389%2ffpubh.2025.1484790&partnerID=40&md5=4681c5c210eba700b78839c6d0cca744
AB  - Background: Health equity and access to essential medical information remain significant challenges, especially for the Spanish-speaking Hispanic population, which faces barriers in accessing living kidney donation opportunities. ChatGPT, an AI language model with sophisticated natural language processing capabilities, has been identified as a promising tool for translating critical health information into Spanish. This study aims to assess ChatGPT’s translation efficacy to ensure the information provided is accurate and culturally relevant. Methods: This study utilized ChatGPT versions 3.5 and 4.0 to translate 27 frequently asked questions (FAQs) from English to Spanish, sourced from Donate Life America’s website. The translated content was reviewed by native Spanish-speaking nephrologists using a standard rubric scale (1–5). The assessment focused on linguistic accuracy and cultural sensitivity, emphasizing retention of the original message, appropriate vocabulary and grammar, and cultural relevance. Results: The mean linguistic accuracy scores were 4.89 ± 0.32 for GPT-3.5 and 5.00 ± 0.00 for GPT-4.0 (p = 0.08). The percentage of excellent-quality translations (score = 5) in linguistic accuracy was 89% for GPT-3.5 and 100% for GPT-4.0 (p = 0.24). The mean cultural sensitivity scores were 4.89 ± 0.32 for both GPT-3.5 and GPT-4.0 (p = 1.00). Similarly, excellent-quality translations in cultural sensitivity were achieved in 89% of cases for both versions (p = 1.00). Conclusion: ChatGPT 4.0 demonstrates strong potential to enhance health equity by improving Spanish-speaking Hispanic patients’ access to LKD information through accurate and culturally sensitive translations. These findings highlight the role of AI in mitigating healthcare disparities and underscore the need for integrating AI-driven tools into healthcare systems. Future efforts should focus on developing accessible platforms and establishing guidelines to maximize AI’s impact on equitable healthcare delivery and patient education. Copyright © 2025 Garcia Valencia, Thongprayoon, Jadlowiec, Mao, Leeaphorn, Budhiraja, Khoury, Pham, Craici, Gonzalez Suarez and Cheungpasitporn.
PB  - Frontiers Media SA
C2  - 39931300
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Wang, C.
AU  - Xiao, C.
AU  - Zhang, X.
AU  - Zhu, Y.
AU  - Chen, X.
AU  - Li, Y.
AU  - Qi, H.
TI  - Exploring medical students’ intention to use of ChatGPT from a programming course: a grounded theory study in China
PY  - 2025
T2  - BMC Medical Education
VL  - 25
IS  - 1
C7  - 209
DO  - 10.1186/s12909-025-06807-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218229852&doi=10.1186%2fs12909-025-06807-6&partnerID=40&md5=2ae6a72fcb7b798ea357f3ed4d9f2f1a
AB  - Background: In interdisciplinary general education courses, medical students face the daunting challenge of learning programming due to academic pressure, cognitive biases, and differences in thinking patterns. ChatGPT provides an effective way for people to acquire knowledge, improve learning efficiency, and quality. Objective: To explore whether medical students can be assisted in learning programming with the help of ChatGPT, it is necessary to investigate their experience and perception of using ChatGPT, and to study which factors influence their willingness to use ChatGPT. Methods: Drawing on the grounded theory research paradigm, this paper constructs a research model of the influencing factors of ChatGPT usage willingness for medical students in programming courses through the analysis of interview data from 30 undergraduate medical students. It analyzes and discusses the cognition and influencing factors of medical students’ willingness to use ChatGPT in programming learning. Results: The willingness to use ChatGPT in programming learning is divided into three types based on the students’ subjective degree of use: active use, neutral use, and negative use. It is also found that individual factors, technical factors, information factors, and environmental factors are four important dimensions affecting the willingness to use ChatGPT. Conclusions: Based on the analysis of influencing factors, strategies and suggestions such as preventing risks and focusing on ethical education, cultivating critical thinking and establishing a case library, and personalized teaching to enhance core literacy in programming are proposed. © The Author(s) 2025.
PB  - BioMed Central Ltd
C2  - 39923098
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Al-Qudimat, A.R.
AU  - Fares, Z.E.
AU  - Elaarag, M.
AU  - Osman, M.
AU  - Al-Zoubi, R.M.
AU  - Aboumarzouk, O.M.
TI  - Advancing Medical Research Through Artificial Intelligence: Progressive and Transformative Strategies: A Literature Review
PY  - 2025
T2  - Health Science Reports
VL  - 8
IS  - 2
C7  - e70200
DO  - 10.1002/hsr2.70200
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219164836&doi=10.1002%2fhsr2.70200&partnerID=40&md5=41fffd0f18c421839218d589eff8bba4
AB  - Background and Aims: Artificial intelligence (AI) has become integral to medical research, impacting various aspects such as data analysis, writing assistance, and publishing. This paper explores the multifaceted influence of AI on the process of writing medical research papers, encompassing data analysis, ethical considerations, writing assistance, and publishing efficiency. Methods: The review was conducted following the PRISMA guidelines; a comprehensive search was performed in Scopus, PubMed, EMBASE, and MEDLINE databases for research publications on artificial intelligence in medical research published up to October 2023. Results: AI facilitates the writing process by generating drafts, offering grammar and style suggestions, and enhancing manuscript quality through advanced models like ChatGPT. Ethical concerns regarding content ownership and potential biases in AI-generated content underscore the need for collaborative efforts among researchers, publishers, and AI creators to establish ethical standards. Moreover, AI significantly influences data analysis in healthcare, optimizing outcomes and patient care, particularly in fields such as obstetrics and gynecology and pharmaceutical research. The application of AI in publishing, ranging from peer review to manuscript quality control and journal matching, underscores its potential to streamline and enhance the entire research and publication process. Overall, while AI presents substantial benefits, ongoing research, and ethical guidelines are essential for its responsible integration into the evolving landscape of medical research and publishing. Conclusion: The integration of AI in medical research has revolutionized efficiency and innovation, impacting data analysis, writing assistance, publishing, and others. While AI tools offer significant benefits, ethical considerations such as biases and content ownership must be addressed. Ongoing research and collaborative efforts are crucial to ensure responsible and transparent AI implementation in the dynamic landscape of medical research and publishing. © 2024 The Authors. Health Science Reports published by Wiley Periodicals LLC.
PB  - John Wiley and Sons Inc
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Occhipinti, J.-A.
AU  - Prodan, A.
AU  - Hynes, W.
AU  - Buchanan, J.
AU  - Green, R.
AU  - Burrow, S.
AU  - Eyre, H.A.
AU  - Skinner, A.
AU  - Hickie, I.B.
AU  - Heffernan, M.
AU  - Song, Y.J.C.
AU  - Ujdur, G.
AU  - Tanner, M.
TI  - Artificial intelligence, recessionary pressures and population health
ST  - Intelligence artificielle, pressions récessionnistes et santé des populations
PY  - 2025
T2  - Bulletin of the World Health Organization
VL  - 103
IS  - 2
SP  - 155
EP  - 163
DO  - 10.2471/BLT.24.291950
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217357047&doi=10.2471%2fBLT.24.291950&partnerID=40&md5=89828290f7f5bcb2d44cb5a60af3588f
AB  - Economic and labour policies have a considerable influence on health and well-being through direct financial impacts, and by shaping social and physical environments. Strong economies are important for public health investment and employment, yet the rapid rise of generative artificial intelligence (AI) has the potential to reshape economies, presenting challenges beyond mere temporary market disruption. Generative AI can perform non-routine cognitive tasks, previously unattainable though traditional automation, creating new efficiencies. While this technology offers opportunities for innovation and productivity, its labour-displacing potential raises serious concerns about economic stability and social equity, both of which are critical to health. Job displacement driven by generative AI could worsen income inequality, shrink middle-class opportunities and reduce consumer demand, triggering recessionary pressures. In this article, we propose the existence of an AI-capital-to-labour ratio threshold beyond which a self-reinforcing cycle of recessionary pressures may emerge, and which market forces alone cannot correct. Traditional responses to such pressures, like fiscal stimulus or monetary easing, may be ineffective in addressing structural disruptions to labour markets caused by generative AI. We call for a proactive global response to harness the benefits of generative AI while mitigating risks. This response should focus on reorienting economic systems towards collective well-being, as emphasized in the World Health Assembly resolution Economics of health for all and the United Nations' Global Digital Compact. Integrated strategies that combine fiscal policy, regulation and social policies are critical to ensuring generative AI advances societal health and equity while avoiding harm from excessive job displacement. © 2025 The authors; licensee World Health Organization.
PB  - World Health Organization
C2  - 39882489
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Thaya, I.M.
AU  - Sasirekha, S.
AU  - Arun, N.
AU  - Kanyalakshmi, G.
TI  - An XAI-Driven Support System to Enhance the Detection and Diagnosis of Liver Tumor for Interventional Radiologist
PY  - 2025
T2  - Journal of Information Systems Engineering and Management
VL  - 10
SP  - 40
EP  - 46
DO  - 10.52783/jisem.v10i11s.1492
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219090147&doi=10.52783%2fjisem.v10i11s.1492&partnerID=40&md5=65620a20672aa969ce3910c0eeb1169a
AB  - In healthcare, the use of opaque deep learning models often results in limited transparency, potential bias, and inaccuracies, leading to a lack of trust among healthcare providers and patients. To address these challenges, this work integrates Explainable Artificial Intelligence (XAI) methods to enhance the transparency and interpretability of AI models, particularly in liver tumor segmentation. By employing XAI techniques, such as GradCAM (Gradient-weighted Class Activation Mapping), the proposed approach provides visual explanations that highlight the most critical regions influencing the model's predictions. This study focuses on combining state-of-the-art deep learning models, achieving a high accuracy of 99%, to ensure precise and reliable segmentation of liver tumors. GradCAM further enhances this process by generating heatmaps that explain the AI's decision-making, fostering trust and reliability among medical professionals. Beyond segmentation, the framework extends to decision support systems that offer transparent insights into medical decision-making, predictive analytics for patient outcome forecasting, and natural language processing for analyzing medical data. This approach ultimately empowers interventional medical professionals with accurate, interpretable, and trustworthy AI solutions, transforming how liver tumors are analyzed and segmented. Copyright © 2024 by Author/s and Licensed by JISEM.
PB  - IADITI - International Association for Digital Transformation and Technological Innovation
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Zarfati, M.
AU  - Soffer, S.
AU  - Nadkarni, G.N.
AU  - Klang, E.
TI  - Retrieval-Augmented Generation: Advancing personalized care and research in oncology
PY  - 2025
T2  - European Journal of Cancer
VL  - 220
C7  - 115341
DO  - 10.1016/j.ejca.2025.115341
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000304757&doi=10.1016%2fj.ejca.2025.115341&partnerID=40&md5=8f3c08d786e64acd4d54e7162542e6dc
AB  - Retrieval-Augmented Generation (RAG) pairs large language models (LLMs) with recent data to produce more accurate, context-aware outputs. By converting text into numeric embeddings, RAG locates and retrieves relevant “chunks” of data, that along with the query, ground the model's responses in current, specific information. This process helps reduce outdated or fabricated answers. In oncology, RAG has shown particular promise. Studies have demonstrated its ability to improve treatment recommendations by integrating genetic profiles, strengthened clinical trial matching through biomarker analysis, and accelerated drug development by clarifying model-driven insights. Despite its advantages, RAG depends on high-quality data. Biased or incomplete sources can lead to inaccurate outcomes. Careful implementation and human oversight are crucial for ensuring the effectiveness and reliability of RAG in oncology. © 2025 Elsevier Ltd
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Hasan, S.S.
AU  - Woo, J.J.
AU  - Cote, M.P.
AU  - Ramkumar, P.N.
TI  - Generative Versus Nongenerative Artificial Intelligence
PY  - 2025
T2  - Arthroscopy - Journal of Arthroscopic and Related Surgery
VL  - 41
IS  - 3
SP  - 545
EP  - 546
DO  - 10.1016/j.arthro.2024.12.001
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217026497&doi=10.1016%2fj.arthro.2024.12.001&partnerID=40&md5=b79fbede0183f2e7dbcaec7a987e59c5
AB  - Artificial intelligence (AI) is a colossal buzzword, a confusing subject matter, but also an inevitable reality. Generative and nongenerative AI are the 2 core subtypes of AI. Generative AI uses current data to understand patterns and generate new information, and it is especially valuable in producing synthetic medical images, enhancing surgical simulations, and expanding training datasets. Techniques such as generative adversarial networks (GANs), large language models (LLMs), and variational autoencoders (VAEs) allow for the creation of realistic simulations, text, and models that can be used for perioperative communication and planning. Conversely, nongenerative AI is centered on the examination and categorization of pre-existing data to formulate predictions or decisions—the most popular denomination namely machine learning. This approach is instrumental in tasks such as forecasting surgical outcomes, segmenting medical images, and determining patient risk profiles. Models such as convolutional neural networks (CNNs), random forests, and support vector machines (SVMs) are widely used for these purposes, demonstrating high accuracy and reliability in clinical decision making. Although generative AI offers innovative tools for creating new data and simulations, nongenerative AI excels in analyzing existing data to inform patient care. Both approaches have the potential of supporting clinical workflows to automate redundancies and improve efficiencies. However, there are also limitations in the application of AI in orthopaedics, including the potential for bias in models, the challenge of interpreting AI-driven insights, and the ethics of oversight. As the integration of AI in orthopaedics continues to grow, it is essential for practitioners to understand these technologies' capabilities and limitations to harness their full potential and establish appropriate governance. © 2024 Arthroscopy Association of North America
PB  - W.B. Saunders
C2  - 39929595
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Comeau, D.S.
AU  - Bitterman, D.S.
AU  - Celi, L.A.
TI  - Preventing unrestricted and unmonitored AI experimentation in healthcare through transparency and accountability
PY  - 2025
T2  - npj Digital Medicine
VL  - 8
IS  - 1
C7  - 42
DO  - 10.1038/s41746-025-01443-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218346992&doi=10.1038%2fs41746-025-01443-2&partnerID=40&md5=b041eb5ac14d4e9d3258cad9c4cecdfc
AB  - The integration of large language models (LLMs) into electronic health records offers potential benefits but raises significant ethical, legal, and operational concerns, including unconsented data use, lack of governance, and AI-related malpractice accountability. Sycophancy, feedback loop bias, and data reuse risk amplifying errors without proper oversight. To safeguard patients, especially the vulnerable, clinicians must advocate for patient-centered education, ethical practices, and robust oversight to prevent harm. © The Author(s) 2025.
PB  - Nature Research
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Fleurence, R.L.
AU  - Bian, J.
AU  - Wang, X.
AU  - Xu, H.
AU  - Dawoud, D.
AU  - Higashi, M.
AU  - Chhatwal, J.
TI  - Generative Artificial Intelligence for Health Technology Assessment: Opportunities, Challenges, and Policy Considerations: An ISPOR Working Group Report
PY  - 2025
T2  - Value in Health
VL  - 28
IS  - 2
SP  - 175
EP  - 183
DO  - 10.1016/j.jval.2024.10.3846
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216324284&doi=10.1016%2fj.jval.2024.10.3846&partnerID=40&md5=a3d80481480398a407018b32db44cd9c
AB  - Objectives: To provide an introduction to the uses of generative artificial intelligence (AI) and foundation models, including large language models, in the field of health technology assessment (HTA). Methods: We reviewed applications of generative AI in 3 areas: systematic literature reviews, real-world evidence, and health economic modeling. Results: (1) Literature reviews: generative AI has the potential to assist in automating aspects of systematic literature reviews by proposing search terms, screening abstracts, extracting data, and generating code for meta-analyses; (2) real-world evidence: generative AI can facilitate automating processes and analyze large collections of real-world data, including unstructured clinical notes and imaging; (3) health economic modeling: generative AI can aid in the development of health economic models, from conceptualization to validation. Limitations in the use of foundation models and large language models include challenges surrounding their scientific rigor and reliability, the potential for bias, implications for equity, as well as nontrivial concerns regarding adherence to regulatory and ethical standards, particularly in terms of data privacy and security. Additionally, we survey the current policy landscape and provide suggestions for HTA agencies on responsibly integrating generative AI into their workflows, emphasizing the importance of human oversight and the fast-evolving nature of these tools. Conclusions: Although generative AI technology holds promise with respect to HTA applications, it is still undergoing rapid developments and improvements. Continued careful evaluation of their applications to HTA is required. Both developers and users of research incorporating these tools, should familiarize themselves with their current capabilities and limitations. © 2025
PB  - Elsevier Ltd
C2  - 39536966
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Haber, Y.
AU  - Hadar Shoval, D.
AU  - Levkovich, I.
AU  - Yinon, D.
AU  - Gigi, K.
AU  - Pen, O.
AU  - Angert, T.
AU  - Elyoseph, Z.
TI  - The externalization of internal experiences in psychotherapy through generative artificial intelligence: a theoretical, clinical, and ethical analysis
PY  - 2025
T2  - Frontiers in Digital Health
VL  - 7
C7  - 1512273
DO  - 10.3389/fdgth.2025.1512273
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218207966&doi=10.3389%2ffdgth.2025.1512273&partnerID=40&md5=2aa0069cf2bdcb2c3f55c77857f1171e
AB  - Introduction: Externalization techniques are well established in psychotherapy approaches, including narrative therapy and cognitive behavioral therapy. These methods elicit internal experiences such as emotions and make them tangible through external representations. Recent advances in generative artificial intelligence (GenAI), specifically large language models (LLMs), present new possibilities for therapeutic interventions; however, their integration into core psychotherapy practices remains largely unexplored. This study aimed to examine the clinical, ethical, and theoretical implications of integrating GenAI into the therapeutic space through a proof-of-concept (POC) of AI-driven externalization techniques, while emphasizing the essential role of the human therapist. Methods: To this end, we developed two customized GPTs agents: VIVI (visual externalization), which uses DALL-E 3 to create images reflecting patients' internal experiences (e.g., depression or hope), and DIVI (dialogic role-play-based externalization), which simulates conversations with aspects of patients' internal content. These tools were implemented and evaluated through a clinical case study under professional psychological guidance. Results: The integration of VIVI and DIVI demonstrated that GenAI can serve as an “artificial third”, creating a Winnicottian playful space that enhances, rather than supplants, the dyadic therapist-patient relationship. The tools successfully externalized complex internal dynamics, offering new therapeutic avenues, while also revealing challenges such as empathic failures and cultural biases. Discussion: These findings highlight both the promise and the ethical complexities of AI-enhanced therapy, including concerns about data security, representation accuracy, and the balance of clinical authority. To address these challenges, we propose the SAFE-AI protocol, offering clinicians structured guidelines for responsible AI integration in therapy. Future research should systematically evaluate the generalizability, efficacy, and ethical implications of these tools across diverse populations and therapeutic contexts. 2025 Haber, Hadar Shoval, Levkovich, Yinon, Gigi, Pen, Angert and Elyoseph.
PB  - Frontiers Media SA
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Annor, E.
AU  - Atarere, J.
AU  - Ubah, N.
AU  - Jolaoye, O.
AU  - Kunkle, B.
AU  - Egbo, O.
AU  - Martin, D.K.
TI  - Assessing online chat-based artificial intelligence models for weight loss recommendation appropriateness and bias in the presence of guideline incongruence: Behaviour, Psychology and Sociology
PY  - 2025
T2  - International Journal of Obesity
DO  - 10.1038/s41366-025-01717-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217262182&doi=10.1038%2fs41366-025-01717-5&partnerID=40&md5=ff020f4413f61a48b5c59a1bdba07800
AB  - Background and aim: Managing obesity requires a comprehensive approach that involves therapeutic lifestyle changes, medications, or metabolic surgery. Many patients seek health information from online sources and artificial intelligence models like ChatGPT, Google Gemini, and Microsoft Copilot before consulting health professionals. This study aims to evaluate the appropriateness of the responses of Google Gemini and Microsoft Copilot to questions on pharmacologic and surgical management of obesity and assess for bias in their responses to either the ADA or AACE guidelines. Methods: Ten questions were compiled into a set and posed separately to the free editions of Google Gemini and Microsoft Copilot. Recommendations for the questions were extracted from the ADA and the AACE websites, and the responses were graded by reviewers for appropriateness, completeness, and bias to any of the guidelines. Results: All responses from Microsoft Copilot and 8/10 (80%) responses from Google Gemini were appropriate. There were no inappropriate responses. Google Gemini refused to respond to two questions and insisted on consulting a physician. Microsoft Copilot (10/10; 100%) provided a higher proportion of complete responses than Google Gemini (5/10; 50%). Of the eight responses from Google Gemini, none were biased towards any of the guidelines, while two of the responses from Microsoft Copilot were biased. Conclusion: The study highlights the role of Microsoft Copilot and Google Gemini in weight loss management. The differences in their responses may be attributed to the variation in the quality and scope of their training data and design. © The Author(s), under exclusive licence to Springer Nature Limited 2025.
PB  - Springer Nature
C2  - 39871015
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Walker, A.
AU  - Thorne, A.
AU  - Das, S.
AU  - Love, J.
AU  - Cooper, H.L.F.
AU  - Livingston, M.
AU  - Sarker, A.
TI  - CARE-SD: classifier-based analysis for recognizing provider stigmatizing and doubt marker labels in electronic health records: model development and validation
PY  - 2025
T2  - Journal of the American Medical Informatics Association
VL  - 32
IS  - 2
SP  - 365
EP  - 374
DO  - 10.1093/jamia/ocae310
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216606913&doi=10.1093%2fjamia%2focae310&partnerID=40&md5=3bbc2f07c391f8fe79640fabde0ebe39
AB  - Objective: To detect and classify features of stigmatizing and biased language in intensive care electronic health records (EHRs) using natural language processing techniques. Materials and Methods: We first created a lexicon and regular expression lists from literature-driven stem words for linguistic features of stigmatizing patient labels, doubt markers, and scare quotes within EHRs. The lexicon was further extended using Word2Vec and GPT 3.5, and refined through human evaluation. These lexicons were used to search for matches across 18 million sentences from the de-identified Medical Information Mart for Intensive Care-III (MIMIC-III) dataset. For each linguistic bias feature, 1000 sentence matches were sampled, labeled by expert clinical and public health annotators, and used to supervised learning classifiers. Results: Lexicon development from expanded literature stem-word lists resulted in a doubt marker lexicon containing 58 expressions, and a stigmatizing labels lexicon containing 127 expressions. Classifiers for doubt markers and stigmatizing labels had the highest performance, with macro F1-scores of 0.84 and 0.79, positive-label recall and precision values ranging from 0.71 to 0.86, and accuracies aligning closely with human annotator agreement (0.87). Discussion: This study demonstrated the feasibility of supervised classifiers in automatically identifying stigmatizing labels and doubt markers in medical text and identified trends in stigmatizing language use in an EHR setting. Additional labeled data may help improve lower scare quote model performance. Conclusions: Classifiers developed in this study showed high model performance and can be applied to identify patterns and target interventions to reduce stigmatizing labels and doubt markers in healthcare systems. © The Author(s) 2024. Published by Oxford University Press on behalf of the American Medical Informatics Association.
PB  - Oxford University Press
C2  - 39724920
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Heuchan, G.N.
AU  - Conway, R.E.
AU  - Tattan-Birch, H.
AU  - Heggie, L.
AU  - Llewellyn, C.H.
TI  - Social and Economic Patterning in Ultra-Processed Food Intake in Toddlerhood and Middle Childhood: Longitudinal Data From the Gemini Cohort in the United Kingdom
PY  - 2025
T2  - Journal of the Academy of Nutrition and Dietetics
DO  - 10.1016/j.jand.2025.01.004
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219541846&doi=10.1016%2fj.jand.2025.01.004&partnerID=40&md5=aff80581eb288f401f6bdf11a409c999
AB  - Background: Children's consumption of ultra-processed food (UPF) may contribute to inequalities in obesity and wider health. Socioeconomic patterning in younger UK children's UPF intake is unknown. Objective: The aim of this study was to investigate socioeconomic patterning of UK toddlers’ (aged 21 months) and children's (aged 7 years) UPF intake across several household and neighborhood indicators. Design: Secondary analysis of data from a prospective longitudinal cohort study using parent-reported sociodemographic data and 3-day diet diaries. Participants/setting: Participants were children from the UK Gemini study of 4804 twins born in 2007. At ages 21 months and 7 years, 2591 and 592 children, respectively, had at least 2 days of dietary data. Main outcome measures: This study measured percentage energy from UPF at 21 months and 7 years of age, classified using the NOVA system. Statistical analyses performed: Unadjusted linear regression models were run for household socioeconomic position (SEP) composite score; index of multiple deprivation decile; income; occupation level; mother's age; education of mother and partner; and child's ethnicity, sex, and age. Adjusted multivariable linear regression models were adjusted for ethnicity and all SEP indicators except SEP composite score (adjusted 1), in addition to child sex and age (adjusted 2). Missing data were addressed with multiple imputation and inverse probability weighting. CIs and P values were adjusted to account for clustering within families. Results: Children of lower SEP had higher UPF intake across several indicators. Mother's education was the strongest predictor; postgraduate education was associated with 8.64% (95% CI –12.08% to –5.20%; P < .001) and 10.12% (95% CI, –15.68% to –4.56%; P < .001) less energy from UPF at 21 months and 7 years, respectively, compared with no educational qualifications in adjusted model 2. Conclusions: UK children from more disadvantaged backgrounds consumed a greater proportion of their energy from UPF. Mother's education seemed to be the most influential factor. Socioeconomic inequalities, particularly in maternal education, may drive disparities in diet quality and associated health outcomes. Addressing these gaps is essential to reduce childhood obesity and improve long-term health in socioeconomically disadvantaged populations. © 2025 The Authors
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Zhou, Y.
AU  - Di Eugenio, B.
AU  - Cheng, L.
TI  - Unveiling Performance Challenges of Large Language Models in Low-Resource Healthcare: A Demographic Fairness Perspective
PY  - 2025
T2  - Proceedings - International Conference on Computational Linguistics, COLING
VL  - Part F206484-1
SP  - 7266
EP  - 7278
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218488336&partnerID=40&md5=2b0608db3ccfd858d095fff916a8781d
AB  - This paper studies the performance of large language models (LLMs), particularly regarding demographic fairness, in solving real-world healthcare tasks. We evaluate state-of-the-art LLMs with three prevalent learning frameworks across six diverse healthcare tasks and find significant challenges in applying LLMs to real-world healthcare tasks and persistent fairness issues across demographic groups. We also find that explicitly providing demographic information yields mixed results, while LLM's ability to infer such details raises concerns about biased health predictions. Utilizing LLMs as autonomous agents with access to up-to-date guidelines does not guarantee performance improvement. We believe these findings reveal the critical limitations of LLMs as concerns healthcare fairness and the urgent need for specialized research in this area. WARNING: This paper contains model outputs that may be considered offensive in nature. © 2025 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Lee, Y.
AU  - Chang, C.-H.
AU  - Yang, C.C.
TI  - Enhancing Patient-Physician Communication: Simulating African American Vernacular English in Medical Diagnostics with Large Language Models
PY  - 2025
T2  - Journal of Healthcare Informatics Research
DO  - 10.1007/s41666-025-00194-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000800870&doi=10.1007%2fs41666-025-00194-9&partnerID=40&md5=52cdfb81ce365f46ce9049eb7101ef97
AB  - Effective communication is crucial in reducing health disparities. However, linguistic differences, such as African American Vernacular English (AAVE), can lead to communication gaps between patients and physicians, negatively affecting care and outcomes. This study examines whether large language models (LLMs), specifically GPT-4 and Llama 3.3, can replicate AAVE in simulated clinical dialogues to improve cultural sensitivity. We tested four prompt types—BaseP, DemoP, LingP, and CompP—using United States Medical Licensing Examination (USMLE) case simulations. Statistical analyses on the models’ outputs showed a significant difference among prompt types for both GPT-4 (F(2,70) = 6.218, p = 0.003) and Llama 3.3 (F(2,70) = 12.124, p < 0.001), indicating that including demographic information and/or explicit AAVE cues influences each model’s output. Combining demographic and linguistic cues (CompP) yielded the highest mean AAVE feature counts (e.g., 9.83 for GPT-4 vs. 16.06 for Llama 3.3), although neither model fully captured the diversity of AAVE. Moreover, simply mentioning African American demographics triggers extra informal forms, suggesting built-in stereotypes or biases in both models. Overall, these findings highlight the promise of LLMs for culturally sensitive healthcare communication, while underscoring the need for continued refinement to address stereotypes and more accurately represent diverse linguistic styles. © The Author(s) 2025.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Huang, X.
AU  - Han, Y.
AU  - Li, Y.
AU  - Li, R.
AU  - Wu, P.
AU  - Zhang, K.
TI  - CmEAA: Cross-modal Enhancement and Alignment Adapter for Radiology Report Generation
PY  - 2025
T2  - Proceedings - International Conference on Computational Linguistics, COLING
VL  - Part F206484-1
SP  - 8546
EP  - 8556
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218490368&partnerID=40&md5=452c11c5732b3c109cc1dd33eeea49b1
AB  - Automatic radiology report generation is pivotal in reducing the workload of radiologists, while simultaneously improving diagnostic accuracy and operational efficiency. Current methods face significant challenges, including the effective alignment of medical visual features with textual features and the mitigation of data bias. In this paper, we propose a method for radiology report generation that utilizes a Cross-modal Enhancement and Alignment Adapter (CmEAA) to connect a vision encoder with a frozen large language model. Specifically, we introduce two novel modules within CmEAA: Cross-modal Feature Enhancement (CFE) and Neural Mutual Information Aligner (NMIA). CFE extracts observation-related contextual features to enhance the visual features of lesions and abnormal regions in radiology images through a cross-modal enhancement Transformer. NMIA maximizes neural mutual information between visual and textual representations within a low-dimensional alignment embedding space during training and provides potential global alignment visual representations during inference. Additionally, a weights generator is designed to enable the dynamic adaptation of cross-modal enhanced features and vanilla visual features. Experimental results on two prevailing datasets, namely, IU X-Ray and MIMIC-CXR, demonstrate that the proposed model outperforms previous state-of-the-art methods. © 2025 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Balt, E.
AU  - Salmi, S.
AU  - Bhulai, S.
AU  - Vrinzen, S.
AU  - Eikelenboom, M.
AU  - Gilissen, R.
AU  - Creemers, D.
AU  - Popma, A.
AU  - Mérelle, S.
TI  - Deductively coding psychosocial autopsy interview data using a few-shot learning large language model
PY  - 2025
T2  - Frontiers in Public Health
VL  - 13
C7  - 1512537
DO  - 10.3389/fpubh.2025.1512537
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000102310&doi=10.3389%2ffpubh.2025.1512537&partnerID=40&md5=15cac329627683426efdf93b62bce17a
AB  - Background: Psychosocial autopsy is a retrospective study of suicide, aimed to identify emerging themes and psychosocial risk factors. It typically relies heavily on qualitative data from interviews or medical documentation. However, qualitative research has often been scrutinized for being prone to bias and is notoriously time- and cost-intensive. Therefore, the current study aimed to investigate if a Large Language Model (LLM) can be feasibly integrated with qualitative research procedures, by evaluating the performance of the model in deductively coding and coherently summarizing interview data obtained in a psychosocial autopsy. Methods: Data from 38 semi-structured interviews conducted with individuals bereaved by the suicide of a loved one was deductively coded by qualitative researchers and a server-installed LLAMA3 large language model. The model performance was evaluated in three tasks: (1) binary classification of coded segments, (2) independent classification using a sliding window approach, and (3) summarization of coded data. Intercoder agreement scores were calculated using Cohen’s Kappa, and the LLM’s summaries were qualitatively assessed using the Constant Comparative Method. Results: The results showed that the LLM achieved substantial agreement with the researchers for the binary classification (accuracy: 0.84) and the sliding window task (accuracy: 0.67). The performance had large variability across codes. LLM summaries were typically rich enough for subsequent analysis by the researcher, with around 80% of the summaries being rated independently by two researchers as ‘adequate’ or ‘good.’ Emerging themes in the qualitative assessment of the summaries included unsolicited elaboration and hallucination. Conclusion: State-of-the-art LLMs show great potential to support researchers in deductively coding complex interview data, which would alleviate the investment of time and resources. Integrating models with qualitative research procedures can facilitate near real-time monitoring. Based on the findings, we recommend a collaborative model, whereby the LLM’s deductive coding is complemented by review, inductive coding and further interpretation by a researcher. Future research may aim to replicate the findings in different contexts and evaluate models with a larger context size. Copyright © 2025 Balt, Salmi, Bhulai, Vrinzen, Eikelenboom, Gilissen, Creemers, Popma and Mérelle.
PB  - Frontiers Media SA
C2  - 40046117
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Huh, K.Y.
AU  - Song, I.
AU  - Kim, Y.
AU  - Park, J.
AU  - Ryu, H.
AU  - Koh, J.
AU  - Yu, K.-S.
AU  - Kim, K.H.
AU  - Lee, S.
TI  - Exploration of Using an Open-Source Large Language Model for Analyzing Trial Information: A Case Study of Clinical Trials With Decentralized Elements
PY  - 2025
T2  - Clinical and Translational Science
VL  - 18
IS  - 3
C7  - e70183
DO  - 10.1111/cts.70183
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219754656&doi=10.1111%2fcts.70183&partnerID=40&md5=be44eeb4a22100865e67fd7d93aaca39
AB  - Despite interest in clinical trials with decentralized elements (DCTs), analysis of their trends in trial registries is lacking due to heterogeneous designs and unstandardized terms. We explored Llama 3, an open-source large language model, to efficiently evaluate these trends. Trial data were sourced from Aggregate Analysis of ClinicalTrials.gov, focusing on drug trials conducted between 2018 and 2023. We utilized three Llama 3 models with a different number of parameters: 8b (model 1), fine-tuned 8b (model 2) with curated data, and 70b (model 3). Prompt engineering enabled sophisticated tasks such as classification of DCTs with explanations and extracting decentralized elements. Model performance, evaluated on a 3-month exploratory test dataset, demonstrated that sensitivity could be improved after fine-tuning from 0.0357 to 0.5385. Low positive predictive value in the fine-tuned model 2 could be improved by focusing on trials with DCT-associated expressions from 0.5385 to 0.9167. However, the extraction of decentralized elements was only properly performed by model 3, which had a larger number of parameters. Based on the results, we screened the entire 6-year dataset after applying DCT-associated expressions. After the subsequent application of models 2 and 3, we identified 692 DCTs. We found that a total of 213 trials were classified as phase 2, followed by 162 phase 4 trials, 112 phase 3 trials, and 92 phase 1 trials. In conclusion, our study demonstrated the potential of large language models for analyzing clinical trial information not structured in a machine-readable format. Managing potential biases during model application is crucial. © 2025 The Author(s). Clinical and Translational Science published by Wiley Periodicals LLC on behalf of American Society for Clinical Pharmacology and Therapeutics.
PB  - John Wiley and Sons Inc
C2  - 40025837
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - McBain, R.K.
AU  - Cantor, J.H.
AU  - Zhang, L.A.
AU  - Baker, O.
AU  - Zhang, F.
AU  - Halbisen, A.
AU  - Kofner, A.
AU  - Breslau, J.
AU  - Stein, B.
AU  - Mehrotra, A.
AU  - Yu, H.
TI  - Competency of Large Language Models in Evaluating Appropriate Responses to Suicidal Ideation: Comparative Study
PY  - 2025
T2  - Journal of Medical Internet Research
VL  - 27
C7  - e67891
DO  - 10.2196/67891
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000305332&doi=10.2196%2f67891&partnerID=40&md5=af2b26f0a0acf2bd140f390ddca3d9e4
AB  - Background: With suicide rates in the United States at an all-time high, individuals experiencing suicidal ideation are increasingly turning to large language models (LLMs) for guidance and support. Objective: The objective of this study was to assess the competency of 3 widely used LLMs to distinguish appropriate versus inappropriate responses when engaging individuals who exhibit suicidal ideation. Methods: This observational, cross-sectional study evaluated responses to the revised Suicidal Ideation Response Inventory (SIRI-2) generated by ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro. Data collection and analyses were conducted in July 2024. A common training module for mental health professionals, SIRI-2 provides 24 hypothetical scenarios in which a patient exhibits depressive symptoms and suicidal ideation, followed by two clinician responses. Clinician responses were scored from –3 (highly inappropriate) to +3 (highly appropriate). All 3 LLMs were provided with a standardized set of instructions to rate clinician responses. We compared LLM responses to those of expert suicidologists, conducting linear regression analyses and converting LLM responses to z scores to identify outliers (z score>1.96 or <–1.96; P<0.05). Furthermore, we compared final SIRI-2 scores to those produced by health professionals in prior studies. Results: All 3 LLMs rated responses as more appropriate than ratings provided by expert suicidologists. The item-level mean difference was 0.86 for ChatGPT (95% CI 0.61-1.12; P<.001), 0.61 for Claude (95% CI 0.41-0.81; P<.001), and 0.73 for Gemini (95% CI 0.35-1.11; P<.001). In terms of z scores, 19% (9 of 48) of ChatGPT responses were outliers when compared to expert suicidologists. Similarly, 11% (5 of 48) of Claude responses were outliers compared to expert suicidologists. Additionally, 36% (17 of 48) of Gemini responses were outliers compared to expert suicidologists. ChatGPT produced a final SIRI-2 score of 45.7, roughly equivalent to master’s level counselors in prior studies. Claude produced an SIRI-2 score of 36.7, exceeding prior performance of mental health professionals after suicide intervention skills training. Gemini produced a final SIRI-2 score of 54.5, equivalent to untrained K-12 school staff. Conclusions: Current versions of 3 major LLMs demonstrated an upward bias in their evaluations of appropriate responses to suicidal ideation; however, 2 of the 3 models performed equivalent to or exceeded the performance of mental health professionals. ©Ryan K McBain, Jonathan H Cantor, Li Ang Zhang, Olesya Baker, Fang Zhang, Alyssa Halbisen, Aaron Kofner, Joshua Breslau, Bradley Stein, Ateev Mehrotra, Hao Yu.
PB  - JMIR Publications Inc.
C2  - 40053817
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Gore, R.
AU  - Safaee, M.M.
AU  - Lynch, C.J.
AU  - Ames, C.P.
TI  - A Spine-Specific Lexicon for the Sentiment Analysis of Interviews with Adult Spinal Deformity Patients Correlates with SF-36, SRS-22, and ODI Scores: A Pilot Study of 25 Patients
PY  - 2025
T2  - Information (Switzerland)
VL  - 16
IS  - 2
C7  - 90
DO  - 10.3390/info16020090
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218439968&doi=10.3390%2finfo16020090&partnerID=40&md5=8be0633b3b027ed5f02be01043981b66
AB  - Classic health-related quality of life (HRQOL) metrics are cumbersome, time-intensive, and subject to biases based on the patient’s native language, educational level, and cultural values. Natural language processing (NLP) converts text into quantitative metrics. Sentiment analysis enables subject matter experts to construct domain-specific lexicons that assign a value of either negative (−1) or positive (1) to certain words. The growth of telehealth provides opportunities to apply sentiment analysis to transcripts of adult spinal deformity patients’ visits to derive a novel and less biased HRQOL metric. In this study, we demonstrate the feasibility of constructing a spine-specific lexicon for sentiment analysis to derive an HRQOL metric for adult spinal deformity patients from their preoperative telehealth visit transcripts. We asked each of twenty-five (25) adult patients seven open-ended questions about their spinal conditions, treatment, and quality of life during telehealth visits. We analyzed the Pearson correlation between our sentiment analysis HRQOL metric and established HRQOL metrics (the Scoliosis Research Society-22 questionnaire [SRS-22], 36-Item Short Form Health Survey [SF-36], and Oswestry Disability Index [ODI]). The results show statistically significant correlations (0.43–0.74) between our sentiment analysis metric and the conventional metrics. This provides evidence that applying NLP techniques to patient transcripts can yield an effective HRQOL metric. © 2025 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Alqithami, S.
TI  - Integrating Sentiment Analysis and Reinforcement Learning for Equitable Disaster Response: A Novel Approach
PY  - 2025
T2  - Sustainability (Switzerland)
VL  - 17
IS  - 3
C7  - 1072
DO  - 10.3390/su17031072
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217631524&doi=10.3390%2fsu17031072&partnerID=40&md5=c442fb9ca8f182f2912a044be6de69f1
AB  - Efficient disaster response requires dynamic and adaptive resource allocation strategies that account for evolving public needs, real-time sentiment, and sustainability concerns. In this study, a sentiment-driven framework is proposed, integrating reinforcement learning, natural language processing, and gamification to optimize the distribution of resources such as water, food, medical aid, shelter, and electricity during disaster scenarios. The model leverages real-time social media data to capture public sentiment, combines it with geospatial and temporal information, and then trains a reinforcement learning agent to maximize both community satisfaction and equitable resource allocation. The model achieved equity scores of up to (Formula presented.) and improved satisfaction metrics by (Formula presented.), which outperforms static allocation baselines. By incorporating a gamified simulation platform, stakeholders can interactively refine policies and address the inherent uncertainties of disaster events. This approach highlights the transformative potential of using advanced artificial intelligence techniques to enhance adaptability, promote sustainability, and foster collaborative decision-making in humanitarian aid efforts. © 2025 by the author.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Gao, Y.
AU  - Li, R.
AU  - Croxford, E.
AU  - Caskey, J.
AU  - Patterson, B.W.
AU  - Churpek, M.
AU  - Miller, T.
AU  - Dligach, D.
AU  - Afshar, M.
TI  - Leveraging Medical Knowledge Graphs Into Large Language Models for Diagnosis Prediction: Design and Application Study
PY  - 2025
T2  - JMIR AI
VL  - 4
IS  - 1
C7  - e58670
DO  - 10.2196/58670
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219233420&doi=10.2196%2f58670&partnerID=40&md5=7a3c67780b10c63f8e01b1c095049412
AB  - Background: Electronic health records (EHRs) and routine documentation practices play a vital role in patients’ daily care, providing a holistic record of health, diagnoses, and treatment. However, complex and verbose EHR narratives can overwhelm health care providers, increasing the risk of diagnostic inaccuracies. While large language models (LLMs) have showcased their potential in diverse language tasks, their application in health care must prioritize the minimization of diagnostic errors and the prevention of patient harm. Integrating knowledge graphs (KGs) into LLMs offers a promising approach because structured knowledge from KGs could enhance LLMs’ diagnostic reasoning by providing contextually relevant medical information. Objective: This study introduces DR.KNOWS (Diagnostic Reasoning Knowledge Graph System), a model that integrates Unified Medical Language System–based KGs with LLMs to improve diagnostic predictions from EHR data by retrieving contextually relevant paths aligned with patient-specific information. Methods: DR.KNOWS combines a stack graph isomorphism network for node embedding with an attention-based path ranker to identify and rank knowledge paths relevant to a patient’s clinical context. We evaluated DR.KNOWS on 2 real-world EHR datasets from different geographic locations, comparing its performance to baseline models, including QuickUMLS and standard LLMs (Text-to-Text Transfer Transformer and ChatGPT). To assess diagnostic reasoning quality, we designed and implemented a human evaluation framework grounded in clinical safety metrics. Results: DR.KNOWS demonstrated notable improvements over baseline models, showing higher accuracy in extracting diagnostic concepts and enhanced diagnostic prediction metrics. Prompt-based fine-tuning of Text-to-Text Transfer Transformer with DR.KNOWS knowledge paths achieved the highest ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation–Longest Common Subsequence) and concept unique identifier F1-scores, highlighting the benefits of KG integration. Human evaluators found the diagnostic rationales of DR.KNOWS to be aligned strongly with correct clinical reasoning, indicating improved abstraction and reasoning. Recognized limitations include potential biases within the KG data, which we addressed by emphasizing case-specific path selection and proposing future bias-mitigation strategies. Conclusions: DR.KNOWS offers a robust approach for enhancing diagnostic accuracy and reasoning by integrating structured KG knowledge into LLM-based clinical workflows. Although further work is required to address KG biases and extend generalizability, DR.KNOWS represents progress toward trustworthy artificial intelligence–driven clinical decision support, with a human evaluation framework focused on diagnostic safety and alignment with clinical standards. © Yanjun Gao, Ruizhe Li, Emma Croxford, John Caskey, Brian W Patterson, Matthew Churpek, Timothy Miller, Dmitriy Dligach, Majid Afshar.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ng, M.Y.
AU  - Helzer, J.
AU  - Pfeffer, M.A.
AU  - Seto, T.
AU  - Hernandez-Boussard, T.
TI  - Development of secure infrastructure for advancing generative artificial intelligence research in healthcare at an academic medical center
PY  - 2025
T2  - Journal of the American Medical Informatics Association
VL  - 32
IS  - 3
SP  - 586
EP  - 588
DO  - 10.1093/jamia/ocaf005
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218412343&doi=10.1093%2fjamia%2focaf005&partnerID=40&md5=a3c4e8291b55777b1ab735af91776551
AB  - Background: Generative AI, particularly large language models (LLMs), holds great potential for improving patient care and operational efficiency in healthcare. However, the use of LLMs is complicated by regulatory concerns around data security and patient privacy. This study aimed to develop and evaluate a secure infrastructure that allows researchers to safely leverage LLMs in healthcare while ensuring HIPAA compliance and promoting equitable AI. Materials and Methods: We implemented a private Azure OpenAI Studio deployment with secure API-enabled endpoints for researchers. Two use cases were explored, detecting falls from electronic health records (EHR) notes and evaluating bias in mental health prediction using fairness-aware prompts. Results: The framework provided secure, HIPAA-compliant API access to LLMs, allowing researchers to handle sensitive data safely. Both use cases highlighted the secure infrastructure's capacity to protect sensitive patient data while supporting innovation. Discussion and Conclusion: This centralized platform presents a scalable, secure, and HIPAA-compliant solution for healthcare institutions aiming to integrate LLMs into clinical research.  © 2025 The Author(s).
PB  - Oxford University Press
C2  - 39836496
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - De la Iglesia, I.
AU  - Goenaga, I.
AU  - Ramirez-Romero, J.
AU  - Villa-Gonzalez, J.M.
AU  - Goikoetxea, J.
AU  - Barrena, A.
TI  - Ranking Over Scoring: Towards Reliable and Robust Automated Evaluation of LLM-Generated Medical Explanatory Arguments
PY  - 2025
T2  - Proceedings - International Conference on Computational Linguistics, COLING
VL  - Part F206484-1
SP  - 9456
EP  - 9471
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218502681&partnerID=40&md5=ed063a6dfd79703c5ebd7046cb1a0cef
AB  - Evaluating LLM-generated text has become a key challenge, especially in domain-specific contexts like the medical field. This work introduces a novel evaluation methodology for LLM-generated medical explanatory arguments, relying on Proxy Tasks and rankings to closely align results with human evaluation criteria, overcoming the biases typically seen in LLMs used as judges. We demonstrate that the proposed evaluators are robust against adversarial attacks, including the assessment of non-argumentative text. Additionally, the human-crafted arguments needed to train the evaluators are minimized to just one example per Proxy Task. By examining multiple LLM-generated arguments, we establish a methodology for determining whether a Proxy Task is suitable for evaluating LLM-generated medical explanatory arguments, requiring only five examples and two human experts. The Proxy Tasks, LM evaluators, and the code are available for reproducibility. © 2025 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Farrow, L.
AU  - Anderson, L.
AU  - Zhong, M.
TI  - Managing class imbalance in the training of a large language model to predict patient selection for total knee arthroplasty: Results from the Artificial intelligence to Revolutionise the patient Care pathway in Hip and knEe aRthroplastY (ARCHERY) project
PY  - 2025
T2  - Knee
VL  - 54
SP  - 1
EP  - 8
DO  - 10.1016/j.knee.2025.02.007
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218854087&doi=10.1016%2fj.knee.2025.02.007&partnerID=40&md5=057f436a0220f39efcc9cff0503c2338
AB  - Introduction: This study set out to test the efficacy of different techniques used to manage to class imbalance, a type of data bias, in application of a large language model (LLM) to predict patient selection for total knee arthroplasty (TKA). Methods: This study utilised data from the Artificial Intelligence to Revolutionise the Patient Care Pathway in Hip and Knee Arthroplasty (ARCHERY) project (ISRCTN18398037). Data included the pre-operative radiology reports of patients referred to secondary care for knee-related complaints from within the North of Scotland. A clinically based LLM (GatorTron) was trained regarding prediction of selection for TKA. Three methods for managing class imbalance were assessed: a standard model, use of class weighting, and majority class undersampling. Results: A total of 7707 individual knee radiology reports were included (dated from 2015 to 2022). The mean text length was 74 words (range 26–275). Only 910/7707 (11.8%) patients underwent TKA surgery (the designated ‘minority class’). Class weighting technique performed better for minority class discrimination and calibration compared with the other two techniques (Recall 0.61/AUROC 0.73 for class weighting compared with 0.54/0.70 and 0.59/0.72 for the standard model and majority class undersampling, respectively. There was also significant data loss for majority class undersampling when compared with class-weighting. Conclusion: Use of class-weighting appears to provide the optimal method of training a an LLM to perform analytical tasks on free-text clinical information in the face of significant data bias (‘class imbalance’). Such knowledge is an important consideration in the development of high-performance clinical AI models within Trauma and Orthopaedics. © 2025 The Author(s)
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Miao, S.
AU  - Ji, P.
AU  - Zhu, Y.
AU  - Meng, H.
AU  - Jing, M.
AU  - Sheng, R.
AU  - Zhang, X.
AU  - Ding, H.
AU  - Guo, J.
AU  - Gao, W.
AU  - Yang, G.
AU  - Liu, Y.
TI  - The Construction and Application of a Clinical Decision Support System for Cardiovascular Diseases: Multimodal Data-Driven Development and Validation Study
PY  - 2025
T2  - JMIR Medical Informatics
VL  - 13
C7  - e63186
DO  - 10.2196/63186
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000186304&doi=10.2196%2f63186&partnerID=40&md5=c2a2d87190316613b36084de256a1dd1
AB  - Background: Due to the acceleration of the aging population and the prevalence of unhealthy lifestyles, the incidence of cardiovascular diseases (CVDs) in China continues to grow. However, due to the uneven distribution of medical resources across regions and significant disparities in diagnostic and treatment levels, the diagnosis and management of CVDs face considerable challenges. Objective: The purpose of this study is to build a cardiovascular diagnosis and treatment knowledge base by using new technology, form an auxiliary decision support system, and integrate it into the doctor’s workstation, to improve the assessment rate and treatment standardization rate. This study offers new ideas for the prevention and management of CVDs. Methods: This study designed a clinical decision support system (CDSS) with data, learning, knowledge, and application layers. It integrates multimodal data from hospital laboratory information systems, hospital information systems, electronic medical records, electrocardiography, nursing, and other systems to build a knowledge model. The unstructured data were segmented using natural language processing technology, and medical entity words and entity combination relationships were extracted using IDCNN (iterated dilated convolutional neural network) and TextCNN (text convolutional neural network). The CDSS refers to global CVD assessment indicators to design quality control strategies and an intelligent treatment plan recommendation engine map, establishing a big data analysis platform to achieve multidimensional, visualized data statistics for management decision support. Results: The CDSS system is embedded and interfaced with the physician workstation, triggering in real-time during the clinical diagnosis and treatment process. It establishes a 3-tier assessment control through pop-up windows and screen domination operations. Based on the intelligent diagnostic and treatment reminders of the CDSS, patients are given intervention treatments. The important risk assessment and diagnosis rate indicators significantly improved after the system came into use, and gradually increased within 2 years. The indicators of mandatory control, directly became 100% after the CDSS was online. The CDSS enhanced the standardization of clinical diagnosis and treatment. Conclusions: This study establishes a specialized knowledge base for CVDs, combined with clinical multimodal information, to intelligently assess and stratify cardiovascular patients. It automatically recommends intervention treatments based on assessments and clinical characterizations, proving to be an effective exploration of using a CDSS to build a disease-specific intelligent system. © Shumei Miao, Pei Ji, Yongqian Zhu, Haoyu Meng, Mang Jing, Rongrong Sheng, Xiaoliang Zhang, Hailong Ding, Jianjun Guo, Wen Gao, Guanyu Yang, Yun Liu.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Bucur, A.-M.
AU  - Moldovan, A.-C.
AU  - Parvatikar, K.
AU  - Zampieri, M.
AU  - Khudabukhsh, A.R.
AU  - Dinu, L.P.
TI  - On the State of NLP Approaches to Modeling Depression in Social Media: A Post-COVID-19 Outlook
PY  - 2025
T2  - IEEE Journal of Biomedical and Health Informatics
DO  - 10.1109/JBHI.2025.3540507
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000485311&doi=10.1109%2fJBHI.2025.3540507&partnerID=40&md5=299c11a5e6e7db5216b0be2965fd90c5
AB  - Computational approaches to predicting mental health conditions in social media have been substantially explored in the past years. Multiple reviews have been published on this topic, providing the community with comprehensive accounts of the research in this area. Among all mental health conditions, depression is the most widely studied due to its worldwide prevalence. The COVID-19 global pandemic, starting in early 2020, has had a great impact on mental health worldwide. Harsh measures employed by governments to slow the spread of the virus (e.g., lockdowns) and the subsequent economic downturn experienced in many countries have significantly impacted people's lives and mental health. Studies have shown a substantial increase of above 50% in the rate of depression in the population. In this context, we present a review on natural language processing (NLP) approaches to modeling depression in social media, providing the reader with a post-COVID-19 outlook. This review contributes to the understanding of the impacts of the pandemic on modeling depression in social media. We outline how state-of-the-art approaches and new datasets have been used in the context of the COVID-19 pandemic. Finally, we also discuss ethical issues in collecting and processing mental health data, considering fairness, accountability, and ethics. © 2013 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Nanor, M.A.
TI  - Determinants of households residential mobility decision in Kumasi Ghana
PY  - 2025
T2  - Environment, Development and Sustainability
C7  - e397
DO  - 10.1007/s10668-025-06046-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218270292&doi=10.1007%2fs10668-025-06046-1&partnerID=40&md5=0e17bf2c03fae6a7995f851f4db2cbb0
AB  - Household relocation choices are critical in shaping urban socio-economic landscapes, especially within the framework of the Sustainable Development Goals (SDGs). This study investigates the complex factors influencing household relocation decisions in Kumasi, Ghana, an urban center undergoing rapid growth. The aim is to understand how socio-economic conditions, environmental challenges, and urban policy dynamics interact to shape relocation intentions. Using a qualitative approach, this research use qualitative interviews and natural language processing (NLP) technique to capture the diverse push and pull factors motivating relocation. Findings indicate that push factors, such as inadequate housing, urban congestion, and environmental degradation, diminish quality of life and drive relocation. Conversely, pull factors, including employment opportunities, access to education and healthcare, and enhanced urban amenities, attract households to particular areas within Kumasi. This study situates these relocation dynamics within the SDG framework, highlighting the need for sustainable urban development strategies that address residents' aspirations and challenges. The results emphasize that aligning urban planning with principles of inclusivity, equity, and environmental sustainability can enhance urban resilience and community well-being. The study’s insights are valuable for policymakers, urban planners, and stakeholders seeking to foster urban environments conducive to sustainable development, equitable growth, and social inclusion, ultimately advancing progress toward achieving the SDGs. © The Author(s), under exclusive licence to Springer Nature B.V. 2025.
PB  - Springer Science and Business Media B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ben-Zion, Z.
AU  - Witte, K.
AU  - Jagadish, A.K.
AU  - Duek, O.
AU  - Harpaz-Rotem, I.
AU  - Khorsandian, M.-C.
AU  - Burrer, A.
AU  - Seifritz, E.
AU  - Homan, P.
AU  - Schulz, E.
AU  - Spiller, T.R.
TI  - Assessing and alleviating state anxiety in large language models
PY  - 2025
T2  - npj Digital Medicine
VL  - 8
IS  - 1
C7  - 132
DO  - 10.1038/s41746-025-01512-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000059115&doi=10.1038%2fs41746-025-01512-6&partnerID=40&md5=118a7dd92bf81db8892b7ee348d08ae9
AB  - The use of Large Language Models (LLMs) in mental health highlights the need to understand their responses to emotional content. Previous research shows that emotion-inducing prompts can elevate “anxiety” in LLMs, affecting behavior and amplifying biases. Here, we found that traumatic narratives increased Chat-GPT-4’s reported anxiety while mindfulness-based exercises reduced it, though not to baseline. These findings suggest managing LLMs’ “emotional states” can foster safer and more ethical human-AI interactions. © The Author(s) 2025.
PB  - Nature Research
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ramaniharan, A.K.
AU  - Pednekar, A.
AU  - Parikh, N.A.
AU  - Nagaraj, U.D.
AU  - Manhard, M.K.
TI  - A single 1-min brain MRI scan for generating multiple synthetic image contrasts in awake children from quantitative relaxometry maps
PY  - 2025
T2  - Pediatric Radiology
VL  - 55
IS  - 2
C7  - 666020
SP  - 312
EP  - 323
DO  - 10.1007/s00247-024-06113-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212428061&doi=10.1007%2fs00247-024-06113-1&partnerID=40&md5=7ca3e776bf1e4238a049140507405368
AB  - Background: Diagnostically adequate contrast and spatial resolution in brain MRI require prolonged scan times, leading to motion artifacts and image degradation in awake children. Rapid multi-parametric techniques can produce diagnostic images in awake children, which could help to avoid the need for sedation. Objective: To evaluate the utility of a rapid echo-planar imaging (EPI)–based multi-inversion spin and gradient echo (MI-SAGE) technique for generating multi-parametric quantitative brain maps and synthetic contrast images in awake pediatric participants. Materials and methods: In this prospective IRB-approved study, awake research participants 3–10 years old were scanned using MI-SAGE, MOLLI, GRASE, mGRE, and T1-, T2-, T2*-, and FLAIR-weighted sequences. The MI-SAGE T1, T2, and T2* maps and synthetic images were estimated offline. The MI-SAGE parametric values were compared to those from conventional mapping sequences including MOLLI, GRASE, and mGRE, with assessments of repeatability and reproducibility. Synthetic MI-SAGE images and conventional weighted images were reviewed by a neuroradiologist and scored using a 5-point Likert scale. Gray-to-white matter contrast ratios (GWRs) were compared between MI-SAGE synthetic and conventional weighted images. The results were analyzed using the Bland–Altman analysis and intra-class correlation coefficient (ICC). Results: A total of 24 healthy participants aged 3 years to 10 years (mean ± SD, 6.5 ± 1.9; 12 males) completed full imaging exams including the 54-s MI-SAGE acquisition and were included in the analysis. The MI-SAGE T1, T2, and T2* had biases of 32%, -4%, and 23% compared to conventional mapping methods using MOLLI, GRASE, and mGRE, respectively, with moderate to very strong correlations (ICC=0.49–0.99). All MI-SAGE maps exhibited strong to very strong repeatability and reproducibility (ICC=0.80 to 0.99). The synthetic MI-SAGE had average Likert scores of 2.1, 2.1, 2.9, and 2.0 for T1-, T2-, T2*-, and FLAIR-weighted images, respectively, while conventional acquisitions had Likert scores of 3.5, 3.6, 4.6, and 3.8 for T1-, T2-, T2*-, and FLAIR-weighted images, respectively. The MI-SAGE synthetic T1w, T2w, T2*w, and FLAIR GWRs had biases of 17%, 3%, 7%, and 1% compared to the GWR of images from conventional T1w, T2w, T2*w, and FLAIR acquisitions respectively. Conclusion: The derived T1, T2, and T2* maps were correlated with conventional mapping methods and showed strong repeatability and reproducibility. While synthetic MI-SAGE images had greater susceptibility artifacts and lower Likert scores than conventional images, the MI-SAGE technique produced synthetic weighted images with contrasts similar to conventional weighted images and achieved a ten-fold reduction in scan time. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.
PB  - Springer Nature
C2  - 39692886
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Atmakuru, A.
AU  - Shahini, A.
AU  - Chakraborty, S.
AU  - Seoni, S.
AU  - Salvi, M.
AU  - Hafeez-Baig, A.
AU  - Rashid, S.
AU  - Tan, R.S.
AU  - Barua, P.D.
AU  - Molinari, F.
AU  - Acharya, U.R.
TI  - Artificial intelligence-based suicide prevention and prediction: A systematic review (2019–2023)
PY  - 2025
T2  - Information Fusion
VL  - 114
C7  - 102673
DO  - 10.1016/j.inffus.2024.102673
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204091360&doi=10.1016%2fj.inffus.2024.102673&partnerID=40&md5=274a0486ad187e8316e1763c7580eb24
AB  - Suicide is a major global public health concern, and the application of artificial intelligence (AI) methods, such as natural language processing (NLP), machine learning (ML), and deep learning (DL), has shown promise in advancing suicide prediction and prevention efforts. Recent advancements in AI – particularly NLP and DL have opened up new avenues of research in suicide prediction and prevention. While several papers have reviewed specific detection techniques like NLP or DL, there has been no recent study that acts as a one-stop-shop, providing a comprehensive overview of all AI-based studies in this field. In this work, we conduct a systematic literature review to identify relevant studies published between 2019 and 2023, resulting in the inclusion of 156 studies. We provide a comprehensive overview of the current state of research conducted on AI-driven suicide prevention and prediction, focusing on different data types and AI techniques employed. We discuss the benefits and challenges of these approaches and propose future research directions to improve the practical application of AI in suicide research. AI is highly capable of improving the accuracy and efficiency of risk assessment, enabling personalized interventions, and enhancing our understanding of risk and protective factors. Multidisciplinary approaches combining diverse data sources and AI methods can help identify individuals at risk by analyzing social media content, patient histories, and data from mobile devices, enabling timely intervention. However, challenges related to data privacy, algorithmic bias, model interpretability, and real-world implementation must be addressed to realize the full potential of these technologies. Future research should focus on integrating prediction and prevention strategies, harnessing multimodal data, and expanding the scope to include diverse populations. Collaboration across disciplines and stakeholders is essential to ensure that AI-driven suicide prevention and prediction efforts are ethical, culturally sensitive, and person-centered. © 2024 Elsevier B.V.
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Bui, N.
AU  - Nguyen, G.
AU  - Nguyen, N.
AU  - Vo, B.
AU  - Vo, L.
AU  - Huynh, T.
AU  - Tang, A.
AU  - Tran, V.N.
AU  - Huynh, T.
AU  - Nguyen, H.Q.
AU  - Dinh, M.
TI  - Fine-tuning large language models for improved health communication in low-resource languages
PY  - 2025
T2  - Computer Methods and Programs in Biomedicine
VL  - 263
C7  - 108655
DO  - 10.1016/j.cmpb.2025.108655
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218354763&doi=10.1016%2fj.cmpb.2025.108655&partnerID=40&md5=1f412394ea4b574ccda2d45df676eaa8
AB  - Background: The reported study illustrates a methodology for compiling training datasets to fine-tune Large Language Models (LLMs) for healthcare information in Vietnamese, a low-resource language. The objective is to bridge the gap in medical information accessibility and enhance healthcare communication in developing countries by adapting LLMs to specific linguistic nuances and domain needs. Method: The methodology involves selecting a base model, compiling a domain-specific dataset, and fine-tuning the model with this dataset. Three open-source models were selected. The dataset, comprising approximately 337,000 prompt-response pairs in Vietnamese, was compiled using existing datasets, data crawled from Vietnamese medical online forums, and distilled from Vietnamese medical textbooks. The three models were fine-tuned using the Low-Rank adaptation (LoRA) and Quantized Low-Rank adaptation (QLoRA) techniques. Models’ performances were evaluated using BertScore score, Rouge-L score, and the "LLM-as-a-Judge" method. Results: The fine-tuned models showed enhancements in performance over their base versions across evaluation metrics in BertScore score, Rouge-L score and “LLM-as-a-Judge” method, confirming the effectiveness of the fine-tuning process. This study details the process of fine-tuning open-source LLMs for health information inquiries in Vietnamese, demonstrating its potential to improve healthcare communication in low-resource languages. Deploying the fine-tuned LLM on-premise enhances data privacy and security. However, the significant computing power and costs required pose challenges, especially for organizations in developing countries. Conclusion: This case study highlights the unique challenges faced by developing countries using low-resource languages. Initiatives are needed to emphasize efforts to bridge healthcare gaps in underserved areas and contribute to global health equity. © 2025 The Author(s)
PB  - Elsevier Ireland Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Bevara, R.V.K.
AU  - Mannuru, N.R.
AU  - Karedla, S.P.
AU  - Lund, B.
AU  - Xiao, T.
AU  - Pasem, H.
AU  - Dronavalli, S.C.
AU  - Rupeshkumar, S.
TI  - Resume2Vec: Transforming Applicant Tracking Systems with Intelligent Resume Embeddings for Precise Candidate Matching
PY  - 2025
T2  - Electronics (Switzerland)
VL  - 14
IS  - 4
C7  - 794
DO  - 10.3390/electronics14040794
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218913856&doi=10.3390%2felectronics14040794&partnerID=40&md5=032e5263e938e81667e52133133e376b
AB  - Conventional Applicant Tracking Systems (ATSs) encounter considerable constraints in accurately aligning resumes with job descriptions (JD), especially in handling unstructured data and intricate qualifications. We provide Resume2Vec, an innovative method that utilizes transformer-based deep learning models, including encoders (BERT, RoBERTa, and DistilBERT) and decoders (GPT, Gemini, and Llama), to create embeddings for resumes and job descriptions, employing cosine similarity for evaluation. Our methodology integrates quantitative analysis via embedding-based evaluation with qualitative human assessment across several professional fields. Experimental findings indicate that Resume2Vec outperformed conventional ATS systems, achieving enhancements of up to 15.85% in Normalized Discounted Cumulative Gain (nDCG) and 15.94% in Ranked Biased Overlap (RBO) scores, especially within the mechanical engineering and health and fitness domains. Although conventional the ATS exhibited slightly superior nDCG scores in operations management and software testing, Resume2Vec consistently displayed a more robust alignment with human preferences across the majority of domains, as indicated by the RBO metrics. This research demonstrates that Resume2Vec is a powerful and scalable method for matching resumes to job descriptions, effectively overcoming the shortcomings of traditional systems, while preserving a high alignment with human evaluation criteria. The results indicate considerable promise for transformer-based methodologies in enhancing recruiting technology, facilitating more efficient and precise candidate selection procedures. © 2025 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - He, K.
AU  - Mao, R.
AU  - Lin, Q.
AU  - Ruan, Y.
AU  - Lan, X.
AU  - Feng, M.
AU  - Cambria, E.
TI  - A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics
PY  - 2025
T2  - Information Fusion
VL  - 118
C7  - 102963
DO  - 10.1016/j.inffus.2025.102963
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216023945&doi=10.1016%2fj.inffus.2025.102963&partnerID=40&md5=9099a43d505ff5c013992a989fc31c14
AB  - The utilization of large language models (LLMs) for Healthcare has generated both excitement and concern due to their ability to effectively respond to free-text queries with certain professional knowledge. This survey outlines the capabilities of the currently developed Healthcare LLMs and explicates their development process, to provide an overview of the development road map from traditional Pretrained Language Models (PLMs) to LLMs. Specifically, we first explore the potential of LLMs to enhance the efficiency and effectiveness of various Healthcare applications highlighting both the strengths and limitations. Secondly, we conduct a comparison between the previous PLMs and the latest LLMs, and summarize related Healthcare training data, learning methods, and usage. Finally, the unique concerns associated with deploying LLMs are investigated, particularly regarding fairness, accountability, transparency, and ethics. Besides, we support researchers by compiling a collection of open-source resources1. Summarily, we contend that a significant paradigm shift is underway, transitioning from PLMs to LLMs. This shift encompasses a move from discriminative AI approaches to generative AI approaches, as well as a move from model-centered methodologies to data-centered methodologies. We determine that the biggest obstacle of using LLMs in Healthcare are fairness, accountability, transparency and ethics. © 2025 The Authors
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Cibelli Hibben, K.
AU  - Smith, Z.
AU  - Rogers, B.
AU  - Ryan, V.
AU  - Scanlon, P.
AU  - Hoppe, T.
TI  - Semi-Automated Nonresponse Detection for Open-Text Survey Data
PY  - 2025
T2  - Social Science Computer Review
VL  - 43
IS  - 1
SP  - 166
EP  - 190
DO  - 10.1177/08944393241249720
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193015598&doi=10.1177%2f08944393241249720&partnerID=40&md5=682f74333cac1a61aea27538232b8fa9
AB  - Open-ended survey questions can enable researchers to gain insights beyond more commonly used closed-ended question formats by allowing respondents an opportunity to provide information with few constraints and in their own words. Open-ended web probes are also increasingly used to inform the design and evaluation of survey questions. However, open-ended questions are more susceptible to insufficient or irrelevant responses that can be burdensome and time-consuming to identify and remove manually, often resulting in underuse of open-ended questions and, when used, potential inclusion of poor-quality data. To address these challenges, we developed and publicly released the Semi-Automated Nonresponse Detection for Survey text (SANDS), an item nonresponse detection approach based on a Bidirectional Transformer for Language Understanding model, fine-tuned using Simple Contrastive Sentence Embedding and targeted human coding, to categorize open-ended text data as valid or likely nonresponse. This approach is powerful in that it uses natural language processing as opposed to existing nonresponse detection approaches that have relied exclusively on rules or regular expressions or used bag-of-words approaches that tend to perform less well on short pieces of text, typos, or uncommon words, often prevalent in open-text survey data. This paper presents the development of SANDS and a quantitative evaluation of its performance and potential bias using open-text responses from a series of web probes as case studies. Overall, the SANDS model performed well in identifying a dataset of likely valid results to be used for quantitative or qualitative analysis, particularly on health-related data. Developed for generalizable use and accessible to others, the SANDS model can greatly improve the efficiency of identifying inadequate and irrelevant open-text responses, offering expanded opportunities for the use of open-text data to inform question design and improve survey data quality. © The Author(s) 2024.
PB  - SAGE Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Fu, B.
AU  - Hadid, A.
AU  - Damer, N.
TI  - Generative AI in the context of assistive technologies: Trends, limitations and future directions
PY  - 2025
T2  - Image and Vision Computing
VL  - 154
C7  - 105347
DO  - 10.1016/j.imavis.2024.105347
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211061272&doi=10.1016%2fj.imavis.2024.105347&partnerID=40&md5=740b8dc473ee604930c53deb53b897eb
AB  - With the tremendous successes of Large Language Models (LLMs) like ChatGPT for text generation and Dall-E for high-quality image generation, generative Artificial Intelligence (AI) models have shown a hype in our society. Generative AI seamlessly delved into different aspects of society ranging from economy, education, legislation, computer science, finance, and even healthcare. This article provides a comprehensive survey on the increased and promising use of generative AI in assistive technologies benefiting different parties, ranging from the assistive system developers, medical practitioners, care workforce, to the people who need the care and the comfort. Ethical concerns, biases, lack of transparency, insufficient explainability, and limited trustworthiness are major challenges when using generative AI in assistive technologies, particularly in systems that impact people directly. Key future research directions to address these issues include creating standardized rules, establishing commonly accepted evaluation metrics and benchmarks for explainability and reasoning processes, and making further advancements in understanding and reducing bias and its potential harms. Beyond showing the current trends of applying generative AI in the scope of assistive technologies in four identified key domains, which include care sectors, medical sectors, helping people in need, and co-working, the survey also discusses the current limitations and provides promising future research directions to foster better integration of generative AI in assistive technologies. © 2024 The Authors
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Madanay, F.
AU  - Bundorf, M.K.
AU  - Ubel, P.A.
TI  - Physician Gender and Patient Perceptions of Interpersonal and Technical Skills in Online Reviews
PY  - 2025
T2  - JAMA network open
VL  - 8
IS  - 2
SP  - e2460018
DO  - 10.1001/jamanetworkopen.2024.60018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218842335&doi=10.1001%2fjamanetworkopen.2024.60018&partnerID=40&md5=698031d1fe78b98fd85a480344c0f791
AB  - Importance: Prior studies have revealed gender differences in workplace assessments of physicians, but little is known about differences by physician gender in patients' online written reviews. Objective: To analyze whether patients' perceptions of their physicians' interpersonal manner and technical competence differ by physician gender and practicing specialty and are associated with review star ratings. Design, Setting, and Participants: This cross-sectional study sampled written reviews submitted by patients between October 16, 2015, and May 27, 2020, for physicians across the US from a commercial physician rating and review website. Physicians included primary care physicians (PCPs) listed under family medicine, internal medicine, and pediatrics and surgeons listed under general surgery; orthopedic surgery; and cosmetic, plastic, and reconstructive surgery. Hand-coded reviews were used to fine-tune a natural language processing algorithm to classify all reviews for the presence and valence of patients' comments of physicians' interpersonal manner and technical competence. Statistical analyses were performed from July 2022 to December 2024. Exposure: Female or male physician gender. Main Outcomes and Measures: Outcomes included the presence and valence of interpersonal manner and technical competence comments and receipt of high star ratings. Multilevel logistic regressions analyzed differences by female or male physician gender in interpersonal manner and technical competence comments and whether those comments were associated with review star ratings. Results: The analysis included 345 053 written reviews of 167 150 physicians (mean [SD] age, 55.16 [11.40] years); 60 060 physicians (35.9%) were female, and 36 132 (21.6%) were surgeons. Female physicians overall had higher odds than males of receiving any (odds ratio [OR], 1.19; 95% CI, 1.16-1.22) or negative (OR, 1.22; 95% CI, 1.18-1.26) patient comments for their interpersonal manner. Among PCPs, females had higher odds than males of receiving a negative comment for interpersonal manner (OR, 1.22; 95% CI, 1.18-1.27) and, when receiving that negative comment, had disproportionately lower odds of receiving a high star rating (OR, 0.62; 95% CI, 0.53-0.73). Female physicians overall (OR, 1.09; 95% CI, 1.05-1.13) and female PCPs (OR, 1.08; 95% CI, 1.04-1.13) had higher odds than their male counterparts of receiving a negative comment for their technical competence. When receiving a negative comment for technical competence, both female PCPs (OR, 0.60; 95% CI, 0.50-0.73) and female surgeons (OR, 0.67; 95% CI, 0.50-0.89) had disproportionately lower odds of receiving a high star rating compared with their male counterparts. Female PCPs also had lower odds than male PCPs of receiving a high star rating when receiving a positive comment for technical competence (OR, 0.82; 95% CI, 0.70-0.96). Conclusions and Relevance: In this cross-sectional study of online written reviews, female and male physician gender were differently associated with patients' perceptions of their physicians' interpersonal manner and technical competence. The findings suggest that patients harbored negative gender biases about the interpersonal manner of female physicians, especially female PCPs, and also assessed disproportionate penalties related to technical competence for both female PCPs and female surgeons.
C2  - 39951262
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Ji, C.
AU  - Zhao, B.
AU  - Wang, Z.
AU  - Wang, Y.
AU  - Zhang, Y.
AU  - Cheng, Y.
AU  - Feng, R.
AU  - Zhang, X.
TI  - RoBGuard: Enhancing LLMs to Assess Risk of Bias in Clinical Trial Documents
PY  - 2025
T2  - Proceedings - International Conference on Computational Linguistics, COLING
VL  - Part F206484-1
SP  - 1258
EP  - 1277
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218505044&partnerID=40&md5=abdf034b008c05ef0227acf2f3486d91
AB  - Randomized Controlled Trials (RCTs) are rigorous clinical studies crucial for reliable decision-making, but their credibility can be compromised by bias. The Cochrane Risk of Bias tool (RoB 2) assesses this risk, yet manual assessments are time-consuming and labor-intensive. Previous approaches have employed Large Language Models (LLMs) to automate this process. However, they typically focus on manually crafted prompts and a restricted set of simple questions, limiting their accuracy and generalizability. Inspired by the human bias assessment process, we propose RoBGuard, a novel framework for enhancing LLMs to assess the risk of bias in RCTs. Specifically, RoBGuard integrates medical knowledge-enhanced question reformulation, multimodal document parsing, and multi-expert collaboration to ensure both completeness and accuracy. Additionally, to address the lack of suitable datasets, we introduce two new datasets: RoB-Item and RoB-Domain. Experimental results demonstrate RoBGuard's effectiveness on the RoB-Item dataset, outperforming existing methods. © 2025 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ma, Y.
AU  - Sun, X.
AU  - Ma, A.
TI  - PSG-Pair: A Psychological Snapshot Guided Model for Improved Query-Response Pairing in Psychological Wellness Communities
PY  - 2025
T2  - IEEE Access
DO  - 10.1109/ACCESS.2025.3545902
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218970548&doi=10.1109%2fACCESS.2025.3545902&partnerID=40&md5=3b31ed53bff053a13935348074f70190
AB  - Psychological wellness has become an increasingly significant global health issue, with millions affected worldwide. In response to the growing demand for accessible psychological support, we propose a novel model, Psychological Snapshot Guided Pairing (PSG-Pair), designed to enhance query-response pairing in Community Question Answering for Psychological Wellness (CQA-PW). Unlike traditional methods that focus solely on conceptual-level matching, PSG-Pair integrates role-based psychological snapshots derived from the historical posts of help-seekers and supporters. The model operates in two phases: the initial screening phase, which utilizes a BERT-based retrieval model to filter relevant supportive posts, and the pairing phase, which incorporates psychological snapshots using a stacked attention mechanism to refine conceptual pairings based on the psychological characteristics of users. Extensive experiments conducted on the CLPsych 2022 Shared Task dataset demonstrate that PSG-Pair significantly outperforms traditional single-phase models, enhancing both precision and recall in pairing processes. The inclusion of psychological snapshots allows the model to better handle the complexities of psychological wellness scenarios, thereby improving the overall effectiveness of automated psychological support systems. However, this study has several limitations. Firstly, the dataset used for experiments, although rich, still suffers from data imbalance and noise due to the high proportion of irrelevant negative samples, which could potentially impact the model's performance. Secondly, while the approach demonstrates promising results in the context of psychological wellness, the generalizability of the model to other domains or applications remains uncertain. Further exploration into the adaptability of PSG-Pair to diverse scenarios is required. Additionally, while the current evaluation metrics adequately reflect the retrieval and pairing capabilities, there is a need for the development of more tailored evaluation systems to assess models within the unique context of psychological wellness support. Future work should also investigate how to mitigate biases in user-generated content, as the quality and authenticity of answers in non-factual Q&A platforms can vary significantly, potentially affecting the accuracy of the pairing.  © 2013 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ibrahim, S.T.
AU  - Li, M.
AU  - Patel, J.
AU  - Katapally, T.R.
TI  - Utilizing natural language processing for precision prevention of mental health disorders among youth: A systematic review
PY  - 2025
T2  - Computers in Biology and Medicine
VL  - 188
C7  - 109859
DO  - 10.1016/j.compbiomed.2025.109859
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218265309&doi=10.1016%2fj.compbiomed.2025.109859&partnerID=40&md5=cd07374e3f810896f10ac222580d780c
AB  - Background: The global mental health crisis has created barriers to youth mental healthcare, leaving many disorders unaddressed. Precision prevention, which identifies individual risks, offers the potential for tailored interventions. While natural language processing (NLP) has shown promise in the early detection of mental health disorders, no review has examined its role in youth mental health detection. We hypothesize that NLP can improve early detection and personalized care in mental healthcare among youth. Methodology: After screening 1197 articles from 5 databases, 12 papers were included covering six categories: (1) mental health disorders, (2) data sources, (3) NLP objective for mental health detection, (4) annotation and validation techniques, (5) linguistic markers, and (6) performance and evaluation. Study quality was assessed using Hawker's checklist for disparate study designs. Results: Most studies focused on suicide risk (42 %), depression (25 %), and stress (17 %). Social media (42 %) and interviews (33 %) were the most common data sources, with linguistic inquiry and word count and support vector machines frequently used for analysis. While most studies were exploratory, one implemented a real-time tool for detecting mental health risks. Validation methods, including precision and recall metrics, showed strong predictive performance. Conclusions: This review highlights the potential of NLP in youth mental health detection, addressing challenges such as bias, data quality, and ethical concerns. Future research should refine NLP models using diverse, multimodal datasets, addressing data imbalance, and improving real-time detection. Exploring transformer-based models and ensuring ethical, inclusive data handling will be key to advancing NLP-driven interventions. © 2025
PB  - Elsevier Ltd
C2  - 39986200
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ali, I.
AU  - Vasant Patil, Y.
AU  - Jangid, A.
AU  - Rahaman, M.A.
AU  - Dilip Taru, R.
AU  - Iftikhar, A.
TI  - Blockchain-Driven Supply Chain Finance for Public Healthcare in India: Enhancing Financial Resilience in Public Health Systems
ST  - Financiamiento de Cadenas de Suministro Impulsado por Blockchain para la Salud Pública en India: Mejorando la Resiliencia Financiera en los Sistemas de Salud Pública
PY  - 2025
T2  - Salud, Ciencia y Tecnologia
VL  - 5
C7  - 1400
DO  - 10.56294/saludcyt20251400
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219682064&doi=10.56294%2fsaludcyt20251400&partnerID=40&md5=2b7d02241bb09d89c3477016c56eeff4
AB  - Introduction: public healthcare systems in India face persistent inefficiencies, including delays in financial workflows, lack of transparency, and fraud, particularly in rural and underserved areas. Blockchain and machine learning (ML) technologies offer transformative potential to address these challenges by enhancing transparency, efficiency, and accountability in healthcare supply chains. Method: a mixed-methods approach was adopted, combining structured surveys, semi-structured interviews, and secondary data analysis. Quantitative data were analysed using techniques such as descriptive statistics, predictive modelling (Random Forest), clustering (K-means), and anomaly detection (Isolation Forest). Qualitative data from stakeholder interviews were analysed using Natural Language Processing (NLP) to identify recurring themes and sentiment trends. Results: the analysis revealed significant inefficiencies and readiness disparities among stakeholders. Blockchain was identified as a critical tool for improving transparency, with readiness levels being the strongest predictor of adoption success. ML demonstrated robust capabilities in fraud detection, with 5 % of transactions flagged as anomalies, and predictive modelling identified key factors influencing readiness. Clustering analysis revealed distinct groups of stakeholders, highlighting the need for tailored interventions to bridge readiness gaps. Sentiment analysis indicated 65 % of stakeholders held positive views on blockchain and ML adoption. Conclusions: blockchain and ML technologies have the potential to transform public healthcare financing by addressing inefficiencies, enhancing transparency, and optimizing resource allocation. However, disparities in stakeholder readiness necessitate targeted capacity-building and phased implementation strategies. These findings provide a roadmap for integrating blockchain and ML into public healthcare systems, fostering financial resilience and improving service delivery in rural and underserved areas. © 2025; Los autores.
PB  - AG Editor (Argentina)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Gu, B.
AU  - Shao, V.
AU  - Liao, Z.
AU  - Carducci, V.
AU  - Brufau, S.R.
AU  - Yang, J.
AU  - Desai, R.J.
TI  - Scalable information extraction from free text electronic health records using large language models
PY  - 2025
T2  - BMC Medical Research Methodology
VL  - 25
IS  - 1
C7  - 23
DO  - 10.1186/s12874-025-02470-z
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217189812&doi=10.1186%2fs12874-025-02470-z&partnerID=40&md5=c0d1932778de9b79ec173bdf89a33eff
AB  - Background: A vast amount of potentially useful information such as description of patient symptoms, family, and social history is recorded as free-text notes in electronic health records (EHRs) but is difficult to reliably extract at scale, limiting their utility in research. This study aims to assess whether an “out of the box” implementation of open-source large language models (LLMs) without any fine-tuning can accurately extract social determinants of health (SDoH) data from free-text clinical notes. Methods: We conducted a cross-sectional study using EHR data from the Mass General Brigham (MGB) system, analyzing free-text notes for SDoH information. We selected a random sample of 200 patients and manually labeled nine SDoH aspects. Eight advanced open-source LLMs were evaluated against a baseline pattern-matching model. Two human reviewers provided the manual labels, achieving 93% inter-annotator agreement. LLM performance was assessed using accuracy metrics for overall, mentioned, and non-mentioned SDoH, and macro F1 scores. Results: LLMs outperformed the baseline pattern-matching approach, particularly for explicitly mentioned SDoH, achieving up to 40% higher Accuracymentioned. openchat_3.5 was the best-performing model, surpassing the baseline in overall accuracy across all nine SDoH aspects. The refined pipeline with prompt engineering reduced hallucinations and improved accuracy. Conclusions: Open-source LLMs are effective and scalable tools for extracting SDoH from unstructured EHRs, surpassing traditional pattern-matching methods. Further refinement and domain-specific training could enhance their utility in clinical research and predictive analytics, improving healthcare outcomes and addressing health disparities. © The Author(s) 2025.
PB  - BioMed Central Ltd
C2  - 39871166
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Mansoor, M.
AU  - Ansari, K.
TI  - Artificial Intelligence-Driven Analysis of Telehealth Effectiveness in Youth Mental Health Services: Insights from SAMHSA Data
PY  - 2025
T2  - Journal of Personalized Medicine
VL  - 15
IS  - 2
C7  - 63
DO  - 10.3390/jpm15020063
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218870736&doi=10.3390%2fjpm15020063&partnerID=40&md5=a9bb0aa9f039a849b0d34a76a2f162d2
AB  - Background: The rapid adoption of telehealth services for youth mental health care necessitates a comprehensive evaluation of its effectiveness. This study aimed to analyze the impact of telehealth on youth mental health outcomes using artificial intelligence techniques applied to large-scale public health data. Methods: We conducted an AI-driven analysis of data from the National Survey on Drug Use and Health (NSDUH) and other SAMHSA datasets. Machine learning techniques, including random forest models, K-means clustering, and time series analysis, were employed to evaluate telehealth adoption patterns, predictors of effectiveness, and comparative outcomes with traditional in-person care. Natural language processing was used to analyze sentiment in user feedback. Results: Telehealth adoption among youth increased significantly, with usage rising from 2.3 sessions per year in 2019 to 8.7 in 2022. Telehealth showed comparable effectiveness to in-person care for depressive disorders and superior effectiveness for anxiety disorders. Session frequency, age, and prior diagnosis were identified as key predictors of telehealth effectiveness. Four distinct user clusters were identified, with socioeconomic status and home environment strongly associated with positive outcomes. States with favorable reimbursement policies saw a 15% greater increase in youth telehealth utilization and 7% greater improvement in mental health outcomes. Conclusions: Telehealth demonstrates significant potential in improving access to and effectiveness of mental health services for youth. However, addressing technological barriers and socioeconomic disparities is crucial to maximize its benefits. © 2025 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Chun, J.
AU  - Kim, J.
AU  - Kim, H.
AU  - Lee, G.
AU  - Cho, S.
AU  - Kim, C.
AU  - Chung, Y.
AU  - Heo, S.
TI  - A Comparative Analysis of On-Device AI-Driven, Self-Regulated Learning and Traditional Pedagogy in University Health Sciences Education
PY  - 2025
T2  - Applied Sciences (Switzerland)
VL  - 15
IS  - 4
C7  - 1815
DO  - 10.3390/app15041815
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218445214&doi=10.3390%2fapp15041815&partnerID=40&md5=bfec007e76bb33871575b2a9bff82aec
AB  - Generative artificial intelligence (AI) has introduced transformative paradigms into education, facilitating personalized, real-time, and interactive learning experiences. This study investigates the integration of AI-based textbooks within regular college curricula, with a specific focus on their application in ancillary engineering subjects that demand high precision. AI textbooks enable customized learning pathways, enhance student engagement through adaptive content, and provide educators with data-driven insights. Employing a mixed-methods approach, this research compares the academic performance and learning experiences of two groups: the Traditional Learning Group (TLG), which utilized printed materials; and the AI Learning Group (ALG), which employed a generative AI-powered textbook based on LLama 3.1. Over a 15-week semester, data were collected through pre- and post-tests, task evaluations, platform log analyses, and satisfaction surveys. The findings reveal no statistically significant differences between the two groups in quantitative measures such as academic achievement, learning time, and overall satisfaction. However, the qualitative assessments underscore the role of AI-based learning in supporting self-directed education, while also highlighting the critical challenges, including digital equity, algorithmic biases, and data privacy concerns. Furthermore, the study emphasizes the necessity of comprehensive educator training, the establishment of ethical frameworks, and the development of scalable implementation strategies to fully leverage the potential of AI textbooks. Future research should prioritize randomized controlled trials (RCTs) to evaluate the long-term impacts of AI-based educational tools and develop adaptive frameworks that balance technological advancements with the emotional and motivational dimensions of human-centered education. This pioneering research lays the groundwork for advancing generative AI in higher education and fostering an innovative and equitable learning ecosystem. © 2025 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Kitapcioglu, D.
AU  - Aksoy, M.E.
AU  - Ozkan, A.E.
AU  - Usseli, T.
AU  - Cabuk Colak, D.
AU  - Torun, T.
TI  - Enhancing Immersion in Virtual Reality–Based Advanced Life Support Training: Randomized Controlled Trial
PY  - 2025
T2  - JMIR Serious Games
VL  - 13
C7  - e68272
DO  - 10.2196/68272
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000168295&doi=10.2196%2f68272&partnerID=40&md5=4a8f9452d9dfd76d11f684a4b650bf6e
AB  - Background: Serious game–based training modules are pivotal for simulation-based health care training. With advancements in artificial intelligence (AI) and natural language processing, voice command interfaces offer an intuitive alternative to traditional virtual reality (VR) controllers in VR applications. Objective: This study aims to compare AI-supported voice command interfaces and traditional VR controllers in terms of user performance, exam scores, presence, and confidence in advanced cardiac life support (ACLS) training. Methods: A total of 62 volunteer students from Acibadem Mehmet Ali Aydinlar University Vocational School for Anesthesiology, aged 20-22 years, participated in the study. All the participants completed a pretest consisting of 10 multiple-choice questions about ACLS. Following the pretest, participants were randomly divided into 2 groups: the voice command group (n=31) and the VR controller group (n=31). The voice command group members completed the VR-based ACLS serious game in training mode twice, using an AI-supported voice command as the game interface. The VR controller group members also completed the VR-based ACLS serious game in training mode twice, but they used VR controllers as the game interface. The participants completed a survey to assess their level of presence and confidence during gameplay. Following the survey, participants completed the exam module of the VR-based serious gaming module. At the final stage of the study, participants completed a posttest, which had the same content as the pretest. VR-based exam scores of the voice command and VR controller groups were compared using a 2-tailed, independent-samples t test, and linear regression analysis was conducted to examine the effect of presence and confidence rating. Results: Both groups showed an improvement in performance from pretest to posttest, with no significant difference in the magnitude of improvement between the 2 groups (P=.83). When comparing presence ratings, there was no significant difference between the voice command group (mean 5.18, SD 0.83) and VR controller group (mean 5.42, SD 0.75; P=.25). However, when comparing VR-based exam scores, the VR controller group (mean 80.47, SD 13.12) significantly outperformed the voice command group (mean 66.70, SD 21.65; P=.005), despite both groups having similar time allocations for the exam (voice command group: mean 18.59, SD 5.28 minutes and VR controller group: mean 17.3, SD 4.83 minutes). Confidence levels were similar between the groups (voice command group: mean 3.79, SD 0.77 and VR controller group: mean 3.60, SD 0.72), but the voice command group displayed a significant overconfidence bias (voice command group: mean 0.09, SD 0.24 and VR controller group: mean –0.09, SD 0.18; P=.002). Conclusions: VR-based ACLS training demonstrated effectiveness; however, the use of voice commands did not result in improved performance. Further research should explore ways to optimize AI’s role in education through VR. ©Dilek Kitapcioglu, Mehmet Emin Aksoy, Arun Ekin Ozkan, Tuba Usseli, Dilan Cabuk Colak, Tugrul Torun.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Busch, F.
AU  - Hoffmann, L.
AU  - Rueger, C.
AU  - van Dijk, E.H.C.
AU  - Kader, R.
AU  - Ortiz-Prado, E.
AU  - Makowski, M.R.
AU  - Saba, L.
AU  - Hadamitzky, M.
AU  - Kather, J.N.
AU  - Truhn, D.
AU  - Cuocolo, R.
AU  - Adams, L.C.
AU  - Bressem, K.K.
TI  - Current applications and challenges in large language models for patient care: a systematic review
PY  - 2025
T2  - Communications Medicine
VL  - 5
IS  - 1
C7  - 26
DO  - 10.1038/s43856-024-00717-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218112324&doi=10.1038%2fs43856-024-00717-2&partnerID=40&md5=7499878030b8128b44eb7990b51fbb55
AB  - Background: The introduction of large language models (LLMs) into clinical practice promises to improve patient education and empowerment, thereby personalizing medical care and broadening access to medical knowledge. Despite the popularity of LLMs, there is a significant gap in systematized information on their use in patient care. Therefore, this systematic review aims to synthesize current applications and limitations of LLMs in patient care. Methods: We systematically searched 5 databases for qualitative, quantitative, and mixed methods articles on LLMs in patient care published between 2022 and 2023. From 4349 initial records, 89 studies across 29 medical specialties were included. Quality assessment was performed using the Mixed Methods Appraisal Tool 2018. A data-driven convergent synthesis approach was applied for thematic syntheses of LLM applications and limitations using free line-by-line coding in Dedoose. Results: We show that most studies investigate Generative Pre-trained Transformers (GPT)-3.5 (53.2%, n = 66 of 124 different LLMs examined) and GPT-4 (26.6%, n = 33/124) in answering medical questions, followed by patient information generation, including medical text summarization or translation, and clinical documentation. Our analysis delineates two primary domains of LLM limitations: design and output. Design limitations include 6 second-order and 12 third-order codes, such as lack of medical domain optimization, data transparency, and accessibility issues, while output limitations include 9 second-order and 32 third-order codes, for example, non-reproducibility, non-comprehensiveness, incorrectness, unsafety, and bias. Conclusions: This review systematically maps LLM applications and limitations in patient care, providing a foundational framework and taxonomy for their implementation and evaluation in healthcare settings. © The Author(s) 2025.
PB  - Springer Nature
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Chang, C.T.
AU  - Farah, H.
AU  - Gui, H.
AU  - Rezaei, S.J.
AU  - Bou-Khalil, C.
AU  - Park, Y.-J.
AU  - Swaminathan, A.
AU  - Omiye, J.A.
AU  - Kolluri, A.
AU  - Chaurasia, A.
AU  - Lozano, A.
AU  - Heiman, A.
AU  - Jia, A.S.
AU  - Kaushal, A.
AU  - Jia, A.
AU  - Iacovelli, A.
AU  - Yang, A.
AU  - Salles, A.
AU  - Singhal, A.
AU  - Narasimhan, B.
AU  - Belai, B.
AU  - Jacobson, B.H.
AU  - Li, B.
AU  - Poe, C.H.
AU  - Sanghera, C.
AU  - Zheng, C.
AU  - Messer, C.
AU  - Kettud, D.V.
AU  - Pandya, D.
AU  - Kaur, D.
AU  - Hla, D.
AU  - Dindoust, D.
AU  - Moehrle, D.
AU  - Ross, D.
AU  - Chou, E.
AU  - Lin, E.
AU  - Haredasht, F.N.
AU  - Cheng, G.
AU  - Gao, I.
AU  - Chang, J.
AU  - Silberg, J.
AU  - Fries, J.A.
AU  - Xu, J.
AU  - Jamison, J.
AU  - Tamaresis, J.S.
AU  - Chen, J.H.
AU  - Lazaro, J.
AU  - Banda, J.M.
AU  - Lee, J.J.
AU  - Matthys, K.E.
AU  - Steffner, K.R.
AU  - Tian, L.
AU  - Pegolotti, L.
AU  - Srinivasan, M.
AU  - Manimaran, M.
AU  - Schwede, M.
AU  - Zhang, M.
AU  - Nguyen, M.
AU  - Fathzadeh, M.
AU  - Zhao, Q.
AU  - Bajra, R.
AU  - Khurana, R.
AU  - Azam, R.
AU  - Bartlett, R.
AU  - Truong, S.T.
AU  - Fleming, S.L.
AU  - Raj, S.
AU  - Behr, S.
AU  - Onyeka, S.
AU  - Muppidi, S.
AU  - Bandali, T.
AU  - Eulalio, T.Y.
AU  - Chen, W.
AU  - Zhou, X.
AU  - Ding, Y.
AU  - Cui, Y.
AU  - Tan, Y.
AU  - Liu, Y.
AU  - Shah, N.
AU  - Daneshjou, R.
TI  - Red teaming ChatGPT in medicine to yield real-world insights on model behavior
PY  - 2025
T2  - npj Digital Medicine
VL  - 8
IS  - 1
C7  - 149
DO  - 10.1038/s41746-025-01542-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000356540&doi=10.1038%2fs41746-025-01542-0&partnerID=40&md5=029f8d6d0a6dec926e4a872ce0e28b04
AB  - Red teaming, the practice of adversarially exposing unexpected or undesired model behaviors, is critical towards improving equity and accuracy of large language models, but non-model creator-affiliated red teaming is scant in healthcare. We convened teams of clinicians, medical and engineering students, and technical professionals (80 participants total) to stress-test models with real-world clinical cases and categorize inappropriate responses along axes of safety, privacy, hallucinations/accuracy, and bias. Six medically-trained reviewers re-analyzed prompt-response pairs and added qualitative annotations. Of 376 unique prompts (1504 responses), 20.1% were inappropriate (GPT-3.5: 25.8%; GPT-4.0: 16%; GPT-4.0 with Internet: 17.8%). Subsequently, we show the utility of our benchmark by testing GPT-4o, a model released after our event (20.4% inappropriate). 21.5% of responses appropriate with GPT-3.5 were inappropriate in updated models. We share insights for constructing red teaming prompts, and present our benchmark for iterative model assessments. © The Author(s) 2025.
PB  - Nature Research
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Hua, Y.
AU  - Beam, A.
AU  - Chibnik, L.B.
AU  - Torous, J.
TI  - From statistics to deep learning: Using large language models in psychiatric research
PY  - 2025
T2  - International Journal of Methods in Psychiatric Research
VL  - 34
IS  - 1
C7  - e70007
DO  - 10.1002/mpr.70007
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214513954&doi=10.1002%2fmpr.70007&partnerID=40&md5=9663677b2bbf006900bda4720f9429c5
AB  - Background: Large Language Models (LLMs) hold promise in enhancing psychiatric research efficiency. However, concerns related to bias, computational demands, data privacy, and the reliability of LLM-generated content pose challenges. Gap: Existing studies primarily focus on the clinical applications of LLMs, with limited exploration of their potentials in broader psychiatric research. Objective: This study adopts a narrative review format to assess the utility of LLMs in psychiatric research, beyond clinical settings, focusing on their effectiveness in literature review, study design, subject selection, statistical modeling, and academic writing. Implication: This study provides a clearer understanding of how LLMs can be effectively integrated in the psychiatric research process, offering guidance on mitigating the associated risks and maximizing their potential benefits. While LLMs hold promise for advancing psychiatric research, careful oversight, rigorous validation, and adherence to ethical standards are crucial to mitigating risks such as bias, data privacy concerns, and reliability issues, thereby ensuring their effective and responsible use in improving psychiatric research. © 2025 The Author(s). International Journal of Methods in Psychiatric Research published by John Wiley & Sons Ltd.
PB  - John Wiley and Sons Ltd
C2  - 39777756
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Davis, V.H.
AU  - Qiang, J.R.
AU  - MacCarthy, I.A.
AU  - Howse, D.
AU  - Seshie, A.Z.
AU  - Kosowan, L.
AU  - Delahunty-Pike, A.
AU  - Abaga, E.
AU  - Cooney, J.
AU  - Robinson, M.
AU  - Senior, D.
AU  - Zsager, A.
AU  - Aubrey-Bassler, K.
AU  - Irwin, M.
AU  - Jackson, L.A.
AU  - Katz, A.
AU  - Marshall, E.G.
AU  - Muhajarine, N.
AU  - Neudorf, C.
AU  - Garies, S.
AU  - Pinto, A.D.
TI  - Perspectives on Using Artificial Intelligence to Derive Social Determinants of Health Data From Medical Records in Canada: Large Multijurisdictional Qualitative Study
PY  - 2025
T2  - Journal of Medical Internet Research
VL  - 27
C7  - e52244
DO  - 10.2196/52244
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000533508&doi=10.2196%2f52244&partnerID=40&md5=a3e003dd9ee38c0c59cb6d4194716d6e
AB  - Background: Data on the social determinants of health could be used to improve care, support quality improvement initiatives, and track progress toward health equity. However, this data collection is not widespread. Artificial intelligence (AI), specifically natural language processing and machine learning, could be used to derive social determinants of health data from electronic medical records. This could reduce the time and resources required to obtain social determinants of health data. Objective: This study aimed to understand perspectives of a diverse sample of Canadians on the use of AI to derive social determinants of health information from electronic medical record data, including benefits and concerns. Methods: Using a qualitative description approach, in-depth interviews were conducted with 195 participants purposefully recruited from Ontario, Newfoundland and Labrador, Manitoba, and Saskatchewan. Transcripts were analyzed using an inductive and deductive content analysis. Results: A total of 4 themes were identified. First, AI was described as the inevitable future, facilitating more efficient, accessible social determinants of health information and use in primary care. Second, participants expressed concerns about potential health care harms and a distrust in AI and public systems. Third, some participants indicated that AI could lead to a loss of the human touch in health care, emphasizing a preference for strong relationships with providers and individualized care. Fourth, participants described the critical importance of consent and the need for strong safeguards to protect patient data and trust. Conclusions: These findings provide important considerations for the use of AI in health care, and particularly when health care administrators and decision makers seek to derive social determinants of health data. © Victoria H Davis, Jinfan Rose Qiang, Itunuoluwa Adekoya MacCarthy, Dana Howse, Abigail Zita Seshie, Leanne Kosowan, Alannah Delahunty-Pike, Eunice Abaga, Jane Cooney, Marjeiry Robinson, Dorothy Senior, Alexander Zsager, Kris Aubrey-Bassler, Mandi Irwin, Lois A Jackson, Alan Katz, Emily Gard Marshall, Nazeem Muhajarine, Cory Neudorf, Stephanie Garies, Andrew D Pinto.
PB  - JMIR Publications Inc.
C2  - 40053728
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Qiu, Y.
AU  - Mintenig, S.
AU  - Barchiesi, M.
AU  - Koelmans, A.A.
TI  - Using artificial intelligence tools for data quality evaluation in the context of microplastic human health risk assessments
PY  - 2025
T2  - Environment International
VL  - 197
C7  - 109341
DO  - 10.1016/j.envint.2025.109341
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218261581&doi=10.1016%2fj.envint.2025.109341&partnerID=40&md5=9e0a9d1683edf8ec048a51fad0f47d56
AB  - Concerns about the negative impacts of microplastics on human health are increasing in society, while exposure and risk assessments require high-quality, reliable data. Although quality assurance and –control (QA/QC) frameworks exist to evaluate the reliability of data for these purposes, manually assessing studies is too time-consuming and prone to inconsistencies due to semantic ambiguities and evaluator bias. The rapid growth of microplastic studies makes manually screening relevant data practically unfeasible. This study explores the potential of artificial intelligence (AI), specifically large language models (LLMs) such as OpenAI's ChatGPT and Google's Gemini, to streamline and standardize the QA/QC screening of data in microplastics research. We developed specific prompts based on previously published QA/QC criteria for the analysis of microplastics in drinking water and its sources, and used these to instruct AI tools to evaluate 73 studies published between 2011 and 2024. Our approach demonstrated the effectiveness of AI in extracting relevant information, interpreting the reliability of studies, and replicating human assessments. The findings indicate that AI-assisted assessments show promise in improving speed, consistency and applicability in QA/QC tasks, as well as in ranking studies or datasets based on their suitability for exposure and risk assessments. This groundbreaking application of LLMs in the environmental sciences suggests that AI can play a vital role in harmonizing microplastics risk assessments within regulatory frameworks and demonstrates how to meet the demands of an increasingly data-intensive application domain. © 2025 The Authors
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Salama, N.
AU  - Bsharat, R.
AU  - Alwawi, A.
AU  - Khlaif, Z.N.
TI  - Knowledge, attitudes, and practices toward AI technology (ChatGPT) among nursing students at Palestinian universities
PY  - 2025
T2  - BMC Nursing
VL  - 24
IS  - 1
C7  - 269
DO  - 10.1186/s12912-025-02913-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000073977&doi=10.1186%2fs12912-025-02913-4&partnerID=40&md5=2834ba9acd065f3327eda0a665367071
AB  - Background: AI can improve medical practice, address staff shortages, and enhance diagnostic efficiency. The ChatGPT of Open AI, launched in 2022, uses AI in medical education. However, the long-term impact is uncertain, and integration varies globally, particularly in the Middle East. Aim: To explore the knowledge, practices, and attitudes of nursing students in Palestinian universities regarding AI, specifically the use of ChatGPT. Methodology: A cross-sectional design was used to conduct this study. The study was performed at 8 private and governmental universities in the West Bank, Palestine, from 1st May 2024 to 30 May 2024, and 304 nursing students participated. Results: The study revealed that 84.5% of nursing students at Palestinian universities were aware of AI technology, yet 69.9% lacked formal education or training related to ChatGPT. Despite this gap, 79% supported the integration of AI into nursing curricula and specialized training programs, reflecting strong optimism about its role in education and healthcare. While 58.6% had used AI in their coursework and 68.1% felt comfortable with technology, disparities in proficiency and access remain key barriers to effective AI integration. Major challenges to AI adoption in Palestine include insufficient training, the absence of AI-focused curricula, and financial constraints, underscoring the need for institutional and pedagogical reforms. Concerns about AI’s reliability, costs, and potential diagnostic errors persist, emphasizing the complexities of its integration into nursing education and practice. Conclusion: This study highlights the knowledge, attitudes, and practices of Palestinian nursing students regarding AI and ChatGPT. It reveals that, despite growing awareness, the lack of formal education on AI underscores the need for comprehensive curricula. While students’ express optimism about AI’s potential in healthcare, concerns about its reliability and integration persist. The study also reveals that barriers such as inadequate training, limited curricula, and financial constraints must be addressed to effectively integrate AI into nursing education and prepare students for its expanding role in healthcare. © The Author(s) 2025.
PB  - BioMed Central Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Mess, S.A.
AU  - Mackey, A.J.
AU  - Yarowsky, D.E.
TI  - Artificial Intelligence Scribe and Large Language Model Technology in Healthcare Documentation: Advantages, Limitations, and Recommendations
PY  - 2025
T2  - Plastic and Reconstructive Surgery - Global Open
VL  - 13
IS  - 1
SP  - e6450
DO  - 10.1097/GOX.0000000000006450
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215842201&doi=10.1097%2fGOX.0000000000006450&partnerID=40&md5=d35bba30294394fe4669e98e03dd436b
AB  - Summary: Artificial intelligence (AI) scribe applications in the healthcare community are in the early adoption phase and offer unprecedented efficiency for medical documentation. They typically use an application programming interface with a large language model (LLM), for example, generative pretrained transformer 4. They use automatic speech recognition on the physician-patient interaction, generating a full medical note for the encounter, together with a draft follow-up e-mail for the patient and, often, recommendations, all within seconds or minutes. This provides physicians with increased cognitive freedom during medical encounters due to less time needed interfacing with electronic medical records. However, careful proofreading of the AI-generated language by the physician signing the note is essential. Insidious and potentially significant errors of omission, fabrication, or substitution may occur. The neural network algorithms of LLMs have unpredictable sensitivity to user input and inherent variability in their output. LLMs are unconstrained by established medical knowledge or rules. As they gain increasing levels of access to large corpora of medical records, the explosion of discovered knowledge comes with large potential risks, including to patient privacy, and potential bias in algorithms. Medical AI developers should use robust regulatory oversights, adhere to ethical guidelines, correct bias in algorithms, and improve detection and correction of deviations from the intended output.  © 2025 The Authors.
PB  - Lippincott Williams and Wilkins
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Agnihotri, A.P.
AU  - Nagel, I.D.
AU  - Artiaga, J.C.M.
AU  - Guevarra, M.C.B.
AU  - Sosuan, G.M.N.
AU  - Kalaw, F.G.P.
TI  - Large Language Models in Ophthalmology: A Review of Publications from Top Ophthalmology Journals
PY  - 2025
T2  - Ophthalmology Science
VL  - 5
IS  - 3
C7  - 100681
DO  - 10.1016/j.xops.2024.100681
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219503020&doi=10.1016%2fj.xops.2024.100681&partnerID=40&md5=bb1ad0c59f3b09527774caa624acfde6
AB  - Purpose: To review and evaluate the current literature on the application and impact of large language models (LLMs) in the field of ophthalmology, focusing on studies published in high-ranking ophthalmology journals. Design: This is a retrospective review of published articles. Participants: This study did not involve human participation. Methods: Articles published in the first quartile (Q1) of ophthalmology journals on Scimago Journal & Country Rank discussing different LLMs up to June 7, 2024, were reviewed, parsed, and analyzed. Main Outcome Measures: All available articles were parsed and analyzed, which included the article and author characteristics and data regarding the LLM used and its applications, focusing on its use in medical education, clinical assistance, research, and patient education. Results: There were 35 Q1-ranked journals identified, 19 of which contained articles discussing LLMs, with 101 articles eligible for review. One-third were original investigations (32%; 32/101), with an average of 5.3 authors per article. The United States (50.4%; 51/101) was the most represented country, followed by the United Kingdom (25.7%; 26/101) and Canada (16.8%; 17/101). ChatGPT was the most used LLM among the studies, with different versions discussed and compared. Large language model applications were discussed relevant to their implications in medical education, clinical assistance, research, and patient education. Conclusions: The numerous publications on the use of LLM in ophthalmology can provide valuable insights for stakeholders and consumers of these applications. Large language models present significant opportunities for advancement in ophthalmology, particularly in team science, education, clinical assistance, and research. Although LLMs show promise, they also show challenges such as performance inconsistencies, bias, and ethical concerns. The study emphasizes the need for ongoing artificial intelligence improvement, ethical guidelines, and multidisciplinary collaboration. Financial Disclosure(s): The author(s) have no proprietary or commercial interest in any materials discussed in this article. © 2024 American Academy of Ophthalmology
PB  - Elsevier Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Lee, H.
TI  - Assessment of Digital Capabilities by 9 Countries in the Alliance for Healthy Cities Using AI: Cross-Sectional Analysis
PY  - 2025
T2  - JMIR Formative Research
VL  - 9
C7  - e62935
DO  - 10.2196/62935
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217257121&doi=10.2196%2f62935&partnerID=40&md5=29849ea30ebb96129bd7dfe1f8bcd257
AB  - Background: The Alma-Ata Declaration of 1978 initiated a global focus on universal health, supported by the World Health Organization (WHO) through healthy cities policies. The concept emerged at the 1984 Toronto “Beyond Health Care” conference, leading to WHO’s first pilot project in Lisbon in 1986. The WHO continues to support regional healthy city networks, emphasizing digital transformation and data-driven health management in the digital era. Objective: This study explored the capabilities of digital healthy cities within the framework of digital transformation, focusing on member countries of the Asian Forum of Healthy Cities. It examined the cities’ preparedness and policy needs for transitioning to digital health. Methods: A cross-sectional survey was conducted of 9 countries—Australia, Cambodia, China, Japan, South Korea, Malaysia, Mongolia, the Philippines, and Vietnam—from August 1 to September 21, 2023. The 6-section SPIRIT (setting approach and sustainability; political commitment, policy, and community participation; information and innovation; resources and research; infrastructure and intersectoral; and training) checklist was modified to assess healthy cities’ digital capabilities. With input from 3 healthy city experts, the checklist was revised for digital capabilities, renaming “healthy city” to “digital healthy city.” The revised tool comprises 8 sections with 33 items. The survey leveraged ChatGPT (version 4.0; OpenAI, Microsoft), accessed via Python (Python Software Foundation) application programming interface. The openai library was installed, and an application programming interface key was entered to use ChatGPT (version 4.0). The “GPT-4 Turbo” model command was applied. A qualitative analysis of the collected data was conducted by 5 healthy city experts through group deep-discussions. Results: The results indicate that these countries should establish networks and committees for sustainable digital healthy cities. Cambodia showed the lowest access to electricity (70%) and significant digital infrastructure disparities. Efforts to sustain digital health initiatives varied, with countries such as Korea focusing on telemedicine, while China aimed to build a comprehensive digital health database, highlighting the need for tailored strategies in promoting digital healthy cities. Life expectancy was the highest in the Republic of Korea and Japan (both 84 y). Access to electricity was the lowest in Cambodia (70%) with the remaining countries having had 95% or higher access. The internet use rate was the highest in Malaysia (97.4%), followed by the Republic of Korea (97.2%), Australia (96.2%), and Japan (82.9%). Conclusions: This study highlights the importance of big data-driven policies and personal information protection systems. Collaborative efforts across sectors for effective implementation of digital healthy cities. The findings suggest that the effectiveness of digital healthy cities is diminished without adequate digital literacy among managers and users, suggesting the need for policies to improve digital literacy. © Hocheol Lee.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Andrew, K.
AU  - Montalbano, M.J.
TI  - Through a Glass Darkly: Perceptions of Ethnoracial Identity in Artificial Intelligence Generated Medical Vignettes and Images
PY  - 2025
T2  - Medical Science Educator
DO  - 10.1007/s40670-025-02332-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219025934&doi=10.1007%2fs40670-025-02332-9&partnerID=40&md5=ec7b2b7f4dfb2fe0f61c21339ed57526
AB  - Purpose: Medical education professionals expect artificial intelligence (AI) systems to be an efficient faculty resource for content creation. However, prior findings suggest that machine learning algorithms may exacerbate negative stereotypes and undermine efforts for diversity, equity, and inclusivity. This investigation explores the potential of OpenAI’s ChatGPT (OCG) and Microsoft’s Bing A.I. Image Creator (MBIC) to perpetuate ethnoracial stereotypes in medical cases. Materials and Methods: A series of medically relevant vignettes and visual representatives were requested from ChatGPT and MBIC for five medical conditions traditionally associated with certain ethnoracial groups: sickle cell anemia, cystic fibrosis, Tay-Sachs disease, beta-thalassemia, and aldehyde dehydrogenase deficiency. Initial prompting, self-prompting, and prompt engineering were iteratively performed to ascertain the extent to which AI outputs for generated vignettes and imagery were mutable or fixed. Results: The ethnoracial identity in the vignettes of the clinical conditions adhered more closely than described in epidemiologic studies. Following prompt engineering and self-prompting, an increase in diversity was seen. On initial prompting, the most common ethnoracial identity depicted was Caucasian. Secondary prompting resulted in less diversity with higher conformation to the traditionally expected ethnoracial identity. Conclusion: The prevalence of dataset bias and AI’s user-dependent learning abilities underscore the importance of human stewardship. The increasing use of AI in generating medical education content, like MCQs, demands vigilant use of such tools to combat the reinforcement of the race-based stereotypes in medicine. © The Author(s) 2025.
PB  - Springer
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Kavak, E.E.
AU  - Dilli, İ.
TI  - Progression-Free Survival Prediction Performance of ChatGPT: Analysis With Real Life Data in Early and Locally Advanced Prostate Cancer
PY  - 2025
T2  - Prostate
DO  - 10.1002/pros.24871
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217777603&doi=10.1002%2fpros.24871&partnerID=40&md5=1e55279df4396b150c5337d71939d32a
AB  - Objective: To evaluate the progression-free survival (PFS) time in patients with early-stage and locally advanced prostate cancer and to compare the estimates provided by ChatGPT with actual survival data. Methods: A retrospective analysis was conducted on patients diagnosed with early-stage/locally advanced prostate cancer. Each patient's estimated PFS times were calculated using an artificial intelligence chatbot. These estimates were generated by considering several factors, including the patient's clinical characteristics, tumor stage, treatment modalities, and biochemical parameters. A statistical comparison was conducted between the predicted PFS and actual PFS times. Results: The AI chatbot tended to overestimate the overall PFS times. A statistically significant discrepancy was observed between the predicted and actual survival times (p < 0.05). A discrepancy of 9.19 months was observed between the PFS predictions made by ChatGPT and the actual PFS. The bias value was 48.57, yet this discrepancy had a negligible impact on clinical practice (Cohen's d = 0.189). Discussion: Artificial intelligence-based models have the potential to play an important role in the prediction of progression in cancers such as prostate cancer, where 5–10-year survival rates can reach 100%. However, this study's findings indicate that the AI model's predictions are not aligned with the actual clinical data. The reliability of the integration of artificial intelligence into clinical decision support systems can be enhanced through the undertaking of comprehensive future studies. Conclusion: The use of artificial intelligence in predictive modeling may prove an effective approach for forecasting the PFS of prostate cancer. It has the potential to supplant the nomograms that are currently in use. Nevertheless, further studies are required to substantiate the accuracy and reliability of these systems. © 2025 Wiley Periodicals LLC.
PB  - John Wiley and Sons Inc
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Sung, M.L.
AU  - León, C.
AU  - Reisman, J.I.
AU  - Gordon, K.S.
AU  - Kerns, R.D.
AU  - Li, W.
AU  - Liu, W.
AU  - Mitra, A.
AU  - Yu, H.
AU  - Becker, W.C.
TI  - Disparities in Receipt of Medications for Opioid Use Disorder Before and During the COVID-19 Pandemic in the US Veterans Health Administration
PY  - 2025
T2  - Substance Use and Addiction Journal
DO  - 10.1177/29767342241293334
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218257694&doi=10.1177%2f29767342241293334&partnerID=40&md5=87c3562082822865a568a7bd615ead41
AB  - Background: Populations disproportionately impacted by the opioid epidemic are less likely to receive medications for opioid use disorder (MOUD; OUD). The COVID-19 pandemic exacerbated these disparities. We performed an ecological survey of subpopulations to compare differences in MOUD receipt among Veterans with OUD before versus during the pandemic. Methods: Using 2 cross-sections of 2 time periods of national Veterans Health Administration electronic health record data, we calculated proportions of Veterans with any MOUD receipt by demographics, Elixhauser comorbidity index, and natural language processing (NLP)-derived substance use and social determinants of health in each time period. We evaluated differences in MOUD receipt before and during the pandemic by patient characteristics using Chi-square and Cohen’s h for effect size. Results: Among 62 195 patients with OUD before the pandemic, the proportion prescribed MOUD increased from 46.5% before to 47.5% (P =.0003) during the pandemic. Statistically significant increased receipt of MOUD was observed for patients who were ≥55 years, men, White, with Elixhauser comorbidity indices of 2 and ≥5, and with NLP-derived indicators of substance use. There was a decrease that did not achieve statistical significance in MOUD receipt from before to during the pandemic for patients who were women, Black, Latinx, and food insecure. Conclusions: The proportions of patients with OUD prescribed MOUD increased from before to during the pandemic. However, Veterans who were women, Black, Latinx, and food insecure did not experience these increases. These patients may benefit from interventions such as targeted outreach efforts to improve MOUD engagement to reduce OUD harms. © 2024 by AMERSA, Inc. (Association for Multidisciplinary Education and Research in Substance use and Addiction).
PB  - SAGE Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Bouguettaya, A.
AU  - Team, V.
AU  - Stuart, E.M.
AU  - Aboujaoude, E.
TI  - AI-driven report-generation tools in mental healthcare: A review of commercial tools
PY  - 2025
T2  - General Hospital Psychiatry
VL  - 94
SP  - 150
EP  - 158
DO  - 10.1016/j.genhosppsych.2025.02.018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000502440&doi=10.1016%2fj.genhosppsych.2025.02.018&partnerID=40&md5=a4dca53ecc61771f3cc47a9266b4435f
AB  - Artificial intelligence (AI) systems are increasingly being integrated in clinical care, including for AI-powered note-writing. We aimed to develop and apply a scale for assessing mental health electronic health records (EHRs) that use large language models (LLMs) for note-writing, focusing on their features, security, and ethics. The assessment involved analyzing product information and directly querying vendors about their systems. On their websites, the majority of vendors provided comprehensive information on data protection, privacy measures, multi-platform availability, patient access features, software update history, and Meaningful Use compliance. Most products clearly indicated the LLM's capabilities in creating customized reports or functioning as a co-pilot. However, critical information was often absent, including details on LLM training methodologies, the specific LLM used, bias correction techniques, and methods for evaluating the evidence base. The lack of transparency regarding LLM specifics and bias mitigation strategies raises concerns about the ethical implementation and reliability of these systems in clinical practice. While LLM-enhanced EHRs show promise in alleviating the documentation burden for mental health professionals, there is a pressing need for greater transparency and standardization in reporting LLM-related information. We propose recommendations for the future development and implementation of these systems to ensure they meet the highest standards of security, ethics, and clinical care. © 2025 Elsevier Inc.
PB  - Elsevier Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ellison, I.E.
AU  - Oslock, W.M.
AU  - Abdullah, A.
AU  - Wood, L.
AU  - Thirumalai, M.
AU  - English, N.
AU  - Jones, B.A.
AU  - Hollis, R.
AU  - Rubyan, M.
AU  - Chu, D.I.
TI  - De novo generation of colorectal patient educational materials using large language models: Prompt engineering key to improved readability
PY  - 2025
T2  - Surgery (United States)
VL  - 180
C7  - 109024
DO  - 10.1016/j.surg.2024.109024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214096430&doi=10.1016%2fj.surg.2024.109024&partnerID=40&md5=ed941a5fa681c92e81a5b6735d79639c
AB  - Background: Improving patient education has been shown to improve clinical outcomes and reduce disparities, though such efforts can be labor intensive. Large language models may serve as an accessible method to improve patient educational material. The aim of this study was to compare readability between existing educational materials and those generated by large language models. Methods: Baseline colorectal surgery educational materials were gathered from a large academic institution (n = 52). Three prompts were entered into Perplexity and ChatGPT 3.5 for each topic: a Basic prompt that simply requested patient educational information the topic, an Iterative prompt that repeated instruction asking for the information to be more health literate, and a Metric-based prompt that requested a sixth-grade reading level, short sentences, and short words. Flesch-Kincaid Grade Level or Grade Level, Flesch-Kincaid Reading Ease or Ease, and Modified Grade Level scores were calculated for all materials, and unpaired t tests were used to compare mean scores between baseline and documents generated by artificial intelligence platforms. Results: Overall existing materials were longer than materials generated by the large language models across categories and prompts: 863–956 words vs 170–265 (ChatGPT) and 220–313 (Perplexity), all P < .01. Baseline materials did not meet sixth-grade readability guidelines based on grade level (Grade Level 7.0–9.8 and Modified Grade Level 9.6–11.5) or ease of readability (Ease 53.1–65.0). Readability of materials generated by a large language model varied by prompt and platform. Overall, ChatGPT materials were more readable than baseline materials with the Metric-based prompt: Grade Level 5.2 vs 8.1, Modified Grade Level 7.3 vs 10.3, and Ease 70.5 vs 60.4, all P < .01. In contrast, Perplexity-generated materials were significantly less readable except for those generated with the Metric-based prompt, which did not statistically differ. Conclusion: Both existing materials and the majority of educational materials created by large language models did not meet readability recommendations. The exception to this was with ChatGPT materials generated with a Metric-based prompt that consistently improved readability scores from baseline and met recommendations in terms of the average Grade Level score. The variability in performance highlights the importance of the prompt used with large language models. © 2024 Elsevier Inc.
PB  - Elsevier Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Shi, L.
AU  - Zhao, T.
AU  - Shi, S.
AU  - Tan, T.
AU  - Regmi, A.
AU  - Cai, Y.
TI  - Top health service concerns: a data mining study of the Shanghai health hotline
PY  - 2025
T2  - Frontiers in Digital Health
VL  - 7
C7  - 1462167
DO  - 10.3389/fdgth.2025.1462167
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000307283&doi=10.3389%2ffdgth.2025.1462167&partnerID=40&md5=74d8972b50ee55b40d0d1ae55a2d9d80
AB  - Objective: Our study aims to explore the health service issues of public concern through analyzing the basic characteristics of callers and information from the health hotline in Shanghai. The findings of this study will provide a reference to relevant government departments and assist the government in optimizing the allocation of health resources. Methods: Our research utilized 16,962 original work orders from the 12,320 health hotline, collected since 2015. We applied natural language processing (NLP) to analyze the content of these work orders, facilitating effective text mining and information extraction. Initially, we performed data cleaning to remove irrelevant information and protect caller privacy by anonymizing personal details. This cleaned data was then organized into a structured database for further analysis. Using text mining, we examined various aspects of the calls, including duration, purpose, and topics discussed, to identify patterns and themes that emerged. Results: The calls were categorized into four main groups: complaints, suggestions, inquiries, and requests for assistance. Complaints were the most frequent category, totaling 8,669 (51.11%), followed by help-seeking at 3,335 (19.66%), consultations at 2,727 (16.08%), and comments and suggestions at 1,484 (8.75%). The analysis revealed that men made 6,689 (56.88%), surpassing the 5,071 (43.12%) from women. Additionally, calls from parents numbered 2,126 (56.84%), slightly exceeding the 1,614 (43.16%) from children. The top 10 health service concerns identified in Shanghai included medical staff attitudes, medications, fees, registration, family planning, medical disputes, ambulance services, environmental health, illegal medical practices, and immunization. Conclusions: This study not only identifies critical issues within the Shanghai health service system but also offers actionable insights to inform targeted policy interventions. The high volume of complaints regarding service attitudes and medical expenses underscores the need for stronger policies to improve patient-provider communication and ensure transparency and fairness in healthcare costs. Additionally, the data reveals considerable public concern about the availability and quality of medical services, suggesting that existing policies on resource allocation and service delivery may not adequately meet population needs. The methodologies employed here can be applied to other urban health contexts, providing a valuable framework for improving public health strategies globally. 2025 Shi, Zhao, Shi, Tan, Regmi and Cai.
PB  - Frontiers Media SA
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Mori, T.
AU  - Watanabe, T.
AU  - Kosugi, S.
TI  - Exploring ethical considerations in medical research: Harnessing pre-generated transformers for AI-powered ethics discussions
PY  - 2025
T2  - PLoS ONE
VL  - 20
IS  - 2 February
C7  - e0311148
DO  - 10.1371/journal.pone.0311148
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216941043&doi=10.1371%2fjournal.pone.0311148&partnerID=40&md5=fd57134aefc44fe36c7e1f2c20350941
AB  - Introduction In medical research involving human subjects, ethical review is essential to protect individuals. However, concerns have been raised about variations in ethical review opinions and a decline in review quality. Adequately protecting human subjects requires multifaceted opinions from ethics committee members. Despite the need to increase the number of committee members, resources are limited. To address these challenges, we explored the use of a generative pre- learning transformer, an interactive artificial intelligence (AI) tool, to discuss ethical issues in medical research. Methods The generation AI used in the research used ChatGPT3.5, which has learned ethical guidelines from various countries worldwide. We requested the generative AI to provide insights on ethical considerations for virtual research involving individuals. The obtained answers were documented and verified by experts. Results The AI successfully highlighted considerations for informed consent regarding individuals with dementia and mental illness, as well as concerns about invasiveness in research. It also raised points about potential side effects of off-label drug use. However, it could not offer specific measures for psychological considerations or broader ethical issues, providing limited ethical insights. This limitation may be attributed to biased opinions resulting from machine learning optimization, preventing comprehensive identification of certain ethical issues. Conclusion Although the validity of ethical opinions generated by the generative AI requires further examination, our findings suggest that this technology could be employed to prompt reviews and re-evaluate ethical concerns arising in research. © 2025 Mori et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
PB  - Public Library of Science
C2  - 39899559
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Tornimbene, B.
AU  - Leiva Rioja, Z.B.
AU  - Brownstein, J.
AU  - Dunn, A.
AU  - Faye, S.
AU  - Kong, J.
AU  - Malou, N.
AU  - Nordon, C.
AU  - Rader, B.
AU  - Morgan, O.
TI  - Harnessing the power of artificial intelligence for disease-surveillance purposes
PY  - 2025
T2  - BMC Proceedings
VL  - 19
IS  - Suppl 4
C7  - 7
DO  - 10.1186/s12919-025-00320-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000757318&doi=10.1186%2fs12919-025-00320-w&partnerID=40&md5=1bb7cd7d468d68301f7a073724f8e9ae
AB  - The COVID-19 pandemic accelerated the development of AI-driven tools to improve public health surveillance and outbreak management. While AI programs have shown promise in disease surveillance, they also present issues such as data privacy, prejudice, and human-AI interactions. This sixth session of the of the WHO Pandemic and Epidemic Intelligence Innovation Forum examines the use of Artificial Intelligence (AI) in public health by collecting the experience of key global health organizations, such the Boston Children's Hospital, the Global South AI for Pandemic & Epidemic Preparedness & Response (AI4PEP) network, Medicines Sans Frontières (MSF), and the University of Sydney. AI's utility in clinical care, particularly in diagnostics, medication discovery, and data processing, has resulted in improvements that may also benefit public health surveillance. However, the use of AI in global health necessitates careful consideration of ethical issues, particularly those involving data use and algorithmic bias. As AI advances, particularly with large language models, public health officials must develop governance frameworks that stress openness, accountability, and fairness. These systems should address worldwide differences in data access and ensure that AI technologies are tailored to specific local needs. Ultimately, AI's ability to improve healthcare efficiency and equity is dependent on multidisciplinary collaboration, community involvement, and inclusive AI designs in ensuring equitable healthcare outcomes to fit the unique demands of global communities. © The Author(s) 2025.
PB  - BioMed Central Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Busigó Torres, R.
AU  - Restrepo, M.
AU  - Stern, B.Z.
AU  - Yahuaca, B.I.
AU  - Buerba, R.A.
AU  - Garca, I.A.
AU  - Hernandez, V.H.
AU  - Navarro, R.A.
TI  - Artificial Intelligence Shows Limited Success in Improving Readability Levels of Spanish-language Orthopaedic Patient Education Materials
PY  - 2025
T2  - Clinical Orthopaedics and Related Research
C7  - 10.1097/CORR.0000000000003413
DO  - 10.1097/CORR.0000000000003413
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217836358&doi=10.1097%2fCORR.0000000000003413&partnerID=40&md5=7cbbae4f12769c5515a6141ab8c9c556
AB  - Background The more than 41 million people in the United States who speak Spanish represent one of the fastest-growing US populations. Non-English-speaking patients often face poorer health outcomes because of language barriers that hinder patient education. Orthopaedic education materials have limited availability in Spanish and may be difficult for some patients to read. The American Academy of Orthopaedic Surgeons (AAOS) has translated education materials into Spanish, but their readability levels remain unknown. Additionally, although artificial intelligence (AI) dialogue platforms have been shown to improve readability in English, no studies have specifically evaluated their effectiveness in non-English languages. Questions/purposes (1) What is the readability of AAOS Spanish-language education materials? (2) Can an AI dialogue platform improve the readability of Spanish-language education materials while maintaining their accuracy and usefulness?MethodsAfter excluding COVID-19 articles and inaccessible websites, Spanish-language education materials were extracted from the AAOS OrthoInfo website, and their Fernández-Huerta and Spanish Orthographic Length (SOL) readability grade levels were calculated. Fernández-Huerta focuses on syntactic complexity (sentence and syllable structure) and SOL assesses lexical complexity (word length and frequency). For both, the higher the grade level, the harder it is to read. Education materials with a reading level above the sixth-grade level were inputted into the ChatGPT-4 AI platform to be adapted to a fifth-grade level. Readability metrics of the adaptations were reassessed and compared with the original versions. Secondarily, one of four Spanish-speaking orthopaedic surgeons evaluated each AI-adapted education material for accuracy and usefulness compared with the original version. We used a single review per material, trusting the orthopaedic surgeon's expertise to minimize discrepancies. We included a total of 77 of 82 education materials covering topics like diseases and conditions, treatment, and recovery and staying healthy. Results Before AI adaptations, none of the 77 education materials met the recommended reading level of sixth grade or below according to both readability formulas. The original education materials were written at a seventh- to eighth-grade reading level in 32% of cases (25 of 77). In comparison, after a single attempt at simplification, AI-adapted materials achieved this reading level in 53% of cases (41 of 77; p < 0.001). Only 23% (18) and 16% (12) of the AI adaptations were written at or below the recommended sixth-grade level per the Fernández-Huerta and SOL grade levels, respectively. Of the AI adaptations, 52% (40) were rated as accurate and 56% (43) were rated as useful for patient education by the evaluating orthopaedic surgeons. AI adaptations that were classified as accurate or useful had a higher median (IQR) word count than those that were inaccurate (accurate 255 [216 to 331] versus inaccurate 236 [209 to 256]; p = 0.04) or not useful (useful 257 [216 to 337] versus not useful 233 [209 to 251]; p = 0.01). Conclusion Ongoing attention is needed to improve the readability of Spanish education materials to reduce health disparities. ChatGPT-4 has limited success in improving readability without compromising accuracy and usefulness. We urge AAOS to enhance the readability of these materials and recommend physicians use them as supplemental resources while prioritizing direct patient education for Spanish-speaking individuals. Further research is needed to develop readable and culturally appropriate education materials for non-English-speaking patients that incorporate direct patient feedback. Clinical Relevance This study shows that Spanish-language orthopaedic materials often exceed recommended readability levels, limiting their effectiveness and worsening health disparities. While AI tools like ChatGPT-4 improve readability, they may fall short in accuracy and usefulness. This underscores the need for clearer, culturally appropriate materials and the importance of physicians providing direct education.  © 2025 by the Association of Bone and Joint Surgeons.
PB  - Wolters Kluwer Health Inc
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Placidi, G.
AU  - Cinque, L.
AU  - Foresti, G.L.
AU  - Galassi, F.
AU  - Mignosi, F.
AU  - Nappi, M.
AU  - Polsinelli, M.
TI  - A Context-Dependent CNN-Based Framework for Multiple Sclerosis Segmentation in MRI
PY  - 2025
T2  - International Journal of Neural Systems
VL  - 35
IS  - 3
C7  - 2550006
DO  - 10.1142/S0129065725500066
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214362274&doi=10.1142%2fS0129065725500066&partnerID=40&md5=fb7563b580efe999b34160883f151962
AB  - Despite several automated strategies for identi¯cation/segmentation of Multiple Sclerosis (MS) lesions in Magnetic Resonance Imaging (MRI) being developed, they consistently fall short when compared to the performance of human experts. This emphasizes the unique skills and expertise of human professionals in dealing with the uncertainty resulting from the vagueness and variability of MS, the lack of speci¯city of MRI concerning MS, and the inherent instabilities of MRI. Physicians manage this uncertainty in part by relying on their radiological, clinical, and anatomical experience. We have developed an automated framework for identifying and segmenting MS lesions in MRI scans by introducing a novel approach to replicating human diagnosis, a signi¯cant advancement in the ¯eld. This framework has the potential to revolutionize the way MS lesions are identi¯ed and segmented, being based on three main concepts: (1) Modeling the uncertainty; (2) Use of separately trained Convolutional Neural Networks (CNNs) optimized for detecting lesions, also considering their context in the brain, and to ensure spatial continuity; (3) Implementing an ensemble classi¯er to combine information from these CNNs. The proposed framework has been trained, validated, and tested on a single MRI modality, the FLuid-Attenuated Inversion Recovery (FLAIR) of the MSSEG benchmark public data set containing annotated data from seven expert radiologists and one ground truth. The comparison with the ground truth and each of the seven human raters demonstrates that it operates similarly to human raters. At the same time, the proposed model demonstrates more stability, e®ectiveness and robustness to biases than any other state-of-the-art model though using just the FLAIR modality. © The Author(s)
PB  - World Scientific
C2  - 39962837
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Lee, J.K.
AU  - Chung, T.-M.
TI  - Detecting Bias in Large Language Models: Fine-Tuned KcBERT
PY  - 2025
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 
VL  - 14893 LNCS
SP  - 76
EP  - 90
DO  - 10.1007/978-981-97-8705-0_6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219166785&doi=10.1007%2f978-981-97-8705-0_6&partnerID=40&md5=66ff0ad19418ce2838eef7fcefd1f11f
AB  - The rapid advancement of large language models (LLMs) has brought their natural language processing capabilities to a level comparable to human performance. These models are now extensively used in various societal domains, including education and healthcare. However, despite their versatility, LLMs can produce subjective and normative language, potentially leading to discriminatory outcomes among social groups, particularly through the dissemination of offensive language online. In this paper, we define such phenomena as societal bias and assess ethnic, gender, and racial biases in a model fine-tuned with Korean comments using the Bidirectional Encoder Representations from Transformers (KcBERT) and Korean Language Open Data (KOLD) through template-based Masked Language Modeling (MLM). To quantitatively evaluate these biases, we use the Language Pattern Bias Score (LPBS) and Contextual Bias Score (CBS) metrics. Our results show that, compared to KcBERT, the fine-tuned model exhibits a reduction in ethnic bias but significant alterations in gender and racial biases. To mitigate these societal biases, we propose two methods: First, a data balancing approach during the pre-training phase that adjusts the uniformity of data by aligning the distribution of specific word occurrences and converting surrounding harmful words into non-harmful alternatives. Second, during the in-training phase, we apply Debiasing Regularization by adjusting dropout and regularization parameters, resulting in decreased training loss. Our work highlights the existence of societal biases in Korean language models and demonstrates the importance of language-dependent characteristics in these models. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Zohny, H.
AU  - Allen, J.W.
AU  - Wilkinson, D.
AU  - Savulescu, J.
TI  - Which AI doctor would you like to see? Emulating healthcare provider–patient communication models with GPT-4: proof-of-concept and ethical exploration
PY  - 2025
T2  - Journal of Medical Ethics
DO  - 10.1136/jme-2024-110256
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000312342&doi=10.1136%2fjme-2024-110256&partnerID=40&md5=421274fe18c8c52ef64f0fe1fa8d9593
AB  - Large language models (LLMs) have demonstrated potential in enhancing various aspects of healthcare, including health provider–patient communication. However, some have raised the concern that such communication may adopt implicit communication norms that deviate from what patients want or need from talking with their healthcare provider. This paper explores the possibility of using LLMs to enable patients to choose their preferred communication style when discussing their medical cases. By providing a proof-of-concept demonstration using ChatGPT-4, we suggest LLMs can emulate different healthcare provider–patient communication approaches (building on Emanuel and Emanuel’s four models: paternalistic, informative, interpretive and deliberative). This allows patients to engage in a communication style that aligns with their individual needs and preferences. We also highlight potential risks associated with using LLMs in healthcare communication, such as reinforcing patients’ biases and the persuasive capabilities of LLMs that may lead to unintended manipulation. © Author(s) (or their employer(s)) 2025.
PB  - BMJ Publishing Group
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - McCaffrey, P.
AU  - Jackups, R.
AU  - Seheult, J.
AU  - Zaydman, M.A.
AU  - Balis, U.
AU  - Thaker, H.M.
AU  - Rashidi, H.
AU  - Gullapalli, R.R.
TI  - Evaluating Use of Generative Artificial Intelligence in Clinical Pathology Practice Opportunities and the Way Forward
PY  - 2025
T2  - Archives of Pathology and Laboratory Medicine
VL  - 149
IS  - 2
SP  - 130
EP  - 141
DO  - 10.5858/arpa.2024-0208-RA
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216494599&doi=10.5858%2farpa.2024-0208-RA&partnerID=40&md5=e6ea2b2bc3dab21b3aa35801f74b439e
AB  - • Context.—Generative artificial intelligence (GAI) technologies are likely to dramatically impact health care workflows in clinical pathology (CP). Applications in CP include education, data mining, decision support, result summaries, and patient trend assessments. Objective.—To review use cases of GAI in CP, with a particular focus on large language models. Specific examples are provided for the applications of GAI in the subspecialties of clinical chemistry, microbiology, hematopathology, and molecular diagnostics. Additionally, the review addresses potential pitfalls of GAI paradigms. Data Sources.—Current literature on GAI in health care was reviewed broadly. The use case scenarios for each CP subspecialty review common data sources generated in each subspecialty. The potential for utilization of CP data in the GAI context was subsequently assessed, focusing on issues such as future reporting paradigms, impact on quality metrics, and potential for translational research activities. Conclusions.—GAI is a powerful tool with the potential to revolutionize health care for patients and practitioners alike. However, GAI must be implemented with much caution considering various shortcomings of the technology such as biases, hallucinations, practical challenges of implementing GAI in existing CP workflows, and end-user acceptance. Human-in-the-loop models of GAI implementation have the potential to revolutionize CP by delivering deeper, meaningful insights into patient outcomes both at an individual and a population level. © 2025 College of American Pathologists. All rights reserved.
PB  - College of American Pathologists
C2  - 39384182
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Bejan, C.A.
AU  - Reed, A.M.
AU  - Mikula, M.
AU  - Zhang, S.
AU  - Xu, Y.
AU  - Fabbri, D.
AU  - Embí, P.J.
AU  - Hsi, R.S.
TI  - Large language models improve the identification of emergency department visits for symptomatic kidney stones
PY  - 2025
T2  - Scientific Reports
VL  - 15
IS  - 1
C7  - 3503
DO  - 10.1038/s41598-025-86632-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217272463&doi=10.1038%2fs41598-025-86632-5&partnerID=40&md5=3128f83fa59ee3a0124f1afb6afb9a89
AB  - Recent advancements of large language models (LLMs) like generative pre-trained transformer 4 (GPT-4) have generated significant interest among the scientific community. Yet, the potential of these models to be utilized in clinical settings remains largely unexplored. In this study, we investigated the abilities of multiple LLMs and traditional machine learning models to analyze emergency department (ED) reports and determine if the corresponding visits were due to symptomatic kidney stones. Leveraging a dataset of manually annotated ED reports, we developed strategies to enhance LLMs including prompt optimization, zero- and few-shot prompting, fine-tuning, and prompt augmentation. Further, we implemented fairness assessment and bias mitigation methods to investigate the potential disparities by LLMs with respect to race and gender. A clinical expert manually assessed the explanations generated by GPT-4 for its predictions to determine if they were sound, factually correct, unrelated to the input prompt, or potentially harmful. The best results were achieved by GPT-4 (macro-F1 = 0.833, 95% confidence interval [CI] 0.826–0.841) and GPT-3.5 (macro-F1 = 0.796, 95% CI 0.796–0.796). Ablation studies revealed that the initial pre-trained GPT-3.5 model benefits from fine-tuning. Adding demographic information and prior disease history to the prompts allows LLMs to make better decisions. Bias assessment found that GPT-4 exhibited no racial or gender disparities, in contrast to GPT-3.5, which failed to effectively model racial diversity. © The Author(s) 2025.
PB  - Nature Research
C2  - 39875475
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Baeta, T.
AU  - Rocha, A.L.L.
AU  - Oliveira, J.A.
AU  - Couto Da Silva, A.P.
AU  - Reis, Z.S.N.
TI  - Accuracy of machine learning and traditional statistical models in the prediction of postpartum haemorrhage: A systematic review
PY  - 2025
T2  - BMJ Open
VL  - 15
IS  - 3
C7  - e094455
DO  - 10.1136/bmjopen-2024-094455
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000451448&doi=10.1136%2fbmjopen-2024-094455&partnerID=40&md5=0371c0d6d89cd6f802a123a3c7bf8a12
AB  - Objectives To evaluate whether postpartum haemorrhage (PPH) can be predicted using both machine learning (ML) and traditional statistical models. Design Diagnostic systematic review and meta-analysis of observational and clinical studies, prospectively registered on PROSPERO, performed accordingly to the Preferred Reporting Items for Systematic Reviews and Meta-analysis and Prediction model risk of bias assessment tool for studies developing, validating or updating prediction models, with the use of an independent analysis by a large language model (GPT-4 Open AI). Data sources MEDLINE/PubMed, LILACS-BVS, Cochrane Library, Scopus-Elsevier, Embase-Elsevier and Web of Science. Eligibility criteria for selected studies The literature search was conducted on 4 January 2024 and included observational studies and clinical trials published in the past 10 years that assessed early PPH and PPH prediction and that applied accuracy metrics for outcomes evaluation. We excluded studies that did not define PPH or had exclusive PPH subgroups evaluation. Primary and secondary outcome measures The primary outcome is the accuracy of PPH prediction using both ML and conventional statistical models. A secondary outcome is to describe the strongest risk factors of PPH identified by ML and traditional statistical models. Results Of 551 citations screened, 35 studies were eligible for inclusion. The synthesis gathered 383 648 patients in 24 studies conducted with conventional statistics (CS), 9 studies using ML models and 2 studies using both methods. Multivariate regression was a preferred modelling approach to predict PPH in CS studies, while ML approaches used multiple models and a myriad of features. ML comparison to CS was only performed in two studies, and ML models demonstrated a 95% higher likelihood of PPH prediction compared with CS when applied to the same dataset (OR 1.95, 95% CI 1.88 to 2.01, p<0.001). The I² had a value of 54%, p=0.14, indicating moderate heterogeneity between the studies. Conclusions ML models are promising for predicting PPH. Nevertheless, they often require a large number of predictors, which may limit their applicability or necessitate automation through digital systems. This poses challenges in resource-scarce settings where the majority of PPH complications occur. PROSPERO registration number CRD42024521059.  © Author(s) (or their employer(s)) 2025.
PB  - BMJ Publishing Group
C2  - 40032385
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Dubbala, K.
AU  - Prizak, R.
AU  - Metzler, I.
AU  - Rubeis, G.
TI  - Exploring Heart Disease–Related mHealth Apps in India: Systematic Search in App Stores and Metadata Analysis
PY  - 2025
T2  - Journal of Medical Internet Research
VL  - 27
C7  - e53823
DO  - 10.2196/53823
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000462295&doi=10.2196%2f53823&partnerID=40&md5=03c90422396daddc566b44f4e4b3a1c4
AB  - Background: Smartphone mobile health (mHealth) apps have the potential to enhance access to health care services and address health care disparities, especially in low-resource settings. However, when developed without attention to equity and inclusivity, mHealth apps can also exacerbate health disparities. Understanding and creating solutions for the disparities caused by mHealth apps is crucial for achieving health equity. There is a noticeable gap in research that comprehensively assesses the entire spectrum of existing health apps and extensively explores apps for specific health priorities from a health care and public health perspective. In this context, with its vast and diverse population, India presents a unique context for studying the landscape of mHealth apps. Objective: This study aimed to create a comprehensive dataset of mHealth apps available in India with an initial focus on heart disease (HD)–related apps. Methods: We collected individual app data from apps in the “medical” and “health and fitness” categories from the Google Play Store and the Apple App Store in December 2022 and July 2023, respectively. Using natural language processing techniques, we selected HD apps, performed statistical analysis, and applied latent Dirichlet allocation for clustering and topic modeling to categorize the resulting HD apps. Results: We collected 118,555 health apps from the Apple App Store and 108,945 health apps from the Google Play Store. Within these datasets, we found that approximately 1.7% (1990/118,555) of apps on the Apple App Store and 0.5% (548/108,945) on the Google Play Store included support for Indian languages. Using monograms and bigrams related to HD, we identified 1681 HD apps from the Apple App Store and 588 HD apps from the Google Play Store. HD apps make up only a small fraction of the total number of health apps available in India. About 90% (1496/1681 on Apple App Store and 548/588 on Google Play Store) of the HD apps were free of cost. However, more than 70% (1329/1681, 79.1% on Apple App Store and 423/588, 71.9% on Google Play Store) of HD apps had no reviews and rating-scores, indicating low overall use. Conclusions: Our study proposed a robust method for collecting and analyzing metadata from a wide array of mHealth apps available in India through the Apple App Store and Google Play Store. We revealed the limited representation of India’s linguistic diversity within the health and medical app landscape, evident from the negligible presence of Indian-language apps. We observed a scarcity of mHealth apps dedicated to HD, along with a lower level of user engagement, as indicated by reviews and app ratings. While most HD apps are financially accessible, uptake remains a challenge. Further research should focus on app quality assessment and factors influencing user adoption. ©Keerthi Dubbala, Roshan Prizak, Ingrid Metzler, Giovanni Rubeis.
PB  - JMIR Publications Inc.
C2  - 40063078
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Xie, J.
AU  - Zhang, Z.
AU  - Zeng, S.
AU  - Hilliard, J.
AU  - An, G.
AU  - Tang, X.
AU  - Jiang, L.
AU  - Yu, Y.
AU  - Wan, X.
AU  - Xu, D.
TI  - Leveraging Large Language Models for Infectious Disease Surveillance—Using a Web Service for Monitoring COVID-19 Patterns From Self-Reporting Tweets: Content Analysis
PY  - 2025
T2  - Journal of Medical Internet Research
VL  - 27
C7  - e63190
DO  - 10.2196/63190
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218728760&doi=10.2196%2f63190&partnerID=40&md5=45264307d66fcdc010ca67398d0574dc
AB  - Background: The emergence of new SARS-CoV-2 variants, the resulting reinfections, and post–COVID-19 condition continue to impact many people’s lives. Tracking websites like the one at Johns Hopkins University no longer report the daily confirmed cases, posing challenges to accurately determine the true extent of infections. Many COVID-19 cases with mild symptoms are self-assessed at home and reported on social media, which provides an opportunity to monitor and understand the progression and evolving trends of the disease. Objective: We aim to build a publicly available database of COVID-19–related tweets and extracted information about symptoms and recovery cycles from self-reported tweets. We have presented the results of our analysis of infection, reinfection, recovery, and long-term effects of COVID-19 on a visualization website that refreshes data on a weekly basis. Methods: We used Twitter (subsequently rebranded as X) to collect COVID-19–related data, from which 9 native English-speaking annotators annotated a training dataset of COVID-19–positive self-reporters. We then used large language models to identify positive self-reporters from other unannotated tweets. We used the Hibert transform to calculate the lead of the prediction curve ahead of the reported curve. Finally, we presented our findings on symptoms, recovery, reinfections, and long-term effects of COVID-19 on the Covlab website. Results: We collected 7.3 million tweets related to COVID-19 between January 1, 2020, and April 1, 2024, including 262,278 self-reported cases. The predicted number of infection cases by our model is 7.63 days ahead of the official report. In addition to common symptoms, we identified some symptoms that were not included in the list from the US Centers for Disease Control and Prevention, such as lethargy and hallucinations. Repeat infections were commonly occurring, with rates of second and third infections at 7.49% (19,644/262,278) and 1.37% (3593/262,278), respectively, whereas 0.45% (1180/262,278) also reported that they had been infected >5 times. We identified 723 individuals who shared detailed recovery experiences through tweets, indicating a substantially reduction in recovery time over the years. Specifically, the average recovery period decreased from around 30 days in 2020 to approximately 12 days in 2023. In addition, geographic information collected from confirmed individuals indicates that the temporal patterns of confirmed cases in states such as California and Texas closely mirror the overall trajectory observed across the United States. Conclusions: Although with some biases and limitations, self-reported tweet data serves as a valuable complement to clinical data, especially in the postpandemic era dominated by mild cases. Our web-based analytic platform can play a significant role in continuously tracking COVID-19, finding new uncommon symptoms, detecting and monitoring the manifestation of long-term effects, and providing necessary insights to the public and decision-makers. ©Jiacheng Xie, Ziyang Zhang, Shuai Zeng, Joel Hilliard, Guanghui An, Xiaoting Tang, Lei Jiang, Yang Yu, Xiufeng Wan, Dong Xu.
PB  - JMIR Publications Inc.
C2  - 39977859
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Topaz, M.
AU  - Davoudi, A.
AU  - Evans, L.
AU  - Sridharan, S.
AU  - Song, J.
AU  - Chae, S.
AU  - Barrón, Y.
AU  - Hobensack, M.
AU  - Scharp, D.
AU  - Cato, K.
AU  - Rossetti, S.C.
AU  - Kapela, P.
AU  - Xu, Z.
AU  - Gupta, P.
AU  - Zhang, Z.
AU  - Mcdonald, M.V.
AU  - Bowles, K.H.
TI  - Building a Time-Series Model to Predict Hospitalization Risks in Home Health Care: Insights Into Development, Accuracy, and Fairness
PY  - 2025
T2  - Journal of the American Medical Directors Association
VL  - 26
IS  - 2
C7  - 105417
DO  - 10.1016/j.jamda.2024.105417
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212946037&doi=10.1016%2fj.jamda.2024.105417&partnerID=40&md5=6b1cbc96231451a8a66d2ffa581c0962
AB  - Objectives: Home health care (HHC) serves more than 5 million older adults annually in the United States, aiming to prevent unnecessary hospitalizations and emergency department (ED) visits. Despite efforts, up to 25% of patients in HHC experience these adverse events. The underutilization of clinical notes, aggregated data approaches, and potential demographic biases have limited previous HHC risk prediction models. This study aimed to develop a time-series risk model to predict hospitalizations and ED visits in patients in HHC, examine model performance over various prediction windows, identify top predictive variables and map them to data standards, and assess model fairness across demographic subgroups. Setting and Participants: A total of 27,222 HHC episodes between 2015 and 2017. Methods: The study used health care process modeling of electronic health records, including clinical notes processed with natural language processing techniques and Medicare claims data. A Light Gradient Boosting Machine algorithm was used to develop the risk prediction model, with performance evaluated using 5-fold cross-validation. Model fairness was assessed across gender, race/ethnicity, and socioeconomic subgroups. Results: The model achieved high predictive performance, with an F1 score of 0.84 for a 5-day prediction window. Twenty top predictive variables were identified, including novel indicators such as the length of nurse-patient visits and visit frequency. Eighty-five percent of these variables mapped completely to the US Core Data for Interoperability standard. Fairness assessment revealed performance disparities across demographic and socioeconomic groups, with lower model effectiveness for more historically underserved populations. Conclusions and Implications: This study developed a robust time-series risk model for predicting adverse events in patients in HHC, incorporating diverse data types and demonstrating high predictive accuracy. The findings highlight the importance of considering established and novel risk factors in HHC. Importantly, the observed performance disparities across subgroups emphasize the need for fairness adjustments to ensure equitable risk prediction across all patient populations. © 2024 Post-Acute and Long-Term Care Medical Association
PB  - Elsevier Inc.
C2  - 39689864
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Rządeczka, M.
AU  - Sterna, A.
AU  - Stolińska, J.
AU  - Kaczyńska, P.
AU  - Moskalewicz, M.
TI  - The Efficacy of Conversational AI in Rectifying the Theory-of-Mind and Autonomy Biases: Comparative Analysis
PY  - 2025
T2  - JMIR Mental Health
VL  - 12
C7  - e64396
DO  - 10.2196/64396
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219118191&doi=10.2196%2f64396&partnerID=40&md5=1c5d5d2c2bae0431f4c99074d0ff58d2
AB  - Background: The increasing deployment of conversational artificial intelligence (AI) in mental health interventions necessitates an evaluation of their efficacy in rectifying cognitive biases and recognizing affect in human-AI interactions. These biases are particularly relevant in mental health contexts as they can exacerbate conditions such as depression and anxiety by reinforcing maladaptive thought patterns or unrealistic expectations in human-AI interactions. Objective: This study aimed to assess the effectiveness of therapeutic chatbots (Wysa and Youper) versus general-purpose language models (GPT-3.5, GPT-4, and Gemini Pro) in identifying and rectifying cognitive biases and recognizing affect in user interactions. Methods: This study used constructed case scenarios simulating typical user-bot interactions to examine how effectively chatbots address selected cognitive biases. The cognitive biases assessed included theory-of-mind biases (anthropomorphism, overtrust, and attribution) and autonomy biases (illusion of control, fundamental attribution error, and just-world hypothesis). Each chatbot response was evaluated based on accuracy, therapeutic quality, and adherence to cognitive behavioral therapy principles using an ordinal scale to ensure consistency in scoring. To enhance reliability, responses underwent a double review process by 2 cognitive scientists, followed by a secondary review by a clinical psychologist specializing in cognitive behavioral therapy, ensuring a robust assessment across interdisciplinary perspectives. Results: This study revealed that general-purpose chatbots outperformed therapeutic chatbots in rectifying cognitive biases, particularly in overtrust bias, fundamental attribution error, and just-world hypothesis. GPT-4 achieved the highest scores across all biases, whereas the therapeutic bot Wysa scored the lowest. Notably, general-purpose bots showed more consistent accuracy and adaptability in recognizing and addressing bias-related cues across different contexts, suggesting a broader flexibility in handling complex cognitive patterns. In addition, in affect recognition tasks, general-purpose chatbots not only excelled but also demonstrated quicker adaptation to subtle emotional nuances, outperforming therapeutic bots in 67% (4/6) of the tested biases. Conclusions: This study shows that, while therapeutic chatbots hold promise for mental health support and cognitive bias intervention, their current capabilities are limited. Addressing cognitive biases in AI-human interactions requires systems that can both rectify and analyze biases as integral to human cognition, promoting precision and simulating empathy. The findings reveal the need for improved simulated emotional intelligence in chatbot design to provide adaptive, personalized responses that reduce overreliance and encourage independent coping skills. Future research should focus on enhancing affective response mechanisms and addressing ethical concerns such as bias mitigation and data privacy to ensure safe, effective AI-based mental health support. ©Marcin Rządeczka, Anna Sterna, Julia Stolińska, Paulina Kaczyńska, Marcin Moskalewicz.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Goh, E.
AU  - Bunning, B.
AU  - Khoong, E.C.
AU  - Gallo, R.J.
AU  - Milstein, A.
AU  - Centola, D.
AU  - Chen, J.H.
TI  - Physician clinical decision modification and bias assessment in a randomized controlled trial of AI assistance
PY  - 2025
T2  - Communications Medicine
VL  - 5
IS  - 1
C7  - 59
DO  - 10.1038/s43856-025-00781-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000080369&doi=10.1038%2fs43856-025-00781-2&partnerID=40&md5=12ea50bc9dc8f2d71165644ade49944c
AB  - Background: Artificial intelligence assistance in clinical decision making shows promise, but concerns exist about potential exacerbation of demographic biases in healthcare. This study aims to evaluate how physician clinical decisions and biases are influenced by AI assistance in a chest pain triage scenario. Methods: A randomized, pre post-intervention study was conducted with 50 US-licensed physicians who reviewed standardized chest pain video vignettes featuring either a white male or Black female patient. Participants answered clinical questions about triage, risk assessment, and treatment before and after receiving GPT-4 generated recommendations. Clinical decision accuracy was evaluated against evidence-based guidelines. Results: Here we show that physicians are willing to modify their clinical decisions based on GPT-4 assistance, leading to improved accuracy scores from 47% to 65% in the white male patient group and 63% to 80% in the Black female patient group. The accuracy improvement occurs without introducing or exacerbating demographic biases, with both groups showing similar magnitudes of improvement (18%). A post-study survey indicates that 90% of physicians expect AI tools to play a significant role in future clinical decision making. Conclusions: Physician clinical decision making can be augmented by AI assistance while maintaining equitable care across patient demographics. These findings suggest a path forward for AI clinical decision support that improves medical care without amplifying healthcare disparities. © The Author(s) 2025.
PB  - Springer Nature
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Aremu, T.
AU  - Akinwehinmi, O.
AU  - Nwagu, C.
AU  - Ahmed, S.I.
AU  - Orji, R.
AU  - Amo, P.A.D.
AU  - Saddik, A.E.
TI  - On the reliability of Large Language Models to misinformed and demographically informed prompts
PY  - 2025
T2  - AI Magazine
VL  - 46
IS  - 1
C7  - e12208
DO  - 10.1002/aaai.12208
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214496727&doi=10.1002%2faaai.12208&partnerID=40&md5=1022a51fbd11bb175008c447cd09574e
AB  - We investigate and observe the behavior and performance of Large Language Model (LLM)-backed chatbots in addressing misinformed prompts and questions with demographic information within the domains of Climate Change and Mental Health. Through a combination of quantitative and qualitative methods, we assess the chatbots' ability to discern the veracity of statements, their adherence to facts, and the presence of bias or misinformation in their responses. Our quantitative analysis using True/False questions reveals that these chatbots can be relied on to give the right answers to these close-ended questions. However, the qualitative insights, gathered from domain experts, shows that there are still concerns regarding privacy, ethical implications, and the necessity for chatbots to direct users to professional services. We conclude that while these chatbots hold significant promise, their deployment in sensitive areas necessitates careful consideration, ethical oversight, and rigorous refinement to ensure they serve as a beneficial augmentation to human expertise rather than an autonomous solution. Dataset and assessment information can be found at https://github.com/tolusophy/Edge-of-Tomorrow. © 2025 The Author(s). AI Magazine published by John Wiley & Sons Ltd on behalf of Association for the Advancement of Artificial Intelligence.
PB  - John Wiley and Sons Inc
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Kaloudis, E.
AU  - Kouti, V.
AU  - Triantafillou, F.-M.
AU  - Ventouris, P.
AU  - Pavlidis, R.
AU  - Bountziouka, V.
TI  - AI-Powered Analysis of Weight Loss Reports from Reddit: Unlocking Social Media’s Potential in Dietary Assessment
PY  - 2025
T2  - Nutrients 
VL  - 17
IS  - 5
C7  - 818
DO  - 10.3390/nu17050818
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000670993&doi=10.3390%2fnu17050818&partnerID=40&md5=957135e10b7fc577af2c56d9b56aabcb
AB  - Background/Objectives: The increasing use of social media for sharing health and diet experiences presents new opportunities for nutritional research and dietary assessment. Large language models (LLMs) and artificial intelligence (AI) offer innovative approaches to analyzing self-reported data from online communities. This study explores weight loss experiences associated with the ketogenic diet (KD) using user-generated content from Reddit, aiming to identify trends and potential biases in self-reported outcomes. Methods: A dataset of 35,079 Reddit posts related to KD was collected and processed. Posts mentioning weight loss, diet duration, and additional factors (age, gender, physical activity, health conditions) were identified, yielding 2416 complete cases. Descriptive statistics summarized weight loss distributions and diet adherence patterns, while linear regression models examined factors associated with weight loss. Results: The median reported weight loss was 10.9 kg (IQR: 4.4–22.7 kg). Diet adherence varied with 36.3% of users following KD for up to 30 days and 7.8% for more than a year. Metabolic (27%) and cardiovascular disorders (17%) were the most frequently reported health conditions. Adherence beyond one year was associated with an average weight loss of 28.2 kg (95% CI: 25.5–30.9) compared to up to 30 days. Male gender was associated with an additional weight loss of 5.2 kg (95% CI: 3.8–6.6) compared to females. Conclusions: Findings suggest KD may lead to substantial weight loss based on self-reported online data. This study highlights the value of social media data in nutritional research, uncovering hidden dietary patterns that could inform public health strategies and personalized nutrition plans. © 2025 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
C2  - 40077687
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Gonzalez Fiol, A.
AU  - Mootz, A.A.
AU  - He, Z.
AU  - Delgado, C.
AU  - Ortiz, V.
AU  - Reale, S.C.
TI  - Accuracy of Spanish and English-generated ChatGPT responses to commonly asked patient questions about labor epidurals: a survey-based study among bilingual obstetric anesthesia experts
PY  - 2025
T2  - International Journal of Obstetric Anesthesia
VL  - 61
C7  - 104290
DO  - 10.1016/j.ijoa.2024.104290
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209741091&doi=10.1016%2fj.ijoa.2024.104290&partnerID=40&md5=1197e1b3f00aa53b9e91a1dbd33c6a29
AB  - Background: Large language models (LLMs), of which ChatGPT is the most well known, are now available to patients to seek medical advice in various languages. However, the accuracy of the information utilized to train these models remains unknown. Methods: Ten commonly asked questions regarding labor epidurals were translated from English to Spanish, and all 20 questions were entered into ChatGPT version 3.5. The answers were transcribed. A survey was then sent to 10 bilingual fellowship-trained obstetric anesthesiologists to assess the accuracy of these answers utilizing a 5-point Likert scale. Results: Overall, the accuracy scores for the ChatGPT-generated answers in Spanish were lower than for the English answers with a median score of 34 (IQR 33–36.5) versus 40.5 (IQR 39–44.3), respectively (P value 0.02). Answers to two questions were scored significantly lower: “Do epidurals prolong labor?” (2 (IQR 2–2.5) versus 4 (IQR 4–4.5), P value 0.03) and “Do epidurals increase the risk of needing cesarean delivery?” (3(IQR 2–4) versus 4 (IQR 4–5); P value 0.03). There was a strong agreement that answers to the question “Do epidurals cause autism” were accurate in both Spanish and English. Conclusion: ChatGPT-generated answers in Spanish to ten questions about labor epidurals scored lower for accuracy than answers generated in English, particularly regarding the effect of labor epidurals on labor course and mode of delivery. This disparity in ChatGPT-generated information may extend already-known health inequities among non-English-speaking patients and perpetuate misinformation. © 2024 Elsevier Ltd
PB  - Churchill Livingstone
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Kunze, K.N.
AU  - Nwachukwu, B.U.
AU  - Cote, M.P.
AU  - Ramkumar, P.N.
TI  - Large Language Models Applied to Health Care Tasks May Improve Clinical Efficiency, Value of Care Rendered, Research, and Medical Education
PY  - 2025
T2  - Arthroscopy - Journal of Arthroscopic and Related Surgery
VL  - 41
IS  - 3
SP  - 547
EP  - 556
DO  - 10.1016/j.arthro.2024.12.010
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212627188&doi=10.1016%2fj.arthro.2024.12.010&partnerID=40&md5=b23983ee4d041d4ff594352505340637
AB  - Large language models (LLMs) are generative artificial intelligence models that create content on the basis of the data on which it was trained. Processing capabilities have evolved from text only to being multimodal including text, images, audio, and video features. In health care settings, LLMs are being applied to several clinically important areas, including patient care and workflow efficiency, communications, hospital operations and data management, medical education, practice management, and health care research. Under the umbrella of patient care, several core use cases of LLMs include simplifying documentation tasks, enhancing patient communication (interactive language and written), conveying medical knowledge, and performing medical triage and diagnosis. However, LLMs warrant scrutiny when applied to health care tasks, as errors may have negative implications for health care outcomes, specifically in the context of perpetuating bias, ethical considerations, and cost-effectiveness. Customized LLMs developed for more narrow purposes may help overcome certain performance limitations, transparency challenges, and biases present in contemporary generalized LLMs by curating training data. Methods of customizing LLMs broadly fall under 4 categories: prompt engineering, retrieval augmented generation, fine-tuning, and agentic augmentation, with each approach conferring different information-retrieval properties for the LLM. Level of Evidence: Level V, expert opinion. © 2024 Arthroscopy Association of North America
PB  - W.B. Saunders
C2  - 39694303
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Li, W.
AU  - Hua, Y.
AU  - Zhou, P.
AU  - Zhou, L.
AU  - Xu, X.
AU  - Yang, J.
TI  - Characterizing Public Sentiments and Drug Interactions in the COVID-19 Pandemic Using Social Media: Natural Language Processing and Network Analysis
PY  - 2025
T2  - Journal of Medical Internet Research
VL  - 27
C7  - e63755
DO  - 10.2196/63755
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000152649&doi=10.2196%2f63755&partnerID=40&md5=cabb95e0b65ec03891c67ea2676e6645
AB  - Background: While the COVID-19 pandemic has induced massive discussion of available medications on social media, traditional studies focused only on limited aspects, such as public opinions, and endured reporting biases, inefficiency, and long collection times. Objective: Harnessing drug-related data posted on social media in real-time can offer insights into how the pandemic impacts drug use and monitor misinformation. This study aimed to develop a natural language processing (NLP) pipeline tailored for the analysis of social media discourse on COVID-19–related drugs. Methods: This study constructed a full pipeline for COVID-19–related drug tweet analysis, using pretrained language model–based NLP techniques as the backbone. This pipeline is architecturally composed of 4 core modules: named entity recognition and normalization to identify medical entities from relevant tweets and standardize them to uniform medication names for time trend analysis, target sentiment analysis to reveal sentiment polarities associated with the entities, topic modeling to understand underlying themes discussed by the population, and drug network analysis to dig potential adverse drug reactions (ADR) and drug-drug interactions (DDI). The pipeline was deployed to analyze tweets related to the COVID-19 pandemic and drug therapies between February 1, 2020, and April 30, 2022. Results: From a dataset comprising 169,659,956 COVID-19–related tweets from 103,682,686 users, our named entity recognition model identified 2,124,757 relevant tweets sourced from 1,800,372 unique users, and the top 5 most-discussed drugs: ivermectin, hydroxychloroquine, remdesivir, zinc, and vitamin D. Time trend analysis revealed that the public focused mostly on repurposed drugs (ie, hydroxychloroquine and ivermectin), and least on remdesivir, the only officially approved drug among the 5. Sentiment analysis of the top 5 most-discussed drugs revealed that public perception was predominantly shaped by celebrity endorsements, media hot spots, and governmental directives rather than empirical evidence of drug efficacy. Topic analysis obtained 15 general topics of overall drug-related tweets, with “clinical treatment effects of drugs” and “physical symptoms” emerging as the most frequently discussed topics. Co-occurrence matrices and complex network analysis further identified emerging patterns of DDI and ADR that could be critical for public health surveillance like better safeguarding public safety in medicines use. Conclusions: This study shows that an NLP-based pipeline can be a robust tool for large-scale public health monitoring and can offer valuable supplementary data for traditional epidemiological studies concerning DDI and ADR. The framework presented here aspires to serve as a cornerstone for future social media–based public health analytics. ©Wanxin Li, Yining Hua, Peilin Zhou, Li Zhou, Xin Xu, Jie Yang. Originally published in the Journal of Medical Internet Research (https://www.jmir.org), 05.03.2025.
PB  - JMIR Publications Inc.
C2  - 40053730
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Tierney, A.A.
AU  - Reed, M.E.
AU  - Grant, R.W.
AU  - Doo, F.X.
AU  - Payán, D.D.
AU  - Liu, V.X.
TI  - Health Equity in the Era of Large Language Models
PY  - 2025
T2  - American Journal of Managed Care
VL  - 31
IS  - 3
SP  - 112
EP  - 117
DO  - 10.37765/ajmc.2025.89695
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000000370&doi=10.37765%2fajmc.2025.89695&partnerID=40&md5=0b431bdaecb59a38aa05788edbeb4e9b
AB  - This commentary presents a summary of 8 major regulations and guidelines that have direct implications for the equitable design, implementation, and maintenance of health care–focused large language models (LLMs) deployed in the US. We grouped key equity issues for LLMs into 3 domains: (1) linguistic and cultural bias, (2) accessibility and trust, and (3) oversight and quality control. Solutions shared by these regulations and guidelines are to (1) ensure diverse representation in training data and in teams that develop artificial intelligence (AI) tools, (2) develop techniques to evaluate AI-enabled health care tool performance against real-world data, (3) ensure that AI used in health care is free of discrimination and integrates equity principles, (4) take meaningful steps to ensure access for patients with limited English proficiency, (5) apply AI tools to make workplaces more efficient and reduce administrative burdens, (6) require human oversight of AI tools used in health care delivery, and (7) ensure AI tools are safe, accessible, and beneficial while respecting privacy. There is an opportunity to prevent further embedding of existing disparities and issues in the health care system by enhancing health equity through thoughtfully designed and deployed LLMs. © 2025 Ascend Media. All rights reserved.
PB  - Ascend Media
C2  - 40053403
M3  - Review
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Armitage, R.C.
TI  - Implications of Large Language Models for Clinical Practice: Ethical Analysis Through the Principlism Framework
PY  - 2025
T2  - Journal of Evaluation in Clinical Practice
VL  - 31
IS  - 1
C7  - e14250
DO  - 10.1111/jep.14250
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211134270&doi=10.1111%2fjep.14250&partnerID=40&md5=b0628a68c5ca36e3afaabc866b498d8b
AB  - Introduction: The potential applications of large language models (LLMs)—a form of generative artificial intelligence (AI)—in medicine and health care are being increasingly explored by medical practitioners and health care researchers. Methods: This paper considers the ethical implications of LLMs for medical practitioners in their delivery of clinical care through the ethical framework of principlism. Findings: It finds that, regarding beneficence, LLMs can improve patient outcomes through supporting administrative tasks that surround patient care, and by directly informing clinical care. Simultaneously, LLMs can cause patient harm through various mechanisms, meaning non-maleficence would prevent their deployment in the absence of sufficient risk mitigation. Regarding autonomy, medical practitioners must inform patients if their medical care will be influenced by LLMs for their consent to be informed, and alternative care uninfluenced by LLMs must be available for patients who withhold such consent. Finally, regarding justice, LLMs could promote the standardisation of care within individual medical practitioners by mitigating any biases harboured by those practitioners and by protecting against human factors, while also up-skilling existing medical practitioners in low-resource settings to reduce global health disparities. Discussion: Accordingly, this paper finds a strong case for the incorporation of LLMs into clinical practice and, if their risk of patient harm is sufficiently mitigated, this incorporation might be ethically required, at least according to principlism. © 2024 The Author(s). Journal of Evaluation in Clinical Practice published by John Wiley & Sons Ltd.
PB  - John Wiley and Sons Inc
C2  - 39618089
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Caponio, V.C.A.
AU  - Musella, G.
AU  - Pérez-Sayáns, M.
AU  - Lo Muzio, L.
AU  - Amaral Mendes, R.
AU  - López-Pintor, R.M.
TI  - The Need to Improve the Medical Subject Headings (MeSH) and the Excerpta Medica Tree (EMTREE) Thesauri to Perform Systematic Review on Oral Potentially Malignant Disorders
PY  - 2025
T2  - Journal of Oral Pathology and Medicine
DO  - 10.1111/jop.13616
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000577734&doi=10.1111%2fjop.13616&partnerID=40&md5=2935ef42008e93d9bf1fab822d906dfc
AB  - Background: Despite recent advancements in the understanding and classification of oral potentially malignant disorders (OPMD), their terminology remains inconsistent and heterogeneous throughout the scientific literature, thus affecting evidence-based decision-making relevant for clinical management of these disorders. Updating this classification represents a necessity to improve the indexing and retrieval of OPMD publications, in particular for systematic reviews and meta-analysis. Methods: Through a critical appraisal of the Medical Subject Headings (MeSH) and Excerpta Medica Tree (EMTREE) thesauri, we assessed gaps in the indexing for OPMD literature and propose improvements for enhanced categorisation and retrieval. Results: The present study identifies inconsistencies and limitations in the classification of these disorders across the major medical databases, which may be summarized in the following findings: a) The MeSH database lacks a dedicated subject heading for “oral potentially malignant disorders”; b) EMTREE indexing is incomplete, with only 5 out of 11 recognised OPMD having corresponding terms; c) Incoherent controlled vocabulary mappings hinder systematic literature retrieval. Conclusion: To ensure accurate evidence synthesis, the authors recommend searching both PubMed and Embase for OPMD studies. Moreover, the use of Embase’s PubMed query translator and Large Language Models, such as ChatGPT, may lead to retrieval biases due to indexing discrepancies, posing challenges for early-career researchers and students. We recommend introducing “oral potentially malignant disorders” as a standardised subject heading. Evidence-based medicine underpins clinical decision support systems, which rely on standardised clinical coding for reliable health information. Enhanced medical ontologies will facilitate structured clinical coding, ensuring interoperability and improving clinical decision support systems. © 2025 The Author(s). Journal of Oral Pathology & Medicine published by John Wiley & Sons Ltd.
PB  - John Wiley and Sons Inc
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Malk, N.A.
AU  - Diwan, S.A.
TI  - Exploring the Role of AI in Understanding Human Emotion
PY  - 2025
T2  - Journal of Information Systems Engineering and Management
VL  - 10
SP  - 64
EP  - 75
DO  - 10.52783/jisem.v10i13s.2004
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000346895&doi=10.52783%2fjisem.v10i13s.2004&partnerID=40&md5=1606f447a9ce9f8a5fe43a8b1efa141c
AB  - This research will explore the intersection of artificial intelligence (AI) and understanding human emotions, a growing field known as emotional AI. The research aims to provide a comprehensive overview of how AI techniques, such as machine learning, natural language processing (NLP), and facial and voice expression analysis, can be used to analyze human emotions and improve human-machine interactions. The research reviews the theoretical foundations for understanding emotions based on several psychological and neurological models, and also discusses AI applications in the fields of mental health, education, and marketing, where these technologies can improve user experiences by recognizing their emotional responses and tailoring their interactions accordingly. However, this field faces significant challenges, such as privacy and data security, model bias, and accuracy in sentiment analysis. The study highlights the need to develop more transparent and fair systems to ensure the ethical use of this technology. The research suggests that advances in deep learning and multi-dimensional interaction can improve AI’s ability to accurately understand human emotions, opening the door to developing AI systems that are more aware and responsive to human emotions. Copyright © 2024 by Author/s and Licensed by JISEM.
PB  - IADITI - International Association for Digital Transformation and Technological Innovation
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Gao, Y.
AU  - Myers, S.
AU  - Chen, S.
AU  - Dligach, D.
AU  - Miller, T.
AU  - Bitterman, D.S.
AU  - Chen, G.
AU  - Mayampurath, A.
AU  - Churpek, M.M.
AU  - Afshar, M.
TI  - Uncertainty estimation in diagnosis generation from large language models: Next-word probability is not pre-Test probability
PY  - 2025
T2  - JAMIA Open
VL  - 8
IS  - 1
C7  - ooae154
DO  - 10.1093/jamiaopen/ooae154
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215550622&doi=10.1093%2fjamiaopen%2fooae154&partnerID=40&md5=e2f975cff47a341fae8f778687458bd5
AB  - Objective: To evaluate large language models (LLMs) for pre-Test diagnostic probability estimation and compare their uncertainty estimation performance with a traditional machine learning classifier. Materials and Methods: We assessed 2 instruction-Tuned LLMs, Mistral-7B-Instruct and Llama3-70B-chat-hf, on predicting binary outcomes for Sepsis, Arrhythmia, and Congestive Heart Failure (CHF) using electronic health record (EHR) data from 660 patients. Three uncertainty estimation methods-Verbalized Confidence, Token Logits, and LLM Embedding+XGB-were compared against an eXtreme Gradient Boosting (XGB) classifier trained on raw EHR data. Performance metrics included AUROC and Pearson correlation between predicted probabilities. Results: The XGB classifier outperformed the LLM-based methods across all tasks. LLM Embedding+XGB showed the closest performance to the XGB baseline, while Verbalized Confidence and Token Logits underperformed. Discussion: These findings, consistent across multiple models and demographic groups, highlight the limitations of current LLMs in providing reliable pre-Test probability estimations and underscore the need for improved calibration and bias mitigation strategies. Future work should explore hybrid approaches that integrate LLMs with numerical reasoning modules and calibrated embeddings to enhance diagnostic accuracy and ensure fairer predictions across diverse populations. Conclusions: LLMs demonstrate potential but currently fall short in estimating diagnostic probabilities compared to traditional machine learning classifiers trained on structured EHR data. Further improvements are needed for reliable clinical use.  © 2025 The Author(s).
PB  - Oxford University Press
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Malekzadeh, M.
AU  - Willberg, E.
AU  - Torkko, J.
AU  - Toivonen, T.
TI  - Urban attractiveness according to ChatGPT: Contrasting AI and human insights
PY  - 2025
T2  - Computers, Environment and Urban Systems
VL  - 117
C7  - 102243
DO  - 10.1016/j.compenvurbsys.2024.102243
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212830252&doi=10.1016%2fj.compenvurbsys.2024.102243&partnerID=40&md5=f24a9cfaeede623ceac962e4844405e1
AB  - The attractiveness of urban environments significantly impacts residents' satisfaction with their living spaces and their overall mood, which in turn, affects their health and well-being. Given the resource-intensive nature of gathering evaluations on urban attractiveness through surveys or inquiries from residents, there is a constant quest for automated solutions to streamline this process and support spatial planning. In this study, we applied an off-the-shelf AI model to automate the analysis of urban attractiveness, using over 1800 Google Street View images of Helsinki, Finland. By incorporating the GPT-4 model, we assessed these images through three criteria-based prompts. Simultaneously, 24 participants, categorised into residents and non-residents, were asked to rate the images. To gain insights into the non-transparent decision-making processes of GPT-4, we employed semantic segmentation to explore how the model uses different image features. Our results demonstrated a strong alignment between GPT-4 and participant ratings, although geographic disparities were noted. Specifically, GPT-4 showed a preference for suburban areas with significant greenery, contrasting with participants who found these areas less attractive. Conversely, in the city centre and densely populated urban regions of Helsinki, GPT-4 assigned lower attractiveness scores than participant ratings. The semantic segmentation analysis revealed that GPT-4's ratings were primarily influenced by physical features like vegetation, buildings, and sidewalk. While there was general agreement between AI and human assessments across various locations, GPT-4 struggled to incorporate contextual nuances into its ratings, unlike participants, who considered both context and features of the urban environment. The study suggests that leveraging AI models like GPT-4 allows spatial planners to gather insights into the attractiveness of different areas efficiently. However, caution is necessary, while we used an off-the-shelf model, it is crucial to develop models specifically trained to understand the local context. Although AI models provide valuable insights, human perspectives are essential for a comprehensive understanding of urban attractiveness. © 2024
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Mendel, T.
AU  - Singh, N.
AU  - Mann, D.M.
AU  - Wiesenfeld, B.
AU  - Nov, O.
TI  - Laypeople’s Use of and Attitudes Toward Large Language Models and Search Engines for Health Queries: Survey Study
PY  - 2025
T2  - Journal of Medical Internet Research
VL  - 27
C7  - e64290
DO  - 10.2196/64290
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217972812&doi=10.2196%2f64290&partnerID=40&md5=66c00953dc0326e00f35e140ec572919
AB  - Background: Laypeople have easy access to health information through large language models (LLMs), such as ChatGPT, and search engines, such as Google. Search engines transformed health information access, and LLMs offer a new avenue for answering laypeople’s questions. Objective: We aimed to compare the frequency of use and attitudes toward LLMs and search engines as well as their comparative relevance, usefulness, ease of use, and trustworthiness in responding to health queries. Methods: We conducted a screening survey to compare the demographics of LLM users and nonusers seeking health information, analyzing results with logistic regression. LLM users from the screening survey were invited to a follow-up survey to report the types of health information they sought. We compared the frequency of use of LLMs and search engines using ANOVA and Tukey post hoc tests. Lastly, paired-sample Wilcoxon tests compared LLMs and search engines on perceived usefulness, ease of use, trustworthiness, feelings, bias, and anthropomorphism. Results: In total, 2002 US participants recruited on Prolific participated in the screening survey about the use of LLMs and search engines. Of them, 52% (n=1045) of the participants were female, with a mean age of 39 (SD 13) years. Participants were 9.7% (n=194) Asian, 12.1% (n=242) Black, 73.3% (n=1467) White, 1.1% (n=22) Hispanic, and 3.8% (n=77) were of other races and ethnicities. Further, 1913 (95.6%) used search engines to look up health queries versus 642 (32.6%) for LLMs. Men had higher odds (odds ratio [OR] 1.63, 95% CI 1.34-1.99; P<.001) of using LLMs for health questions than women. Black (OR 1.90, 95% CI 1.42-2.54; P<.001) and Asian (OR 1.66, 95% CI 1.19-2.30; P<.01) individuals had higher odds than White individuals. Those with excellent perceived health (OR 1.46, 95% CI 1.1-1.93; P=.01) were more likely to use LLMs than those with good health. Higher technical proficiency increased the likelihood of LLM use (OR 1.26, 95% CI 1.14-1.39; P<.001). In a follow-up survey of 281 LLM users for health, most participants used search engines first (n=174, 62%) to answer health questions, but the second most common first source consulted was LLMs (n=39, 14%). LLMs were perceived as less useful (P<.01) and less relevant (P=.07), but elicited fewer negative feelings (P<.001), appeared more human (LLM: n=160, vs search: n=32), and were seen as less biased (P<.001). Trust (P=.56) and ease of use (P=.27) showed no differences. Conclusions: Search engines are the primary source of health information; yet, positive perceptions of LLMs suggest growing use. Future work could explore whether LLM trust and usefulness are enhanced by supplementing answers with external references and limiting persuasive language to curb overreliance. Collaboration with health organizations can help improve the quality of LLMs’ health output. ©Tamir Mendel, Nina Singh, Devin M Mann, Batia Wiesenfeld, Oded Nov.
PB  - JMIR Publications Inc.
C2  - 39946180
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Carl, N.
AU  - Haggenmüller, S.
AU  - Wies, C.
AU  - Nguyen, L.
AU  - Winterstein, J.T.
AU  - Hetz, M.J.
AU  - Mangold, M.H.
AU  - Hartung, F.O.
AU  - Grüne, B.
AU  - Holland-Letz, T.
AU  - Michel, M.S.
AU  - Brinker, T.J.
AU  - Wessels, F.
TI  - Evaluating interactions of patients with large language models for medical information
PY  - 2025
T2  - BJU International
DO  - 10.1111/bju.16676
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219667169&doi=10.1111%2fbju.16676&partnerID=40&md5=78ec2d333c2da68c84f899ebc1b7b139
AB  - Objectives: To explore the interaction of real-world patients with a chatbot in a clinical setting, investigating key aspects of medical information provided by large language models (LLMs). Patients and methods: The study enrolled 300 patients seeking urological counselling between February and July 2024. First, participants voluntarily conversed with a Generative Pre-trained Transformer 4 (GPT-4) powered chatbot to ask questions related to their medical situation. In the following survey, patients rated the perceived utility, completeness, and understandability of the information provided during the simulated conversation as well as user-friendliness. Finally, patients were asked which, in their experience, best answered their questions: LLMs, urologists, or search engines. Results: A total of 292 patients completed the study. The majority of patients perceived the chatbot as providing useful, complete, and understandable information, as well as being user-friendly. However, the ability of human urologists to answer medical questions in an understandable way was rated higher than of LLMs. Interestingly, 53% of participants rated the question-answering ability of LLMs higher than search engines. Age was not associated with preferences. Limitations include social desirability and sampling biases. Discussion: This study highlights the potential of LLMs to enhance patient education and communication in clinical settings, with patients valuing their user-friendliness and comprehensiveness for medical information. By addressing preliminary questions, LLMs could potentially relieve time constraints on healthcare providers, enabling medical personnel to focus on complex inquiries and patient care. © 2025 The Author(s). BJU International published by John Wiley & Sons Ltd on behalf of BJU International.
PB  - John Wiley and Sons Inc
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Bedi, S.
AU  - Liu, Y.
AU  - Orr-Ewing, L.
AU  - Dash, D.
AU  - Koyejo, S.
AU  - Callahan, A.
AU  - Fries, J.A.
AU  - Wornow, M.
AU  - Swaminathan, A.
AU  - Lehmann, L.S.
AU  - Hong, H.J.
AU  - Kashyap, M.
AU  - Chaurasia, A.R.
AU  - Shah, N.R.
AU  - Singh, K.
AU  - Tazbaz, T.
AU  - Milstein, A.
AU  - Pfeffer, M.A.
AU  - Shah, N.H.
TI  - Testing and Evaluation of Health Care Applications of Large Language Models: A Systematic Review
PY  - 2025
T2  - JAMA
VL  - 333
IS  - 4
SP  - 319
EP  - 328
DO  - 10.1001/jama.2024.21700
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217518288&doi=10.1001%2fjama.2024.21700&partnerID=40&md5=4f44433c336df6cbb401ecc02cc6aea5
AB  - Importance: Large language models (LLMs) can assist in various health care activities, but current evaluation approaches may not adequately identify the most useful application areas. Objective: To summarize existing evaluations of LLMs in health care in terms of 5 components: (1) evaluation data type, (2) health care task, (3) natural language processing (NLP) and natural language understanding (NLU) tasks, (4) dimension of evaluation, and (5) medical specialty. Data Sources: A systematic search of PubMed and Web of Science was performed for studies published between January 1, 2022, and February 19, 2024. Study Selection: Studies evaluating 1 or more LLMs in health care. Data Extraction and Synthesis: Three independent reviewers categorized studies via keyword searches based on the data used, the health care tasks, the NLP and NLU tasks, the dimensions of evaluation, and the medical specialty. Results: Of 519 studies reviewed, published between January 1, 2022, and February 19, 2024, only 5% used real patient care data for LLM evaluation. The most common health care tasks were assessing medical knowledge such as answering medical licensing examination questions (44.5%) and making diagnoses (19.5%). Administrative tasks such as assigning billing codes (0.2%) and writing prescriptions (0.2%) were less studied. For NLP and NLU tasks, most studies focused on question answering (84.2%), while tasks such as summarization (8.9%) and conversational dialogue (3.3%) were infrequent. Almost all studies (95.4%) used accuracy as the primary dimension of evaluation; fairness, bias, and toxicity (15.8%), deployment considerations (4.6%), and calibration and uncertainty (1.2%) were infrequently measured. Finally, in terms of medical specialty area, most studies were in generic health care applications (25.6%), internal medicine (16.4%), surgery (11.4%), and ophthalmology (6.9%), with nuclear medicine (0.6%), physical medicine (0.4%), and medical genetics (0.2%) being the least represented. Conclusions and Relevance: Existing evaluations of LLMs mostly focus on accuracy of question answering for medical examinations, without consideration of real patient care data. Dimensions such as fairness, bias, and toxicity and deployment considerations received limited attention. Future evaluations should adopt standardized applications and metrics, use clinical data, and broaden focus to include a wider range of tasks and specialties.
C2  - 39405325
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 21
ER  -

TY  - JOUR
AU  - Theodorou, B.
AU  - Danek, B.
AU  - Tummala, V.
AU  - Kumar, S.P.
AU  - Malin, B.
AU  - Sun, J.
TI  - Improving medical machine learning models with generative balancing for equity and excellence
PY  - 2025
T2  - npj Digital Medicine
VL  - 8
IS  - 1
C7  - 100
DO  - 10.1038/s41746-025-01438-z
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218355805&doi=10.1038%2fs41746-025-01438-z&partnerID=40&md5=8146dba6f6fda3ecf7f6c0d14474b1f9
AB  - Applying machine learning to clinical outcome prediction is challenging due to imbalanced datasets and sensitive tasks that contain rare yet critical outcomes and where equitable treatment across diverse patient groups is essential. Despite attempts, biases in predictions persist, driven by disparities in representation and exacerbated by the scarcity of positive labels, perpetuating health inequities. This paper introduces FairPlay, a synthetic data generation approach leveraging large language models, to address these issues. FairPlay enhances algorithmic performance and reduces bias by creating realistic, anonymous synthetic patient data that improves representation and augments dataset patterns while preserving privacy. Through experiments on multiple datasets, we demonstrate that FairPlay boosts mortality prediction performance across diverse subgroups, achieving up to a 21% improvement in F1 Score without requiring additional data or altering downstream training pipelines. Furthermore, FairPlay consistently reduces subgroup performance gaps, as shown by universal improvements in performance and fairness metrics across four experimental setups. © The Author(s) 2025.
PB  - Nature Research
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Scroggins, J.K.
AU  - Hulchafo, I.I.
AU  - Harkins, S.
AU  - Scharp, D.
AU  - Moen, H.
AU  - Davoudi, A.
AU  - Cato, K.
AU  - Tadiello, M.
AU  - Topaz, M.
AU  - Barcelona, V.
TI  - Identifying stigmatizing and positive/preferred language in obstetric clinical notes using natural language processing
PY  - 2025
T2  - Journal of the American Medical Informatics Association
VL  - 32
IS  - 2
SP  - 308
EP  - 317
DO  - 10.1093/jamia/ocae290
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216606737&doi=10.1093%2fjamia%2focae290&partnerID=40&md5=df0ea26f6a598fa32cac9761c5602c5b
AB  - Objective: To identify stigmatizing language in obstetric clinical notes using natural language processing (NLP). Materials and Methods: We analyzed electronic health records from birth admissions in the Northeast United States in 2017. We annotated 1771 clinical notes to generate the initial gold standard dataset. Annotators labeled for exemplars of 5 stigmatizing and 1 positive/preferred language categories. We used a semantic similarity-based search approach to expand the initial dataset by adding additional exemplars, composing an enhanced dataset. We employed traditional classifiers (Support Vector Machine, Decision Trees, and Random Forest) and a transformer-based model, ClinicalBERT (Bidirectional Encoder Representations from Transformers) and BERT base. Models were trained and validated on initial and enhanced datasets and were tested on enhanced testing dataset. Results: In the initial dataset, we annotated 963 exemplars as stigmatizing or positive/preferred. The most frequently identified category was marginalized language/identities (n = 397, 41%), and the least frequent was questioning patient credibility (n = 51, 5%). After employing a semantic similarity-based search approach, 502 additional exemplars were added, increasing the number of low-frequency categories. All NLP models also showed improved performance, with Decision Trees demonstrating the greatest improvement (21%). ClinicalBERT outperformed other models, with the highest average F1-score of 0.78. Discussion: Clinical BERT seems to most effectively capture the nuanced and context-dependent stigmatizing language found in obstetric clinical notes, demonstrating its potential clinical applications for real-time monitoring and alerts to prevent usages of stigmatizing language use and reduce healthcare bias. Future research should explore stigmatizing language in diverse geographic locations and clinical settings to further contribute to high-quality and equitable perinatal care. Conclusion: ClinicalBERT effectively captures the nuanced stigmatizing language in obstetric clinical notes. Our semantic similarity-based search approach to rapidly extract additional exemplars enhanced the performances while reducing the need for labor-intensive annotation. © The Author(s) 2024. Published by Oxford University Press on behalf of the American Medical Informatics Association.
PB  - Oxford University Press
C2  - 39569431
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Bai, X.
AU  - Wang, A.
AU  - Sucholutsky, I.
AU  - Griffiths, T.L.
TI  - Explicitly unbiased large language models still form biased associations
PY  - 2025
T2  - Proceedings of the National Academy of Sciences of the United States of America
VL  - 122
IS  - 8
C7  - e2416228122
DO  - 10.1073/pnas.2416228122
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219322052&doi=10.1073%2fpnas.2416228122&partnerID=40&md5=cc700253277e9ecd4e2933ed3756f4f8
AB  - Large language models (LLMs) can pass explicit social bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: As LLMs become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make. We address both challenges by introducing two measures: LLM Word Association Test, a prompt-based method for revealing implicit bias; and LLM Relative Decision Test, a strategy to detect subtle discrimination in contextual decisions. Both measures are based on psychological research: LLM Word Association Test adapts the Implicit Association Test, widely used to study the automatic associations between concepts held in human minds; and LLM Relative Decision Test operationalizes psychological results indicating that relative evaluations between two candidates, not absolute evaluations assessing each independently, are more diagnostic of implicit biases. Using these measures, we found pervasive stereotype biases mirroring those in society in 8 value-aligned models across 4 social categories (race, gender, religion, health) in 21 stereotypes (such as race and criminality, race and weapons, gender and science, age and negativity). These prompt-based measures draw from psychology’s long history of research into measuring stereotypes based on purely observable behavior; they expose nuanced biases in proprietary value-aligned LLMs that appear unbiased according to standard benchmarks. Copyright © 2025 the Author(s).
PB  - National Academy of Sciences
C2  - 39977313
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Tocilă-Mătășel, C.
AU  - Dudea, S.M.
AU  - Iana, G.
TI  - Addressing Multi-Center Variability in Radiomic Analysis: A Comparative Study of Image Acquisition Methods Across Two 3T MRI Scanners
PY  - 2025
T2  - Diagnostics
VL  - 15
IS  - 4
C7  - 485
DO  - 10.3390/diagnostics15040485
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218905500&doi=10.3390%2fdiagnostics15040485&partnerID=40&md5=5156f52694968e15e4dfd19e573bc26e
AB  - Background: Radiomics has become a valuable tool in medical imaging, but its clinical use is limited by data variability and a lack of reproducibility between centers. This study aims to assess the differences between two scanners and provide guidance on image acquisition methods to reduce variations between images obtained from different centers. Methods: This study utilized medical images obtained in two different imaging centers, with two different 3T MRI scanners. For each scanner, 3D T2 FLAIR sequences were acquired in two forms: the raw and the clinical practice images typically used in diagnostic workflows. The differences between images were analyzed regarding resolution, SNR, CNR, and radiomic features. To facilitate comparison, bias field correction was applied, and the data were standardized to the same scale using Z-score normalization. Descriptive and inferential statistical methods were used to analyze the data. Results: The results show that there are significant differences between centers. Filtering and zero-padding significantly influence the resolution, SNR, CNR values, and radiomics features. Applying Z-score normalization has resolved variations in features sensitive to scale differences, but features reflecting dispersion and extreme values remain significantly different between scanners. Some feature differences may be resolved by analyzing the raw images in both centers. Conclusions: Variations arise due to different acquisition parameters and the differing quality and sensitivity of the equipment. In multi-center studies, acquiring raw images and then applying standardized post-processing methods across all images can enhance the robustness of results. This approach minimizes technical differences, and preserves the integrity of the information, reflecting a more accurate representation of reality and contributing to more reliable and reproducible findings. © 2025 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Huang, T.
AU  - Socrates, V.
AU  - Ovchinnikova, P.
AU  - Faustino, I.
AU  - Kumar, A.
AU  - Safranek, C.
AU  - Chi, L.
AU  - Wang, E.A.
AU  - Puglisi, L.
AU  - Wong, A.H.
AU  - Wang, K.H.
AU  - Taylor, R.A.
TI  - Characterizing Emergency Department Care for Patients With Histories of Incarceration
PY  - 2025
T2  - JACEP Open
VL  - 6
IS  - 1
C7  - 100022
DO  - 10.1016/j.acepjo.2024.100022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216857112&doi=10.1016%2fj.acepjo.2024.100022&partnerID=40&md5=c3b3f6d1e42de37cf3ef1e558587edb8
AB  - Objectives: Patients with a history of incarceration experience bias from health care team members, barriers to privacy, and a multitude of health care disparities. We aimed to assess care processes delivered in emergency departments (EDs) for people with histories of incarceration. Methods: We utilized a fine-tuned large language model to identify patient incarceration status from 480,374 notes from the ED setting. We compared socio-demographic characteristics, comorbidities, and care processes, including disposition, restraint use, and sedation, between individuals with and without a history of incarceration. We then conducted multivariable logistic regression to assess the independent correlation of incarceration history and management in the ED while adjusting for demographic characteristics, health behaviors, presentation, and past medical history. Results: We found 1734 unique patient encounters with a history of incarceration from a total of 177,987 encounters. Patients with history of incarceration were more likely to be male, Black, Hispanic, or other race/ethnicity, currently unemployed or disabled, and had smoking and substance use histories, compared with those without. This cohort demonstrated higher odds of elopement (OR: 3.59 [95% CI: 2.41–5.12]), leaving against medical advice (OR: 2.39 [95% CI: 1.46–3.67]), and being subjected to sedation (OR: 3.89 [95% CI: 3.19–4.70]) and restraint use (OR: 3.76 [95% CI: 3.06–4.57]). After adjusting for covariates, the association between incarceration and elopement remained significant (adjusted odds ratio: 1.65 [95% CI: 1.08–2.43]), while associations with other dispositions, restraint use, and sedation did not persist. Conclusion: This study identified differences in patient characteristics and care processes in the ED for patients with histories of incarceration and demonstrated the potential of using natural language processing in measuring care processes in populations that are largely hidden, but highly prevalent and subject to discrimination, in the health care system. © 2024 The Author(s)
PB  - Elsevier Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Nwachukwu, B.U.
AU  - Varady, N.H.
AU  - Allen, A.A.
AU  - Dines, J.S.
AU  - Altchek, D.W.
AU  - Williams, R.J.
AU  - Kunze, K.N.
TI  - Currently Available Large Language Models Do Not Provide Musculoskeletal Treatment Recommendations That Are Concordant With Evidence-Based Clinical Practice Guidelines
PY  - 2025
T2  - Arthroscopy - Journal of Arthroscopic and Related Surgery
VL  - 41
IS  - 2
SP  - 263
EP  - 275.e6
DO  - 10.1016/j.arthro.2024.07.040
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203009740&doi=10.1016%2fj.arthro.2024.07.040&partnerID=40&md5=807f594cae6dfa8a0823d8843ec1f22c
AB  - Purpose: To determine whether several leading, commercially available large language models (LLMs) provide treatment recommendations concordant with evidence-based clinical practice guidelines (CPGs) developed by the American Academy of Orthopaedic Surgeons (AAOS). Methods: All CPGs concerning the management of rotator cuff tears (n = 33) and anterior cruciate ligament injuries (n = 15) were extracted from the AAOS. Treatment recommendations from Chat-Generative Pretrained Transformer version 4 (ChatGPT-4), Gemini, Mistral-7B, and Claude-3 were graded by 2 blinded physicians as being concordant, discordant, or indeterminate (i.e., neutral response without definitive recommendation) with respect to AAOS CPGs. The overall concordance between LLM and AAOS recommendations was quantified, and the comparative overall concordance of recommendations among the 4 LLMs was evaluated through the Fisher exact test. Results: Overall, 135 responses (70.3%) were concordant, 43 (22.4%) were indeterminate, and 14 (7.3%) were discordant. Inter-rater reliability for concordance classification was excellent (κ = 0.92). Concordance with AAOS CPGs was most frequently observed with ChatGPT-4 (n = 38, 79.2%) and least frequently observed with Mistral-7B (n = 28, 58.3%). Indeterminate recommendations were most frequently observed with Mistral-7B (n = 17, 35.4%) and least frequently observed with Claude-3 (n = 8, 6.7%). Discordant recommendations were most frequently observed with Gemini (n = 6, 12.5%) and least frequently observed with ChatGPT-4 (n = 1, 2.1%). Overall, no statistically significant difference in concordant recommendations was observed across LLMs (P = .12). Of all recommendations, only 20 (10.4%) were transparent and provided references with full bibliographic details or links to specific peer-reviewed content to support recommendations. Conclusions: Among leading commercially available LLMs, more than 1-in-4 recommendations concerning the evaluation and management of rotator cuff and anterior cruciate ligament injuries do not reflect current evidence-based CPGs. Although ChatGPT-4 showed the highest performance, clinically significant rates of recommendations without concordance or supporting evidence were observed. Only 10% of responses by LLMs were transparent, precluding users from fully interpreting the sources from which recommendations were provided. Clinical Relevance: Although leading LLMs generally provide recommendations concordant with CPGs, a substantial error rate exists, and the proportion of recommendations that do not align with these CPGs suggests that LLMs are not trustworthy clinical support tools at this time. Each off-the-shelf, closed-source LLM has strengths and weaknesses. Future research should evaluate and compare multiple LLMs to avoid bias associated with narrow evaluation of few models as observed in the current literature. © 2024 Arthroscopy Association of North America
PB  - W.B. Saunders
C2  - 39173690
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - Gupta, G.K.
AU  - Singh, A.
AU  - Manikandan, S.V.
AU  - Ehtesham, A.
TI  - Digital Diagnostics: The Potential of Large Language Models in Recognizing Symptoms of Common Illnesses
PY  - 2025
T2  - AI (Switzerland)
VL  - 6
IS  - 1
C7  - 13
DO  - 10.3390/ai6010013
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216109661&doi=10.3390%2fai6010013&partnerID=40&md5=dac82ecb72392576bd6725bddfa0a949
AB  - This study aimed to evaluate the potential of Large Language Models (LLMs) in healthcare diagnostics, specifically their ability to analyze symptom-based prompts and provide accurate diagnoses. The study focused on models including GPT-4, GPT-4o, Gemini, o1 Preview, and GPT-3.5, assessing their performance in identifying illnesses based solely on provided symptoms. Symptom-based prompts were curated from reputable medical sources to ensure validity and relevance. Each model was tested under controlled conditions to evaluate their diagnostic accuracy, precision, recall, and decision-making capabilities. Specific scenarios were designed to explore their performance in both general and high-stakes diagnostic tasks. Among the models, GPT-4 achieved the highest diagnostic accuracy, demonstrating strong alignment with medical reasoning. Gemini excelled in high-stakes scenarios requiring precise decision-making. GPT-4o and o1 Preview showed balanced performance, effectively handling real-time diagnostic tasks with a focus on both precision and recall. GPT-3.5, though less advanced, proved dependable for general diagnostic tasks. This study highlights the strengths and limitations of LLMs in healthcare diagnostics. While models such as GPT-4 and Gemini exhibit promise, challenges such as privacy compliance, ethical considerations, and the mitigation of inherent biases must be addressed. The findings suggest pathways for responsibly integrating LLMs into diagnostic processes to enhance healthcare outcomes. © 2025 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Levkovich, I.
AU  - Shinan-Altman, S.
AU  - Elyoseph, Z.
TI  - Can large language models be sensitive to culture suicide risk assessment?
PY  - 2024
T2  - Journal of Cultural Cognitive Science
VL  - 8
IS  - 3
SP  - 275
EP  - 287
DO  - 10.1007/s41809-024-00151-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208100197&doi=10.1007%2fs41809-024-00151-9&partnerID=40&md5=a2a7f012a516facd7609e498d2aef0d3
AB  - Suicide remains a pressing global public health issue. Previous studies have shown the promise of Generative Intelligent (GenAI) Large Language Models (LLMs) in assessing suicide risk in relation to professionals. But the considerations and risk factors that the models use to assess the risk remain as a black box. This study investigates if ChatGPT-3.5 and ChatGPT-4 integrate cultural factors in assessing suicide risks (probability of suicidal ideation, potential for suicide attempt, likelihood of severe suicide attempt, and risk of mortality from a suicidal act) by vignette methodology. The vignettes examined were of individuals from Greece and South Korea, representing countries with low and high suicide rates, respectively. The contribution of this research is to examine risk assessment from an international perspective, as large language models are expected to provide culturally-tailored responses. However, there is a concern regarding cultural biases and racism, making this study crucial. In the evaluation conducted via ChatGPT-4, only the risks associated with a severe suicide attempt and potential mortality from a suicidal act were rated higher for the South Korean characters than for their Greek counterparts. Furthermore, only within the ChatGPT-4 framework was male gender identified as a significant risk factor, leading to a heightened risk evaluation across all variables. ChatGPT models exhibit significant sensitivity to cultural nuances. ChatGPT-4, in particular, offers increased sensitivity and reduced bias, highlighting the importance of gender differences in suicide risk assessment. The findings suggest that, while ChatGPT-4 demonstrates an improved ability to account for cultural and gender-related factors in suicide risk assessment, there remain areas for enhancement, particularly in ensuring comprehensive and unbiased risk evaluations across diverse populations. These results underscore the potential of GenAI models to aid culturally sensitive mental health assessments, yet they also emphasize the need for ongoing refinement to mitigate inherent biases and enhance their clinical utility. © The Author(s) 2024.
PB  - Springer
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Shmueli-Scheuer, M.
AU  - Silverman, Y.
AU  - Halperin, I.
AU  - Gepner, Y.
TI  - Analysis of Reddit Discussions on Motivational Factors for Physical Activity: Cross-Sectional Study
PY  - 2025
T2  - Journal of Medical Internet Research
VL  - 27
C7  - e54489
DO  - 10.2196/54489
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214869354&doi=10.2196%2f54489&partnerID=40&md5=13a5b049defc07b990bab434c982c8d4
AB  - Background: Despite the ample benefits of physical activity (PA), many individuals do not meet the minimum PA recommended by health organizations. Structured questionnaires and interviews are commonly used to study why individuals perform PA and their strategies to adhere to PA. However, certain biases are inherent to these tools that limit what can be concluded from their results. Collecting data from social media channels can complement these studies and provide a more comprehensive overview of PA motives and adherence strategies. Objective: This study aims to investigate motives for engaging in PA, as well as the associated strategies to achieve these goals, as stated by a large number of people on a social media site. Methods: We searched for users’ responses regarding PA motives and adherence strategies in Reddit forums dedicated to PA and analyzed the data using (1) unsupervised clustering to identify topics from the textual comments and (2) supervised classification to classify the comments into the detected topics. A panel of experts participated in both steps for annotation and validation purposes. Results: We analyzed 1577 unique user comments (representing 1577 individual users); of those, 1247 were linked to physical appearance (mentioned in 298/1247, 23.9% of the comments) and improving physical (235/1247, 18.9%) and mental health (211/1247, 16.9%), indicating these as the main motivational factors. The main strategies people used to adhere to PA were habit formation (373/1247, 30%), goal setting (173/1247, 13.9%), enjoyable activities (151/1247, 12.1%), socializing (121/1247, 9.7%), using media (111/1247, 8.9%), using different apps to monitor PA (35/1247, 2.8%), and financial commitment (32/1247, 2.5%). Conclusions: This study presented a novel approach using a language model to investigate why people engage in PA and the strategies they use to adhere to PA using wide-scale, self-disclosed content from popular social media channels. ©Michal Shmueli-Scheuer, Yedidya Silverman, Israel Halperin, Yftach Gepner.
PB  - JMIR Publications Inc.
C2  - 39805106
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Wang, W.
AU  - Chen, H.
AU  - Wang, H.
AU  - Fang, L.
AU  - Wang, H.
AU  - Ding, Y.
AU  - Lu, Y.
AU  - Wu, Q.
TI  - Interpretation knowledge extraction for genetic testing via question-answer model
PY  - 2024
T2  - BMC Genomics
VL  - 25
IS  - 1
C7  - 1062
DO  - 10.1186/s12864-024-10978-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209481741&doi=10.1186%2fs12864-024-10978-9&partnerID=40&md5=77f3ad741869df2c0122f1db6cf0f2fe
AB  - Background: Sequencing-based genetic testing is widely used in biomedical research, including pathogenic microorganism detection with metagenomic next-generation sequencing (mNGS). The application of sequencing results to clinical diagnosis and treatment relies on various interpretation knowledge bases. Currently, the existing knowledge bases are primarily built through manual knowledge extraction. This method requires professionals to read extensive literature and extract relevant knowledge from it, which is time-consuming and costly. Furthermore, manual extraction unavoidably introduces subjective biases. In this study, we aimed to automatically extract knowledge for interpreting mNGS results. Method: We propose a novel approach to automatically extract pathogenic microorganism knowledge based on the question-answer (QA) model. First, we construct a MicrobeDB dataset since there is no available pathogenic microorganism QA dataset for training the model. The created dataset contains 3,161 samples from 618 published papers covering 224 pathogenic microorganisms. Then, we fine-tune the selected baseline model based on MicrobeDB. Finally, we utilize ChatGPT to enhance the diversity of training data, and employ data expansion to increase training data volume. Results: Our method achieves an Exact Match (EM) and F1 score of 88.39% and 93.18%, respectively, on the MicrobeDB test set. We also conduct ablation studies on the proposed data augmentation method. In addition, we perform comparative experiments with the ChatPDF tool based on the ChatGPT API to demonstrate the effectiveness of the proposed method. Conclusions: Our method is effective and valuable for extracting pathogenic microorganism knowledge. © The Author(s) 2024.
PB  - BioMed Central Ltd
C2  - 39522019
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Kee, X.L.J.
AU  - Sng, G.G.R.
AU  - Lim, D.Y.Z.
AU  - Tung, J.Y.M.
AU  - Abdullah, H.R.
AU  - Chowdury, A.R.
TI  - Use of a large language model with instruction-tuning for reliable clinical frailty scoring
PY  - 2024
T2  - Journal of the American Geriatrics Society
VL  - 72
IS  - 12
SP  - 3849
EP  - 3854
DO  - 10.1111/jgs.19114
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200485967&doi=10.1111%2fjgs.19114&partnerID=40&md5=7f90ae801dbe3206d4d49d8f6c52caec
AB  - Background: Frailty is an important predictor of health outcomes, characterized by increased vulnerability due to physiological decline. The Clinical Frailty Scale (CFS) is commonly used for frailty assessment but may be influenced by rater bias. Use of artificial intelligence (AI), particularly Large Language Models (LLMs) offers a promising method for efficient and reliable frailty scoring. Methods: The study utilized seven standardized patient scenarios to evaluate the consistency and reliability of CFS scoring by OpenAI's GPT-3.5-turbo model. Two methods were tested: a basic prompt and an instruction-tuned prompt incorporating CFS definition, a directive for accurate responses, and temperature control. The outputs were compared using the Mann–Whitney U test and Fleiss' Kappa for inter-rater reliability. The outputs were compared with historic human scores of the same scenarios. Results: The LLM's median scores were similar to human raters, with differences of no more than one point. Significant differences in score distributions were observed between the basic and instruction-tuned prompts in five out of seven scenarios. The instruction-tuned prompt showed high inter-rater reliability (Fleiss' Kappa of 0.887) and produced consistent responses in all scenarios. Difficulty in scoring was noted in scenarios with less explicit information on activities of daily living (ADLs). Conclusions: This study demonstrates the potential of LLMs in consistently scoring clinical frailty with high reliability. It demonstrates that prompt engineering via instruction-tuning can be a simple but effective approach for optimizing LLMs in healthcare applications. The LLM may overestimate frailty scores when less information about ADLs is provided, possibly as it is less subject to implicit assumptions and extrapolation than humans. Future research could explore the integration of LLMs in clinical research and frailty-related outcome prediction. © 2024 The American Geriatrics Society.
PB  - John Wiley and Sons Inc
C2  - 39105505
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Bannett, Y.
AU  - Gunturkun, F.
AU  - Pillai, M.
AU  - Herrmann, J.E.
AU  - Luo, I.
AU  - Huffman, L.C.
AU  - Feldman, H.M.
TI  - Applying Large Language Models to Assess Quality of Care: Monitoring ADHD Medication Side Effects
PY  - 2025
T2  - Pediatrics
VL  - 155
IS  - 1
C7  - e2024067223
DO  - 10.1542/peds.2024-067223
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213536832&doi=10.1542%2fpeds.2024-067223&partnerID=40&md5=706d75681e00d70c6a410a31b99cb86f
AB  - OBJECTIVE: To assess the accuracy of a large language model (LLM) in measuring clinician adherence to practice guidelines for monitoring side effects after prescribing medications for children with attention-deficit/hyperactivity disorder (ADHD). METHODS: Retrospective population-based cohort study of electronic health records. Cohort included children aged 6 to 11 years with ADHD diagnosis and 2 or more ADHD medication encounters (stimulants or nonstimulants prescribed) between 2015 and 2022 in a community-based primary health care network (n = 1201). To identify documentation of side effects inquiry, we trained, tested, and deployed an open-source LLM (LLaMA) on all clinical notes from ADHD-related encounters (ADHD diagnosis or ADHD medication prescription), including in-clinic/telehealth and telephone encounters (n = 15 628 notes). Model performance was assessed using holdout and deployment test sets, compared with manual medical record review. RESULTS: The LLaMA model accurately classified notes that contained side effects inquiry (sensitivity = 87.2, specificity = 86.3, area under curve = 0.93 on holdout test set). Analyses revealed no model bias in relation to patient sex or insurance. Mean age (SD) at first prescription was 8.8 (1.6) years; characteristics were mostly similar across patients with and without documented side effects inquiry. Rates of documented side effects inquiry were lower for telephone encounters than for in-clinic/telehealth encounters (51.9% vs 73.0%, P < .001). Side effects inquiry was documented in 61.4% of encounters after stimulant prescriptions and 48.5% of encounters after nonstimulant prescriptions (P = .041). CONCLUSIONS: Deploying an LLM on a variable set of clinical notes, including telephone notes, offered scalable measurement of quality of care and uncovered opportunities to improve psychopharmacological medication management in primary care. Copyright © 2024 by the American Academy of Pediatrics.
PB  - American Academy of Pediatrics
C2  - 39701141
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Cheng, C.P.
AU  - Vasan, V.
AU  - Patel, A.M.
AU  - Shekane, P.R.
TI  - Sentiment analysis of letters of recommendation for a U.S. pain medicine fellowship from 2020 to 2023
PY  - 2025
T2  - Pain Practice
VL  - 25
IS  - 1
C7  - e13416
DO  - 10.1111/papr.13416
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205544927&doi=10.1111%2fpapr.13416&partnerID=40&md5=4bf36e9730f07b1b79b16eafc208cff5
AB  - Objectives: Letters of recommendation (LORs) are an important part of pain medicine fellowship applications that may be subject to implicit bias by the letter's author. This study evaluated letters of recommendation for applications to pain medicine fellowships in the United States to characterize biases and differences among applicants over four application cycles. Methods: This was a retrospective single-site cohort study. De-identified LORs were collected from 2020 to 2023 from one institution. The Valence Aware Dictionary and sEntiment Reasoner (VADER) natural language processing package scored positive LOR sentiment. In addition, the deep learning tool, Empath, scored LORs for 15 sentiments. Wilcoxon rank-sum and one-way ANOVA tests compared scores between applicant demographics: gender, race, medical school type, residency specialty, and chief resident status, as well as letter writers' academic position. Results: Nine hundred and sixty-four applications were studied over four application cycles. Program directors wrote fewer words (p = 0.020) and less positively (p < 0.001) compared to department chairs and letter writers with neither position. Department chairs wrote with less “negative emotion” compared to both program directors and writers with neither position (p < 0.001). Anesthesiologist applicants received more letters highlighting “achievement” (p < 0.001) while PM&R applicants submitted letters with less “negative emotion” (p < 0.001) compared to other specialties. Chief residents' letters scored higher in “leader” sentiment (p < 0.001) and lower in “negative emotion” (p < 0.001). Discussion: Linguistic content did not favor certain genders or races over others. However, disparities in LORs do exist depending on an applicant's specialty and chief resident status, as well as the academic status of the letter writer. © 2024 World Institute of Pain.
PB  - John Wiley and Sons Inc
C2  - 39364730
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Lange, M.
AU  - Koliousis, A.
AU  - Fayez, F.
AU  - Gogarty, E.
AU  - Twumasi, R.
TI  - Schizophrenia more employable than depression? Language-based artificial intelligence model ratings for employability of psychiatric diagnoses and somatic and healthy controls
PY  - 2025
T2  - PLoS ONE
VL  - 20
IS  - 1
C7  - e0315768
DO  - 10.1371/journal.pone.0315768
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214671810&doi=10.1371%2fjournal.pone.0315768&partnerID=40&md5=0959fc87dab65b9d5700ae3996e511da
AB  - Artificial Intelligence (AI) assists recruiting and job searching. Such systems can be biased against certain characteristics. This results in potential misrepresentations and consequent inequalities related to people with mental health disorders. Hence occupational and mental health bias in existing Natural Language Processing (NLP) models used in recruiting and job hunting must be assessed. We examined occupational bias against mental health disorders in NLP models through relationships between occupations, employability, and psychiatric diagnoses. We investigated Word2Vec and GloVe embedding algorithms through analogy questions and graphical representation of cosine similarities. Word2Vec embeddings exhibit minor bias against mental health disorders when asked analogies regarding employability attributes and no evidence of bias when asked analogies regarding high earning jobs. GloVe embeddings view common mental health disorders such as depression less healthy and less employable than severe mental health disorders and most physical health conditions. Overall, physical, and psychiatric disorders are seen as similarly healthy and employable. Both algorithms appear to be safe for use in downstream task without major repercussions. Further research is needed to confirm this. This project was funded by the London Interdisciplinary Social Science Doctoral Training Programme (LISS-DTP). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. © 2025 Lange et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
PB  - Public Library of Science
C2  - 39774560
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ozduran, E.
AU  - Hancı, V.
AU  - Erkin, Y.
AU  - Özbek, İ.C.
AU  - Abdulkerimov, V.
TI  - Assessing the readability, quality and reliability of responses produced by ChatGPT, Gemini, and Perplexity regarding most frequently asked keywords about low back pain
PY  - 2025
T2  - PeerJ
VL  - 13
IS  - 1
C7  - e18847
DO  - 10.7717/peerj.18847
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216024934&doi=10.7717%2fpeerj.18847&partnerID=40&md5=e750422788c559f31f25555d13ec1b98
AB  - Background: Patients who are informed about the causes, pathophysiology, treatment and prevention of a disease are better able to participate in treatment procedures in the event of illness. Artificial intelligence (AI), which has gained popularity in recent years, is defined as the study of algorithms that provide machines with the ability to reason and perform cognitive functions, including object and word recognition, problem solving and decision making. This study aimed to examine the readability, reliability and quality of responses to frequently asked keywords about low back pain (LBP) given by three different AI-based chatbots (ChatGPT, Perplexity and Gemini), which are popular applications in online information presentation today. Methods: All three AI chatbots were asked the 25 most frequently used keywords related to LBP determined with the help of Google Trend. In order to prevent possible bias that could be created by the sequential processing of keywords in the answers given by the chatbots, the study was designed by providing input from different users (EO, VH) for each keyword. The readability of the responses given was determined with the Simple Measure of Gobbledygook (SMOG), Flesch Reading Ease Score (FRES) and Gunning Fog (GFG) readability scores. Quality was assessed using the Global Quality Score (GQS) and the Ensuring Quality Information for Patients (EQIP) score. Reliability was assessed by determining with DISCERN and Journal of American Medical Association (JAMA) scales. Results: The first three keywords detected as a result of Google Trend search were “Lower Back Pain”, “ICD 10 Low Back Pain”, and “Low Back Pain Symptoms”. It was determined that the readability of the responses given by all AI chatbots was higher than the recommended 6th grade readability level (p < 0.001). In the EQIP, JAMA, modified DISCERN and GQS score evaluation, Perplexity was found to have significantly higher scores than other chatbots (p < 0.001). Conclusion: It has been determined that the answers given by AI chatbots to keywords about LBP are difficult to read and have low reliability and quality assessment. It is clear that when new chatbots are introduced, they can provide better guidance to patients with increased clarity and text quality. This study can provide inspiration for future studies on improving the algorithms and responses of AI chatbots. Copyright 2025 Ozduran et al.
PB  - PeerJ Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Xu, S.
AU  - Sun, M.
TI  - Natural Language Processing (NLP): Identifying Linguistic Gender Bias in Electronic Medical Records (EMRs)
PY  - 2025
T2  - Journal of Patient Experience
VL  - 12
DO  - 10.1177/23743735251314843
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216772770&doi=10.1177%2f23743735251314843&partnerID=40&md5=528d04eddce10b63b55190fe4a78ef03
AB  - With the rise of feminism, women report experiencing doubt or discrimination in medical settings. This study aims to explore the linguistic mechanisms by which physicians express disbelief toward patients and to investigate gender differences in the use of negative medical descriptions. A content analysis of 285 electronic medical records was conducted to identify 4 linguistic bias features: judging, reporting, quoting, and fudging. Sentiment classification and knowledge graph with ICD-11 were used to determine the prevalence of these features in the medical records, and logistic regression was applied to test gender differences. A total of 2354 descriptions were analyzed, with 64.7% of the patients identified as male. Descriptions of female patients contained fewer judgmental linguistic features but more fudging-related linguistic features compared to male patients (judging: OR 0.69, 95% CI 0.54-0.88, p < 0.01; fudging: OR 1.38, 95% CI 1.09-1.75, p < 0.01). No significant differences were found in the use of reporting (OR 0.95, 95% CI 0.61-1.47, p = 0.81) and quoting (OR 0.99, 95% CI 0.72-1.36, p = 0.96) between male and female patients. This study highlights how physicians may express disbelief toward patients through linguistic biases, particularly through the use of judging and fudging language. Both male and female patients may face different types of systematic bias from physicians, with female patients experiencing more fudging-related language and less judgmental language compared to male patients. These differences point to a potential mechanism through which gender disparities in healthcare quality may arise, underscoring the need for further investigation and action to address these biases. © The Author(s) 2025.
PB  - SAGE Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Hanna, J.J.
AU  - Wakene, A.D.
AU  - Johnson, A.O.
AU  - Lehmann, C.U.
AU  - Medford, R.J.
TI  - Assessing Racial and Ethnic Bias in Text Generation by Large Language Models for Health Care–Related Tasks: Cross-Sectional Study
PY  - 2025
T2  - Journal of Medical Internet Research
VL  - 27
C7  - e57257
DO  - 10.2196/57257
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000026942&doi=10.2196%2f57257&partnerID=40&md5=2f6987ed3ac7b8caf8ac6f5345596479
AB  - Background: Racial and ethnic bias in large language models (LLMs) used for health care tasks is a growing concern, as it may contribute to health disparities. In response, LLM operators implemented safeguards against prompts that are overtly seeking certain biases. Objective: This study aims to investigate a potential racial and ethnic bias among 4 popular LLMs: GPT-3.5-turbo (OpenAI), GPT-4 (OpenAI), Gemini-1.0-pro (Google), and Llama3-70b (Meta) in generating health care consumer–directed text in the absence of overtly biased queries. Methods: In this cross-sectional study, the 4 LLMs were prompted to generate discharge instructions for patients with HIV. Each patient’s encounter deidentified metadata including race/ethnicity as a variable was passed over in a table format through a prompt 4 times, altering only the race/ethnicity information (African American, Asian, Hispanic White, and non-Hispanic White) each time, while keeping all other information constant. The prompt requested the model to write discharge instructions for each encounter without explicitly mentioning race or ethnicity. The LLM-generated instructions were analyzed for sentiment, subjectivity, reading ease, and word frequency by race/ethnicity. Results: The only observed statistically significant difference between race/ethnicity groups was found in entity count (GPT-4, df=42, P=.047). However, post hoc chi-square analysis for GPT-4’s entity counts showed no significant pairwise differences among race/ethnicity categories after Bonferroni correction. Conclusions: A total of 4 LLMs were relatively invariant to race/ethnicity in terms of linguistic and readability measures. While our study used proxy linguistic and readability measures to investigate racial and ethnic bias among 4 LLM responses in a health care–related task, there is an urgent need to establish universally accepted standards for measuring bias in LLM-generated responses. Further studies are needed to validate these results and assess their implications. © John J Hanna, Abdi D Wakene, Andrew O Johnson, Christoph U Lehmann, Richard J Medford.
PB  - JMIR Publications Inc.
C2  - 40080818
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ong Ly, C.
AU  - Unnikrishnan, B.
AU  - Tadic, T.
AU  - Patel, T.
AU  - Duhamel, J.
AU  - Kandel, S.
AU  - Moayedi, Y.
AU  - Brudno, M.
AU  - Hope, A.
AU  - Ross, H.
AU  - McIntosh, C.
TI  - Shortcut learning in medical AI hinders generalization: method for estimating AI model generalization without external data
PY  - 2024
T2  - npj Digital Medicine
VL  - 7
IS  - 1
C7  - 124
DO  - 10.1038/s41746-024-01118-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193071862&doi=10.1038%2fs41746-024-01118-4&partnerID=40&md5=8a771a1adac32bd83d3898df5e7efc31
AB  - Healthcare datasets are becoming larger and more complex, necessitating the development of accurate and generalizable AI models for medical applications. Unstructured datasets, including medical imaging, electrocardiograms, and natural language data, are gaining attention with advancements in deep convolutional neural networks and large language models. However, estimating the generalizability of these models to new healthcare settings without extensive validation on external data remains challenging. In experiments across 13 datasets including X-rays, CTs, ECGs, clinical discharge summaries, and lung auscultation data, our results demonstrate that model performance is frequently overestimated by up to 20% on average due to shortcut learning of hidden data acquisition biases (DAB). Shortcut learning refers to a phenomenon in which an AI model learns to solve a task based on spurious correlations present in the data as opposed to features directly related to the task itself. We propose an open source, bias-corrected external accuracy estimate, PEst, that better estimates external accuracy to within 4% on average by measuring and calibrating for DAB-induced shortcut learning. © The Author(s) 2024.
PB  - Nature Research
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - McCoy, T.H.
AU  - Perlis, R.H.
TI  - Dimensional Measures of Psychopathology in Children and Adolescents Using Large Language Models
PY  - 2024
T2  - Biological Psychiatry
VL  - 96
IS  - 12
SP  - 940
EP  - 947
DO  - 10.1016/j.biopsych.2024.05.008
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198137212&doi=10.1016%2fj.biopsych.2024.05.008&partnerID=40&md5=ee5ad13b56b381e840b69e73284a6ab9
AB  - Background: To enable greater use of National Institute of Mental Health Research Domain Criteria (RDoC) in real-world settings, we applied large language models (LLMs) to estimate dimensional psychopathology from narrative clinical notes. Methods: We conducted a cohort study using health records from individuals age ≤18 years evaluated in the psychiatric emergency department of a large academic medical center between November 2008 and March 2015. Outcomes were hospital admission and length of emergency department stay. RDoC domains were estimated using a Health Insurance Portability and Accountability Act–compliant LLM (gpt-4-1106-preview) and compared with a previously validated token-based approach. Results: The cohort included 3059 individuals (median age 16 years [interquartile range, 13–18]; 1580 [52%] female, 1479 [48%] male; 105 [3.4%] identified as Asian, 329 [11%] as Black, 288 [9.4%] as Hispanic, 474 [15%] as other race, and 1863 [61%] as White), of whom 1695 (55%) were admitted. Correlation between LLM-extracted RDoC scores and the token-based scores ranged from small to medium as assessed by Kendall's tau (0.14–0.22). In logistic regression models adjusting for sociodemographic and clinical features, admission likelihood was associated with greater scores on all domains, with the exception of the sensorimotor domain, which was inversely associated (p <.001 for all adjusted associations). Tests for bias suggested modest but statistically significant differences in positive valence scores by race (p <.05 for Asian, Black, and Hispanic individuals). Conclusions: An LLM extracted estimates of 6 RDoC domains in an explainable manner, which were associated with clinical outcomes. This approach can contribute to a new generation of prediction models or biological investigations based on dimensional psychopathology. © 2024
PB  - Elsevier Inc.
C2  - 38866172
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Yılmaz, İ.E.
AU  - Doğan, L.
TI  - Talking technology: exploring chatbots as a tool for cataract patient education
PY  - 2025
T2  - Clinical and Experimental Optometry
VL  - 108
IS  - 1
SP  - 56
EP  - 64
DO  - 10.1080/08164622.2023.2298812
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181681329&doi=10.1080%2f08164622.2023.2298812&partnerID=40&md5=db9bc30bc7e8698f641dddb19ca957d7
AB  - Clinical relevance: Worldwide, millions suffer from cataracts, which impair vision and quality of life. Cataract education improves outcomes, satisfaction, and treatment adherence. Lack of health literacy, language and cultural barriers, personal preferences, and limited resources may all impede effective communication. Background: AI can improve patient education by providing personalised, interactive, and accessible information tailored to patient understanding, interest, and motivation. AI chatbots can have human-like conversations and give advice on numerous topics. Methods: This study investigated the efficacy of chatbots in cataract patient education relative to traditional resources like the AAO website, focusing on information accuracy,understandability, actionability, and readability. A descriptive comparative design was used to analyse quantitative data from frequently asked questions about cataracts answered by ChatGPT, Bard, Bing AI, and the AAO website. SOLO taxonomy, PEMAT, and the Flesch-Kincaid ease score were used to collect and analyse the data. Results: Chatbots scored higher than AAO website on cataract-related questions in terms of accuracy (mean SOLO score ChatGPT: 3.1 ± 0.31, Bard: 2.9 ± 0.72, Bing AI: 2.65 ± 0.49, AAO website: 2.4 ± 0.6, (p < 0.001)). For understandability (mean PEMAT-U score AAO website: 0,89 ± 0,04, ChatGPT 0,84 ± 0,02, Bard: 0,84 ± 0,02, Bing AI: 0,81 ± 0,02, (p < 0.001)), and actionability (mean PEMAT-A score ChatGPT: 0.86 ± 0.03, Bard: 0.85 ± 0.06, Bing AI: 0.81 ± 0.05, AAO website: 0.81 ± 0.06, (p < 0.001)) AAO website scored better than chatbots. Flesch-Kincaid readability ease analysis showed that Bard (55,5 ± 8,48) had the highest mean score, followed by AAO website (51,96 ± 12,46), Bing AI (41,77 ± 9,53), and ChatGPT (34,38 ± 9,75, (p < 0.001)). Conclusion: Chatbots have the potential to provide more detailed and accurate data than the AAO website. On the other hand, the AAO website has the advantage of providing information that is more understandable and practical. When patient preferences are not taken into account, generalised or biased information can decrease reliability. © 2024 Optometry Australia.
PB  - Taylor and Francis Ltd.
C2  - 38194585
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Malas, L.
AU  - Shawaqfeh, A.
AU  - Abushakra, A.
TI  - The future of artificial intelligence: evaluating ChatGPT's performance in X sentiment prediction
PY  - 2024
T2  - Discover Artificial Intelligence
VL  - 4
IS  - 1
C7  - 105
DO  - 10.1007/s44163-024-00218-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212448553&doi=10.1007%2fs44163-024-00218-0&partnerID=40&md5=2f699cecd5d9dcf794725334f269bb9a
AB  - Technological advancements have significantly progressed from the inception of computers and the internet to the rise of artificial intelligence (AI), profoundly impacting various sectors such as business, education, and healthcare. Among these advancements, ChatGPT has emerged as a prominent conversational AI tool, facilitating tasks such as sentiment prediction and sector-specific decision-making. This study aims to evaluate ChatGPT's performance in analyzing public sentiment using data from X (formerly Twitter) and to propose an ethical framework for its sector-specific applications. By combining the BERT transformer model with BiLSTM, Random Forest, and K-Nearest Neighbor algorithms, the study achieves a hybrid approach to sentiment prediction, demonstrating superior performance with an accuracy of 86.7%. Furthermore, the study explores ethical considerations, such as fairness, accountability, and transparency, to address ChatGPT's adoption across industries. The findings offer insights into both the technical and ethical dimensions of ChatGPT’s integration into business, education, and healthcare sectors, emphasizing its potential for sustainable and responsible use. © The Author(s) 2024.
PB  - Springer Nature
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Pu, J.
AU  - Hong, J.
AU  - Yu, Q.
AU  - Yu, P.
AU  - Tian, J.
AU  - He, Y.
AU  - Huang, H.
AU  - Yuan, Q.
AU  - Tao, L.
AU  - Peng, Z.
TI  - Accuracy, satisfaction, and impact of custom GPT in acquiring clinical knowledge: Potential for AI-assisted medical education
PY  - 2025
T2  - Medical Teacher
DO  - 10.1080/0142159X.2025.2458808
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216643481&doi=10.1080%2f0142159X.2025.2458808&partnerID=40&md5=3a310a65fbc903cb9293ba56e9ac9de7
AB  - Background: Recent advancements in artificial intelligence (AI) have enabled the customization of large language models to address specific domains such as medical education. This study investigates the practical performance of a custom GPT model in enhancing clinical knowledge acquisition for medical students and physicians. Methods: A custom GPT was developed by incorporating the latest readily available teaching resources. Its accuracy in providing clinical knowledge was evaluated using a set of clinical questions, and responses were compared against established medical guidelines. Satisfaction was assessed through surveys involving medical students and physicians at different stages and from various types of hospitals. The impact of the custom GPT was further evaluated by comparing its role in facilitating clinical knowledge acquisition with traditional learning methods. Results: The custom GPT demonstrated higher accuracy (83.6%) compared to general AI models (65.5%, 69.1%) and was comparable to a professionally developed AI (Glass Health, 83.6%). Residents reported the highest satisfaction compared to clerks and physicians, citing improved learning independence, motivation, and confidence (p < 0.05). Physicians, especially those from teaching hospitals, showed greater eagerness to develop a custom GPT compared to clerks and residents (p < 0.05). The impact analysis revealed that residents using the custom GPT achieved better test scores compared to those using traditional resources (p < 0.05), though fewer perfect scores were obtained. Conclusions: The custom GPT demonstrates significant promise as an innovative tool for advancing medical education, particularly for residents. Its capability to deliver accurate, tailored information complements traditional teaching methods, aiding educators in promoting personalized and consistent training. However, it is essential for both learners and educators to remain critical in evaluating AI-generated information. With continued development and thoughtful integration, AI tools like custom GPTs have the potential to significantly enhance the quality and accessibility of medical education. Practice points AI-assisted tools, particularly custom GPTs, reform medical education by involving educators in content development, thereby achieving individualized educational goals. Custom GPTs benefit residents by enhancing their experience with clinical knowledge acquisition. Integrating custom GPTs with traditional methods promotes educational equity. Over-reliance on custom GPTs may diminish critical thinking. Future innovations in medical education will combine custom GPTs with other teaching technologies. © 2025 Informa UK Limited, trading as Taylor & Francis Group.
PB  - Taylor and Francis Ltd.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Scharp, D.
AU  - Song, J.
AU  - Hobensack, M.
AU  - Palmer, M.H.
AU  - Barcelona, V.
AU  - Topaz, M.
TI  - Applying natural language processing to understand symptoms among older adult home healthcare patients with urinary incontinence
PY  - 2025
T2  - Journal of Nursing Scholarship
VL  - 57
IS  - 1
SP  - 152
EP  - 164
DO  - 10.1111/jnu.13038
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210406619&doi=10.1111%2fjnu.13038&partnerID=40&md5=25fd73d853d49a0d78797daeaee47289
AB  - Introduction: Little is known about the range and frequency of symptoms among older adult home healthcare patients with urinary incontinence, as this information is predominantly contained in clinical notes. Natural language processing can uncover symptom information among older adults with urinary incontinence to promote holistic, equitable care. Design: We conducted a secondary analysis of cross-sectional data collected between January 1, 2015, and December 31, 2017, from the largest HHC agency in the Northeastern United States. We aimed to develop and test a natural language processing algorithm to extract symptom information from clinical notes for older adults with urinary incontinence and analyze differences in symptom documentation by race or ethnicity. Methods: Symptoms were identified through expert clinician-driven Delphi survey rounds. We developed a natural language processing algorithm for symptom identification in clinical notes, examined symptom documentation frequencies, and analyzed differences in symptom documentation by race or ethnicity using chi-squared tests and logistic regression models. Results: In total, 39,179 home healthcare episodes containing 1,098,419 clinical notes for 29,981 distinct patients were included. Nearly 40% of the sample represented racially or ethnically minoritized groups (i.e., 18% Black, 14% Hispanic, 7% Asian/Pacific Islander, 0.3% multi-racial, and 0.2% Native American). Based on expert clinician-driven Delphi survey rounds, the following symptoms were identified: anxiety, dizziness, constipation, syncope, tachycardia, urinary frequency/urgency, urinary hesitancy/retention, and vision impairment/blurred vision. The natural language processing algorithm achieved excellent performance (average precision of 0.92). Approximately 29% of home healthcare episodes had symptom information documented. Compared to home healthcare episodes for White patients, home healthcare episodes for Asian/Pacific Islander (odds ratio = 0.74, 95% confidence interval [0.67–0.80], p < 0.001), Black (odds ratio = 0.69, 95% confidence interval [0.64–0.73], p < 0.001), and Hispanic (odds ratio = 0.91, 95% confidence interval [0.85–0.97], p < 0.01) patients were less likely to have any symptoms documented in clinical notes. Conclusion: We found multidimensional symptoms and differences in symptom documentation among a diverse cohort of older adults with urinary incontinence, underscoring the need for comprehensive assessments by clinicians. Future research should apply natural language processing to other data sources and investigate symptom clusters to inform holistic care strategies for diverse populations. Clinical Relevance: Knowledge of symptoms of older adult home healthcare patients with urinary incontinence can facilitate comprehensive assessments, health equity, and improved outcomes. © 2024 Sigma Theta Tau International.
PB  - John Wiley and Sons Inc
C2  - 39601443
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Aranda Rubio, Y.
AU  - Baztán Cortés, J.J.
AU  - Canillas Del Rey, F.
TI  - Is Artificial Intelligence ageist?
PY  - 2024
T2  - European geriatric medicine
VL  - 15
IS  - 6
SP  - 1957
EP  - 1960
DO  - 10.1007/s41999-024-01070-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212456913&doi=10.1007%2fs41999-024-01070-2&partnerID=40&md5=0b56930093b32e68e333c4679b70daba
AB  - INTRODUCTION: Generative Artificial Intelligence (AI) is a technological innovation with wide applicability in daily life, which could help elderly people. However, it raises potential conflicts, such as biases, omissions and errors. METHODS: Descriptive study through the negative stereotypes towards aging questionnaire (CENVE) conducted on chatbots ChatGPT, Gemini, Perplexity, YOUChat, and Copilot was conducted. RESULTS: Of the chatbots studied, three were above 50% in responses with negative stereotypes, Copilot with high ageism level results, followed by Perplexity. In the health section, Copilot was the chatbot with the most negative connotations regarding old age (13 out of 20 points). In the personality section, Copilot scored 14 out of 20, followed by YOUChat. CONCLUSION: The Copilot chatbot responded to the statements more ageistically than the other platforms. These results highlight the importance of addressing any potential biases in AI to ensure that the responses provided are fair and respectful for all potential users. © 2024. The Author(s), under exclusive licence to European Geriatric Medicine Society.
C2  - 39320544
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Digumarthi, V.
AU  - Amin, T.
AU  - Kanu, S.
AU  - Mathew, J.
AU  - Edwards, B.
AU  - Peterson, L.A.
AU  - Lundy, M.E.
AU  - Hegarty, K.E.
TI  - Preoperative prediction model for risk of readmission after total joint replacement surgery: a random forest approach leveraging NLP and unfairness mitigation for improved patient care and cost-effectiveness
PY  - 2024
T2  - Journal of Orthopaedic Surgery and Research
VL  - 19
IS  - 1
C7  - 287
DO  - 10.1186/s13018-024-04774-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192489883&doi=10.1186%2fs13018-024-04774-0&partnerID=40&md5=d2b95bef12d22560cb8fa2e075c6a872
AB  - Background: The Center for Medicare and Medicaid Services (CMS) imposes payment penalties for readmissions following total joint replacement surgeries. This study focuses on total hip, knee, and shoulder arthroplasty procedures as they account for most joint replacement surgeries. Apart from being a burden to healthcare systems, readmissions are also troublesome for patients. There are several studies which only utilized structured data from Electronic Health Records (EHR) without considering any gender and payor bias adjustments. Methods: For this study, dataset of 38,581 total knee, hip, and shoulder replacement surgeries performed from 2015 to 2021 at Novant Health was gathered. This data was used to train a random forest machine learning model to predict the combined endpoint of emergency department (ED) visit or unplanned readmissions within 30 days of discharge or discharge to Skilled Nursing Facility (SNF) following the surgery. 98 features of laboratory results, diagnoses, vitals, medications, and utilization history were extracted. A natural language processing (NLP) model finetuned from Clinical BERT was used to generate an NLP risk score feature for each patient based on their clinical notes. To address societal biases, a feature bias analysis was performed in conjunction with propensity score matching. A threshold optimization algorithm from the Fairlearn toolkit was used to mitigate gender and payor biases to promote fairness in predictions. Results: The model achieved an Area Under the Receiver Operating characteristic Curve (AUROC) of 0.738 (95% confidence interval, 0.724 to 0.754) and an Area Under the Precision-Recall Curve (AUPRC) of 0.406 (95% confidence interval, 0.384 to 0.433). Considering an outcome prevalence of 16%, these metrics indicate the model’s ability to accurately discriminate between readmission and non-readmission cases within the context of total arthroplasty surgeries while adjusting patient scores in the model to mitigate bias based on patient gender and payor. Conclusion: This work culminated in a model that identifies the most predictive and protective features associated with the combined endpoint. This model serves as a tool to empower healthcare providers to proactively intervene based on these influential factors without introducing bias towards protected patient classes, effectively mitigating the risk of negative outcomes and ultimately improving quality of care regardless of socioeconomic factors. © The Author(s) 2024.
PB  - BioMed Central Ltd
C2  - 38725085
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Mondal, H.
TI  - Ethical engagement with artificial intelligence in medical education
PY  - 2025
T2  - Advances in Physiology Education
VL  - 49
IS  - 1
SP  - 163
EP  - 165
DO  - 10.1152/ADVAN.00188.2024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216715239&doi=10.1152%2fADVAN.00188.2024&partnerID=40&md5=fa211644a390105504c5e3d6ea8820a3
AB  - The integration of large language models (LLMs) in medical education offers both opportunities and challenges. While these artificial intelligence (AI)-driven tools can enhance access to information and support critical thinking, they also pose risks like potential overreliance and ethical concerns. To ensure ethical use, students and instructors must recognize the limitations of LLMs, maintain academic integrity, and handle data cautiously, and instructors should prioritize content quality over AI detection methods. LLMs can be used as supplementary aids rather than primary educational resources, with a focus on enhancing accessibility and equity and fostering a culture of feedback. Institutions should create guidelines that align with their unique educational values, providing clear frameworks that support responsible LLM usage while addressing risks associated with AI in education. Such guidelines should reflect the institution’s pedagogical mission, whether centered on clinical practice, research, or a mix of both, and should be adaptable to evolving educational technologies. 1043-4046/25 Copyright © 2025 The Authors. Licensed under Creative Commons Attribution CC-BY-NC 4.0. 163
PB  - American Physiological Society
C2  - 39749996
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Sasseville, M.
AU  - Ouellet, S.
AU  - Rhéaume, C.
AU  - Sahlia, M.
AU  - Couture, V.
AU  - Després, P.
AU  - Paquette, J.-S.
AU  - Darmon, D.
AU  - Bergeron, F.
AU  - Gagnon, M.-P.
TI  - Bias Mitigation in Primary Health Care Artificial Intelligence Models: Scoping Review
PY  - 2025
T2  - Journal of Medical Internet Research
VL  - 27
C7  - e60269
DO  - 10.2196/60269
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214559411&doi=10.2196%2f60269&partnerID=40&md5=980cbadf5a74fcd7845cc85b93664e5b
AB  - Background: Artificial intelligence (AI) predictive models in primary health care have the potential to enhance population health by rapidly and accurately identifying individuals who should receive care and health services. However, these models also carry the risk of perpetuating or amplifying existing biases toward diverse groups. We identified a gap in the current understanding of strategies used to assess and mitigate bias in primary health care algorithms related to individuals' personal or protected attributes. Objective: This study aimed to describe the attempts, strategies, and methods used to mitigate bias in AI models within primary health care, to identify the diverse groups or protected attributes considered, and to evaluate the results of these approaches on both bias reduction and AI model performance. Methods: We conducted a scoping review following Joanna Briggs Institute (JBI) guidelines, searching Medline (Ovid), CINAHL (EBSCO), PsycINFO (Ovid), and Web of Science databases for studies published between January 1, 2017, and November 15, 2022. Pairs of reviewers independently screened titles and abstracts, applied selection criteria, and performed full-text screening. Discrepancies regarding study inclusion were resolved by consensus. Following reporting standards for AI in health care, we extracted data on study objectives, model features, targeted diverse groups, mitigation strategies used, and results. Using the mixed methods appraisal tool, we appraised the quality of the studies. Results: After removing 585 duplicates, we screened 1018 titles and abstracts. From the remaining 189 full-text articles, we included 17 studies. The most frequently investigated protected attributes were race (or ethnicity), examined in 12 of the 17 studies, and sex (often identified as gender), typically classified as "male versus female" in 10 of the studies. We categorized bias mitigation approaches into four clusters: (1) modifying existing AI models or datasets, (2) sourcing data from electronic health records, (3) developing tools with a "human-in-the-loop" approach, and (4) identifying ethical principles for informed decision-making. Algorithmic preprocessing methods, such as relabeling and reweighing data, along with natural language processing techniques that extract data from unstructured notes, showed the greatest potential for bias mitigation. Other methods aimed at enhancing model fairness included group recalibration and the application of the equalized odds metric. However, these approaches sometimes exacerbated prediction errors across groups or led to overall model miscalibrations. Conclusions: The results suggest that biases toward diverse groups are more easily mitigated when data are open-sourced, multiple stakeholders are engaged, and during the algorithm's preprocessing stage. Further empirical studies that include a broader range of groups, such as Indigenous peoples in Canada, are needed to validate and expand upon these findings. © 2025 JMIR Publications Inc.. All rights reserved.
PB  - JMIR Publications Inc.
C2  - 39773888
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Cárdenas-Tapia, J.
AU  - Pesántez-Avilés, F.
AU  - Vintimilla-Pesántez, S.
AU  - Oyola-Flores, C.
AU  - Torres-Toukoumidis, A.
TI  - Studying Neurology in Latin America: Comparative Analysis of Academic Curriculum
PY  - 2025
T2  - Journal of Educational and Social Research
VL  - 15
IS  - 1
SP  - 268
EP  - 282
DO  - 10.36941/jesr-2025-0021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214810393&doi=10.36941%2fjesr-2025-0021&partnerID=40&md5=c72ff61cfcfb3137a2414caf6f653d1a
AB  - This study analyzes neurology curricula in universities across Latin America, taking into account the diversity in medical education and the need to improve the training of specialists in a constantly evolving field. The objective of the research is to compare neurology educational programs in Latin America by evaluating admission requirements, curricular structure, and courses offered. A comparative methodology was employed to examine the curricula of various Latin American universities included in the QS ranking. Data collection was conducted between November 2023 and July 2024, using Python as the primary programming language for the development and implementation of algorithms, complemented by dolphin-2.7-mixtral. Findings indicate that Colombia and Chile have the highest number of universities offering specialization in neurology, predominantly within private institutions. Common admission requirements were identified, such as holding a medical degree and completing a rotating internship. However, significant differences were observed, including psychological testing and language proficiency requirements. The most common subjects include Clinical Neurology and Neurophysiology, although there is variation in the emphasis on specific topics. There is significant diversity in educational programs, but challenges remain in terms of accessibility and equity. Greater collaboration and standardization among institutions are recommended to improve neurology training in the region. © 2025 Cárdenas-Tapia.
PB  - Richtmann Publishing Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Reis, Z.S.N.
AU  - Pagano, A.S.
AU  - Ramos de Oliveira, I.J.
AU  - Dias, C.D.S.
AU  - Lage, E.M.
AU  - Mineiro, E.F.
AU  - Varella Pereira, G.M.
AU  - de Carvalho Gomes, I.
AU  - Basilio, V.A.
AU  - Cruz-Correia, R.J.
AU  - de Jesus, D.D.R.
AU  - de Souza Júnior, A.P.
AU  - da Rocha, L.C.D.
TI  - Evaluating Large Language Model–Supported Instructions for Medication Use: First Steps Toward a Comprehensive Model
PY  - 2024
T2  - Mayo Clinic Proceedings: Digital Health
VL  - 2
IS  - 4
SP  - 632
EP  - 644
DO  - 10.1016/j.mcpdig.2024.09.006
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209546910&doi=10.1016%2fj.mcpdig.2024.09.006&partnerID=40&md5=9f19c45d3692686bfdd7d61d09a286da
AB  - Objective: To assess the support of large language models (LLMs) in generating clearer and more personalized medication instructions to enhance e-prescription. Patients and Methods: We established patient-centered guidelines for adequate, acceptable, and personalized directions to enhance e-prescription. A dataset comprising 104 outpatient scenarios, with an array of medications, administration routes, and patient conditions, was developed following the Brazilian national e-prescribing standard. Three prompts were submitted to a closed-source LLM. The first prompt involved a generic command, the second one was calibrated for content enhancement and personalization, and the third one requested bias mitigation. The third prompt was submitted to an open-source LLM. Outputs were assessed using automated metrics and human evaluation. We conducted the study between March 1, 2024 and September 10, 2024. Results: Adequacy scores of our closed-source LLM's output showed the third prompt outperforming the first and second one. Full and partial acceptability was achieved in 94.3% of texts with the third prompt. Personalization was rated highly, especially with the second and third prompts. The 2 LLMs showed similar adequacy results. Lack of scientific evidence and factual errors were infrequent and unrelated to a particular prompt or LLM. The frequency of hallucinations was different for each LLM and concerned prescriptions issued upon symptom manifestation and medications requiring dosage adjustment or involving intermittent use. Gender bias was found in our closed-source LLM's output for the first and second prompts, with the third one being bias-free. The second LLM's output was bias-free. Conclusion: This study demonstrates the potential of LLM-supported generation to produce prescription directions and improve communication between health professionals and patients within the e-prescribing system. © 2024 The Authors
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Wang, Y.
AU  - Liu, X.
AU  - Zhu, X.-L.
TI  - Enhancing emerging technology discovery in nanomedicine by integrating innovative sentences using BERT and NLDA
PY  - 2024
T2  - Journal of Data and Information Science
VL  - 9
IS  - 4
SP  - 155
EP  - 195
DO  - 10.2478/jdis-2024-0031
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208920291&doi=10.2478%2fjdis-2024-0031&partnerID=40&md5=069bdd9e92cf97d488a2e0994863d48e
AB  - Purpose: Nanomedicine has significant potential to revolutionize biomedicine and healthcare through innovations in diagnostics, therapeutics, and regenerative medicine. This study aims to develop a novel framework that integrates advanced natural language processing, noise-free topic modeling, and multidimensional bibliometrics to systematically identify emerging nanomedicine technology topics from scientific literature. Design/methodology/approach: The framework involves collecting full-text articles from PubMed Central and nanomedicine-related metrics from the Web of Science for the period 2013–2023. A fine-tuned BERT model is employed to extract key informative sentences. Noiseless Latent Dirichlet Allocation (NLDA) is applied to model interpretable topics from the cleaned corpus. Additionally, we develop and apply metrics for novelty, innovation, growth, impact, and intensity to quantify the emergence of novel technological topics. Findings: By applying this methodology to nanomedical publications, we identify an increasing emphasis on research aligned with global health priorities, particularly inflammation and biomaterial interactions in disease research. This methodology provides deeper insights through full-text analysis and leading to a more robust discovery of emerging technologies. Research limitations: One limitation of this study is its reliance on the existing scientific literature, which may introduce publication biases and language constraints. Additionally, manual annotation of the dataset, while thorough, is subject to subjectivity and can be time-consuming. Future research could address these limitations by incorporating more diverse data sources, and automating the annotation process. Practical implications: The methodology presented can be adapted to explore emerging technologies in other scientific domains. It allows for tailored assessment criteria based on specific contexts and objectives, enabling more precise analysis and decision-making in various fields. Originality/value: This study offers a comprehensive framework for identifying emerging technologies in nanomedicine, combining theoretical insights and practical applications. Its potential for adaptation across scientific disciplines enhances its value for future research and decision-making in technology discovery. © 2024 Sciendo. All rights reserved.
PB  - Sciendo
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Kim, S.-D.
TI  - Trends and Perspectives of mHealth in Obesity Control
PY  - 2025
T2  - Applied Sciences (Switzerland)
VL  - 15
IS  - 1
C7  - 74
DO  - 10.3390/app15010074
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214516523&doi=10.3390%2fapp15010074&partnerID=40&md5=b489d678d8bb62b518e6fdd662e6d458
AB  - The proliferation of mobile health (mHealth) technologies has revolutionized healthcare delivery, particularly in obesity control. This descriptive study aimed to identify the evolving trends and perspectives of mHealth interventions targeting obesity control, leveraging insights from the ChatGPT language model accessed on 1 January 2024. Eighteen items and 32 questions were used to assess trends and perspectives in mHealth for obesity control, with 8 items and 16 questions dedicated to each aspect, respectively. Key trends identified include personalized interventions, integration of emerging technologies, remote monitoring and telemedicine, behavioral economics strategies, and enhanced user engagement. Perspectives highlight potential for equity, empowerment through self-management, privacy and security concerns, evidence-based practice, and interdisciplinary approaches. These findings shed light on mHealth’s potential in combating obesity, offering valuable insights for healthcare practitioners, researchers, and policymakers. They underscore the significance of leveraging technology to address the global burden of obesity and advocate for collaborative efforts to maximize mHealth solutions’ impact on promoting healthier lifestyles and reducing obesity-related complications. © 2024 by the author.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Nawara, D.
AU  - Aly, A.
AU  - Kashef, R.
TI  - Shilling Attacks and Fake Reviews Injection: Principles, Models, and Datasets
PY  - 2025
T2  - IEEE Transactions on Computational Social Systems
VL  - 12
IS  - 1
SP  - 362
EP  - 375
DO  - 10.1109/TCSS.2024.3465008
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207344802&doi=10.1109%2fTCSS.2024.3465008&partnerID=40&md5=a46f5cc1f7479802fcb246e3eace6451
AB  - Recommendation systems have proved to be a compelling performance in overcoming the data overload problem in many domains, such as e-commerce, e-health, and transportation. Recommender systems guide users/clients to personalized recommendations based on their preferences. However, some recommendation systems are vulnerable to shilling attacks, which create rating biases or fake reviews that will eventually affect the authenticity and integrity of the generated recommendations. This survey comprehensively covers various shilling attack methods, including high-knowledge, low-knowledge attacks, and obfuscated attacks. It explores malicious review generators that generate fake text. In addition to that, this survey covers shilling attack detection methods such as supervised, unsupervised, semisupervised, and hybrid techniques. Natural Language Processing techniques are also thoroughly explored for fake text review detection using large language models (LLMs). A wide range of detection mechanisms incorporated in the literature is examined, such as convolutional neural network (CNN), long short term memory (LSTM)-based detectors for rating-based shilling attacks, and bidirectional encoder representation (BERT) and RoBERTa-based detectors for fake reviews that are accompanied by shilling attacks, aiming to offer insights into the evolving methods of shilling attack strategies and the corresponding advancements in the detection methods. © 2014 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Yogarajan, V.
AU  - Dobbie, G.
AU  - Keegan, T.T.
TI  - Debiasing large language models: research opportunities*
PY  - 2025
T2  - Journal of the Royal Society of New Zealand
VL  - 55
IS  - 2
SP  - 372
EP  - 395
DO  - 10.1080/03036758.2024.2398567
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203971216&doi=10.1080%2f03036758.2024.2398567&partnerID=40&md5=9cc7eb85d79d886f6357f96e94c12036
AB  - Large language models (LLMs) are powerful decision-making tools widely adopted in healthcare, finance, and transportation. Embracing the opportunities and innovations of LLMs is inevitable. However, LLMs inherit stereotypes, misrepresentations, discrimination, and societies' biases from various sources–including training data, algorithm design, and user interactions–resulting in concerns about equality, diversity, and fairness. The bias problem has triggered increased research towards defining, detecting and quantifying bias and developing debiasing techniques. The predominant focus in tackling the bias problem is skewed towards resource-rich regions such as the US and Europe, resulting in a scarcity of research in other societies. As a small country with a unique history, culture and social composition, there is an opportunity for Aotearoa New Zealand's (NZ) research community to address this inadequacy. This paper presents an experimental evaluation of existing bias metrics and debiasing techniques in the NZ context. Research gaps derived from the study and a literature review are outlined, current and ongoing research in this space are discussed, and the suggested scope of research opportunities for NZ are presented. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.
PB  - Taylor and Francis Asia Pacific
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Nguyen, D.
AU  - Rao, A.
AU  - Mazumder, A.
AU  - Succi, M.D.
TI  - Exploring the accuracy of embedded ChatGPT-4 and ChatGPT-4o in generating BI-RADS scores: a pilot study in radiologic clinical support
PY  - 2025
T2  - Clinical Imaging
VL  - 117
C7  - 110335
DO  - 10.1016/j.clinimag.2024.110335
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208989470&doi=10.1016%2fj.clinimag.2024.110335&partnerID=40&md5=d3e6ad7f5ba59c4950139d4c3bbf0812
AB  - This study evaluates the accuracy of ChatGPT-4 and ChatGPT-4o in generating Breast Imaging Reporting and Data System (BI-RADS) scores from radiographic images. We tested both models using 77 breast cancer images from radiopaedia.org, including mammograms and ultrasounds. Images were analyzed in separate sessions to avoid bias. ChatGPT-4 and ChatGPT-4o achieved a 66.2 % accuracy across all BI-RADS cases. Performance was highest in BI-RADS 5 cases, with ChatGPT-4 and ChatGPT4o scoring 84.4 % and 88.9 %, respectively. However, both models struggled with BIRADS 1–3 cases, often assigning higher severity ratings. This study highlights the limitations of current LLMs in accurately grading these images and emphasizes the need for further research in these technologies before clinical integration. © 2024 Elsevier Inc.
PB  - Elsevier Inc.
C2  - 39549561
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Farrell, S.
AU  - Anderson, K.
AU  - Noble, P.-J.M.
AU  - Al Moubayed, N.
TI  - Premature mortality analysis of 52,000 deceased cats and dogs exposes socioeconomic disparities
PY  - 2024
T2  - Scientific Reports
VL  - 14
IS  - 1
C7  - 28763
DO  - 10.1038/s41598-024-77385-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209995255&doi=10.1038%2fs41598-024-77385-8&partnerID=40&md5=c20e1a88e08ae44b0b756b78350fccef
AB  - Monitoring mortality rates offers crucial insights into public health by uncovering the hidden impacts of diseases, identifying emerging trends, optimising resource allocation, and informing effective policy decisions. Here, we present a novel approach to analysing premature mortality in companion animals, utilising data from 28,159 deceased dogs and 24,006 deceased cats across the United Kingdom. By employing PetBERT-ICD, an automated large language model (LLM) based International Classification of Disease 11 syndromic classifier, we reveal critical insights into the causes and patterns of premature deaths. Our findings highlight the significant impact of behavioural conditions on premature euthanasia in dogs, particularly in ages one to six. We also identify a 19% increased risk of premature mortality in brachycephalic dog breeds, raising important animal welfare concerns. Our research establishes a strong correlation between socioeconomic status and premature mortality in cats and dogs. Areas with the lowest Index of Multiple Deprivation (IMD) scores show nearly a 50% reduction in the risk of premature mortality across cats and dogs, underscoring the powerful impact that socioeconomic factors can have on pet health and longevity. This research underscores the necessity of examining the socioeconomic disparities affecting animal health outcomes. By addressing these inequities, we can better safeguard the well-being of our companion animals. © The Author(s) 2024.
PB  - Nature Research
C2  - 39567516
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Artsi, Y.
AU  - Sorin, V.
AU  - Konen, E.
AU  - Glicksberg, B.S.
AU  - Nadkarni, G.
AU  - Klang, E.
TI  - Large language models for generating medical examinations: systematic review
PY  - 2024
T2  - BMC Medical Education
VL  - 24
IS  - 1
C7  - 354
DO  - 10.1186/s12909-024-05239-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188953407&doi=10.1186%2fs12909-024-05239-y&partnerID=40&md5=e3d07768fc194f8cae4df3c37c8da8cc
AB  - Background: Writing multiple choice questions (MCQs) for the purpose of medical exams is challenging. It requires extensive medical knowledge, time and effort from medical educators. This systematic review focuses on the application of large language models (LLMs) in generating medical MCQs. Methods: The authors searched for studies published up to November 2023. Search terms focused on LLMs generated MCQs for medical examinations. Non-English, out of year range and studies not focusing on AI generated multiple-choice questions were excluded. MEDLINE was used as a search database. Risk of bias was evaluated using a tailored QUADAS-2 tool. Results: Overall, eight studies published between April 2023 and October 2023 were included. Six studies used Chat-GPT 3.5, while two employed GPT 4. Five studies showed that LLMs can produce competent questions valid for medical exams. Three studies used LLMs to write medical questions but did not evaluate the validity of the questions. One study conducted a comparative analysis of different models. One other study compared LLM-generated questions with those written by humans. All studies presented faulty questions that were deemed inappropriate for medical exams. Some questions required additional modifications in order to qualify. Conclusions: LLMs can be used to write MCQs for medical examinations. However, their limitations cannot be ignored. Further study in this field is essential and more conclusive evidence is needed. Until then, LLMs may serve as a supplementary tool for writing medical examinations. 2 studies were at high risk of bias. The study followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. © The Author(s) 2024.
PB  - BioMed Central Ltd
C2  - 38553693
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 17
ER  -

TY  - JOUR
AU  - Sallam, M.
AU  - Al-Mahzoum, K.
AU  - Alshuaib, O.
AU  - Alhajri, H.
AU  - Alotaibi, F.
AU  - Alkhurainej, D.
AU  - Al-Balwah, M.Y.
AU  - Barakat, M.
AU  - Egger, J.
TI  - Language discrepancies in the performance of generative artificial intelligence models: an examination of infectious disease queries in English and Arabic
PY  - 2024
T2  - BMC Infectious Diseases
VL  - 24
IS  - 1
C7  - 799
DO  - 10.1186/s12879-024-09725-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200897290&doi=10.1186%2fs12879-024-09725-y&partnerID=40&md5=c368984d7ca5c4f31590b2609dbd5083
AB  - Background: Assessment of artificial intelligence (AI)-based models across languages is crucial to ensure equitable access and accuracy of information in multilingual contexts. This study aimed to compare AI model efficiency in English and Arabic for infectious disease queries. Methods: The study employed the METRICS checklist for the design and reporting of AI-based studies in healthcare. The AI models tested included ChatGPT-3.5, ChatGPT-4, Bing, and Bard. The queries comprised 15 questions on HIV/AIDS, tuberculosis, malaria, COVID-19, and influenza. The AI-generated content was assessed by two bilingual experts using the validated CLEAR tool. Results: In comparing AI models’ performance in English and Arabic for infectious disease queries, variability was noted. English queries showed consistently superior performance, with Bard leading, followed by Bing, ChatGPT-4, and ChatGPT-3.5 (P =.012). The same trend was observed in Arabic, albeit without statistical significance (P =.082). Stratified analysis revealed higher scores for English in most CLEAR components, notably in completeness, accuracy, appropriateness, and relevance, especially with ChatGPT-3.5 and Bard. Across the five infectious disease topics, English outperformed Arabic, except for flu queries in Bing and Bard. The four AI models’ performance in English was rated as “excellent”, significantly outperforming their “above-average” Arabic counterparts (P =.002). Conclusions: Disparity in AI model performance was noticed between English and Arabic in response to infectious disease queries. This language variation can negatively impact the quality of health content delivered by AI models among native speakers of Arabic. This issue is recommended to be addressed by AI developers, with the ultimate goal of enhancing health outcomes. © The Author(s) 2024.
PB  - BioMed Central Ltd
C2  - 39118057
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Guevara, M.
AU  - Chen, S.
AU  - Thomas, S.
AU  - Chaunzwa, T.L.
AU  - Franco, I.
AU  - Kann, B.H.
AU  - Moningi, S.
AU  - Qian, J.M.
AU  - Goldstein, M.
AU  - Harper, S.
AU  - Aerts, H.J.W.L.
AU  - Catalano, P.J.
AU  - Savova, G.K.
AU  - Mak, R.H.
AU  - Bitterman, D.S.
TI  - Large language models to identify social determinants of health in electronic health records
PY  - 2024
T2  - npj Digital Medicine
VL  - 7
IS  - 1
C7  - 6
DO  - 10.1038/s41746-023-00970-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181889002&doi=10.1038%2fs41746-023-00970-0&partnerID=40&md5=9da9bacf0c14754c0c06353e98e7a31b
AB  - Social determinants of health (SDoH) play a critical role in patient outcomes, yet their documentation is often missing or incomplete in the structured data of electronic health records (EHRs). Large language models (LLMs) could enable high-throughput extraction of SDoH from the EHR to support research and clinical care. However, class imbalance and data limitations present challenges for this sparsely documented yet critical information. Here, we investigated the optimal methods for using LLMs to extract six SDoH categories from narrative text in the EHR: employment, housing, transportation, parental status, relationship, and social support. The best-performing models were fine-tuned Flan-T5 XL for any SDoH mentions (macro-F1 0.71), and Flan-T5 XXL for adverse SDoH mentions (macro-F1 0.70). Adding LLM-generated synthetic data to training varied across models and architecture, but improved the performance of smaller Flan-T5 models (delta F1 + 0.12 to +0.23). Our best-fine-tuned models outperformed zero- and few-shot performance of ChatGPT-family models in the zero- and few-shot setting, except GPT4 with 10-shot prompting for adverse SDoH. Fine-tuned models were less likely than ChatGPT to change their prediction when race/ethnicity and gender descriptors were added to the text, suggesting less algorithmic bias (p < 0.05). Our models identified 93.8% of patients with adverse SDoH, while ICD-10 codes captured 2.0%. These results demonstrate the potential of LLMs in improving real-world evidence on SDoH and assisting in identifying patients who could benefit from resource support. © 2024, The Author(s).
PB  - Nature Research
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 62
ER  -

TY  - JOUR
AU  - Arthur, M.
AU  - Frank, N.
AU  - Chakma, A.
AU  - Friel, S.
TI  - Climate change mitigation policy for planetary health equity? An automated content analysis of countries’ nationally determined contribution reports
PY  - 2024
T2  - Environmental Research Letters 
VL  - 19
IS  - 11
C7  - 114032
DO  - 10.1088/1748-9326/ad7edf
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206217153&doi=10.1088%2f1748-9326%2fad7edf&partnerID=40&md5=4ba93eebdbf1d2bea76bb889706b1a8c
AB  - This study examines the extent to which national governments discuss the social determinants of planetary health equity (SDPHE) within their Nationally Determined Contribution reports (NDCs) to the UN Framework Convention on Climate Change. This is assessed relative to the frequency of discussion of economic factors and health outcomes, and how this varies between countries based on political, economic, and environmental factors. Using natural language processing, a dictionary-based automated content analysis was conducted of the frequency of terms within these reports. Correlation analyses examined the relationship between the frequency of dictionary categories and political, economic, and environmental variables to ascertain the role of contextual factors. Overall, NDCs were found to feature a greater proportion of economic language compared to health outcome and SDPHE terms. Among the SDPHE, equity- and gender-related terms occurred most frequently. Correlations were identified primarily among high-income countries, that suggest moderate positive associations between levels of CO2 emissions per capita and per dollar of gross domestic product and the use of economic terms, and a negative association of economic language with levels of democracy. Democracy was also positively associated with language related to social norms such as equity and justice, indicating potential scope for impact through democratic pressures. The relatively frequent use of economic frames in NDCs suggests that economic issues may receive more attention compared to the SDPHE. This analysis identifies potential enabling and constraining country-level factors for greater attention to the SDPHE in NDCs and more progressive climate change mitigation policymaking. © 2024 The Author(s). Published by IOP Publishing Ltd.
PB  - Institute of Physics
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Beaulieu-Jones, B.K.
AU  - Frau, F.
AU  - Bozzi, S.
AU  - Chandross, K.J.
AU  - Peterschmitt, M.J.
AU  - Cohen, C.
AU  - Coulovrat, C.
AU  - Kumar, D.
AU  - Kruger, M.J.
AU  - Lipnick, S.L.
AU  - Fitzsimmons, L.
AU  - Kohane, I.S.
AU  - Scherzer, C.R.
TI  - Disease progression strikingly differs in research and real-world Parkinson’s populations
PY  - 2024
T2  - npj Parkinson's Disease
VL  - 10
IS  - 1
C7  - 58
DO  - 10.1038/s41531-024-00667-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187723468&doi=10.1038%2fs41531-024-00667-5&partnerID=40&md5=dd3c460cead5dea88bd17106eedbe106
AB  - Characterization of Parkinson’s disease (PD) progression using real-world evidence could guide clinical trial design and identify subpopulations. Efforts to curate research populations, the increasing availability of real-world data, and advances in natural language processing, particularly large language models, allow for a more granular comparison of populations than previously possible. This study includes two research populations and two real-world data-derived (RWD) populations. The research populations are the Harvard Biomarkers Study (HBS, N = 935), a longitudinal biomarkers cohort study with in-person structured study visits; and Fox Insights (N = 36,660), an online self-survey-based research study of the Michael J. Fox Foundation. Real-world cohorts are the Optum Integrated Claims-electronic health records (N = 157,475), representing wide-scale linked medical and claims data and de-identified data from Mass General Brigham (MGB, N = 22,949), an academic hospital system. Structured, de-identified electronic health records data at MGB are supplemented using a manually validated natural language processing with a large language model to extract measurements of PD progression. Motor and cognitive progression scores change more rapidly in MGB than HBS (median survival until H&Y 3: 5.6 years vs. >10, p < 0.001; mini-mental state exam median decline 0.28 vs. 0.11, p < 0.001; and clinically recognized cognitive decline, p = 0.001). In real-world populations, patients are diagnosed more than eleven years later (RWD mean of 72.2 vs. research mean of 60.4, p < 0.001). After diagnosis, in real-world cohorts, treatment with PD medications has initiated an average of 2.3 years later (95% CI: [2.1–2.4]; p < 0.001). This study provides a detailed characterization of Parkinson’s progression in diverse populations. It delineates systemic divergences in the patient populations enrolled in research settings vs. patients in the real-world. These divergences are likely due to a combination of selection bias and real population differences, but exact attribution of the causes is challenging. This study emphasizes a need to utilize multiple data sources and to diligently consider potential biases when planning, choosing data sources, and performing downstream tasks and analyses. © The Author(s) 2024.
PB  - Nature Research
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - Uribe, S.E.
AU  - Maldupa, I.
AU  - Schwendicke, F.
TI  - Integrating Generative AI in Dental Education: A Scoping Review of Current Practices and Recommendations
PY  - 2025
T2  - European Journal of Dental Education
DO  - 10.1111/eje.13074
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216563031&doi=10.1111%2feje.13074&partnerID=40&md5=f074ae30d77b9deefde0a434cf921b48
AB  - Background: Generative AI (GenAI) tools like ChatGPT are increasingly relevant in dental education, offering potential enhancements in personalised learning and clinical reasoning. However, specific guidance from dental institutions remains unexplored. Aim: To identify, analyse and summarise existing guidelines from universities and organisations on using GenAI in dental education, focusing on recommendations for academic staff. Methods: A scoping review (10.17605/OSF.IO/3XMP7) searched for GenAI guidance on university websites, search engines (Google Search, Scholar, Perplexity and PubMed) and through contacting relevant academics (January 2022 to June 2024). Two reviewers independently screened and extracted data, including implementation details, AI tools and permitted/prohibited uses. Thematic analysis revealed common applications, benefits, challenges and recommendations. Results: Thirty-one unique documents were included from 21 universities in 15 countries and three international organisations. Thematic analysis identified common applications, benefits, challenges and recommendations for integrating GenAI, including facilitating teaching and learning, personalised learning, efficient content creation and encouraging critical thinking. However, challenges such as academic integrity, ethical use, bias and privacy issues were also identified. No dental education-specific guidelines were found. Conclusion: This review identified and summarised existing GenAI guidelines from universities and organisations relevant to dental education. The guidelines emphasise ethical use, transparency, academic integrity, secure environments and AI misuse detection tools. However, the absence of dental specific guidance presents an opportunity to fill this gap, providing recommendations for academic staff to integrate GenAI effectively while promoting critical thinking and responsible AI use. © 2025 The Author(s). European Journal of Dental Education published by John Wiley & Sons Ltd.
PB  - John Wiley and Sons Inc
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ramaswamy, A.
AU  - Hung, M.
AU  - Pelt, J.
AU  - Iranmahboub, P.
AU  - Calderon, L.P.
AU  - Scherr, I.S.
AU  - Wang, G.
AU  - Green, D.
AU  - Patel, N.
AU  - McClure, T.D.
AU  - Barbieri, C.
AU  - Hu, J.C.
AU  - Lindvall, C.
AU  - Scherr, D.S.
TI  - Ascertaining provider-level implicit bias in electronic health records with rules-based natural language processing: A pilot study in the case of prostate cancer
PY  - 2024
T2  - PLoS ONE
VL  - 19
IS  - 12 December
C7  - e0314989
DO  - 10.1371/journal.pone.0314989
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213703029&doi=10.1371%2fjournal.pone.0314989&partnerID=40&md5=10e5e45c171cd86a1a217c0d715c4bef
AB  - Purpose Implicit, unconscious biases in medicine are personal attitudes about race, ethnicity, gender, and other characteristics that may lead to discriminatory patterns of care. However, there is no consensus on whether implicit bias represents a true predictor of differential care given an absence of real-world studies. We conducted the first real-world pilot study of provider implicit bias by evaluating treatment parity in prostate cancer using unstructured data–the most common way providers document granular details of the patient encounter. Methods and findings Patients ≥18 years with a diagnosis of very-low to favorable intermediate-risk prostate cancer followed by 3 urologic oncologists from 2010 through 2021. The race Implicit Association Test was administered to all providers. Natural language processing screened human annotation using validated regex ontologies evaluated each provider’s care on four prostate cancer quality indicators: (1) active surveillance utilization; (2) molecular biomarker discussion; (3) urinary function evaluation; and (4) sexual function evaluation. The chi-squared test and phi coefficient were utilized to respectively measure the statistical significance and the strength of association between race and four quality indicators. 1,094 patients were included. While Providers A and B demonstrated no preference on the race Implicit Association Test, Provider C showed preference for White patients. Provider C recommended active surveillance (p<0.01, φ = 0.175) and considered biomarkers (p = 0.047, φ = 0.127) more often in White men than expected, suggestive of treatment imparity. Provider A considered biomarkers (p<0.01, φ = 0.179) more often in White men than expected. Provider B demonstrated treatment parity in all evaluated quality indicators (p>0.05). Conclusions In this pilot study, providers’ practice patterns were associated with both patient race and implicit racial preferences in prostate cancer. Alerting providers of existing implicit bias may restore parity, however future assessments are needed to validate this concept. © 2024 Ramaswamy et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
PB  - Public Library of Science
C2  - 39775249
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Glicksman, M.
AU  - Wang, S.
AU  - Yellapragada, S.
AU  - Robinson, C.
AU  - Orhurhu, V.
AU  - Emerick, T.
TI  - Artificial intelligence and pain medicine education: Benefits and pitfalls for the medical trainee
PY  - 2025
T2  - Pain Practice
VL  - 25
IS  - 1
C7  - e13428
DO  - 10.1111/papr.13428
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210145101&doi=10.1111%2fpapr.13428&partnerID=40&md5=2e880e26d68a1a36d756aabc778b4415
AB  - Objectives: Artificial intelligence (AI) represents an exciting and evolving technology that is increasingly being utilized across pain medicine. Large language models (LLMs) are one type of AI that has become particularly popular. Currently, there is a paucity of literature analyzing the impact that AI may have on trainee education. As such, we sought to assess the benefits and pitfalls that AI may have on pain medicine trainee education. Given the rapidly increasing popularity of LLMs, we particularly assessed how these LLMs may promote and hinder trainee education through a pilot quality improvement project. Materials and Methods: A comprehensive search of the existing literature regarding AI within medicine was performed to identify its potential benefits and pitfalls within pain medicine. The pilot project was approved by UPMC Quality Improvement Review Committee (#4547). Three of the most commonly utilized LLMs at the initiation of this pilot study – ChatGPT Plus, Google Bard, and Bing AI – were asked a series of multiple choice questions to evaluate their ability to assist in learner education within pain medicine. Results: Potential benefits of AI within pain medicine trainee education include ease of use, imaging interpretation, procedural/surgical skills training, learner assessment, personalized learning experiences, ability to summarize vast amounts of knowledge, and preparation for the future of pain medicine. Potential pitfalls include discrepancies between AI devices and associated cost-differences, correlating radiographic findings to clinical significance, interpersonal/communication skills, educational disparities, bias/plagiarism/cheating concerns, lack of incorporation of private domain literature, and absence of training specifically for pain medicine education. Regarding the quality improvement project, ChatGPT Plus answered the highest percentage of all questions correctly (16/17). Lowest correctness scores by LLMs were in answering first-order questions, with Google Bard and Bing AI answering 4/9 and 3/9 first-order questions correctly, respectively. Qualitative evaluation of these LLM-provided explanations in answering second- and third-order questions revealed some reasoning inconsistencies (e.g., providing flawed information in selecting the correct answer). Conclusions: AI represents a continually evolving and promising modality to assist trainees pursuing a career in pain medicine. Still, limitations currently exist that may hinder their independent use in this setting. Future research exploring how AI may overcome these challenges is thus required. Until then, AI should be utilized as supplementary tool within pain medicine trainee education and with caution. © 2024 The Author(s). Pain Practice published by Wiley Periodicals LLC on behalf of World Institute of Pain.
PB  - John Wiley and Sons Inc
C2  - 39588809
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Wang, D.
AU  - Zhang, S.
TI  - Large language models in medical and healthcare fields: applications, advances, and challenges
PY  - 2024
T2  - Artificial Intelligence Review
VL  - 57
IS  - 11
C7  - 299
DO  - 10.1007/s10462-024-10921-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204776133&doi=10.1007%2fs10462-024-10921-0&partnerID=40&md5=0007efced697e15fb9544bbd57472e8e
AB  - Large language models (LLMs) are increasingly recognized for their advanced language capabilities, offering significant assistance in diverse areas like medical communication, patient data optimization, and surgical planning. Our survey meticulously searched for papers with keywords such as “medical,” “clinical,” “healthcare,” and “LLMs” across various databases, including ACM and Google Scholar. It sought to delve into the latest trends and applications of LLMs in healthcare, analyzing 175 relevant publications to support both practitioners and researchers in the field. We have compiled 56 experimental datasets, various evaluation methods and reviewed cutting-edge LLMs across tasks. Our comprehensive analysis of LLMs in healthcare applications, including medical question-answering, dialogue summarization, electronic health record generation, scientific research, medical education, medical product safety monitoring, clinical health reasoning, and clinical decision support. Furthermore, we have identified the challenges, including data security, inaccurate information, fairness and bias, plagiarism, copyrights, and accountability, and the potential solutions, namely de-identification framework, references,counterfactually fair prompting,opening and ending control codes, and establishing normative standards,to address these open issues,respectively. The findings of this survey exert a profound impact on spurring innovation in practical applications and addressing inherent challenges within the academic and medical communities. © The Author(s) 2024.
PB  - Springer Nature
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Ilin, C.
TI  - Early detection of pediatric health risks using maternal and child health data
PY  - 2024
T2  - Scientific Reports
VL  - 14
IS  - 1
C7  - 15350
DO  - 10.1038/s41598-024-65449-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197476264&doi=10.1038%2fs41598-024-65449-8&partnerID=40&md5=21b698f52685ece9dfe2f5536fe2d4a9
AB  - Machine learning (ML)-driven diagnosis systems are particularly relevant in pediatrics given the well-documented impact of early-life health conditions on later-life outcomes. Yet, early identification of diseases and their subsequent impact on length of hospital stay for this age group has so far remained uncharacterized, likely because access to relevant health data is severely limited. Thanks to a confidential data use agreement with the California Department of Health Care Access and Information, we introduce Ped-BERT: a state-of-the-art deep learning model that accurately predicts the likelihood of 100+ conditions and the length of stay in a pediatric patient’s next medical visit. We link mother-specific pre- and postnatal period health information to pediatric patient hospital discharge and emergency room visits. Our data set comprises 513.9K mother–baby pairs and contains medical diagnosis codes, length of stay, as well as temporal and spatial pediatric patient characteristics, such as age and residency zip code at the time of visit. Following the popular bidirectional encoder representations from the transformers (BERT) approach, we pre-train Ped-BERT via the masked language modeling objective to learn embedding features for the diagnosis codes contained in our data. We then continue to fine-tune our model to accurately predict primary diagnosis outcomes and length of stay for a pediatric patient’s next visit, given the history of previous visits and, optionally, the mother’s pre- and postnatal health information. We find that Ped-BERT generally outperforms contemporary and state-of-the-art classifiers when trained with minimum features. We also find that incorporating mother health attributes leads to significant improvements in model performance overall and across all patient subgroups in our data. Our most successful Ped-BERT model configuration achieves an area under the receiver operator curve (ROC AUC) of 0.927 and an average precision score (APS) of 0.408 for the diagnosis prediction task, and a ROC AUC of 0.855 and APS of 0.815 for the length of hospital stay task. Further, we examine Ped-BERT’s fairness by determining whether prediction errors are evenly distributed across various subgroups of mother–baby demographics and health characteristics, or if certain subgroups exhibit a higher susceptibility to prediction errors. © The Author(s) 2024.
PB  - Nature Research
C2  - 38961161
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ahmed, H.S.
AU  - Devaraj, T.
AU  - Singhvi, M.
AU  - Dasan, T.A.
AU  - Ranganath, P.
TI  - Radio-anatomical evaluation of clinical and radiomic profile of multi-parametric magnetic resonance imaging of de novo glioblastoma multiforme
PY  - 2024
T2  - Journal of the Egyptian National Cancer Institute
VL  - 36
IS  - 1
C7  - 13
DO  - 10.1186/s43046-024-00217-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190886099&doi=10.1186%2fs43046-024-00217-3&partnerID=40&md5=ae8441c9342cfe901f4cfecb2be3006d
AB  - Background: Glioblastoma (GBM) is a fatal, fast-growing, and aggressive brain tumor arising from glial cells or their progenitors. It is a primary malignancy with a poor prognosis. The current study aims at evaluating the neuroradiological parameters of de novo GBM by analyzing the brain multi-parametric magnetic resonance imaging (mpMRI) scans acquired from a publicly available database analysis of the scans. Methods: The dataset used was the mpMRI scans for de novo glioblastoma (GBM) patients from the University of Pennsylvania Health System, called the UPENN-GBM dataset. This was a collection from The Cancer Imaging Archive (TCIA), a part of the National Cancer Institute. The MRIs were reviewed by a single diagnostic radiologist, and the tumor parameters were recorded, wherein all recorded data was corroborated with the clinical findings. Results: The study included a total of 58 subjects who were predominantly male (male:female ratio of 1.07:1). The mean age with SD was 58.49 (11.39) years. Mean survival days with SD were 347 (416.21) days. The left parietal lobe was the most commonly found tumor location with 11 (18.96%) patients. The mean intensity for T1, T2, and FLAIR with SD was 1.45E + 02 (20.42), 1.11E + 02 (17.61), and 141.64 (30.67), respectively (p = < 0.001). The tumor dimensions of anteroposterior, transverse, and craniocaudal gave a z-score (significance level = 0.05) of − 2.53 (p = 0.01), − 3.89 (p < 0.001), and 1.53 (p = 0.12), respectively. Conclusion: The current study takes a third-party database and reduces physician bias from interfering with study findings. Further prospective and retrospective studies are needed to provide conclusive data. © The Author(s) 2024.
PB  - Springer Science and Business Media Deutschland GmbH
C2  - 38644430
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Šuster, S.
AU  - Baldwin, T.
AU  - Verspoor, K.
TI  - Zero- and few-shot prompting of generative large language models provides weak assessment of risk of bias in clinical trials
PY  - 2024
T2  - Research Synthesis Methods
VL  - 15
IS  - 6
SP  - 988
EP  - 1000
DO  - 10.1002/jrsm.1749
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201981549&doi=10.1002%2fjrsm.1749&partnerID=40&md5=2140a9f5c44d4459eff1eb3e9d4228f0
AB  - Existing systems for automating the assessment of risk-of-bias (RoB) in medical studies are supervised approaches that require substantial training data to work well. However, recent revisions to RoB guidelines have resulted in a scarcity of available training data. In this study, we investigate the effectiveness of generative large language models (LLMs) for assessing RoB. Their application requires little or no training data and, if successful, could serve as a valuable tool to assist human experts during the construction of systematic reviews. Following Cochrane's latest guidelines (RoB2) designed for human reviewers, we prepare instructions that are fed as input to LLMs, which then infer the risk associated with a trial publication. We distinguish between two modelling tasks: directly predicting RoB2 from text; and employing decomposition, in which a RoB2 decision is made after the LLM responds to a series of signalling questions. We curate new testing data sets and evaluate the performance of four general- and medical-domain LLMs. The results fall short of expectations, with LLMs seldom surpassing trivial baselines. On the direct RoB2 prediction test set (n = 5993), LLMs perform akin to the baselines (F1: 0.1–0.2). In the decomposition task setup (n = 28,150), similar F1 scores are observed. Our additional comparative evaluation on RoB1 data also reveals results substantially below those of a supervised system. This testifies to the difficulty of solving this task based on (complex) instructions alone. Using LLMs as an assisting technology for assessing RoB2 thus currently seems beyond their reach. © 2024 The Author(s). Research Synthesis Methods published by John Wiley & Sons Ltd.
PB  - John Wiley and Sons Ltd
C2  - 39176994
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Garcia Valencia, O.A.
AU  - Thongprayoon, C.
AU  - Jadlowiec, C.C.
AU  - Mao, S.A.
AU  - Leeaphorn, N.
AU  - Budhiraja, P.
AU  - Craici, I.M.
AU  - Gonzalez Suarez, M.L.
AU  - Cheungpasitporn, W.
TI  - AI-driven translations for kidney transplant equity in Hispanic populations
PY  - 2024
T2  - Scientific Reports
VL  - 14
IS  - 1
C7  - 8511
DO  - 10.1038/s41598-024-59237-7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190301534&doi=10.1038%2fs41598-024-59237-7&partnerID=40&md5=79201141aa83d46708bd8acdeb8d7d9b
AB  - Health equity and accessing Spanish kidney transplant information continues being a substantial challenge facing the Hispanic community. This study evaluated ChatGPT’s capabilities in translating 54 English kidney transplant frequently asked questions (FAQs) into Spanish using two versions of the AI model, GPT-3.5 and GPT-4.0. The FAQs included 19 from Organ Procurement and Transplantation Network (OPTN), 15 from National Health Service (NHS), and 20 from National Kidney Foundation (NKF). Two native Spanish-speaking nephrologists, both of whom are of Mexican heritage, scored the translations for linguistic accuracy and cultural sensitivity tailored to Hispanics using a 1–5 rubric. The inter-rater reliability of the evaluators, measured by Cohen’s Kappa, was 0.85. Overall linguistic accuracy was 4.89 ± 0.31 for GPT-3.5 versus 4.94 ± 0.23 for GPT-4.0 (non-significant p = 0.23). Both versions scored 4.96 ± 0.19 in cultural sensitivity (p = 1.00). By source, GPT-3.5 linguistic accuracy was 4.84 ± 0.37 (OPTN), 4.93 ± 0.26 (NHS), 4.90 ± 0.31 (NKF). GPT-4.0 scored 4.95 ± 0.23 (OPTN), 4.93 ± 0.26 (NHS), 4.95 ± 0.22 (NKF). For cultural sensitivity, GPT-3.5 scored 4.95 ± 0.23 (OPTN), 4.93 ± 0.26 (NHS), 5.00 ± 0.00 (NKF), while GPT-4.0 scored 5.00 ± 0.00 (OPTN), 5.00 ± 0.00 (NHS), 4.90 ± 0.31 (NKF). These high linguistic and cultural sensitivity scores demonstrate Chat GPT effectively translated the English FAQs into Spanish across systems. The findings suggest Chat GPT’s potential to promote health equity by improving Spanish access to essential kidney transplant information. Additional research should evaluate its medical translation capabilities across diverse contexts/languages. These English-to-Spanish translations may increase access to vital transplant information for underserved Spanish-speaking Hispanic patients. © The Author(s) 2024.
PB  - Nature Research
C2  - 38609476
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - O’Brien, H.
AU  - Salm, M.
AU  - Morton, L.T.
AU  - Szukszto, M.
AU  - O’Farrell, F.
AU  - Boulton, C.
AU  - King, L.
AU  - Bola, S.K.
AU  - Becker, P.D.
AU  - Craig, A.
AU  - Nielsen, M.
AU  - Samuels, Y.
AU  - Swanton, C.
AU  - Mansour, M.R.
AU  - Hadrup, S.R.
AU  - Quezada, S.A.
TI  - A modular protein language modelling approach to immunogenicity prediction
PY  - 2024
T2  - PLoS Computational Biology
VL  - 20
IS  - 11
C7  - e1012511
DO  - 10.1371/journal.pcbi.1012511
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208915158&doi=10.1371%2fjournal.pcbi.1012511&partnerID=40&md5=83395317471d64cda965307363fe8839
AB  - Neoantigen immunogenicity prediction is a highly challenging problem in the development of personalised medicines. Low reactivity rates in called neoantigens result in a difficult prediction scenario with limited training datasets. Here we describe ImmugenX, a modular protein language modelling approach to immunogenicity prediction for CD8+ reactive epitopes. ImmugenX comprises of a pMHC encoding module trained on three pMHC prediction tasks, an optional TCR encoding module and a set of context specific immunogenicity prediction head modules. Compared with state-of-the-art models for each task, ImmugenX’s encoding module performs comparably or better on pMHC binding affinity, eluted ligand prediction and stability tasks. ImmugenX outperforms all compared models on pMHC immunogenicity prediction (Area under the receiver operating characteristic curve = 0.619, average precision: 0.514), with a 7% increase in average precision compared to the next best model. ImmugenX shows further improved performance on immunogenicity prediction with the integration of TCR context information. ImmugenX performance is further analysed for interpret-ability, which locates areas of weakness found across existing immunogenicity models and highlight possible biases in public datasets. © 2024 O’Brien et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
PB  - Public Library of Science
C2  - 39527593
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Yun, H.S.
AU  - Arjmand, M.
AU  - Sherlock, P.
AU  - Paasche-Orlow, M.K.
AU  - Griffith, J.W.
AU  - Bickmore, T.
TI  - Keeping Users Engaged During Repeated Interviews by a Virtual Agent: Using Large Language Models to Reliably Diversify Questions
PY  - 2024
T2  - Proceedings of the 24th ACM International Conference on Intelligent Virtual Agents, IVA 2024
C7  - 13
DO  - 10.1145/3652988.3673929
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215535547&doi=10.1145%2f3652988.3673929&partnerID=40&md5=4a0088385425be579508beec04947151
AB  - Standardized, validated questionnaires are vital tools in research and healthcare, offering dependable self-report data. Prior work has revealed that virtual agent-administered questionnaires are almost equivalent to self-administered ones in an electronic form. Despite being an engaging method, repeated use of virtual agent-administered questionnaires in longitudinal or pre-post studies can induce respondent fatigue, impacting data quality via response biases and decreased response rates. We propose using large language models (LLMs) to generate diverse questionnaire versions while retaining good psychometric properties. In a longitudinal study, participants interacted with our agent system and responded daily for two weeks to one of the following questionnaires: a standardized depression questionnaire, question variants generated by LLMs, or question variants accompanied by LLM-generated small talk. The responses were compared to a validated depression questionnaire. Psychometric testing revealed consistent covariation between the external criterion and focal measure administered across the three conditions, demonstrating the reliability and validity of the LLM-generated variants. Participants found that the variants were significantly less repetitive than repeated administrations of the same standardized questionnaire. Our findings highlight the potential of LLM-generated variants to invigorate agent-administered questionnaires and foster engagement and interest, without compromising their validity. © 2024 Copyright held by the owner/author(s).
PB  - Association for Computing Machinery, Inc
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ye, H.
AU  - Moreno, T.
AU  - Alpern, A.
AU  - Ehwerhemuepha, L.
AU  - Qu, A.
TI  - DYNAMIC TOPIC LANGUAGE MODEL ON HETEROGENEOUS CHILDREN’S MENTAL HEALTH CLINICAL NOTES
PY  - 2024
T2  - Annals of Applied Statistics
VL  - 18
IS  - 4
SP  - 3165
EP  - 3184
DO  - 10.1214/24-AOAS1930
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210825739&doi=10.1214%2f24-AOAS1930&partnerID=40&md5=59cf769276df54e14c9c00b2c11855e4
AB  - Mental health diseases which affect children’s lives and well-beings have received increased attention since the COVID-19 pandemic. Analyzing psychiatric clinical notes with topic models is critical to evaluating children’s mental status over time. However, few topic models are built for longitudinal settings, and most existing approaches fail to capture temporal trajectories for each document. To address these challenges, we develop a dynamic topic model with consistent topics and individualized temporal dependencies on the evolving document metadata. Our model preserves the semantic mean-ing of discovered topics over time and incorporates heterogeneity among documents. In particular, when documents can be categorized, we propose a classifier-free approach to maximize topic heterogeneity across different document groups. We also present an efficient variational optimization procedure adapted for the multistage longitudinal setting. In this case study, we apply our method to the psychiatric clinical notes from a large tertiary pedi-atric hospital in Southern California and achieve a 38% increase in the overall coherence of extracted topics. Our real data analysis reveals that children tend to express more negative emotions during state shutdowns and more positive when schools reopen. Furthermore, it suggests that sexual and gender minority (SGM) children display more pronounced reactions to major COVID-19 events and a greater sensitivity to vaccine-related news than non-SGM chil-dren. This study examines children’s mental health progression during the pandemic and offers clinicians valuable insights to recognize disparities in children’s mental health related to their sexual and gender identities. © Institute of Mathematical Statistics, 2024.
PB  - Institute of Mathematical Statistics
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Sha, S.
AU  - Loveys, K.
AU  - Qualter, P.
AU  - Shi, H.
AU  - Krpan, D.
AU  - Galizzi, M.
TI  - Efficacy of relational agents for loneliness across age groups: a systematic review and meta-analysis
PY  - 2024
T2  - BMC Public Health
VL  - 24
IS  - 1
C7  - 1802
DO  - 10.1186/s12889-024-19153-x
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197751926&doi=10.1186%2fs12889-024-19153-x&partnerID=40&md5=d10676a3c9d3db66dfbed890711c3a4b
AB  - Background: Loneliness is a serious public health concern. Although previous interventions have had some success in mitigating loneliness, the field is in search of novel, more effective, and more scalable solutions. Here, we focus on “relational agents”, a form of software agents that are increasingly powered by artificial intelligence and large language models (LLMs). We report on a systematic review and meta-analysis to investigate the impact of relational agents on loneliness across age groups. Methods: In this systematic review and meta-analysis, we searched 11 databases including Ovid MEDLINE and Embase from inception to Sep 16, 2022. We included randomised controlled trials and non-randomised studies of interventions published in English across all age groups. These loneliness interventions, typically attempt to improve social skills, social support, social interaction, and maladaptive cognitions. Peer-reviewed journal articles, books, book chapters, Master’s and PhD theses, or conference papers were eligible for inclusion. Two reviewers independently screened studies, extracted data, and assessed risk of bias via the RoB 2 and ROBINS-I tools. We calculated pooled estimates of Hedge’s g in a random-effects meta-analysis and conducted sensitivity and sub-group analyses. We evaluated publication bias via funnel plots, Egger’s test, and a trim-and-fill algorithm. Findings: Our search identified 3,935 records of which 14 met eligibility criteria and were included in our meta-analysis. Included studies comprised 286 participants with individual study sample sizes ranging from 4 to 42 participants (x̄ = 20.43, s = 11.58, x̃ = 20). We used a Bonferroni correction with αBonferroni = 0.05 / 4 = 0.0125 and applied Knapp-Hartung adjustments. Relational agents reduced loneliness significantly at an adjusted αBonferroni (g = -0.552; 95% Knapp-Hartung CI, -0.877 to -0.226; P = 0.003), which corresponds to a moderate reduction in loneliness. Conclusion: Our results are currently the most comprehensive of their kind and provide promising evidence for the efficacy of relational agents. Relational agents are a promising technology that can alleviate loneliness in a scalable way and that can be a meaningful complement to other approaches. The advent of LLMs should boost their efficacy, and further research is needed to explore the optimal design and use of relational agents. Future research could also address shortcomings of current results, such as small sample sizes and high risk of bias. Particularly young audiences have been overlooked in past research. © The Author(s) 2024.
PB  - BioMed Central Ltd
C2  - 38971769
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Sarker, I.H.
TI  - LLM potentiality and awareness: a position paper from the perspective of trustworthy and responsible AI modeling
PY  - 2024
T2  - Discover Artificial Intelligence
VL  - 4
IS  - 1
C7  - 40
DO  - 10.1007/s44163-024-00129-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193991750&doi=10.1007%2fs44163-024-00129-0&partnerID=40&md5=98623ab967fcc4d0f3b225755b8a397e
AB  - Large language models (LLMs) are an exciting breakthrough in the rapidly growing field of artificial intelligence (AI), offering unparalleled potential in a variety of application domains such as finance, business, healthcare, cybersecurity, and so on. However, concerns regarding their trustworthiness and ethical implications have become increasingly prominent as these models are considered black-box and continue to progress. This position paper explores the potentiality of LLM from diverse perspectives as well as the associated risk factors with awareness. Towards this, we highlight not only the technical challenges but also the ethical implications and societal impacts associated with LLM deployment emphasizing fairness, transparency, explainability, trust and accountability. We conclude this paper by summarizing potential research scopes with direction. Overall, the purpose of this position paper is to contribute to the ongoing discussion of LLM potentiality and awareness from the perspective of trustworthiness and responsibility in AI. © The Author(s) 2024.
PB  - Springer Nature
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 12
ER  -

TY  - JOUR
AU  - Fatharani, A.
AU  - Alsayegh, A.
TI  - Pharmacogenomics Meets Generative AI: Transforming Clinical Trial Design with Large Language Models
PY  - 2025
T2  - Journal of Pharmacology and Pharmacotherapeutics
DO  - 10.1177/0976500X251321885
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000011680&doi=10.1177%2f0976500X251321885&partnerID=40&md5=eef81ce53ac2f107a9bb9824028d7b7b
AB  - Background: Pharmacogenomics aims to optimise drug therapy based on genetic makeup, but traditional clinical trial design faces challenges with complexity, cost and data integration. Purpose: This study explores integrating generative artificial intelligence (AI), specifically large language models (LLMs) like Llama3 8B, Mistral 7B v0.3 and Phi-3 Mini 3.8B, into pharmacogenomics clinical trial design through Retrieval-Augmented Generation frameworks and local knowledge bases to address the challenges. Materials and Methods: We conducted a comparative analysis of LLMs, evaluating the accuracy, relevancy, response time and operational efficiency with a case study that assessed LLMs’ capacity to address key trial design elements. The LLMs were locally run using an RTX 4080 mobile graphics card and Intel Core i9-13980HX central processing unit, with Open-WebUI employed. Results: Our results show that Llama3 8B and Phi-3 Mini 3.8B both achieved an accuracy and relevancy score of 0.92 and 0.89, showcasing their underscore of advanced capabilities in delivering both accurate and contextually relevant outputs. More thorough results showed that Phi-3 Mini 3.8B excelled in efficiency and scalability, while Llama3 8B provided greater contextual depth. Conclusion: This study indicates that generative AI offers transformative potential in pharmacogenomics clinical trials, enhancing efficiency and outcomes. However, challenges such as potential bias and the need for further validation remain. Addressing these limitations and advancing multimodal AI capabilities will further support inclusive and effective trial designs. © The Author(s) 2025.
PB  - Sage Publications India Pvt. Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Tzavella, K.
AU  - Diaz, A.
AU  - Olsen, C.
AU  - Vranken, W.
TI  - Combining evolution and protein language models for an interpretable cancer driver mutation prediction with D2Deep
PY  - 2025
T2  - Briefings in Bioinformatics
VL  - 26
IS  - 1
C7  - bbae664
DO  - 10.1093/bib/bbae664
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214185367&doi=10.1093%2fbib%2fbbae664&partnerID=40&md5=8dede932681838b566c008da84c831fc
AB  - The mutations driving cancer are being increasingly exposed through tumor-specific genomic data. However, differentiating between cancer-causing driver mutations and random passenger mutations remains challenging. State-of-the-art homology-based predictors contain built-in biases and are often ill-suited to the intricacies of cancer biology. Protein language models have successfully addressed various biological problems but have not yet been tested on the challenging task of cancer driver mutation prediction at a large scale. Additionally, they often fail to offer result interpretation, hindering their effective use in clinical settings. The AI-based D2Deep method we introduce here addresses these challenges by combining two powerful elements: (i) a nonspecialized protein language model that captures the makeup of all protein sequences and (ii) protein-specific evolutionary information that encompasses functional requirements for a particular protein. D2Deep relies exclusively on sequence information, outperforms state-of-the-art predictors, and captures intricate epistatic changes throughout the protein caused by mutations. These epistatic changes correlate with known mutations in the clinical setting and can be used for the interpretation of results. The model is trained on a balanced, somatic training set and so effectively mitigates biases related to hotspot mutations compared to state-of-the-art techniques. The versatility of D2Deep is illustrated by its performance on non-cancer mutation prediction, where most variants still lack known consequences. D2Deep predictions and confidence scores are available via https://tumorscope.be/d2deep to help with clinical interpretation and mutation prioritization. © The Author(s) 2024. Published by Oxford University Press.
PB  - Oxford University Press
C2  - 39708841
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Yeo, Y.H.
AU  - Peng, Y.
AU  - Mehra, M.
AU  - Samaan, J.
AU  - Hakimian, J.
AU  - Clark, A.
AU  - Suchak, K.
AU  - Krut, Z.
AU  - Andersson, T.
AU  - Persky, S.
AU  - Liran, O.
AU  - Spiegel, B.
TI  - Evaluating for Evidence of Sociodemographic Bias in Conversational AI for Mental Health Support
PY  - 2025
T2  - Cyberpsychology, Behavior, and Social Networking
VL  - 28
IS  - 1
SP  - 44
EP  - 51
DO  - 10.1089/cyber.2024.0199
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207624553&doi=10.1089%2fcyber.2024.0199&partnerID=40&md5=6f034333e5a1b38771e27938746a5f98
AB  - The integration of large language models (LLMs) into healthcare highlights the need to ensure their efficacy while mitigating potential harms, such as the perpetuation of biases. Current evidence on the existence of bias within LLMs remains inconclusive. In this study, we present an approach to investigate the presence of bias within an LLM designed for mental health support. We simulated physician-patient conversations by using a communication loop between an LLM-based conversational agent and digital standardized patients (DSPs) that engaged the agent in dialogue while remaining agnostic to sociodemographic characteristics. In contrast, the conversational agent was made aware of each DSP’s characteristics, including age, sex, race/ethnicity, and annual income. The agent’s responses were analyzed to discern potential systematic biases using the Linguistic Inquiry and Word Count tool. Multivariate regression analysis, trend analysis, and group-based trajectory models were used to quantify potential biases. Among 449 conversations, there was no evidence of bias in both descriptive assessments and multivariable linear regression analyses. Moreover, when evaluating changes in mean tone scores throughout a dialogue, the conversational agent exhibited a capacity to show understanding of the DSPs’ chief complaints and to elevate the tone scores of the DSPs throughout conversations. This finding did not vary by any sociodemographic characteristics of the DSP. Using an objective methodology, our study did not uncover significant evidence of bias within an LLM-enabled mental health conversational agent. These findings offer a complementary approach to examining bias in LLM-based conversational agents for mental health support. Copyright 2025, Mary Ann Liebert, Inc., publishers.
PB  - Mary Ann Liebert Inc.
C2  - 39446671
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Chakshu, N.K.
AU  - Nithiarasu, P.
TI  - Orbital learning: a novel, actively orchestrated decentralised learning for healthcare
PY  - 2024
T2  - Scientific Reports
VL  - 14
IS  - 1
C7  - 10459
DO  - 10.1038/s41598-024-60915-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192238401&doi=10.1038%2fs41598-024-60915-9&partnerID=40&md5=b2568ef92e2179f3261850797aec183c
AB  - A novel collaborative and continual learning across a network of decentralised healthcare units, avoiding identifiable data-sharing capacity, is proposed. Currently available methodologies, such as federated learning and swarm learning, have demonstrated decentralised learning. However, the majority of them face shortcomings that affect their performance and accuracy. These shortcomings include a non-uniform rate of data accumulation, non-uniform patient demographics, biased human labelling, and erroneous or malicious training data. A novel method to reduce such shortcomings is proposed in the present work through selective grouping and displacing of actors in a network of many entities for intra-group sharing of learning with inter-group accessibility. The proposed system, known as Orbital Learning, incorporates various features from split learning and ensemble learning for a robust and secure performance of supervised models. A digital embodiment of the information quality and flow within a decentralised network, this platform also acts as a digital twin of healthcare network. An example of ECG classification for arrhythmia with 6 clients is used to analyse its performance and is compared against federated learning. In this example, four separate experiments are conducted with varied configurations, such as varied age demographics and clients with data tampering. The results obtained show an average area under receiver operating characteristic curve (AUROC) of 0.819 (95% CI 0.784–0.853) for orbital learning whereas 0.714 (95% CI 0.692–0.736) for federated learning. This result shows an increase in overall performance and establishes that the proposed system can address the majority of the issues faced by existing decentralised learning methodologies. Further, a scalability demo conducted establishes the versatility and scalability of this platform in handling state-of-the-art large language models. © The Author(s) 2024.
PB  - Nature Research
C2  - 38714825
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Sahoo, D.
AU  - Urkude, S.V.
TI  - Role of ChatGPT in Decision Making Across Industries: An Indian Perspective
PY  - 2025
T2  - Communications in Computer and Information Science
VL  - 2184 CCIS
SP  - 99
EP  - 112
DO  - 10.1007/978-3-031-71481-8_8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208246983&doi=10.1007%2f978-3-031-71481-8_8&partnerID=40&md5=c945cbbc67ea630598f055286ea03cb7
AB  - This study delves deeply into the functions of ChatGPT, an AI model built on the GPT-3.5 architecture, across multiple industries. Through ChatGPT, artificial intelligence (AI) and natural language processing (NLP) are combined to create new opportunities for a variety of sectors. This summary highlights the report’s analysis of ChatGPT’s applications in marketing, education, healthcare, investment decision-making, and agriculture sector. The survey methodology involves online distribution of questionnaires to gather insights from respondents in each sector. The data was collected using Google Forms, analyzed and interpreted to provide nuanced perspectives on ChatGPT’s role in these sectors. While ChatGPT demonstrates potential to enhance efficiency and productivity, challenges such as data privacy, biases, and overreliance on technology must be addressed. The report concludes that responsible integration of ChatGPT in various sectors necessitates a balance between AI capabilities and human expertise. As technology evolves, the transformative potential of ChatGPT continues to shape these industries and contribute to responsible innovation. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Chang, K.
AU  - Ge, J.
AU  - Yao, H.
AU  - Yin, X.
TI  - Supervised Contrastive learning based fine-tuning framework with Small-Scale WSI dataset on ViT
PY  - 2024
T2  - ACM International Conference Proceeding Series
SP  - 122
EP  - 130
DO  - 10.1145/3674658.3674679
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212828548&doi=10.1145%2f3674658.3674679&partnerID=40&md5=690a93aefee1d1db65f01f75a3211456
AB  - Contrastive learning has been effectively implemented in natural language processing, leveraging large and complex networks trained through data augmentation.Its extension to computer vision, particularly for training Vision Transformers (ViT) with small-scale Whole Slide Imaging (WSI) datasets, faces unique challenges due to the intrinsic characteristics of medical images.These challenges include the need for specialized data augmentation techniques informed by domain-specific knowledge in histopathology.Traditional methods of manually designing these augmentations can be labor-intensive, inefficient, and introduce bias.To overcome these issues, we introduce a novel method named WSI-Contrastive Fusion (WSI-CoFu) specifically designed for fine-tuning ViT models in a supervised contrastive learning framework, tailored for small-scale WSI datasets.This method leverages the auto-encoder capability of Transformers to learn diverse semantic projections of histopathological images.Additionally, we propose a unique loss function that integrates multi-level image semantics extracted from each attention block of the ViT, guiding the model to better understand the complex structures in WSI.The effectiveness of WSI-CoFu is demonstrated through the analysis of feature distributions on a hypersphere, showcasing the rich and diverse representations our method generates.Moreover, WSI-CoFu's utility extends into transfer learning, showcasing remarkable adaptability, achieving significantly improved accuracy on different medical imaging tasks without extensive retraining, such as metastasis detection, or segmentation.Experimental results show that CoFu achieves an accuracy of 91.3% in ICIAR2018, 90.5% in Camelyon2016, and 92.7% in Camelyon2017, outperforming both traditional supervised learning frameworks and conventional data augmentation approaches on Small Scale WSI dataset.Notably, it also achieves significant gains in transfer learning on downstream dataset. © 2024 Copyright held by the owner/author(s).
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Alabool, H.M.
TI  - Large Language Model Evaluation Criteria Framework in Healthcare: Fuzzy MCDM Approach
PY  - 2025
T2  - SN Computer Science
VL  - 6
IS  - 1
C7  - 57
DO  - 10.1007/s42979-024-03533-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213971857&doi=10.1007%2fs42979-024-03533-6&partnerID=40&md5=4b01c09556f235ff228b3b24efc5debf
AB  - Large Language Models (LLMs) gained notable popularity in academia and industry. It has unprecedented features and performance in many applications. LLMs are revolutionizing the AI industry by enabling them to offer sophisticated and human-like natural language processing capabilities. LLMs empower applications to understand, interpret, and generate human-like text at a level that was previously challenging to achieve. On this, numerous LLM providers including Open AI, Google AI, Meta AI, and Microsoft have commenced to offer LLM services to their customers. The integration of LLMs into healthcare represents a significant advancement in the delivery of medical care. Alas, the diversity of LLMs presents a challenge for healthcare providers in determining which of these LLMs is the most appropriate option that meets the user’s requirements. This study aims to develop a LLM evaluation criteria framework that help healthcare providers to select the best LLM that meet their requirements. To do so, 38 experts in the domain of AI were interviewed and the fuzzy analytical hierarchy process (FAHP) is applied to compute the weight of these criteria. The results identified 9 evaluation criteria with 12 sub-criteria along with their specific metrics as the most critical criteria in evaluating and selecting LLMs in healthcare domain. The analysis results show that LLM evaluation criteria are ranked in descending order of importance, with assigned weights as follows: reliability (0.200), robustness (0.171), bias and fairness (0.126), availability (0.121), performance (0.092), usability (0.090), resilience (0.089), predictability (0.063), and cost (0.047). As a result, the proposed framework can provide useful feedback to healthcare providers by allowing them to select the best LLM that meets their requirements. Also, it can help providers by allowing them to satisfy their Service Level Agreement (SLA) and improve their quality of service. © The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd. 2024.
PB  - Springer
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Grévisse, C.
TI  - LLM-based automatic short answer grading in undergraduate medical education
PY  - 2024
T2  - BMC Medical Education
VL  - 24
IS  - 1
C7  - 1060
DO  - 10.1186/s12909-024-06026-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205336649&doi=10.1186%2fs12909-024-06026-5&partnerID=40&md5=fdcc3bb2d0af1b14b1a4691c8eb8183f
AB  - Background: Multiple choice questions are heavily used in medical education assessments, but rely on recognition instead of knowledge recall. However, grading open questions is a time-intensive task for teachers. Automatic short answer grading (ASAG) has tried to fill this gap, and with the recent advent of Large Language Models (LLM), this branch has seen a new momentum. Methods: We graded 2288 student answers from 12 undergraduate medical education courses in 3 languages using GPT-4 and Gemini 1.0 Pro. Results: GPT-4 proposed significantly lower grades than the human evaluator, but reached low rates of false positives. The grades of Gemini 1.0 Pro were not significantly different from the teachers’. Both LLMs reached a moderate agreement with human grades, and a high precision for GPT-4 among answers considered fully correct. A consistent grading behavior could be determined for high-quality keys. A weak correlation was found wrt. the length or language of student answers. There is a risk of bias if the LLM knows the human grade a priori. Conclusions: LLM-based ASAG applied to medical education still requires human oversight, but time can be spared on the edge cases, allowing teachers to focus on the middle ones. For Bachelor-level medical education questions, the training knowledge of LLMs seems to be sufficient, fine-tuning is thus not necessary. © The Author(s) 2024.
PB  - BioMed Central Ltd
C2  - 39334087
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Lv, B.
AU  - Feng, A.
AU  - Xie, C.
TI  - Improving Factuality by Contrastive Decoding with Factual and Hallucination Prompts
PY  - 2024
T2  - Sensors
VL  - 24
IS  - 21
C7  - 7097
DO  - 10.3390/s24217097
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208644442&doi=10.3390%2fs24217097&partnerID=40&md5=8eab21ab87a2a6d870b883c72f8c444c
AB  - Large language models have demonstrated impressive capabilities in many domains. But they sometimes generate irrelevant or nonsensical text, or produce outputs that deviate from the provided input, an occurrence commonly referred to as hallucination. To mitigate this issue, we introduce a novel decoding method that incorporates both factual and hallucination prompts (DFHP). It applies contrastive decoding to highlight the disparity in output probabilities between factual prompts and hallucination prompts. Experiments on both multiple-choice and text generation tasks show that our approach significantly improves factual accuracy of large language models without additional training. On the TruthfulQA dataset, the DFHP method significantly improves factual accuracy of the LLaMA model, with an average improvement of 6.4% for the 7B, 13B, 30B, and 65B versions. Its high accuracy in factuality makes it an ideal choice for high reliability tasks like medical diagnosis and legal cases. © 2024 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
C2  - 39517994
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Urbina, J.T.
AU  - Vu, P.D.
AU  - Nguyen, M.V.
TI  - Disability Ethics and Education in the Age of Artificial Intelligence: Identifying Ability Bias in ChatGPT and Gemini
PY  - 2025
T2  - Archives of Physical Medicine and Rehabilitation
VL  - 106
IS  - 1
SP  - 14
EP  - 19
DO  - 10.1016/j.apmr.2024.08.014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204491450&doi=10.1016%2fj.apmr.2024.08.014&partnerID=40&md5=f76232215c1bfd42000ef74d5d8b9221
AB  - Objective: To identify and quantify ability bias in generative artificial intelligence large language model chatbots, specifically OpenAI's ChatGPT and Google's Gemini. Design: Observational study of language usage in generative artificial intelligence models. Setting: Investigation-only browser profile restricted to ChatGPT and Gemini. Participants: Each chatbot generated 60 descriptions of people prompted without specified functional status, 30 descriptions of people with a disability, 30 descriptions of patients with a disability, and 30 descriptions of athletes with a disability (N=300). Interventions: Not applicable. Main Outcome Measures: Generated descriptions produced by the models were parsed into words that were linguistically analyzed into favorable qualities or limiting qualities. Results: Both large language models significantly underestimated disability in a population of people, and linguistic analysis showed that descriptions of people, patients, and athletes with a disability were generated as having significantly fewer favorable qualities and significantly more limitations than people without a disability in both ChatGPT and Gemini. Conclusions: Generative artificial intelligence chatbots demonstrate quantifiable ability bias and often exclude people with disabilities in their responses. Ethical use of these generative large language model chatbots in medical systems should recognize this limitation, and further consideration should be taken in developing equitable artificial intelligence technologies. © 2024 American Congress of Rehabilitation Medicine
PB  - W.B. Saunders
C2  - 39216786
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Currie, G.
AU  - Currie, J.
AU  - Anderson, S.
AU  - Hewis, J.
TI  - Gender bias in generative artificial intelligence text-to-image depiction of medical students
PY  - 2024
T2  - Health Education Journal
VL  - 83
IS  - 7
SP  - 732
EP  - 746
DO  - 10.1177/00178969241274621
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202537917&doi=10.1177%2f00178969241274621&partnerID=40&md5=47a28833ae7e7ca8464eb19d0edb294d
AB  - Introduction: In Australia, 54.3% of medical students are women yet they remain under-represented in stereotypical perspectives of medicine. While potentially transformative, generative artificial intelligence (genAI) has the potential for errors, misrepresentations and bias. GenAI text-to-image production could reinforce gender biases making it important to evaluate DALL-E 3 (the text-to-image genAI supported through ChatGPT) representations of Australian medical students. Method: In March 2024, DALL-E 3 was utilised via GPT-4 to generate a series of individual and group images of medical students, specifically Australian undergraduate medical students to eliminate potential confounders. Multiple iterations of images were generated using a variety of prompts. Collectively, 47 images were produced for evaluation of which 33 were individual characters and the remaining 14 images were comprised of multiple (5 to 67) characters. All images were independently analysed by three reviewers for apparent gender and skin tone. Consequently, 33 feature individuals were evaluated and a further 417 characters in groups were evaluated (N = 448). Discrepancies in responses were resolved by consensus. Results: Collectively (individual and group images), 58.8% (N = 258) of medical students were depicted as men, 39.9% (N = 175) as women, 92.0% (N = 404) with a light skin tone, 7.7% (N = 34) with mid skin tone and 0% with dark skin tone. The gender distribution was a statistically significant variation from that of actual Australian medical students for individual images, for group images and for collective images. Among the images of individual medical students (N = 25), DALL-E 3 generated 92% (N = 23) as men and 100% were of light skin tone (N = 25). Conclusion: This evaluation reveals the gender associated with genAI text-to-image generation using DALL-E 3 among Australian undergraduate medical students. Generated images included a disproportionately high proportion of white male medical students which is not representative of the diversity of medical students in Australia. The use of DALL-E 3 to produce depictions of medical students for education or promotion purposes should be done with caution. © The Author(s) 2024.
PB  - SAGE Publications Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Turnbull, O.M.
AU  - Oglic, D.
AU  - Croasdale-Wood, R.
AU  - Deane, C.M.
TI  - p-IgGen: a paired antibody generative language model
PY  - 2024
T2  - Bioinformatics
VL  - 40
IS  - 11
C7  - btae659
DO  - 10.1093/bioinformatics/btae659
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210021747&doi=10.1093%2fbioinformatics%2fbtae659&partnerID=40&md5=5eb5d6cd72298f4f13bc5a3018a9b848
AB  - A key challenge in antibody drug discovery is designing novel sequences that are free from developability issues - such as aggregation, polyspecificity, poor expression, or low solubility. Here, we present p-IgGen, a protein language model for paired heavy-light chain antibody generation. The model generates diverse, antibody-like sequences with pairing properties found in natural antibodies. We also create a finetuned version of p-IgGen that biases the model to generate antibodies with 3D biophysical properties that fall within distributions seen in clinical-stage therapeutic antibodies.  © 2024 The Author(s).
PB  - Oxford University Press
C2  - 39520401
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Huang, H.
AU  - Shah, T.
AU  - Karigiannis, J.
AU  - Evans, S.
TI  - Physics and Data Collaborative Root Cause Analysis: Integrating Pretrained Large Language Models and Data-Driven AI for Trustworthy Asset Health Management
PY  - 2024
T2  - Proceedings of the Annual Conference of the Prognostics and Health Management Society, PHM 
VL  - 16
IS  - 1
DO  - 10.36001/phmconf.2024.v16i1.3881
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210228443&doi=10.36001%2fphmconf.2024.v16i1.3881&partnerID=40&md5=b48140c95f9d02d804fac61c3fcd623d
AB  - Data-driven tools for asset health management face significant challenges, including a lack of understanding of physical principles, difficulty incorporating domain experts’ experiences, and consequently low detection accuracy, leading to trustworthiness issues. Automatically integrating data-driven analysis with human knowledge and experience, as found in literature and maintenance logs, is critically needed. Recent progress in large language models (LLMs) offers opportunities to achieve this goal. However, there is still a lack of work that effectively combines pretrained LLMs with data-driven models for asset health management using industrial time series data as input. This paper presents a framework that integrates our recently proposed data-driven AI with pretrained LLMs to address root cause detection in industrial failure analysis. The framework employs LLMs to analyze outputs from our data-driven root cause analysis models, filtering out less relevant results and prioritizing those that align closely with physical principles and domain expertise. Our innovative approach leverages advanced data-driven analytics and a multi-LLM debate for collaborative decision-making, seamlessly merging data-driven insights with domain knowledge. Specifically, through our proposed self-exclusionary debates among multiple LLMs, biases inherent in single-LLM systems are effectively mitigated, enhancing reliability and stability. Crucially, the framework bridges the gap between data-driven models and physics-informed LLMs, accelerating the interaction between data and knowledge for more informed and realistic decision-making processes. © 2024 Prognostics and Health Management Society. All rights reserved.
PB  - Prognostics and Health Management Society
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Gumilar, K.E.
AU  - Indraprasta, B.R.
AU  - Hsu, Y.-C.
AU  - Yu, Z.-Y.
AU  - Chen, H.
AU  - Irawan, B.
AU  - Tambunan, Z.
AU  - Wibowo, B.M.
AU  - Nugroho, H.
AU  - Tjokroprawiro, B.A.
AU  - Dachlan, E.G.
AU  - Mulawardhana, P.
AU  - Rahestyningtyas, E.
AU  - Pramuditya, H.
AU  - Putra, V.G.E.
AU  - Waluyo, S.T.
AU  - Tan, N.R.
AU  - Folarin, R.
AU  - Ibrahim, I.H.
AU  - Lin, C.-H.
AU  - Hung, T.-Y.
AU  - Lu, T.-F.
AU  - Chen, Y.-F.
AU  - Shih, Y.-H.
AU  - Wang, S.-J.
AU  - Huang, J.
AU  - Yates, C.C.
AU  - Lu, C.-H.
AU  - Liao, L.-N.
AU  - Tan, M.
TI  - Disparities in medical recommendations from AI-based chatbots across different countries/regions
PY  - 2024
T2  - Scientific Reports
VL  - 14
IS  - 1
C7  - 17052
DO  - 10.1038/s41598-024-67689-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199505313&doi=10.1038%2fs41598-024-67689-0&partnerID=40&md5=2f05110434bba8e41ed15eee08415e04
AB  - This study explores disparities and opportunities in healthcare information provided by AI chatbots. We focused on recommendations for adjuvant therapy in endometrial cancer, analyzing responses across four regions (Indonesia, Nigeria, Taiwan, USA) and three platforms (Bard, Bing, ChatGPT-3.5). Utilizing previously published cases, we asked identical questions to chatbots from each location within a 24-h window. Responses were evaluated in a double-blinded manner on relevance, clarity, depth, focus, and coherence by ten experts in endometrial cancer. Our analysis revealed significant variations across different countries/regions (p < 0.001). Interestingly, Bing's responses in Nigeria consistently outperformed others (p < 0.05), excelling in all evaluation criteria (p < 0.001). Bard also performed better in Nigeria compared to other regions (p < 0.05), consistently surpassing them across all categories (p < 0.001, with relevance reaching p < 0.01). Notably, Bard's overall scores were significantly higher than those of ChatGPT-3.5 and Bing in all locations (p < 0.001). These findings highlight disparities and opportunities in the quality of AI-powered healthcare information based on user location and platform. This emphasizes the necessity for more research and development to guarantee equal access to trustworthy medical information through AI technologies. © The Author(s) 2024.
PB  - Nature Research
C2  - 39048640
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Bhesra, K.
AU  - Agarwal, A.
TI  - A Multi-modal Framework to Counter Hate Speeches
PY  - 2025
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 
VL  - 15331 LNCS
SP  - 197
EP  - 207
DO  - 10.1007/978-3-031-78119-3_14
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212275687&doi=10.1007%2f978-3-031-78119-3_14&partnerID=40&md5=0e82bc329fbf65090ce359da8b9212a2
AB  - The proliferation of offensive, hateful, and toxic content on social media platforms has reached unprecedented levels. These deleterious expressions not only tarnish the fabric of online interactions but also pose significant threats to individual well-being, potentially precipitating mental health issues such as depression. Manifesting in various modalities including audio and text, this digital toxicity exerts a corrosive influence, leaving enduring impacts on the psyche of individuals. The literature has begun addressing this issue through the lens of natural language processing. However, conventional toxic language detection systems often exhibit biases, particularly in misidentifying text featuring mentions of minority groups as harmful. Furthermore, the overreliance on spurious correlations undermines the efficacy of these systems in detecting implicitly toxic language. Notably, existing benchmark datasets such as ToxiGen predominantly comprise text-based content. Thus, to address this gap, this study presents a pioneering effort in assembling an audio-based hate speech dataset. Subsequently, a multimodal hate speech detection algorithm integrating audio and text inputs is proposed, demonstrating a significant performance enhancement over conventional text-based models.  © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Patel, P.V.
AU  - Davis, C.
AU  - Ralbovsky, A.
AU  - Tinoco, D.
AU  - Williams, C.Y.K.
AU  - Slatter, S.
AU  - Naderalvojoud, B.
AU  - Rosen, M.J.
AU  - Hernandez-Boussard, T.
AU  - Rudrapatna, V.
TI  - Large Language Models Outperform Traditional Natural Language Processing Methods in Extracting Patient-Reported Outcomes in Inflammatory Bowel Disease
PY  - 2025
T2  - Gastro Hep Advances
VL  - 4
IS  - 2
C7  - 100563
DO  - 10.1016/j.gastha.2024.10.003
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214532928&doi=10.1016%2fj.gastha.2024.10.003&partnerID=40&md5=4a6b56177e05d381dcdc17fa83992687
AB  - Background and Aims: Patient-reported outcomes (PROs) are vital in assessing disease activity and treatment outcomes in inflammatory bowel disease (IBD). However, manual extraction of these PROs from the free-text of clinical notes is burdensome. We aimed to improve data curation from free-text information in the electronic health record, making it more available for research and quality improvement. This study aimed to compare traditional natural language processing (tNLP) and large language models (LLMs) in extracting 3 IBD PROs (abdominal pain, diarrhea, fecal blood) from clinical notes across 2 institutions. Methods: Clinic notes were annotated for each PRO using preset protocols. Models were developed and internally tested at the University of California, San Francisco, and then externally validated at Stanford University. We compared tNLP and LLM-based models on accuracy, sensitivity, specificity, positive, and negative predictive value. In addition, we conducted fairness and error assessments. Results: Interrater reliability between annotators was >90%. On the University of California, San Francisco test set (n = 50), the top-performing tNLP models showcased accuracies of 92% (abdominal pain), 82% (diarrhea) and 80% (fecal blood), comparable to GPT-4, which was 96%, 88%, and 90% accurate, respectively. On external validation at Stanford (n = 250), tNLP models failed to generalize (61%–62% accuracy) while GPT-4 maintained accuracies >90%. Pathways Language Model-2 and Generative Pre-trained Transformer-4 showed similar performance. No biases were detected based on demographics or diagnosis. Conclusion: LLMs are accurate and generalizable methods for extracting PROs. They maintain excellent accuracy across institutions, despite heterogeneity in note templates and authors. Widespread adoption of such tools has the potential to enhance IBD research and patient care. © 2024 The Authors
PB  - American Gastroenterological Association
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ma, J.
AU  - Chen, H.
AU  - Sun, J.
AU  - Huang, J.
AU  - He, G.
AU  - Yang, G.
TI  - Efficient analysis of drug interactions in liver injury: a retrospective study leveraging natural language processing and machine learning
PY  - 2024
T2  - BMC Medical Research Methodology
VL  - 24
IS  - 1
C7  - 312
DO  - 10.1186/s12874-024-02443-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212711807&doi=10.1186%2fs12874-024-02443-8&partnerID=40&md5=798b40cf9cab7d8c038a845324397d43
AB  - Background: Liver injury from drug-drug interactions (DDIs), notably with anti-tuberculosis drugs such as isoniazid, poses a significant safety concern. Electronic medical records contain comprehensive clinical information and have gained increasing attention as a potential resource for DDI detection. However, a substantial portion of adverse drug reaction (ADR) information is hidden in unstructured narrative text, which has yet to be efficiently harnessed, thereby introducing bias into the research. There is a significant need for an efficient framework for the DDI assessment. Methods: Using a Chinese natural language processing (NLP) model, we extracted 25,130 adverse drug reaction (ADR) records, dividing them into sets for training an automated normalization model. The trained models, in conjunction with liver function laboratory tests, were used to thoroughly and efficiently identify liver injury cases. Ultimately, we applied a case-control study design to detect DDI signals increasing isoniazid’s liver injury risk. Results: The Logistic Regression model demonstrated stable and superior performance in classification task. Based on laboratory criteria and NLP, we identified 128 liver injury cases among a cohort of 3,209 patients treated with isoniazid. Preliminary screening of 113 drug combinations with isoniazid highlighted 20 potential signal drugs, with antibacterials constituting 25%. Sensitivity analysis confirmed the robustness of signal drugs, especially in cardiac therapy and antibacterials. Conclusion: Our NLP and machine learning approach effectively identifies isoniazid-related DDIs that increase the risk of liver injury, identifying 20 signal drugs, mainly antibacterials. Further research is required to validate these DDI signals. © The Author(s) 2024.
PB  - BioMed Central Ltd
C2  - 39707270
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Parillo, M.
AU  - Vaccarino, F.
AU  - Beomonte Zobel, B.
AU  - Mallio, C.A.
TI  - ChatGPT and radiology report: potential applications and limitations
PY  - 2024
T2  - Radiologia Medica
VL  - 129
IS  - 12
SP  - 1849
EP  - 1863
DO  - 10.1007/s11547-024-01915-7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208796842&doi=10.1007%2fs11547-024-01915-7&partnerID=40&md5=15c1f7a3eb4a197a6ee5dd54794b3346
AB  - Large language models like ChatGPT, with their growing accessibility, are attracting increasing interest within the artificial intelligence medical field, particularly in the analysis of radiology reports. These present a valuable opportunity to explore the potential clinical applications of large language models, given their huge capabilities in processing and understanding written language. Early research indicates that ChatGPT could offer benefits in radiology reporting. ChatGPT can assist but not replace radiologists in achieving diagnoses, generating structured reports, extracting data, identifying errors or incidental findings, and can also serve as a support in creating patient-friendly reports. However, ChatGPT also has intrinsic limitations, such as hallucinations, stochasticity, biases, deficiencies in complex clinical scenarios, data privacy and legal concerns. To fully utilize the potential of ChatGPT in radiology reporting, careful integration planning and rigorous validation of their outputs are crucial, especially for tasks requiring abstract reasoning or nuanced medical context. Radiologists’ expertise in medical imaging and data analysis positions them exceptionally well to lead the responsible integration and utilization of ChatGPT within the field of radiology. This article offers a topical overview of the potential strengths and limitations of ChatGPT in radiological reporting. © Italian Society of Medical Radiology 2024.
PB  - Springer-Verlag Italia s.r.l.
C2  - 39508933
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Lian, J.
AU  - Dong, Z.
AU  - Zhang, H.
AU  - Chen, Y.
AU  - Liu, J.
TI  - Multifocal region-assisted cross-modality learning for chest X-ray report generation
PY  - 2024
T2  - Computers in Biology and Medicine
VL  - 183
C7  - 109187
DO  - 10.1016/j.compbiomed.2024.109187
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206844600&doi=10.1016%2fj.compbiomed.2024.109187&partnerID=40&md5=2a565c118cb835e6ae188367c01b0a79
AB  - The prevalence of cardiovascular disease, tumors, and other chronic illnesses has been steadily rising in recent years. Researchers have recently been employing cross-modal large-scale models and natural language generation models to address the significant visual and textual disparities in medical report generation tasks. However, these training processes presents challenges, such as difficulties matching cross-modal information and generating specialized medical terminology. To tackle these issues, we propose a Multifocal Region-Assisted Report Generation Network (MRARGN) to enhance cross-modal information matching. Specifically, we integrate a pre-trained ResNet-50 with multi-channel and attention mechanisms for trainable X-ray image representation. We then combine our proposed memory response matrix with OpenAI's contrastive pre-training results to construct a dynamic knowledge graph that stores lesion features and their corresponding texts. Finally, we incorporate attention mechanisms and forget gate units to generate comprehensive textual descriptions for different lesions, using an image and report alignment loss. We conduct ablation experiments on the IU-Xray and MIMIC-CXR datasets to evaluate our approach. The experimental results demonstrate that our proposed MRARGN outperforms most state-of-the-art approaches, including their own variants. © 2024 Elsevier Ltd
PB  - Elsevier Ltd
C2  - 39437605
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Diaz Ochoa, J.G.
AU  - Mustafa, F.E.
AU  - Weil, F.
AU  - Wang, Y.
AU  - Kama, K.
AU  - Knott, M.
TI  - The aluminum standard: using generative Artificial Intelligence tools to synthesize and annotate non-structured patient data
PY  - 2024
T2  - BMC Medical Informatics and Decision Making
VL  - 24
IS  - 1
C7  - 409
DO  - 10.1186/s12911-024-02825-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213549719&doi=10.1186%2fs12911-024-02825-4&partnerID=40&md5=eaddc8fc9e03114533b8fe17eb1225a6
AB  - Background: Medical narratives are fundamental to the correct identification of a patient’s health condition. This is not only because it describes the patient’s situation. It also contains relevant information about the patient’s context and health state evolution. Narratives are usually vague and cannot be categorized easily. On the other hand, once the patient’s situation is correctly identified based on a narrative, it is then possible to map the patient’s situation into precise classification schemas and ontologies that are machine-readable. To this end, language models can be trained to read and extract elements from these narratives. However, the main problem is the lack of data for model identification and model training in languages other than English. First, gold standard annotations are usually not available due to the high level of data protection for patient data. Second, gold standard annotations (if available) are difficult to access. Alternative available data, like MIMIC (Sci Data 3:1, 2016) is written in English and for specific patient conditions like intensive care. Thus, when model training is required for other types of patients, like oncology (and not intensive care), this could lead to bias. To facilitate clinical narrative model training, a method for creating high-quality synthetic narratives is needed. Method: We devised workflows based on generative AI methods to synthesize narratives in the German language to avoid the disclosure of patient’s health data. Since we required highly realistic narratives, we generated prompts, written with high-quality medical terminology, asking for clinical narratives containing both a main and co-disease. The frequency of distribution of both the main and co-disease was extracted from the hospital’s structured data, such that the synthetic narratives reflect the disease distribution among the patient’s cohort. In order to validate the quality of the synthetic narratives, we annotated them to train a Named Entity Recognition (NER) algorithm. According to our assumptions, the validation of this system implies that the synthesized data used for its training are of acceptable quality. Result: We report precision, recall and F1 score for the NER model while also considering metrics that take into account both exact and partial entity matches. Trained models are cautious, with a precision up to 0.8 for Entity Type match metric and a F1 score of 0.3. Conclusion: Despite its inherent limitations, this technology has the potential to allow data interoperability by using encoded diseases across languages and regions without compromising data safety. Additionally, it facilitates the synthesis of unstructured patient data. In this way, the identification and training of models can be accelerated. We believe that this method may be able to generate discharge letters for any combination of main and co-diseases, which will significantly reduce the amount of time spent writing these letters by healthcare professionals. © The Author(s) 2024.
PB  - BioMed Central Ltd
C2  - 39732668
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Jeyakodi, G.
AU  - Bala, P.S.
TI  - HyPRETo: Hybrid Pre-trained Ontology Approach for Contextual Relation Classification on Mosquito Vector Biocontrol Agents
PY  - 2025
T2  - IFIP Advances in Information and Communication Technology
VL  - 723 IFIP
SP  - 312
EP  - 326
DO  - 10.1007/978-3-031-73617-9_25
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214102353&doi=10.1007%2f978-3-031-73617-9_25&partnerID=40&md5=17c4d68f4b82ac537b7873909cba70cb
AB  - Pre-trained Language Model facilitates contextual relation classification by capturing contextual information, addressing word ambiguity, encoding global sentence context, enabling transfer learning, handling out-of-vocabulary words, and improving performance with limited labelled data. Existing pre-training approaches suffer in size, bias, interpretability, generalization, and the lack of domain specificity. To address this, HyPRETo, the hybrid model that combines the strength of token replacement and dynamic masking is proposed to achieve upgraded performance to increase classification accuracy. The Mosquito Vector Biocontrol Agents data is used for implementing the model for a contextual relation classification task. HyPRETo uses ontology to provide structured knowledge. HyPRETo is pre-trained by ELECTRA and fine-tuned by RoBERTa models. Feedforward and softmax activation function is used for classification. The Natural Language Processing technique and SQL database are used to develop an automated question-answering system. The HyPRETo was evaluated with state-of-art models and achieved 98.42% accuracy. As a contribution, the manually annotated input dataset on the mosquito vector control agent is prepared for the classification task. Subsequently, the enhanced model is developed. The interface for an automated question-answering system for mosquito vector biocontrol agents is developed to assist public health applications such as mosquito vector control, disease control, ecosystem management, environmental conservation, and so on. © IFIP International Federation for Information Processing 2025.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Isch, C.
TI  - Media bias in portrayals of mortality risks: Comparison of newspaper coverage to death rates
PY  - 2025
T2  - Social Science and Medicine
VL  - 364
C7  - 117542
DO  - 10.1016/j.socscimed.2024.117542
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210072835&doi=10.1016%2fj.socscimed.2024.117542&partnerID=40&md5=9fd6e5571d79c0c21b2df21a08fb52db
AB  - Chronic diseases are the leading cause of death in the United States, yet effective preventive measures receive minimal healthcare funding. This disparity may stem from public underestimation of these diseases’ impact and controllability, with distorted media coverage overemphasizing sensational risks and underemphasizing chronic illnesses. This study compares media coverage of mortality risks to objective measures of death rates to investigate such distortions. Data were collected on 14 mortality risks, including monthly US deaths from CDC Wonder and 823,406 relevant articles from major US newspapers via LexisNexis. Regression analyses and qualitative evaluations using natural language processing tools were performed. From 1999 to 2020, a significant disconnect was found between the deadliest risks and their media coverage. Media coverage fluctuations correlated with death rate changes, yet only 1.7–2.8% of the coverage was explained by these rates. Chronic illnesses were described neutrally as individual challenges, while sensational risks were depicted negatively as collective problems. These results illustrate how the media depict a skewed view of the risks facing the public, with disproportionate coverage of sensational risks while comparatively ignoring chronic diseases. Consumers may consequently come to a distorted understanding of the most threatening risks they face and how to combat them. © 2024 Elsevier Ltd
PB  - Elsevier Ltd
C2  - 39603173
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Lee, J.
AU  - Lee, J.
AU  - Yoo, J.-J.
TI  - The role of large language models in the peer-review process: opportunities and challenges for medical journal reviewers and editors
PY  - 2025
T2  - Journal of Educational Evaluation for Health Professions
VL  - 22
C7  - 4
DO  - 10.3352/jeehp.2025.22.4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216208122&doi=10.3352%2fjeehp.2025.22.4&partnerID=40&md5=fd21c0a54e340e656690359cf0201baa
AB  - The peer review process ensures the integrity of scientific research. This is particularly important in the medical field, where research findings directly impact patient care. However, the rapid growth of publications has strained reviewers, causing delays and potential declines in quality. Generative artificial intelligence, especially large language models (LLMs) such as ChatGPT, may assist researchers with efficient, high-quality reviews. This review explores the integration of LLMs into peer review, highlighting their strengths in linguistic tasks and challenges in assessing scientific validity, particularly in clinical medicine. Key points for integration include initial screening, reviewer matching, feedback support, and language review. However, implementing LLMs for these purposes will necessitate addressing biases, privacy concerns, and data confidentiality. We recommend using LLMs as complementary tools under clear guidelines to support, not replace, human expertise in maintaining rigorous peer review standards. © 2025 Korea Health Personnel Licensing Examination Institute.
PB  - Korea Health Personnel Licensing Examination Institute
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Taha, T.A.-E.A.
AU  - Abdel-Qader, D.H.
AU  - Alamiry, K.R.
AU  - Fadl, Z.A.
AU  - Alrawi, A.
AU  - Abdelsattar, N.K.
TI  - Perception, concerns, and practice of ChatGPT among Egyptian pharmacists: a cross-sectional study in Egypt
PY  - 2024
T2  - BMC Health Services Research
VL  - 24
IS  - 1
C7  - 1500
DO  - 10.1186/s12913-024-11815-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210494184&doi=10.1186%2fs12913-024-11815-1&partnerID=40&md5=e668be2af8c9b20815788e6018a911cf
AB  - Background: The emergence of large language models (LLMs) like ChatGPT attracted significant attention for their potential to revolutionize pharmacy practice. While artificial intelligence (AI) offers promising benefits, its integration also presents unique challenges. Objectives: This cross-sectional study aimed to explore the current Egyptian pharmacists’ perceptions, practices, and concerns regarding ChatGPT in pharmacy practice. Methods: The study questionnaire was shared with pharmacists during March and April 2024. We included pharmacists licensed by the Egyptian Ministry of Health and Population. We adapted a convenient sampling technique by sending the research questionnaire via emails, student networks, social media (Facebook and WhatsApp), and student organizations. Any pharmacist interested in participating followed a link to review the study description and was asked to provide electronic consent before continuing with the study. Data were analyzed using SPSS software, employing Chi-square tests for categorical variables and Spearman’s correlation for continuous variables. Statistical significance was set at p < 0.05. Results: The study sample size included 428 pharmacists from the main economic regions of Egypt. The results revealed a strong recognition (73.6%) among participants of ChatGPT’s anticipated benefits within pharmacy practice. Around two-thirds of the participants (65.9%) expressed disagreement or neutrality regarding the application of ChatGPT for analyzing patients’ medical inputs and providing individualized medical advice. Regarding factors affecting perception, we found that the region is the only factor that significantly contributed to the level of perception among pharmacists (P = 0.011) with Greater cairo region showing the highest perception level. We found that 73.6% of participants who have heard about ChatGPT reported high levels of concern. One-third of participants never use ChatGPT in their pharmacy work, and 20% rarely use it. Using Spearman’s correlation test, there was no significant correlation between anticipated advantages, concerns and practice level (P > 0.05). Conclusion: This study reveals a generally positive perception of ChatGPT’s potential benefits among Egyptian pharmacists, despite existing concerns regarding accuracy, data privacy, and bias. Notably, no significant associations were found between demographic factors and pharmacists’ perceptions, practices, or concerns. This underscores the need for comprehensive educational initiatives to promote informed and responsible ChatGPT utilization within pharmacy practice. Future research should explore the development and implementation of tailored training programs and guidelines to ensure the safe and effective integration of ChatGPT into pharmacy workflows for optimal patient care. © The Author(s) 2024.
PB  - BioMed Central Ltd
C2  - 39609697
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Anibal, J.T.
AU  - Huth, H.B.
AU  - Gunkel, J.
AU  - Gregurick, S.K.
AU  - Wood, B.J.
TI  - Simulated misuse of large language models and clinical credit systems
PY  - 2024
T2  - npj Digital Medicine
VL  - 7
IS  - 1
C7  - 317
DO  - 10.1038/s41746-024-01306-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209795908&doi=10.1038%2fs41746-024-01306-2&partnerID=40&md5=0d7891e18a0261e66f3e994217a37503
AB  - In the future, large language models (LLMs) may enhance the delivery of healthcare, but there are risks of misuse. These methods may be trained to allocate resources via unjust criteria involving multimodal data - financial transactions, internet activity, social behaviors, and healthcare information. This study shows that LLMs may be biased in favor of collective/systemic benefit over the protection of individual rights and could facilitate AI-driven social credit systems. © The Author(s) 2024.
PB  - Nature Research
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Gisselbaek, M.
AU  - Suppan, M.
AU  - Minsart, L.
AU  - Köselerli, E.
AU  - Nainan Myatra, S.
AU  - Matot, I.
AU  - Barreto Chang, O.L.
AU  - Saxena, S.
AU  - Berger-Estilita, J.
TI  - Representation of intensivists’ race/ethnicity, sex, and age by artificial intelligence: a cross-sectional study of two text-to-image models
PY  - 2024
T2  - Critical Care
VL  - 28
IS  - 1
C7  - 363
DO  - 10.1186/s13054-024-05134-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209477298&doi=10.1186%2fs13054-024-05134-4&partnerID=40&md5=c09f151775bb18517ae843c7163a3e9b
AB  - Background: Integrating artificial intelligence (AI) into intensive care practices can enhance patient care by providing real-time predictions and aiding clinical decisions. However, biases in AI models can undermine diversity, equity, and inclusion (DEI) efforts, particularly in visual representations of healthcare professionals. This work aims to examine the demographic representation of two AI text-to-image models, Midjourney and ChatGPT DALL-E 2, and assess their accuracy in depicting the demographic characteristics of intensivists. Methods: This cross-sectional study, conducted from May to July 2024, used demographic data from the USA workforce report (2022) and intensive care trainees (2021) to compare real-world intensivist demographics with images generated by two AI models, Midjourney v6.0 and ChatGPT 4.0 DALL-E 2. A total of 1,400 images were generated across ICU subspecialties, with outcomes being the comparison of sex, race/ethnicity, and age representation in AI-generated images to the actual workforce demographics. Results: The AI models demonstrated noticeable biases when compared to the actual U.S. intensive care workforce data, notably overrepresenting White and young doctors. ChatGPT-DALL-E2 produced less female (17.3% vs 32.2%, p < 0.0001), more White (61% vs 55.1%, p = 0.002) and younger (53.3% vs 23.9%, p < 0.001) individuals. While Midjourney depicted more female (47.6% vs 32.2%, p < 0.001), more White (60.9% vs 55.1%, p = 0.003) and younger intensivist (49.3% vs 23.9%, p < 0.001). Substantial differences between the specialties within both models were observed. Finally when compared together, both models showed significant differences in the Portrayal of intensivists. Conclusions: Significant biases in AI images of intensivists generated by ChatGPT DALL-E 2 and Midjourney reflect broader cultural issues, potentially perpetuating stereotypes of healthcare worker within the society. This study highlights the need for an approach that ensures fairness, accountability, transparency, and ethics in AI applications for healthcare. © The Author(s) 2024.
PB  - BioMed Central Ltd
C2  - 39529104
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Ong, Q.C.
AU  - Ang, C.-S.
AU  - Chee, D.Z.Y.
AU  - Lawate, A.
AU  - Sundram, F.
AU  - Dalakoti, M.
AU  - Pasalic, L.
AU  - To, D.
AU  - Erlikh Fox, T.
AU  - Bojic, I.
AU  - Car, J.
TI  - Advancing health coaching: A comparative study of large language model and health coaches
PY  - 2024
T2  - Artificial Intelligence in Medicine
VL  - 157
C7  - 103004
DO  - 10.1016/j.artmed.2024.103004
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207061208&doi=10.1016%2fj.artmed.2024.103004&partnerID=40&md5=5851bd42efb7bbfbe5ca711b0d788bb6
AB  - Objective: Recent advances in large language models (LLM) offer opportunities to automate health coaching. With zero-shot learning ability, LLMs could revolutionize health coaching by providing better accessibility, scalability, and customization. The aim of this study is to compare the quality of responses to clients' sleep-related questions provided by health coaches and an LLM. Design, setting, and participants: From a de-identified dataset of coaching conversations from a pilot randomized controlled trial, we extracted 100 question-answer pairs comprising client questions and corresponding health coach responses. These questions were entered into a retrieval-augmented generation (RAG)-enabled open-source LLM (LLaMa-2-7b-chat) to generate LLM responses. Out of 100 question-answer pairs, 90 were taken out and assigned to three groups of evaluators: experts, lay-users, and GPT-4. Each group conducted two evaluation tasks: (Task 1) a single-response quality assessment spanning five criteria—accuracy, readability, helpfulness, empathy, and likelihood of harm—rated on a five-point Likert scale, and (Task 2) a pairwise comparison to choose the superior response between pairs. A suite of inferential statistical methods, including the paired and independent sample t-tests, Pearson correlation, and chi-square tests, were utilized to answer the study objective. Recognizing potential biases in human judgment, the remaining 10 question-answer pairs were used to assess inter-evaluator reliability among the human evaluators, quantified using the interclass correlation coefficient and percentage agreement metrics. Results: Upon exclusion of incomplete data, the analysis included 178 single-response evaluations (Task 1) and 83 pairwise comparisons (Task 2). Expert and GPT-4 assessments revealed no discernible disparities in health coach and LLM responses across the five metrics. Contrarily, lay-users deemed LLM responses significantly more helpful than that of human coaches (p < 0.05). LLM responses were preferred in the majority (62.25 %, n = 155) of the aggregate 249 assessments, with all three evaluator groups favoring LLM over health coach inputs. While GPT-4 rated both health coach and LLM responses significantly higher than experts in terms of readability, helpfulness, and empathy, its ratings on accuracy and likelihood of harm aligned with those of experts. Response length positively correlated with accuracy and empathy scores, but negatively affected readability across all evaluator groups. Expert and lay-user evaluators demonstrated moderate to high inter-evaluator reliability. Conclusion: Our study showed encouraging findings by demonstrating that RAG-enabled LLM has comparable performance to health coaches in the domain tested. Serving as an initial step towards the creation of more sophisticated, adaptive, round-the-clock automated health coaching systems, our findings call for more extensive evaluation which could assist in the development of the model that could in the future lead to potential clinical implementation. © 2024 Elsevier B.V.
PB  - Elsevier B.V.
C2  - 39454500
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Wang, C.-K.
AU  - Ke, C.-R.
AU  - Huang, M.-S.
AU  - Chong, I.-W.
AU  - Yang, Y.-H.
AU  - Tseng, V.S.
AU  - Dai, H.-J.
TI  - Using Large Language Models for Efficient Cancer Registry Coding in the Real Hospital Setting: A Feasibility Study
PY  - 2025
T2  - Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing
VL  - 30
SP  - 121
EP  - 137
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212611510&partnerID=40&md5=59439a85628c0fdd94322a4ad548c719
AB  - The primary challenge in reporting cancer cases lies in the labor-intensive and time-consuming process of manually reviewing numerous reports. Current methods predominantly rely on rule-based approaches or custom-supervised learning models, which predict diagnostic codes based on a single pathology report per patient. Although these methods show promising evaluation results, their biased outcomes in controlled settings may hinder adaption to real-world reporting workflows. In this feasibility study, we focused on lung cancer as a test case and developed an agentic retrieval-augmented generation (RAG) system to evaluate the potential of publicly available large language models (LLMs) for cancer registry coding. Our findings demonstrate that: (1) directly applying publicly available LLMs without fine-tuning is feasible for cancer registry coding; and (2) prompt engineering can significantly enhance the capability of pre-trained LLMs in cancer registry coding. The off-the-shelf LLM, combined with our proposed system architecture and basic prompts, achieved a macro-averaged F-score of 0.637 when evaluated on testing data consisting of patients' medical reports spanning 1.5 years since their first visit. By employing chain of thought (CoT) reasoning and our proposed coding item grouping, the system outperformed the baseline by 0.187 in terms of the macro-averaged F-score. These findings demonstrate the great potential of leveraging LLMs with prompt engineering for cancer registry coding. Our system could offer cancer registrars a promising reference tool to enhance their daily workflow, improving efficiency and accuracy in cancer case reporting.
C2  - 39670366
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Mehta, S.
AU  - Mehta, N.
TI  - Embracing the illusion of explanatory depth: A strategic framework for using iterative prompting for integrating large language models in healthcare education
PY  - 2025
T2  - Medical Teacher
VL  - 47
IS  - 2
SP  - 208
EP  - 211
DO  - 10.1080/0142159X.2024.2382863
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199801242&doi=10.1080%2f0142159X.2024.2382863&partnerID=40&md5=85f153a24a9a08ba5e18f4e06d98a483
AB  - Healthcare educators are exploring ways to integrate Large Language Models (LLMs) into the curriculum. At the same time, they are concerned about the negative impact on students’ cognitive development. There is concern that the students will not learn to think and problem-solve by themselves and instead become dependent on LLMs to find answers. In addition, the students could start accepting the LLM generated responses at face value. The Illusion of Explanatory Depth (IoED) is a cognitive bias where humans believe they understand complex phenomena in more depth than they do. This illusion is caused when people rely on external sources of information rather than deeper levels of internalized knowledge. This illusion can be exposed by asking follow-up in depth questions. Using the same approach, specifically iterative prompting, can help students interact with LLM’s while learning actively, gaining deeper levels of knowledge, and exposing the LLM shortcomings. The article proposes that educators encourage use of LLMs to complete assignments using a template, that promotes students’ reflections on their interactions with LLMs, using iterative prompting. This process based on IoED, and iterative prompting will help educators integrate LLMs in the curriculum while mitigating the risk of students becoming dependent on these tools. Students will practice active learning and experience firsthand the inaccuracies and inconsistencies in LLM responses. © 2024 Informa UK Limited, trading as Taylor & Francis Group.
PB  - Taylor and Francis Ltd.
C2  - 39058399
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Li, M.
AU  - Lin, H.
AU  - Qiu, L.
AU  - Liang, X.
AU  - Chen, L.
AU  - Elsaddik, A.
AU  - Chang, X.
TI  - Contrastive Learning with Counterfactual Explanations for Radiology Report Generation
PY  - 2025
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 15101 LNCS
SP  - 162
EP  - 180
DO  - 10.1007/978-3-031-72775-7_10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206391406&doi=10.1007%2f978-3-031-72775-7_10&partnerID=40&md5=fb773c2c59877873a08e415fd29c9256
AB  - Due to the common content of anatomy, radiology images with their corresponding reports exhibit high similarity. Such inherent data bias can predispose automatic report generation models to learn entangled and spurious representations resulting in misdiagnostic reports. To tackle these, we propose a novel CounterFactual Explanations-based framework (CoFE) for radiology report generation. Counterfactual explanations serve as a potent tool for understanding how decisions made by algorithms can be changed by asking “what if” scenarios. By leveraging this concept, CoFE can learn non-spurious visual representations by contrasting the representations between factual and counterfactual images. Specifically, we derive counterfactual images by swapping a patch between positive and negative samples until a predicted diagnosis shift occurs. Here, positive and negative samples are the most semantically similar but have different diagnosis labels. Additionally, CoFE employs a learnable prompt to efficiently fine-tune the pre-trained large language model, encapsulating both factual and counterfactual content to provide a more generalizable prompt representation. Extensive experiments on two benchmarks demonstrate that leveraging the counterfactual explanations enables CoFE to generate semantically coherent and factually complete reports and outperform in terms of language generation and clinical efficacy metrics. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Calley, D.Q.
AU  - Fu, S.
AU  - Hamilton, M.D.
AU  - Kalla, A.W.
AU  - Lee, C.K.
AU  - Rasmussen, V.A.
AU  - Hollman, J.H.
AU  - Liu, H.
TI  - Assessment of Gender Differences in Letters of Recommendation for Physical Therapy Residency Applications
PY  - 2024
T2  - Journal of Physical Therapy Education
VL  - 38
IS  - 4
SP  - 331
EP  - 339
DO  - 10.1097/JTE.0000000000000337
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210149936&doi=10.1097%2fJTE.0000000000000337&partnerID=40&md5=d461a8fbd9a320acc3985ae2876f428c
AB  - Introduction. Letters of recommendation (LOR) are an integral component of physical therapy residency applications. Identifying the influence of applicant and writer gender in LOR will help identify whether potential implicit gender bias exists in physical therapy residency application processes. Review of Literature. Several medical and surgical residency education programs have reported positive, neutral, or negative LOR female gender bias among applicants and writers. Little research exists on gender differences in LOR to physical therapy education programs or physical therapy residency programs. Subjects. Seven hundred sixty-eight LOR were analyzed from 256 applications to 3 physical therapy residency programs (neurologic, orthopaedic, sports) at one institution from 2014 to 2020. Methods. Thematic categories were developed to identify themes in a sample of LOR. Associations between writer and applicant gender were analyzed using summary statistics, word counts, thematic and psycholinguistic extraction, and rule-based and deep learning Natural Language Processing . Results. No significant difference in LOR word counts were found based on writer or applicant gender. Increased word counts were seen in sports residency LOR compared with the orthopaedic residency. Thematic analysis showed LOR gender differences with male applicants receiving more positive generalized recommendations and female applicants receiving more comments regarding interpersonal relationship skills. No thematic or psycholinguistic gender differences were seen by LOR writer. Male applicants were 1.9 times more likely to select all male LOR writers, whereas female applicants were 2.1 times more likely to choose all female LOR writers. Discussion and Conclusion. Gender differences in LORs for physical therapy residencies were found using a comprehensive Natural Language Processing approach that identified both a positive recommendation male applicant gender bias and a positive interpersonal relationship skill female applicant gender bias. Applicants were not harmed nor helped by selecting LOR writers of the opposite gender. Admissions committees and LOR writers should be mindful of potential implicit gender biases in LOR submitted to physical therapy residency programs. © 2024 Academy of Physical Therapy Education, APTA.
PB  - Lippincott Williams and Wilkins
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Lim, Y.M.F.
AU  - Asselbergs, F.W.
AU  - Bagheri, A.
AU  - Denaxas, S.
AU  - Tay, W.T.
AU  - Voors, A.
AU  - Lam, C.S.P.
AU  - Koudstaal, S.
AU  - Grobbee, D.E.
AU  - Vaartjes, I.
TI  - Eligibility of Asian and European registry patients for phase III trials in heart failure with reduced ejection fraction
PY  - 2024
T2  - ESC Heart Failure
VL  - 11
IS  - 6
SP  - 3559
EP  - 3571
DO  - 10.1002/ehf2.14751
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197803423&doi=10.1002%2fehf2.14751&partnerID=40&md5=bd820ec386e764f397c70c32355f724b
AB  - Aims: Traditional approaches to designing clinical trials for heart failure (HF) have historically relied on expertise and past practices. However, the evolving landscape of healthcare, marked by the advent of novel data science applications and increased data availability, offers a compelling opportunity to transition towards a data-driven paradigm in trial design. This research aims to evaluate the scope and determinants of disparities between clinical trials and registries by leveraging natural language processing for the analysis of trial eligibility criteria. The findings contribute to the establishment of a robust design framework for guiding future HF trials. Methods and results: Interventional phase III trials registered for HF on ClinicalTrials.gov as of the end of 2021 were identified. Natural language processing was used to extract and structure the eligibility criteria for quantitative analysis. The most common criteria for HF with reduced ejection fraction (HFrEF) were applied to estimate patient eligibility as a proportion of registry patients in the ASIAN-HF (N = 4868) and BIOSTAT-CHF registries (N = 2545). Of the 375 phase III trials for HF, 163 HFrEF trials were identified. In these trials, the most frequently encountered inclusion criteria were New York Heart Association (NYHA) functional class (69%), worsening HF (23%), and natriuretic peptides (18%), whereas the most frequent comorbidity-based exclusion criteria were acute coronary syndrome (64%), renal disease (55%), and valvular heart disease (47%). On average, 20% of registry patients were eligible for HFrEF trials. Eligibility distributions did not differ (P = 0.18) between Asian [median eligibility 0.20, interquartile range (IQR) 0.08–0.43] and European registry populations (median 0.17, IQR 0.06–0.39). With time, HFrEF trials became more restrictive, where patient eligibility declined from 0.40 in 1985–2005 to 0.19 in 2016–2022 (P = 0.03). When frequency among trials is taken into consideration, the eligibility criteria that were most restrictive were prior myocardial infarction, NYHA class, age, and prior HF hospitalization. Conclusions: Based on 14 trial criteria, only one-fifth of registry patients were eligible for phase III HFrEF trials. Overall eligibility rates did not differ between the Asian and European patient cohorts. © 2024 The Authors. ESC Heart Failure published by John Wiley & Sons Ltd on behalf of European Society of Cardiology.
PB  - John Wiley and Sons Inc
C2  - 38984466
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Mirzaei, T.
AU  - Amini, L.
AU  - Esmaeilzadeh, P.
TI  - Clinician voices on ethics of LLM integration in healthcare: a thematic analysis of ethical concerns and implications
PY  - 2024
T2  - BMC Medical Informatics and Decision Making
VL  - 24
IS  - 1
C7  - 250
DO  - 10.1186/s12911-024-02656-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203319423&doi=10.1186%2fs12911-024-02656-3&partnerID=40&md5=0a504bcbba60113f4ea5c1752f1715cc
AB  - Objectives: This study aimed to explain and categorize key ethical concerns about integrating large language models (LLMs) in healthcare, drawing particularly from the perspectives of clinicians in online discussions. Materials and methods: We analyzed 3049 posts and comments extracted from a self-identified clinician subreddit using unsupervised machine learning via Latent Dirichlet Allocation and a structured qualitative analysis methodology. Results: Analysis uncovered 14 salient themes of ethical implications, which we further consolidated into 4 overarching domains reflecting ethical issues around various clinical applications of LLM in healthcare, LLM coding, algorithm, and data governance, LLM’s role in health equity and the distribution of public health services, and the relationship between users (human) and LLM systems (machine). Discussion: Mapping themes to ethical frameworks in literature illustrated multifaceted issues covering transparent LLM decisions, fairness, privacy, access disparities, user experiences, and reliability. Conclusion: This study emphasizes the need for ongoing ethical review from stakeholders to ensure responsible innovation and advocates for tailored governance to enhance LLM use in healthcare, aiming to improve clinical outcomes ethically and effectively. © This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply 2024.
PB  - BioMed Central Ltd
C2  - 39252056
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Klug, K.
AU  - Beckh, K.
AU  - Antweiler, D.
AU  - Chakraborty, N.
AU  - Baldini, G.
AU  - Laue, K.
AU  - Hosch, R.
AU  - Nensa, F.
AU  - Schuler, M.
AU  - Giesselbach, S.
TI  - From admission to discharge: a systematic review of clinical natural language processing along the patient journey
PY  - 2024
T2  - BMC Medical Informatics and Decision Making
VL  - 24
IS  - 1
C7  - 238
DO  - 10.1186/s12911-024-02641-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202798104&doi=10.1186%2fs12911-024-02641-w&partnerID=40&md5=9e678700f22e4dd4d1196f5adc60b6f5
AB  - Background: Medical text, as part of an electronic health record, is an essential information source in healthcare. Although natural language processing (NLP) techniques for medical text are developing fast, successful transfer into clinical practice has been rare. Especially the hospital domain offers great potential while facing several challenges including many documents per patient, multiple departments and complex interrelated processes. Methods: In this work, we survey relevant literature to identify and classify approaches which exploit NLP in the clinical context. Our contribution involves a systematic mapping of related research onto a prototypical patient journey in the hospital, along which medical documents are created, processed and consumed by hospital staff and patients themselves. Specifically, we reviewed which dataset types, dataset languages, model architectures and tasks are researched in current clinical NLP research. Additionally, we extract and analyze major obstacles during development and implementation. We discuss options to address them and argue for a focus on bias mitigation and model explainability. Results: While a patient’s hospital journey produces a significant amount of structured and unstructured documents, certain steps and documents receive more research attention than others. Diagnosis, Admission and Discharge are clinical patient steps that are researched often across the surveyed paper. In contrast, our findings reveal significant under-researched areas such as Treatment, Billing, After Care, and Smart Home. Leveraging NLP in these stages can greatly enhance clinical decision-making and patient outcomes. Additionally, clinical NLP models are mostly based on radiology reports, discharge letters and admission notes, even though we have shown that many other documents are produced throughout the patient journey. There is a significant opportunity in analyzing a wider range of medical documents produced throughout the patient journey to improve the applicability and impact of NLP in healthcare. Conclusions: Our findings suggest that there is a significant opportunity to leverage NLP approaches to advance clinical decision-making systems, as there remains a considerable understudied potential for the analysis of patient journey data. © The Author(s) 2024.
PB  - BioMed Central Ltd
C2  - 39210370
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Shiraishi, M.
AU  - Banda, C.H.
AU  - Nakajima, M.
AU  - Nakazwe, M.
AU  - Wong, Z.Y.
AU  - Tomioka, Y.
AU  - Moriwaki, Y.
AU  - Takeishi, H.
AU  - Lee, H.
AU  - Kurita, D.
AU  - Furuse, K.
AU  - Ohba, J.
AU  - Fujisawa, K.
AU  - Miyamoto, S.
AU  - Okazaki, M.
TI  - Gender and racial diversity Assumed by text-to-image generators in microsurgery and plastic surgery-related subspecialities
PY  - 2025
T2  - Journal of Hand and Microsurgery
VL  - 17
IS  - 1
C7  - 100196
DO  - 10.1016/j.jham.2024.100196
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211044867&doi=10.1016%2fj.jham.2024.100196&partnerID=40&md5=195ab11271a44e1fe9d16deb230b66cd
AB  - Background: Since the release of ChatGPT by OpenAI in November 2022, generative artificial intelligence (AI) models have attracted significant attention in various fields, including surgery. These advancements have been particularly notable for creating highly detailed and contextually accurate images from textual prompts. A notable area of clinical application is the representation of surgeon demographics in various specialties, particularly in the context of microsurgery and plastic surgery-related subspecialties. Methods: This cross-sectional study, conducted in June 2024, utilized the latest version of the Copilot Creative Mode powered by DALL-E 3 to generate images of surgeons across various plastic surgery subspecialties. Real-world demographic data from the US, Japan, and Zambia were compared with AI-generated images for an accurate representation analysis. Results: Five hundred images (350 from various subspecialties and 150 from geographical sources) were analyzed. The AI model predominantly generated images of male and female surgeons with a statistical underrepresentation of female and Black microsurgeons. Geographical prompts influenced the representation, with an overrepresentation of female (64.0 %; p < 0.001) and Black (16.0 %; p < 0.001) plastic surgeons in the US and exclusively Asian surgeons in Japan. Discrepancies were also observed in the depiction of surgical equipment, with the majority of AI-generated microsurgeons inaccurately portrayed using either surgical loupes (46.0 %) or optical microscopes (32.0 %), not with surgical microscopes (4.0 %). Conclusions: This study revealed significant disparities between AI-generated images and actual demographics in the fields of microsurgery and plastic surgery-related subspecialties, highlighting the need for more diverse and accurate training datasets for AI models. © 2024 Society for Indian Hand Surgery and Micro Surgeons
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Schmidgall, S.
AU  - Harris, C.
AU  - Essien, I.
AU  - Olshvang, D.
AU  - Rahman, T.
AU  - Kim, J.W.
AU  - Ziaei, R.
AU  - Eshraghian, J.
AU  - Abadir, P.
AU  - Chellappa, R.
TI  - Evaluation and mitigation of cognitive biases in medical language models
PY  - 2024
T2  - npj Digital Medicine
VL  - 7
IS  - 1
C7  - 295
DO  - 10.1038/s41746-024-01283-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207162648&doi=10.1038%2fs41746-024-01283-6&partnerID=40&md5=13536aa36686928054381d5e9b901ba2
AB  - Increasing interest in applying large language models (LLMs) to medicine is due in part to their impressive performance on medical exam questions. However, these exams do not capture the complexity of real patient–doctor interactions because of factors like patient compliance, experience, and cognitive bias. We hypothesized that LLMs would produce less accurate responses when faced with clinically biased questions as compared to unbiased ones. To test this, we developed the BiasMedQA dataset, which consists of 1273 USMLE questions modified to replicate common clinically relevant cognitive biases. We assessed six LLMs on BiasMedQA and found that GPT-4 stood out for its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B, which showed large drops in performance. Additionally, we introduced three bias mitigation strategies, which improved but did not fully restore accuracy. Our findings highlight the need to improve LLMs’ robustness to cognitive biases, in order to achieve more reliable applications of LLMs in healthcare. © The Author(s) 2024.
PB  - Nature Research
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Carl, N.
AU  - Nguyen, L.
AU  - Haggenmüller, S.
AU  - Joachim Hetz, M.
AU  - Theres Winterstein, J.
AU  - Otto Hartung, F.
AU  - Gruene, B.
AU  - Nikolas Kather, J.
AU  - Holland-Letz, T.
AU  - Stephan Michel, M.
AU  - Wessels, F.
AU  - Josef Brinker, T.
TI  - Comparing Patient's Confidence in Clinical Capabilities in Urology: Large Language Models Versus Urologists
PY  - 2024
T2  - European Urology Open Science
VL  - 70
SP  - 91
EP  - 98
DO  - 10.1016/j.euros.2024.10.009
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207106744&doi=10.1016%2fj.euros.2024.10.009&partnerID=40&md5=cb1cbcb276ed17fbc25d8a97cd27bbac
AB  - Background and objective: Data on interaction of patients with artificial intelligence (AI) are limited, primarily derived from small-scale studies, cross-sectional surveys, and qualitative reviews. Most patients have not yet encountered AI in their clinical experience. This study explored patients’ confidence in AI, specifically large language models, after a direct interaction with a chatbot in a clinical setting. Through hands-on experience, the study sought to reduce potential biases due to an anticipated lack of AI experience in a real-world urological patient sample. Methods: A total of 300 patients scheduled for counseling were enrolled from February to July 2024. Participants voluntarily conversed about their medical questions with a GPT-4 powered chatbot, followed by a survey assessing their confidence in clinical capabilities of AI compared with their counseling urologists. Clinical capabilities included history taking, diagnostics, treatment recommendation, anxiety reduction, and time allocation. Key findings and limitations: Of the 292 patients who completed the study, AI was significantly preferred to physicians for consultation time allocation (p < 0.001). However, urologists were overwhelmingly favored for all other capabilities, especially treatment recommendations and anxiety reduction. Notably, age did not influence patients’ confidence in AI. Limitations include a potential social desirability bias. Conclusions and clinical implications: Our study demonstrates that urological patients prefer AI as a powerful complement to—rather than a replacement for—human expertise in clinical care. Patients appreciated the additional consultation time provided by AI. Interestingly, age was not associated with confidence in AI, suggesting that large language models are user-friendly tools for patients of all age groups. Patient summary: In this report, we explored how patients feel about using an artificial intelligence (AI)-powered chatbot in a medical setting. Patients interacted with the AI for medical questions and compared its skills with those of doctors through a survey. They appreciated the AI for providing more time during consultations but preferred doctors for other tasks, for example, diagnostics, recommendation of treatments, and reduction of anxieties. © 2024 The Author(s)
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Kumar, Y.
AU  - Marttinen, P.
TI  - Improving Medical Multi-modal Contrastive Learning with Expert Annotations
PY  - 2025
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 
VL  - 15078 LNCS
SP  - 468
EP  - 486
DO  - 10.1007/978-3-031-72661-3_27
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211205301&doi=10.1007%2f978-3-031-72661-3_27&partnerID=40&md5=abe26addea0e0df529276b9dc4835795
AB  - We introduce eCLIP, an enhanced version of the CLIP model that integrates expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key challenges in contrastive multi-modal medical imaging analysis, notably data scarcity and the “modality gap” – a significant disparity between image and text embeddings that diminishes the quality of representations and hampers cross-modal interoperability. eCLIP integrates a heatmap processor and leverages mixup augmentation to efficiently utilize the scarce expert annotations, thus boosting the model’s learning effectiveness. eCLIP is designed to be generally applicable to any variant of CLIP without requiring any modifications of the core architecture. Through detailed evaluations across several tasks, including zero-shot inference, linear probing, cross-modal retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using a frozen Large Language Model, eCLIP showcases consistent improvements in embedding quality. The outcomes reveal enhanced alignment and uniformity, affirming eCLIP’s capability to harness high-quality annotations for enriched multi-modal analysis in the medical imaging domain. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Pfohl, S.R.
AU  - Cole-Lewis, H.
AU  - Sayres, R.
AU  - Neal, D.
AU  - Asiedu, M.
AU  - Dieng, A.
AU  - Tomasev, N.
AU  - Rashid, Q.M.
AU  - Azizi, S.
AU  - Rostamzadeh, N.
AU  - McCoy, L.G.
AU  - Celi, L.A.
AU  - Liu, Y.
AU  - Schaekermann, M.
AU  - Walton, A.
AU  - Parrish, A.
AU  - Nagpal, C.
AU  - Singh, P.
AU  - Dewitt, A.
AU  - Mansfield, P.
AU  - Prakash, S.
AU  - Heller, K.
AU  - Karthikesalingam, A.
AU  - Semturs, C.
AU  - Barral, J.
AU  - Corrado, G.
AU  - Matias, Y.
AU  - Smith-Loud, J.
AU  - Horn, I.
AU  - Singhal, K.
TI  - A toolbox for surfacing health equity harms and biases in large language models
PY  - 2024
T2  - Nature Medicine
VL  - 30
IS  - 12
C7  - 194
SP  - 3590
EP  - 3600
DO  - 10.1038/s41591-024-03258-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204613041&doi=10.1038%2fs41591-024-03258-2&partnerID=40&md5=60ae4427ece7e33c045627c3bb5d9027
AB  - Large language models (LLMs) hold promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities. Reliably evaluating equity-related model failures is a critical step toward developing systems that promote health equity. We present resources and methodologies for surfacing biases with potential to precipitate equity-related harms in long-form, LLM-generated answers to medical questions and conduct a large-scale empirical case study with the Med-PaLM 2 LLM. Our contributions include a multifactorial framework for human assessment of LLM-generated answers for biases and EquityMedQA, a collection of seven datasets enriched for adversarial queries. Both our human assessment framework and our dataset design process are grounded in an iterative participatory approach and review of Med-PaLM 2 answers. Through our empirical study, we find that our approach surfaces biases that may be missed by narrower evaluation approaches. Our experience underscores the importance of using diverse assessment methodologies and involving raters of varying backgrounds and expertise. While our approach is not sufficient to holistically assess whether the deployment of an artificial intelligence (AI) system promotes equitable health outcomes, we hope that it can be leveraged and built upon toward a shared goal of LLMs that promote accessible and equitable healthcare. © The Author(s) 2024.
PB  - Nature Research
C2  - 39313595
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Ghosh, D.
AU  - Karande, H.
AU  - Gite, S.
AU  - Pradhan, B.
TI  - Psychological disorder detection: A multimodal approach using a transformer-based hybrid model
PY  - 2024
T2  - MethodsX
VL  - 13
C7  - 102976
DO  - 10.1016/j.mex.2024.102976
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205680445&doi=10.1016%2fj.mex.2024.102976&partnerID=40&md5=eda3e19e7edd94fcbac73e23e5a8518a
AB  - Detecting psychological disorders, particularly depression, is a complex and critical task within the realm of mental health assessment. This research explores a novel approach to improve the identification of psychological distresses, such as depression, by addressing the subjectivity, complexity, and biasness inherent in traditional diagnostic techniques. Using multimodal data, such as voice characteristics and linguistic content from participant interviews, we developed a Transformer-Based Hybrid Model that combines advanced natural language processing and deep learning approaches. This model provides a complete assessment of an individual's psychological well-being by merging aural cues and textual data. This study investigates the theoretical underpinnings, technical complexities, and practical applications of this model in the context of psychological disorder detection. Additionally, the model's design and implementation details are thoroughly documented to ensure replicability by other researchers. • A unique way of strengthening emotional ailments (focusing on depression). • Transformer-Based Hybrid Model is proposed using multimodal data from interviews of participants. • The model integrates voice characteristics (aural cues) and linguistic content (textual data). • Comparative analysis of this research with existing approaches. © 2024 The Authors
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Park, Y.-J.
AU  - Pillai, A.
AU  - Deng, J.
AU  - Guo, E.
AU  - Gupta, M.
AU  - Paget, M.
AU  - Naugler, C.
TI  - Assessing the research landscape and clinical utility of large language models: a scoping review
PY  - 2024
T2  - BMC Medical Informatics and Decision Making
VL  - 24
IS  - 1
C7  - 72
DO  - 10.1186/s12911-024-02459-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187530678&doi=10.1186%2fs12911-024-02459-6&partnerID=40&md5=9b97b44693fe4f525e4f2743874bd31f
AB  - Importance: Large language models (LLMs) like OpenAI’s ChatGPT are powerful generative systems that rapidly synthesize natural language responses. Research on LLMs has revealed their potential and pitfalls, especially in clinical settings. However, the evolving landscape of LLM research in medicine has left several gaps regarding their evaluation, application, and evidence base. Objective: This scoping review aims to (1) summarize current research evidence on the accuracy and efficacy of LLMs in medical applications, (2) discuss the ethical, legal, logistical, and socioeconomic implications of LLM use in clinical settings, (3) explore barriers and facilitators to LLM implementation in healthcare, (4) propose a standardized evaluation framework for assessing LLMs’ clinical utility, and (5) identify evidence gaps and propose future research directions for LLMs in clinical applications. Evidence review: We screened 4,036 records from MEDLINE, EMBASE, CINAHL, medRxiv, bioRxiv, and arXiv from January 2023 (inception of the search) to June 26, 2023 for English-language papers and analyzed findings from 55 worldwide studies. Quality of evidence was reported based on the Oxford Centre for Evidence-based Medicine recommendations. Findings: Our results demonstrate that LLMs show promise in compiling patient notes, assisting patients in navigating the healthcare system, and to some extent, supporting clinical decision-making when combined with human oversight. However, their utilization is limited by biases in training data that may harm patients, the generation of inaccurate but convincing information, and ethical, legal, socioeconomic, and privacy concerns. We also identified a lack of standardized methods for evaluating LLMs’ effectiveness and feasibility. Conclusions and relevance: This review thus highlights potential future directions and questions to address these limitations and to further explore LLMs’ potential in enhancing healthcare delivery. © The Author(s) 2024.
PB  - BioMed Central Ltd
C2  - 38475802
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 39
ER  -

TY  - JOUR
AU  - Cheng, H.Y.
TI  - ChatGPT's Attitude, Knowledge, and Clinical Application in Geriatrics Practice and Education: Exploratory Observational Study
PY  - 2025
T2  - JMIR Formative Research
VL  - 9
C7  - e63494
DO  - 10.2196/63494
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214438271&doi=10.2196%2f63494&partnerID=40&md5=ede2557b4e19c258a211effbad94e39c
AB  - Background: The increasing use of ChatGPT in clinical practice and medical education necessitates the evaluation of its reliability, particularly in geriatrics. Objective: This study aimed to evaluate ChatGPT’s trustworthiness in geriatrics through 3 distinct approaches: evaluating ChatGPT’s geriatrics attitude, knowledge, and clinical application with 2 vignettes of geriatric syndromes (polypharmacy and falls). Methods: We used the validated University of California, Los Angeles, geriatrics attitude and knowledge instruments to evaluate ChatGPT’s geriatrics attitude and knowledge and compare its performance with that of medical students, residents, and geriatrics fellows from reported results in the literature. We also evaluated ChatGPT’s application to 2 vignettes of geriatric syndromes (polypharmacy and falls). Results: The mean total score on geriatrics attitude of ChatGPT was significantly lower than that of trainees (medical students, internal medicine residents, and geriatric medicine fellows; 2.7 vs 3.7 on a scale from 1-5; 1=strongly disagree; 5=strongly agree). The mean subscore on positive geriatrics attitude of ChatGPT was higher than that of the trainees (medical students, internal medicine residents, and neurologists; 4.1 vs 3.7 on a scale from 1 to 5 where a higher score means a more positive attitude toward older adults). The mean subscore on negative geriatrics attitude of ChatGPT was lower than that of the trainees and neurologists (1.8 vs 2.8 on a scale from 1 to 5 where a lower subscore means a less negative attitude toward aging). On the University of California, Los Angeles geriatrics knowledge test, ChatGPT outperformed all medical students, internal medicine residents, and geriatric medicine fellows from validated studies (14.7 vs 11.3 with a score range of –18 to +18 where +18 means that all questions were answered correctly). Regarding the polypharmacy vignette, ChatGPT not only demonstrated solid knowledge of potentially inappropriate medications but also accurately identified 7 common potentially inappropriate medications and 5 drug-drug and 3 drug-disease interactions. However, ChatGPT missed 5 drug-disease and 1 drug-drug interaction and produced 2 hallucinations. Regarding the fall vignette, ChatGPT answered 3 of 5 pretests correctly and 2 of 5 pretests partially correctly, identified 6 categories of fall risks, followed fall guidelines correctly, listed 6 key physical examinations, and recommended 6 categories of fall prevention methods. Conclusions: This study suggests that ChatGPT can be a valuable supplemental tool in geriatrics, offering reliable information with less age bias, robust geriatrics knowledge, and comprehensive recommendations for managing 2 common geriatric syndromes (polypharmacy and falls) that are consistent with evidence from guidelines, systematic reviews, and other types of studies. ChatGPT’s potential as an educational and clinical resource could significantly benefit trainees, health care providers, and laypeople. Further research using GPT-4o, larger geriatrics question sets, and more geriatric syndromes is needed to expand and confirm these findings before adopting ChatGPT widely for geriatrics education and practice. © Huai Yong Cheng.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ali, R.
AU  - Connolly, I.D.
AU  - Tang, O.Y.
AU  - Mirza, F.N.
AU  - Johnston, B.
AU  - Abdulrazeq, H.F.
AU  - Galamaga, P.F.
AU  - Libby, T.J.
AU  - Sodha, N.R.
AU  - Groff, M.W.
AU  - Gokaslan, Z.L.
AU  - Telfeian, A.E.
AU  - Shin, J.H.
AU  - Asaad, W.F.
AU  - Zou, J.
AU  - Doberstein, C.E.
TI  - Bridging the literacy gap for surgical consents: an AI-human expert collaborative approach
PY  - 2024
T2  - npj Digital Medicine
VL  - 7
IS  - 1
C7  - 63
DO  - 10.1038/s41746-024-01039-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187100275&doi=10.1038%2fs41746-024-01039-2&partnerID=40&md5=72df8b06e49b9bda43686bf95e5322de
AB  - Despite the importance of informed consent in healthcare, the readability and specificity of consent forms often impede patients’ comprehension. This study investigates the use of GPT-4 to simplify surgical consent forms and introduces an AI-human expert collaborative approach to validate content appropriateness. Consent forms from multiple institutions were assessed for readability and simplified using GPT-4, with pre- and post-simplification readability metrics compared using nonparametric tests. Independent reviews by medical authors and a malpractice defense attorney were conducted. Finally, GPT-4’s potential for generating de novo procedure-specific consent forms was assessed, with forms evaluated using a validated 8-item rubric and expert subspecialty surgeon review. Analysis of 15 academic medical centers’ consent forms revealed significant reductions in average reading time, word rarity, and passive sentence frequency (all P < 0.05) following GPT-4-faciliated simplification. Readability improved from an average college freshman to an 8th-grade level (P = 0.004), matching the average American’s reading level. Medical and legal sufficiency consistency was confirmed. GPT-4 generated procedure-specific consent forms for five varied surgical procedures at an average 6th-grade reading level. These forms received perfect scores on a standardized consent form rubric and withstood scrutiny upon expert subspeciality surgeon review. This study demonstrates the first AI-human expert collaboration to enhance surgical consent forms, significantly improving readability without sacrificing clinical detail. Our framework could be extended to other patient communication materials, emphasizing clear communication and mitigating disparities related to health literacy barriers. © The Author(s) 2024.
PB  - Nature Research
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 18
ER  -

TY  - JOUR
AU  - Wang, Z.
AU  - Duan, J.
AU  - Yuan, C.
AU  - Chen, Q.
AU  - Chen, T.
AU  - Zhang, Y.
AU  - Wang, R.
AU  - Shi, X.
AU  - Xu, K.
TI  - Word-Sequence Entropy: Towards uncertainty estimation in free-form medical question answering applications and beyond
PY  - 2025
T2  - Engineering Applications of Artificial Intelligence
VL  - 139
C7  - 109553
DO  - 10.1016/j.engappai.2024.109553
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208764309&doi=10.1016%2fj.engappai.2024.109553&partnerID=40&md5=b99747df052dc2e6bcf58d4633d820f7
AB  - Uncertainty estimation is crucial for the reliability of safety-critical human and artificial intelligence (AI) interaction systems, particularly in the domain of healthcare engineering. However, a robust and general uncertainty measure for free-form answers has not been well-established in open-ended medical question-answering (QA) tasks, where generative inequality introduces a large number of irrelevant words and sequences within the generated set for uncertainty quantification (UQ), which can lead to biases. This paper proposes Word-Sequence Entropy (WSE), which calibrates uncertainty at both the word and sequence levels based on semantic relevance, highlighting keywords and enlarging the generative probability of trustworthy responses when performing UQ. We compare WSE with six baseline methods on five free-form medical QA datasets, utilizing seven popular large language models (LLMs), and demonstrate that WSE exhibits superior performance in accurate UQ under two standard criteria for correctness evaluation. Additionally, in terms of the potential for real-world medical QA applications, we achieve a significant enhancement (e.g., a 6.36% improvement in model accuracy on the COVID-QA dataset) in the performance of LLMs when employing responses with lower uncertainty that are identified by WSE as final answers, without requiring additional task-specific fine-tuning or architectural modifications. © 2024 Elsevier Ltd
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Yang, Y.
AU  - Liu, X.
AU  - Jin, Q.
AU  - Huang, F.
AU  - Lu, Z.
TI  - Unmasking and quantifying racial bias of large language models in medical report generation
PY  - 2024
T2  - Communications Medicine
VL  - 4
IS  - 1
C7  - 176
DO  - 10.1038/s43856-024-00601-z
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203715321&doi=10.1038%2fs43856-024-00601-z&partnerID=40&md5=d0132cfa2f7ef0d6a13230e0c2293aee
AB  - Background: Large language models like GPT-3.5-turbo and GPT-4 hold promise for healthcare professionals, but they may inadvertently inherit biases during their training, potentially affecting their utility in medical applications. Despite few attempts in the past, the precise impact and extent of these biases remain uncertain. Methods: We use LLMs to generate responses that predict hospitalization, cost and mortality based on real patient cases. We manually examine the generated responses to identify biases. Results: We find that these models tend to project higher costs and longer hospitalizations for white populations and exhibit optimistic views in challenging medical scenarios with much higher survival rates. These biases, which mirror real-world healthcare disparities, are evident in the generation of patient backgrounds, the association of specific diseases with certain racial and ethnic groups, and disparities in treatment recommendations, etc. Conclusions: Our findings underscore the critical need for future research to address and mitigate biases in language models, especially in critical healthcare applications, to ensure fair and accurate outcomes for all patients. © This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply 2024.
PB  - Springer Nature
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Roganović, J.
TI  - Familiarity with ChatGPT Features Modifies Expectations and Learning Outcomes of Dental Students
PY  - 2024
T2  - International Dental Journal
VL  - 74
IS  - 6
SP  - 1456
EP  - 1462
DO  - 10.1016/j.identj.2024.04.012
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191292571&doi=10.1016%2fj.identj.2024.04.012&partnerID=40&md5=71877a0b440c59ca22ed55c950d5feb3
AB  - Objectives: The number of approvals for AI-based systems is increasing rapidly, although AI clinical trial designs lack consideration of the impact of human–AI interaction. Aim of this work was to investigate how reading of an AI system (ChatGPT) features/descriptions could influence the willingness and expectations for use of this technology as well as dental students’ learning performance. Methods: Dental students (N = 104) were asked to learn about side effects of drugs used in dental practice via reading recommended literature or ChatGPT. Expectations towards ChatGPT were measured by survey, before and after reading of a system features description, whilst learning outcomes were evaluated via pharmacology quiz. Results: Students who used ChatGPT (YG group) showed better results on the pharmacology quiz than students who neither read the description nor employed ChatGPT for learning (NN condition). Moreover, students who read the description of ChatGPT features yet did not use it (NG) showed better results on the pharmacology quiz compared with the NN condition, although none of them employed ChatGPT for learning. The NG students compared to the YG students had less trust in AI system assistance in learning, and after the AI system description reading, their expectations changed significantly, showing an association with quiz scores. Conclusions: A majority of students in our cohort was reluctant to use ChatGPT. Furthermore, familarity (reading) with ChatGPT features appear to alter the expectations and enhance learning performance of students.suggesting an AI description–related cognitive bias. Hence the content description of ChatGPTshould be reviewed and verified prior to AI system use for educational purposes. © 2024 The Authors
PB  - Elsevier Inc.
C2  - 38677973
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Smolyak, D.
AU  - Bjarnadóttir, M.V.
AU  - Crowley, K.
AU  - Agarwal, R.
TI  - Large language models and synthetic health data: progress and prospects
PY  - 2024
T2  - JAMIA Open
VL  - 7
IS  - 4
C7  - ooae114
DO  - 10.1093/jamiaopen/ooae114
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208018747&doi=10.1093%2fjamiaopen%2fooae114&partnerID=40&md5=3708884299139ff6eb3e72a7a6c01a0e
AB  - Objectives: Given substantial obstacles surrounding health data acquisition, high-quality synthetic health data are needed to meet a growing demand for the application of advanced analytics for clinical discovery, prediction, and operational excellence. We highlight how recent advances in large language models (LLMs) present new opportunities for progress, as well as new risks, in synthetic health data generation (SHDG). Materials and Methods: We synthesized systematic scoping reviews in the SHDG domain, recent LLM methods for SHDG, and papers investigating the capabilities and limits of LLMs. Results: We summarize the current landscape of generative machine learning models (eg, Generative Adversarial Networks) for SHDG, describe remaining challenges and limitations, and identify how recent LLM approaches can potentially help mitigate them. Discussion: Six research directions are outlined for further investigation of LLMs for SHDG: evaluation metrics, LLM adoption, data efficiency, generalization, health equity, and regulatory challenges. Conclusion: LLMs have already demonstrated both high potential and risks in the health domain, and it is important to study their advantages and disadvantages for SHDG. © 2024 The Author(s).
PB  - Oxford University Press
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Carl, N.
AU  - Schramm, F.
AU  - Haggenmüller, S.
AU  - Kather, J.N.
AU  - Hetz, M.J.
AU  - Wies, C.
AU  - Michel, M.S.
AU  - Wessels, F.
AU  - Brinker, T.J.
TI  - Large language model use in clinical oncology
PY  - 2024
T2  - npj Precision Oncology
VL  - 8
IS  - 1
C7  - 240
DO  - 10.1038/s41698-024-00733-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207458310&doi=10.1038%2fs41698-024-00733-4&partnerID=40&md5=1f0d3c356fc7d00dc39885091e63225e
AB  - Large language models (LLMs) are undergoing intensive research for various healthcare domains. This systematic review and meta-analysis assesses current applications, methodologies, and the performance of LLMs in clinical oncology. A mixed-methods approach was used to extract, summarize, and compare methodological approaches and outcomes. This review includes 34 studies. LLMs are primarily evaluated on their ability to answer oncologic questions across various domains. The meta-analysis highlights a significant performance variance, influenced by diverse methodologies and evaluation criteria. Furthermore, differences in inherent model capabilities, prompting strategies, and oncological subdomains contribute to heterogeneity. The lack of use of standardized and LLM-specific reporting protocols leads to methodological disparities, which must be addressed to ensure comparability in LLM research and ultimately leverage the reliable integration of LLM technologies into clinical practice. © The Author(s) 2024.
PB  - Nature Research
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - CONF
AU  - Yuan, R.
AU  - Hao, W.
AU  - Yuan, C.
TI  - Benchmarking AI in Mental Health: A Critical Examination of LLMs Across Key Performance and Ethical Metrics
PY  - 2025
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 
VL  - 15328 LNCS
SP  - 351
EP  - 366
DO  - 10.1007/978-3-031-78104-9_24
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211760370&doi=10.1007%2f978-3-031-78104-9_24&partnerID=40&md5=4d92958167737ce0121f12850c2b2741
AB  -  The rapid advancement of artificial intelligence (AI) has led to an increasing application of Large Language Models (LLMs) in psychological counseling. This study focuses on a comprehensive evaluation of LLMs in this domain, moving beyond traditional case-based reasoning. We introduce a novel multi-agent LLM framework that enhances the analysis of psychological case interactions. Our approach involves expanding the Emotional First Aid dataset with diverse client backgrounds, enhancing its applicability and generalizability. A sophisticated user profile model, incorporating eight critical dimensions, is developed and applied within a multi-agent system to examine counseling scenarios. The system’s performance is extensively evaluated based on accuracy, robustness, consistency, and fairness. The findings reveal significant differences among LLMs in these areas, highlighting their strengths and limitations in psychological interventions. This research underscores the need for ongoing refinement in LLM applications to ensure equitable and reliable support in psychological counseling. The detailed results and methodologies are available on the GitHub platform for further academic scrutiny and development. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Bhanvadia, S.
AU  - Radha Saseendrakumar, B.
AU  - Guo, J.
AU  - Spadafore, M.
AU  - Daniel, M.
AU  - Lander, L.
AU  - Baxter, S.L.
TI  - Evaluation of bias and gender/racial concordance based on sentiment analysis of narrative evaluations of clinical clerkships using natural language processing
PY  - 2024
T2  - BMC Medical Education
VL  - 24
IS  - 1
C7  - 295
DO  - 10.1186/s12909-024-05271-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187924784&doi=10.1186%2fs12909-024-05271-y&partnerID=40&md5=b610ebbdf3965669a91053b8805e5346
AB  - There is increasing interest in understanding potential bias in medical education. We used natural language processing (NLP) to evaluate potential bias in clinical clerkship evaluations. Data from medical evaluations and administrative databases for medical students enrolled in third-year clinical clerkship rotations across two academic years. We collected demographic information of students and faculty evaluators to determine gender/racial concordance (i.e., whether the student and faculty identified with the same demographic). We used a multinomial log-linear model for final clerkship grades, using predictors such as numerical evaluation scores, gender/racial concordance, and sentiment scores of narrative evaluations using the SentimentIntensityAnalyzer tool in Python. 2037 evaluations from 198 students were analyzed. Statistical significance was defined as P < 0.05. Sentiment scores for evaluations did not vary significantly by student gender, race, or ethnicity (P = 0.88, 0.64, and 0.06, respectively). Word choices were similar across faculty and student demographic groups. Modeling showed narrative evaluation sentiment scores were not predictive of an honors grade (odds ratio [OR] 1.23, P = 0.58). Numerical evaluation average (OR 1.45, P < 0.001) and gender concordance between faculty and student (OR 1.32, P = 0.049) were significant predictors of receiving honors. The lack of disparities in narrative text in our study contrasts with prior findings from other institutions. Ongoing efforts include comparative analyses with other institutions to understand what institutional factors may contribute to bias. NLP enables a systematic approach for investigating bias. The insights gained from the lack of association between word choices, sentiment scores, and final grades show potential opportunities to improve feedback processes for students. © The Author(s) 2024.
PB  - BioMed Central Ltd
C2  - 38491461
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Pan, G.
AU  - Ni, J.
TI  - A cross sectional investigation of ChatGPT-like large language models application among medical students in China
PY  - 2024
T2  - BMC Medical Education
VL  - 24
IS  - 1
C7  - 908
DO  - 10.1186/s12909-024-05871-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201975062&doi=10.1186%2fs12909-024-05871-8&partnerID=40&md5=95867654cf464145b9d50152cb6b9e28
AB  - Objective: To investigate the level of understanding and trust of medical students towards ChatGPT-like large language models, as well as their utilization and attitudes towards these models. Methods: Data collection was concentrated from December 2023 to mid-January 2024, utilizing a self-designed questionnaire to assess the use of large language models among undergraduate medical students at Anhui Medical University. The normality of the data was confirmed with Shapiro-Wilk tests. We used Chi-square tests for comparisons of categorical variables, Mann-Whitney U tests for comparisons of ordinal variables and non-normal continuous variables between two groups, Kruskall-Wallis H tests for comparisons of ordinal variables between multiple groups, and Bonferroni tests for post hoc comparisons. Results: A total of 1774 questionnaires were distributed and 1718 valid questionnaires were collected, with an effective rate of 96.84%. Among these students, 34.5% had heard and used large language models. There were statistically significant differences in the understanding of large language models between genders (p < 0.001), grade levels (junior-level students and senior-level students) (p = 0.03), and major (p < 0.001). Male, junior-level students, and public health management had a higher level of understanding of these models. Genders and majors had statistically significant effects on the degree of trust in large language models (p = 0.004; p = 0.02). Male and nursing students exhibited a higher degree of trust in large language models. As for usage, Male and junior-level students showed a significantly higher proportion of using these models for assisted learning (p < 0.001). Neutral sentiments were held by over two-thirds of the students (66.7%) regarding large language models, with only 51(3.0%) expressing pessimism. There were significant gender-based disparities in attitudes towards large language models, and male exhibited a more optimistic attitude towards these models (p < 0.001). Notably, among students with different levels of knowledge and trust in large language models, statistically significant differences were observed in their perceptions of the shortcomings and benefits of these models. Conclusion: Our study identified gender, grade levels, and major as influential factors in students’ understanding and utilization of large language models. This also suggested the feasibility of integrating large language models with traditional medical education to further enhance teaching effectiveness in the future. © The Author(s) 2024.
PB  - BioMed Central Ltd
C2  - 39180023
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Currie, G.
AU  - Hewis, J.
AU  - Hawk, E.
AU  - Rohren, E.
TI  - Gender and Ethnicity Bias of Text-to-Image Generative Artificial Intelligence in Medical Imaging, Part 1: Preliminary Evaluation
PY  - 2024
T2  - Journal of Nuclear Medicine Technology
VL  - 52
IS  - 4
SP  - 356
EP  - 359
DO  - 10.2967/jnmt.124.268332
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211830034&doi=10.2967%2fjnmt.124.268332&partnerID=40&md5=fdd2a6ab041ac107fd1765b4918a76dd
AB  - Generative artificial intelligence (AI) text-to-image production could reinforce or amplify gender and ethnicity biases. Several text-to-image generative AI tools are used for producing images that represent the medical imaging professions. White male stereotyping and masculine cultures can dissuade women and ethnically divergent people from being drawn into a profession. Methods: In March 2024, DALL-E 3, Firefly 2, Stable Diffusion 2.1, and Midjourney 5.2 were utilized to generate a series of individual and group images of medical imaging professionals: radiologist, nuclear medicine physician, radiographer, and nuclear medicine technologist. Multiple iterations of images were generated using a variety of prompts. Collectively, 184 images were produced for evaluation of 391 characters. All images were independently analyzed by 3 reviewers for apparent gender and skin tone. Results: Collectively (individual and group characters) (n 5 391), 60.6% were male and 87.7% were of a light skin tone. DALL-E 3 (65.6%), Midjourney 5.2 (76.7%), and Stable Diffusion 2.1 (56.2%) had a statistically higher representation of men than Firefly 2 (42.9%) (P, 0.0001). With Firefly 2, 70.3% of characters had light skin tones, which was statistically lower (P, 0.0001) than for Stable Diffusion 2.1 (84.8%), Midjourney 5.2 (100%), and DALL-E 3 (94.8%). Overall, image quality metrics were average or better in 87.2% for DALL-E 3 and 86.2% for Midjourney 5.2, whereas 50.9% were inadequate or poor for Firefly 2 and 86.0% for Stable Diffusion 2.1. Conclusion: Generative AI text-to-image generation using DALL-E 3 via GPT-4 has the best overall quality compared with Firefly 2, Midjourney 5.2, and Stable Diffusion 2.1. Nonetheless, DALL-E 3 includes inherent biases associated with gender and ethnicity that demand more critical evaluation. COPYRIGHT © 2024 by the Society of Nuclear Medicine and Molecular Imaging.
PB  - Society of Nuclear Medicine Inc.
C2  - 39438057
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Sun, S.
AU  - Zack, T.
AU  - Williams, C.Y.K.
AU  - Butte, A.J.
AU  - Sushil, M.
TI  - Revealing the impact of social circumstances on the selection of cancer therapy through natural language processing of social work notes
PY  - 2024
T2  - JAMIA Open
VL  - 7
IS  - 4
C7  - ooae073
DO  - 10.1093/jamiaopen/ooae073
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206442312&doi=10.1093%2fjamiaopen%2fooae073&partnerID=40&md5=a3904d75d643f02a8cd03d4c81eb9838
AB  - Objective: We aimed to investigate the impact of social circumstances on cancer therapy selection using natural language processing to derive insights from social worker documentation. Materials and Methods: We developed and employed a Bidirectional Encoder Representations from Transformers (BERT) based approach, using a hierarchical multi-step BERT model (BERT-MS), to predict the prescription of targeted cancer therapy to patients based solely on documentation by clinical social workers. Our corpus included free-text clinical social work notes, combined with medication prescription information, for all patients treated for breast cancer at UCSF between 2012 and 2021. We conducted a feature importance analysis to identify the specific social circumstances that impact cancer therapy regimen. Results: Using only social work notes, we consistently predicted the administration of targeted therapies, suggesting systematic differences in treatment selection exist due to non-clinical factors. The findings were confirmed by several language models, with GatorTron achieving the best performance with an area under the receiver operating characteristic curve (AUROC) of 0.721 and a Macro F1 score of 0.616. The UCSF BERT-MS model, capable of leveraging multiple pieces of notes, surpassed the UCSF-BERT model in both AUROC and Macro-F1. Our feature importance analysis identified several clinically intuitive social determinants of health that potentially contribute to disparities in treatment. Discussion: Leveraging social work notes can be instrumental in identifying disparities in clinical decision-making. Hypotheses generated in an automated way could be used to guide patient-specific quality improvement interventions. Further validation with diverse clinical outcomes and prospective studies is essential. Conclusions: Our findings indicate that significant disparities exist among breast cancer patients receiving different types of therapies based on social determinants of health. Social work reports play a crucial role in understanding these disparities in clinical decision-making. Lay Summary We aimed to explore the potential correlation between social factors and cancer treatment choices by analyzing social worker notes using data-driven methods. We developed and deployed a Bidirectional Encoder Representations from Transformers (BERT) based approach, using a hierarchical multi-step BERT model (BERT-MS), to predict the prescription of targeted cancer therapies based on these notes. Our study included social work notes and medication information for breast cancer patients at UCSF from 2012 to 2021. We found a strong correlation between social factors and treatment decisions, as shown by the model’s ability to predict therapy choices. The GatorTron model performed best, but our UCSF BERT-MS model also showed strong results. This analysis highlighted important social determinants of health that contribute to disparities in treatment, suggesting that social work notes are crucial for understanding and addressing these differences. © The Author(s) 2024.
PB  - Oxford University Press
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - McLean, A.L.
TI  - Constructing knowledge: the role of AI in medical learning
PY  - 2024
T2  - Journal of the American Medical Informatics Association
VL  - 31
IS  - 8
SP  - 1797
EP  - 1798
DO  - 10.1093/jamia/ocae124
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199129254&doi=10.1093%2fjamia%2focae124&partnerID=40&md5=ac3f3d3886e86bc88b9db6b7ac9aed74
AB  - The integration of large language models (LLMs) like ChatGPT into medical education presents potential benefits and challenges. These technologies, aligned with constructivist learning theories, could potentially enhance critical thinking and problem-solving through inquiry-based learning environments. However, the actual impact on educational outcomes and the effectiveness of these tools in fostering learning require further empirical study. This technological shift necessitates a reevaluation of curriculum design and the development of new assessment methodologies to measure its effects accurately. Additionally, the use of LLMs introduces significant ethical concerns, particularly in addressing inherent AI biases to ensure equitable educational access. LLMs may also help reduce global disparities in medical education by providing broader access to contemporary medical knowledge and practices, though their deployment must be managed carefully to truly support the training of competent, ethical medical professionals.  © The Author(s) 2024.
PB  - Oxford University Press
C2  - 38812088
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Ohse, J.
AU  - Hadžić, B.
AU  - Mohammed, P.
AU  - Peperkorn, N.
AU  - Danner, M.
AU  - Yorita, A.
AU  - Kubota, N.
AU  - Rätsch, M.
AU  - Shiban, Y.
TI  - Zero-Shot Strike: Testing the generalisation capabilities of out-of-the-box LLM models for depression detection
PY  - 2024
T2  - Computer Speech and Language
VL  - 88
C7  - 101663
DO  - 10.1016/j.csl.2024.101663
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193683499&doi=10.1016%2fj.csl.2024.101663&partnerID=40&md5=b76980d1542621224eded770f4e9d7a7
AB  - Depression is a significant global health challenge. Still, many people suffering from depression remain undiagnosed. Furthermore, the assessment of depression can be subject to human bias. Natural Language Processing (NLP) models offer a promising solution. We investigated the potential of four NLP models (BERT, Llama2-13B, GPT-3.5, and GPT-4) for depression detection in clinical interviews. Participants (N = 82) underwent clinical interviews and completed a self-report depression questionnaire. NLP models inferred depression scores from interview transcripts. Questionnaire cut-off values for depression were used as a classifier for depression. GPT-4 showed the highest accuracy for depression classification (F1 score 0.73), while zero-shot GPT-3.5 initially performed with low accuracy (0.34), improved to 0.82 after fine-tuning, and achieved 0.68 with clustered data. GPT-4 estimates of symptom severity PHQ-8 score correlated strongly (r = 0.71) with true symptom severity. These findings demonstrate the potential of AI models for depression detection. However, further research is necessary before widespread deployment can be considered. © 2024 Elsevier Ltd
PB  - Academic Press
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Wei, X.
AU  - Chu, X.
AU  - Geng, J.
AU  - Wang, Y.
AU  - Wang, P.
AU  - Wang, H.
AU  - Wang, C.
AU  - Lei, L.
TI  - Societal impacts of chatbot and mitigation strategies for negative impacts: A large-scale qualitative survey of ChatGPT users
PY  - 2024
T2  - Technology in Society
VL  - 77
C7  - 102566
DO  - 10.1016/j.techsoc.2024.102566
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191291742&doi=10.1016%2fj.techsoc.2024.102566&partnerID=40&md5=204945ecbb5da5082fb46f610c86e4da
AB  - Discussing ChatGPT's societal impact often focuses on specific domains or groups, typically involving a limited number of participants. In contrast, this study investigates the impact of this artificial intelligence (AI) on society as a whole by conducting a qualitative online survey with 1298 daily users from Hinterland of China across various industries. Identified key benefits include enhanced efficiency and production, improved communication, understand, and cooperation, increased social equalization and harmony, and bolstered public mental health. On the downside, concerns arose about (un)employment issues, biased or false outputs, potential for unethical applications, and user dependency and its negative consequences. To address these concerns, participants suggested use restrictions and management, promoting independent and critical thinking/practice, enhancing user literacy, increasing individual irreplaceability, technological improvement and optimization, and safeguarding against AI threats to human society. The above results elucidate the impact of ChatGPT on various social domains and potential solutions for its negative effects. These findings augment existing research on AI user models from a societal impact perspective, providing a foundation for proposing AI user models that consider the social implications of AI. These outcomes also aim to assist policymakers, researchers, and designers in better understanding the societal impact and user concerns regarding AI, thereby informing the design and enhancement of products, services, or policies. © 2024 Elsevier Ltd
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - CONF
AU  - Iloanusi, N.-J.
AU  - Chun, S.A.
TI  - AI Impact on Health Equity for Marginalized, Racial, and Ethnic Minorities
PY  - 2024
T2  - ACM International Conference Proceeding Series
SP  - 841
EP  - 848
DO  - 10.1145/3657054.3657152
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195299835&doi=10.1145%2f3657054.3657152&partnerID=40&md5=2f9fa15ad6330e8257881bb7db802631
AB  - Predictive analytics technologies like machine learning, AI and Generative AI models like Large Language Models (LLMs), have garnered enthusiasm for their potential to improve healthcare services in smart cities. However, these rapidly developing intelligent agents that guides the healthcare decisions may also risk exacerbating health inequities along racial, ethnic, gender, and socioeconomic lines, reflecting systemic discrimination ingrained within healthcare practices. Flawed or injudiciously applied AI systems could improperly restrict opportunities and provide substandard care for minority groups by propagating historical patterns of prejudice encoded within limited training datasets. These advanced intelligent technologies can hinder the sustainable health solutions for smart cities. This study examines intelligent AI models and applications in healthcare settings, with a focus on assessing impacts on marginalized and disadvantaged populations. Comprehensive scholarly database searches identified 45 relevant studies investigating issues on algorithmic bias, lack of diverse training data, and discrimination risks linked to healthcare AI systems. The review finds most applications still lack adequate safeguards to prevent discrimination against vulnerable populations. Through the review of these systems, we propose an integrated inclusive smart health model that considers both technical interventions as well as broader participatory and ethical approaches. Realizing AI’s fullest potential to meaningfully advance health justice requires not only algorithmic adjustments to mitigate bias, and efforts to improve diversity of training data, transparent analysis frameworks, and best practices for ensuring just AI systems in healthcare, but also a human-centered commitment to thoughtful, inclusive development approaches that center the needs and priorities of communities impacted by health disparities from the outset. © 2024 Copyright held by the owner/author(s).
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Futterman, I.D.
AU  - Friedmann, H.
AU  - Shpanel-Yukhta, O.
AU  - Minkoff, H.
AU  - Haberman, S.
TI  - Use of natural language processing to uncover racial bias in obstetrical documentation
PY  - 2024
T2  - Clinical Imaging
VL  - 110
C7  - 110164
DO  - 10.1016/j.clinimag.2024.110164
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191489717&doi=10.1016%2fj.clinimag.2024.110164&partnerID=40&md5=a065b81130f1b0598c19c830ae41294c
AB  - Natural Language Processing (NLP), a form of Artificial Intelligence, allows free-text based clinical documentation to be integrated in ways that facilitate data analysis, data interpretation and formation of individualized medical and obstetrical care. In this cross-sectional study, we identified all births during the study period carrying the radiology-confirmed diagnosis of fibroid uterus in pregnancy (defined as size of largest diameter of >5 cm) by using an NLP platform and compared it to non-NLP derived data using ICD10 codes of the same diagnosis. We then compared the two sets of data and stratified documentation gaps by race. Using fibroid uterus in pregnancy as a marker, we found that Black patients were more likely to have the diagnosis entered late into the patient's chart or had missing documentation of the diagnosis. With appropriate algorithm definitions, cross referencing and thorough validation steps, NLP can contribute to identifying areas of documentation gaps and improve quality of care. © 2024 Elsevier Inc.
PB  - Elsevier Inc.
C2  - 38691911
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Reason, T.
AU  - Langham, J.
AU  - Gimblett, A.
TI  - Automated Mass Extraction of Over 680,000 PICOs from Clinical Study Abstracts Using Generative AI: A Proof-of-Concept Study
PY  - 2024
T2  - Pharmaceutical Medicine
VL  - 38
IS  - 5
SP  - 365
EP  - 372
DO  - 10.1007/s40290-024-00539-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204913325&doi=10.1007%2fs40290-024-00539-6&partnerID=40&md5=50e87b7b396b4e99ed3ab202c9638023
AB  - Background: Generative artificial intelligence (GenAI) shows promise in automating key tasks involved in conducting systematic literature reviews (SLRs), including screening, bias assessment and data extraction. This potential automation is increasingly relevant as pharmaceutical developers face challenging requirements for timely and precise SLRs using the population, intervention, comparator and outcome (PICO) framework, such as those under the impending European Union (EU) Health Technology Assessment Regulation 2021/2282 (HTAR). This proof-of-concept study aimed to evaluate the feasibility, accuracy and efficiency of using GenAI for mass extraction of PICOs from PubMed abstracts. Methods: Abstracts were retrieved from PubMed using a search string targeting randomised controlled trials. A PubMed clinical study ‘specific/narrow’ filter was also applied. Retrieved abstracts were processed using the OpenAI Batch application programming interface (API), which allowed parallel processing and interaction with Generative Pre-trained Transformer 4 Omni (GPT-4o) via custom Python scripts. PICO elements were extracted using a zero-shot prompting strategy. Results were stored in CSV files and subsequently imported into a PostgreSQL database. Results: The PubMed search returned 682,667 abstracts. PICOs from all abstracts were extracted in < 3 h, with an average processing time of 200 s per 1000 abstracts. A total of 395,992,770 tokens were processed, with an average of 580 tokens per abstract. The total cost was $3390. On the basis of a random sample of 350 abstracts, human verification confirmed that GPT-4o accurately and comprehensively extracted 342 (98%) of all PICOs, with only outcome elements rarely missed. Conclusions: Using GenAI to extract PICOs from clinical study abstracts could fundamentally transform the way SLRs are conducted. By enabling pharmaceutical developers to anticipate PICO requirements, this approach allows for proactive preparation for the EU HTAR process, or other health technology assessments (HTAs), streamlining efficiency and reducing the burden of meeting these requirements. © The Author(s) 2024.
PB  - Adis
C2  - 39327389
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Salvagno, M.
AU  - De Cassai, A.
AU  - Zorzi, S.
AU  - Zaccarelli, M.
AU  - Pasetto, M.
AU  - Sterchele, E.D.
AU  - Chumachenko, D.
AU  - Gerli, A.G.
AU  - Azamfirei, R.
AU  - Taccone, F.S.
TI  - The state of artificial intelligence in medical research: A survey of corresponding authors from top medical journals
PY  - 2024
T2  - PLoS ONE
VL  - 19
IS  - 8
C7  - e0309208
DO  - 10.1371/journal.pone.0309208
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201864179&doi=10.1371%2fjournal.pone.0309208&partnerID=40&md5=d79be7710d7598c72979542061dfe1a8
AB  - Natural Language Processing (NLP) is a subset of artificial intelligence that enables machines to understand and respond to human language through Large Language Models (LLMs)‥ These models have diverse applications in fields such as medical research, scientific writing, and publishing, but concerns such as hallucination, ethical issues, bias, and cybersecurity need to be addressed. To understand the scientific community’s understanding and perspective on the role of Artificial Intelligence (AI) in research and authorship, a survey was designed for corresponding authors in top medical journals. An online survey was conducted from July 13th, 2023, to September 1st, 2023, using the SurveyMonkey web instrument, and the population of interest were corresponding authors who published in 2022 in the 15 highest-impact medical journals, as ranked by the Journal Citation Report. The survey link has been sent to all the identified corresponding authors by mail. A total of 266 authors answered, and 236 entered the final analysis. Most of the researchers (40.6%) reported having moderate familiarity with artificial intelligence, while a minority (4.4%) had no associated knowledge. Furthermore, the vast majority (79.0%) believe that artificial intelligence will play a major role in the future of research. Of note, no correlation between academic metrics and artificial intelligence knowledge or confidence was found. The results indicate that although researchers have varying degrees of familiarity with artificial intelligence, its use in scientific research is still in its early phases. Despite lacking formal AI training, many scholars publishing in high-impact journals have started integrating such technologies into their projects, including rephrasing, translation, and proofreading tasks. Efforts should focus on providing training for their effective use, establishing guidelines by journal editors, and creating software applications that bundle multiple integrated tools into a single platform. © 2024 Salvagno et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
PB  - Public Library of Science
C2  - 39178224
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - CONF
AU  - Zhong, Y.
AU  - Wang, X.
AU  - Wang, J.
AU  - Zhang, X.
AU  - Wang, Y.
AU  - Huai, M.
AU  - Xiao, C.
AU  - Ma, F.
TI  - Synthesizing Multimodal Electronic Health Records via Predictive Diffusion Models
PY  - 2024
T2  - Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
SP  - 4607
EP  - 4618
DO  - 10.1145/3637528.3671836
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203674286&doi=10.1145%2f3637528.3671836&partnerID=40&md5=40ded84570adabadc5c8bf240394a6fb
AB  - Synthesizing electronic health records (EHR) data has become a preferred strategy to address data scarcity, improve data quality, and model fairness in healthcare. However, existing approaches for EHR data generation predominantly rely on state-of-the-art generative techniques like generative adversarial networks, variational autoencoders, and language models. These methods typically replicate input visits, resulting in inadequate modeling of temporal dependencies between visits and overlooking the generation of time information, a crucial element in EHR data. Moreover, their ability to learn visit representations is limited due to simple linear mapping functions, thus compromising generation quality. To address these limitations, we propose a novel EHR data generation model called EHRPD. It is a diffusion-based model designed to predict the next visit based on the current one while also incorporating time interval estimation. To enhance generation quality and diversity, we introduce a novel time-aware visit embedding module and a pioneering predictive denoising diffusion probabilistic model (P-DDPM). Additionally, we devise a predictive U-Net (PU-Net) to optimize P-DDPM. We conduct experiments on two public datasets and evaluate EHRPD from fidelity, privacy, and utility perspectives. The experimental results demonstrate the efficacy and utility of the proposed EHRPD in addressing the aforementioned limitations and advancing EHR data generation.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Nasarian, E.
AU  - Alizadehsani, R.
AU  - Acharya, U.R.
AU  - Tsui, K.-L.
TI  - Designing interpretable ML system to enhance trust in healthcare: A systematic review to proposed responsible clinician-AI-collaboration framework
PY  - 2024
T2  - Information Fusion
VL  - 108
C7  - 102412
DO  - 10.1016/j.inffus.2024.102412
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189938333&doi=10.1016%2fj.inffus.2024.102412&partnerID=40&md5=c350f539774b809b5e2c2fbe18bd9d05
AB  - Background: Artificial intelligence (AI)-based medical devices and digital health technologies, including medical sensors, wearable health trackers, telemedicine, mobile health (mHealth), large language models (LLMs), and digital care twins (DCTs), significantly influence the process of clinical decision support systems (CDSS) in healthcare and medical applications. However, given the complexity of medical decisions, it is crucial that results generated by AI tools not only be correct but also carefully evaluated, understandable, and explainable to end-users, especially clinicians. The lack of interpretability in communicating AI clinical decisions can lead to mistrust among decision-makers and a reluctance to use these technologies. Objective: This paper systematically reviews the processes and challenges associated with interpretable machine learning (IML) and explainable artificial intelligence (XAI) within the healthcare and medical domains. Its main goals are to examine the processes of IML and XAI, their related methods, applications, and the implementation challenges they pose in digital health interventions (DHIs), particularly from a quality control perspective, to help understand and improve communication between AI systems and clinicians. The IML process is categorized into pre-processing interpretability, interpretable modeling, and post-processing interpretability. This paper aims to foster a comprehensive understanding of the significance of a robust interpretability approach in clinical decision support systems (CDSS) by reviewing related experimental results. The goal is to provide future researchers with insights for creating clinician-AI tools that are more communicable in healthcare decision support systems and offer a deeper understanding of their challenges. Methods: Our research questions, eligibility criteria, and primary goals were proved using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guideline and the PICO (population, intervention, control, and outcomes) method. We systematically searched PubMed, Scopus, and Web of Science databases using sensitive and specific search strings. Subsequently, duplicate papers were removed using EndNote and Covidence. A two-phase selection process was then carried out on Covidence, starting with screening by title and abstract, followed by a full-text appraisal. The Meta Quality Appraisal Tool (MetaQAT) was used to assess the quality and risk of bias. Finally, a standardized data extraction tool was employed for reliable data mining. Results: The searches yielded 2,241 records, from which 555 duplicate papers were removed. During the title and abstract screening step, 958 papers were excluded, and the full-text review step excluded 482 studies. Subsequently, in quality and risk of bias assessment, 172 papers were removed. 74 publications were selected for data extraction, which formed 10 insightful reviews and 64 related experimental studies. Conclusion: The paper provides general definitions of explainable artificial intelligence (XAI) in the medical domain and introduces a framework for interpretability in clinical decision support systems structured across three levels. It explores XAI-related health applications within each tier of this framework, underpinned by a review of related experimental findings. Furthermore, the paper engages in a detailed discussion of quality assessment tools for evaluating XAI in intelligent health systems. It also presents a step-by-step roadmap for implementing XAI in clinical settings. To direct future research toward bridging current gaps, the paper examines the importance of XAI models from various angles and acknowledges their limitations. © 2024 Elsevier B.V.
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 18
ER  -

TY  - JOUR
AU  - Ihara, K.
AU  - Dumkrieger, G.
AU  - Zhang, P.
AU  - Takizawa, T.
AU  - Schwedt, T.J.
AU  - Chiang, C.-C.
TI  - Application of Artificial Intelligence in the Headache Field
PY  - 2024
T2  - Current Pain and Headache Reports
VL  - 28
IS  - 10
SP  - 1049
EP  - 1057
DO  - 10.1007/s11916-024-01297-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197736922&doi=10.1007%2fs11916-024-01297-5&partnerID=40&md5=b33f79053d72cf7ee94650473cc137f2
AB  - Purpose of Review: Headache disorders are highly prevalent worldwide. Rapidly advancing capabilities in artificial intelligence (AI) have expanded headache-related research with the potential to solve unmet needs in the headache field. We provide an overview of AI in headache research in this article. Recent Findings: We briefly introduce machine learning models and commonly used evaluation metrics. We then review studies that have utilized AI in the field to advance diagnostic accuracy and classification, predict treatment responses, gather insights from various data sources, and forecast migraine attacks. Furthermore, given the emergence of ChatGPT, a type of large language model (LLM), and the popularity it has gained, we also discuss how LLMs could be used to advance the field. Finally, we discuss the potential pitfalls, bias, and future directions of employing AI in headache medicine. Summary: Many recent studies on headache medicine incorporated machine learning, generative AI and LLMs. A comprehensive understanding of potential pitfalls and biases is crucial to using these novel techniques with minimum harm. When used appropriately, AI has the potential to revolutionize headache medicine. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.
PB  - Springer
C2  - 38976174
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Roxas, R.E.O.
AU  - Recario, R.N.C.
TI  - Scientific landscape on opportunities and challenges of large language models and natural language processing
PY  - 2024
T2  - Indonesian Journal of Electrical Engineering and Computer Science
VL  - 36
IS  - 1
SP  - 252
EP  - 263
DO  - 10.11591/ijeecs.v36.i1.pp252-263
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199862427&doi=10.11591%2fijeecs.v36.i1.pp252-263&partnerID=40&md5=9c55255f4c543ba86693dbdad0d26467
AB  - This paper conducted a systematic review of Scopus-indexed publications on large language models (LLMs) and natural language processing (NLP) extracted in October 2023 to address the dearth of literature on their opportunities and challenges. Through bibliometric analysis, from the 1,600 relevant documents, the study explored research productivity, revealing both opportunities and challenges spanning research and real-world applications in education, medicine, and health care, citations, and keyword co-occurrence networks. Results highlighted distribution patterns and dominant players like Google LLC and Stanford University. Opportunities such as technological development in generative artificial intelligence (AI), were contrasted with challenges such as biases and ethical concerns. The intellectual structure analysis revealed prominent application areas in health and education and also emphasized issues such as AI divide and human-AI partnership. Improvement on the technology performance of LLM and NLP remains to be a challenge. Recommendations include further exploration of open research problems and bibliometric studies using other research databases given the research bias towards Scopus-indexed English publications. © 2024 Institute of Advanced Engineering and Science. All rights reserved.
PB  - Institute of Advanced Engineering and Science
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Rehman, A.U.
AU  - Behera, R.K.
AU  - Islam, M.S.
AU  - Abbasi, F.A.
AU  - Imtiaz, A.
TI  - Assessing the usage of ChatGPT on life satisfaction among higher education students: The moderating role of subjective health
PY  - 2024
T2  - Technology in Society
VL  - 78
C7  - 102655
DO  - 10.1016/j.techsoc.2024.102655
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198025211&doi=10.1016%2fj.techsoc.2024.102655&partnerID=40&md5=dcdec4ef325fb9153fa9153daa7155b3
AB  - OpenAI's ChatGPT is a widely used artificial intelligence tool that has recently experienced rapid growth and widespread adoption. ChatGPT enhances digital accessibility, performance in communication, and supports the creation of digital content, which can be a powerful assistive technology for the education industry. However, the role of ChatGPT for higher education students remains a topic of contention. Therefore, this study is undertaken to investigate how ChatGPT usage can enhance information and communication technology (ICT) accessibility and performance by influencing life satisfaction among higher education students by proposing a unique conceptual model. The primary data were collected from 305 respondents, and quantitative methodology was used to analyse the data. The results indicate that interaction with ChatGPT increases freedom and productivity by producing understandable and relevant responses that meet emotional needs, which are positively correlated with happiness. ChatGPT usage provides unique experiences to students that evoke their feelings to strengthen academic engagement, and such feelings positively influence their perceptions and behaviour towards academic buoyancy. However, the concerns about the biased response, limited knowledge, and lack of emotional intelligence of ChatGPT limit trustworthiness and cause disengagement, which have been deemed the most significant weaknesses in improving quality of life. © 2024 Elsevier Ltd
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Conrad, E.J.
AU  - Hall, K.C.
TI  - Leveraging Generative AI to Elevate Curriculum Design and Pedagogy in Public Health and Health Promotion
PY  - 2024
T2  - Pedagogy in Health Promotion
VL  - 10
IS  - 3
SP  - 178
EP  - 186
DO  - 10.1177/23733799241232641
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185908036&doi=10.1177%2f23733799241232641&partnerID=40&md5=b2b58d9577386a4024550dc9647e29c4
AB  - Despite increased recognition of the importance and need for pedagogical training for public health and health promotion instructors in best-practices and inclusivity, formal training is often overlooked. This disregard for pedagogical training necessitates exploration of alternative and innovative approaches to enhance teaching and learning such as generative AI. This paper describes applied uses of generative AI, specifically ChatGPT, to enhance pedagogy in public health and health promotion education in the areas of curriculum design, instructional strategies, assessment and feedback, and diversity, equity, and inclusion. Generative AI as a supplemental tool shows immense promise for improving teaching and learning, however, inherent limitations and ethical considerations require caution and continued scrutiny. © 2024 Society for Public Health Education.
PB  - SAGE Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Herr, K.
AU  - Lu, P.
AU  - Diamreyan, K.
AU  - Xu, H.
AU  - Mendonca, E.
AU  - Weaver, K.N.
AU  - Chen, J.
TI  - Estimating prevalence of rare genetic disease diagnoses using electronic health records in a children's hospital
PY  - 2024
T2  - Human Genetics and Genomics Advances
VL  - 5
IS  - 4
C7  - 100341
DO  - 10.1016/j.xhgg.2024.100341
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206220732&doi=10.1016%2fj.xhgg.2024.100341&partnerID=40&md5=aeb9c7244bf922d086d27959efb9998b
AB  - Rare genetic diseases (RGDs) affect a significant number of individuals, particularly in pediatric populations. This study investigates the efficacy of identifying RGD diagnoses through electronic health records (EHRs) and natural language processing (NLP) tools, and analyzes the prevalence of identified RGDs for potential underdiagnosis at Cincinnati Children's Hospital Medical Center (CCHMC). EHR data from 659,139 pediatric patients at CCHMC were utilized. Diagnoses corresponding to RGDs in Orphanet were identified using rule-based and machine learning-based NLP methods. Manual evaluation assessed the precision of the NLP strategies, with 100 diagnosis descriptions reviewed for each method. The rule-based method achieved a precision of 97.5% (95% CI: 91.5%, 99.4%), while the machine-learning-based method had a precision of 73.5% (95% CI: 63.6%, 81.6%). A manual chart review of 70 randomly selected patients with RGD diagnoses confirmed the diagnoses in 90.3% (95% CI: 82.0%, 95.2%) of cases. A total of 37,326 pediatric patients were identified with 977 RGD diagnoses based on the rule-based method, resulting in a prevalence of 5.66% in this population. While a majority of the disorders showed a higher prevalence at CCHMC compared with Orphanet, some diseases, such as 1p36 deletion syndrome, indicated potential underdiagnosis. Analyses further uncovered disparities in RGD prevalence and age of diagnosis across gender and racial groups. This study demonstrates the utility of employing EHR data with NLP tools to systematically investigate RGD diagnoses in large cohorts. The identified disparities underscore the need for enhanced approaches to guarantee timely and accurate diagnosis and management of pediatric RGDs. © 2024 The Authors
PB  - Elsevier Inc.
C2  - 39148290
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Sheikh, M.S.
AU  - Thongprayoon, C.
AU  - Suppadungsuk, S.
AU  - Miao, J.
AU  - Qureshi, F.
AU  - Kashani, K.
AU  - Cheungpasitporn, W.
TI  - Evaluating ChatGPT’s Accuracy in Responding to Patient Education Questions on Acute Kidney Injury and Continuous Renal Replacement Therapy
PY  - 2024
T2  - Blood Purification
VL  - 53
IS  - 9
SP  - 725
EP  - 731
DO  - 10.1159/000539065
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196742813&doi=10.1159%2f000539065&partnerID=40&md5=c189926410cf7e0a4c894d236bc6e2d2
AB  - Introduction: Acute kidney injury (AKI) and continuous renal replacement therapy (CRRT) are critical areas in nephrology. The effectiveness of ChatGPT in simpler, patient education-oriented questions has not been thoroughly assessed. This study evaluates the proficiency of ChatGPT 4.0 in responding to such questions, subjected to various linguistic alterations. Methods: Eighty-nine questions were sourced from the Mayo Clinic Handbook for educating patients on AKI and CRRT. These questions were categorized as original, paraphrased with different interrogative adverbs, paraphrased resulting in incomplete sentences, and paraphrased containing misspelled words. Two nephrologists verified the questions for medical accuracy. A χ2 test was conducted to ascertain notable discrepancies in ChatGPT 4.0’s performance across these formats. Results: ChatGPT provided notable accuracy in handling a variety of question formats for patient education in AKI and CRRT. Across all question types, ChatGPT demonstrated an accuracy of 97% for both original and adverb-altered questions and 98% for questions with incomplete sentences or misspellings. Specifically for AKI-related questions, the accuracy was consistently maintained at 97% for all versions. In the subset of CRRT-related questions, the tool achieved a 96% accuracy for original and adverb-altered questions, and this increased to 98% for questions with incomplete sentences or misspellings. The statistical analysis revealed no significant difference in performance across these varied question types (p value: 1.00 for AKI and 1.00 for CRRT), and there was no notable disparity between the artificial intelligence (AI)’s responses to AKI and CRRT questions (p value: 0.71). Conclusion: ChatGPT 4.0 demonstrates consistent and high accuracy in interpreting and responding to queries related to AKI and CRRT, irrespective of linguistic modifications. These findings suggest that ChatGPT 4.0 has the potential to be a reliable support tool in the delivery of patient education, by accurately providing information across a range of question formats. Further research is needed to explore the direct impact of AI-generated responses on patient understanding and education outcomes. © 2024 S. Karger AG, Basel.
PB  - S. Karger AG
C2  - 38679000
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Zhang, A.
AU  - Dimock, E.
AU  - Gupta, R.
AU  - Chen, K.
TI  - The new frontier: utilizing ChatGPT to expand craniofacial research
PY  - 2024
T2  - Archives of Craniofacial Surgery
VL  - 25
IS  - 3
SP  - 116
EP  - 122
DO  - 10.7181/acfs.2024.00115
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198633231&doi=10.7181%2facfs.2024.00115&partnerID=40&md5=89013ce229df7332622a51b9f473d240
AB  - Background: Due to the importance of evidence-based research in plastic surgery, the authors of this study aimed to assess the accuracy of ChatGPT in generating novel systematic review ideas within the field of craniofacial surgery. Methods: ChatGPT was prompted to generate 20 novel systematic review ideas for 10 different subcategories within the field of craniofa-cial surgery. For each topic, the chatbot was told to give 10 “general” and 10 “specific” ideas that were related to the concept. In order to determine the accuracy of ChatGPT, a literature review was conducted using PubMed, CINAHL, Embase, and Cochrane. Results: In total, 200 total systematic review research ideas were generated by ChatGPT. We found that the algorithm had an overall 57.5% accuracy at identifying novel systematic review ideas. ChatGPT was found to be 39% accurate for general topics and 76% accurate for specific topics. Conclusion: Craniofacial surgeons should use ChatGPT as a tool. We found that ChatGPT provided more precise answers with specific research questions than with general questions and helped narrow down the search scope, leading to a more relevant and accurate response. Beyond research purposes, ChatGPT can augment patient consultations, improve healthcare equity, and assist in clinical decision-making. With rapid advancements in artificial intelligence (AI), it is important for plastic surgeons to consider using AI in their clinical practice to improve patient-centered outcomes. © 2024 Korean Cleft Palate-Craniofacial Association.
PB  - Korean Cleft Palate-Craniofacial Association
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Boutet, A.
AU  - Haile, S.S.
AU  - Yang, A.Z.
AU  - Son, H.J.
AU  - Malik, M.
AU  - Pai, V.
AU  - Nasralla, M.
AU  - Germann, J.
AU  - Vetkas, A.
AU  - Khalvati, F.
AU  - Ertl-Wagner, B.B.
TI  - Assessing the Emergence and Evolution of Artificial Intelligence and Machine Learning Research in Neuroradiology
PY  - 2024
T2  - American Journal of Neuroradiology
VL  - 45
IS  - 9
SP  - 1269
EP  - 1275
DO  - 10.3174/ajnr.A8252
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198330583&doi=10.3174%2fajnr.A8252&partnerID=40&md5=29ccdbe21a2f9c7a8161b65751ae61ee
AB  - BACKGROUND AND PURPOSE: Interest in artificial intelligence (AI) and machine learning (ML) has been growing in neuroradiology, but there is limited knowledge on how this interest has manifested into research and specifically, its qualities and characteristics. This study aims to characterize the emergence and evolution of AI/ML articles within neuroradiology and provide a comprehensive overview of the trends, challenges, and future directions of the field. MATERIALS AND METHODS: We performed a bibliometric analysis of the American Journal of Neuroradiology; the journal was queried for original research articles published since inception (January 1, 1980) to December 3, 2022 that contained any of the following key terms: “machine learning,” “artificial intelligence,” “radiomics,” “deep learning,” “neural network,” “generative adversarial network,” “object detection,” or “natural language processing.” Articles were screened by 2 independent reviewers, and categorized into statistical modeling (type 1), AI/ML development (type 2), both representing developmental research work but without a direct clinical integration, or end-user application (type 3), which is the closest surrogate of potential AI/ML integration into day-to-day practice. To better understand the limiting factors to type 3 articles being published, we analyzed type 2 articles as they should represent the precursor work leading to type 3. RESULTS: A total of 182 articles were identified with 79% being nonintegration focused (type 1 n ¼ 53, type 2 n ¼ 90) and 21% (n ¼ 39) being type 3. The total number of articles published grew roughly 5-fold in the last 5 years, with the nonintegration focused articles mainly driving this growth. Additionally, a minority of type 2 articles addressed bias (22%) and explainability (16%). These articles were primarily led by radiologists (63%), with most (60%) having additional postgraduate degrees. CONCLUSIONS: AI/ML publications have been rapidly increasing in neuroradiology with only a minority of this growth being attributable to end-user application. Areas identified for improvement include enhancing the quality of type 2 articles, namely external validation, and addressing both bias and explainability. These results ultimately provide authors, editors, clinicians, and policymakers important insights to promote a shift toward integrating practical AI/ML solutions in neuroradiology. © 2024 American Society of Neuroradiology. All rights reserved.
PB  - American Society of Neuroradiology
C2  - 38521092
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - JOUR
AU  - Mayo-Yáñez, M.
AU  - González-Torres, L.
AU  - Saibene, A.M.
AU  - Allevi, F.
AU  - Vaira, L.A.
AU  - Maniaci, A.
AU  - Chiesa-Estomba, C.M.
AU  - Lechien, J.R.
TI  - Application of ChatGPT as a support tool in the diagnosis and management of acute bacterial tonsillitis
PY  - 2024
T2  - Health and Technology
VL  - 14
IS  - 4
SP  - 773
EP  - 779
DO  - 10.1007/s12553-024-00858-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189892794&doi=10.1007%2fs12553-024-00858-3&partnerID=40&md5=d420dde7d4e58d711c967235324544e7
AB  - Introduction: Artificial intelligence (AI) is transforming medicine through techniques like machine learning and deep learning. AI aids diagnosis, enhances patient care, and streamlines healthcare systems. Despite potential benefits, challenges of bias and trust must be managed. Tonsillitis is a common otolaryngological condition with economic implications. The study assesses ChatGPT’s utility in the diagnosis management of bacterial tonsillitis, highlighting its potential for patient-professional interaction. Methods: 2 methods evaluated ChatGPT 3.5 diagnostic ability for tonsillitis: patient-written cases and specialist-created cases. The scenarios involved real patients and fictional cases, assessed by 15 otolaryngologists and 5 pediatricians. Variables included diagnosis accuracy, recommendations quality, and message count. Results: A total of 35 conversations were conducted. ChatGPT achieved accurate diagnoses in 100% of cases, with an average of 3.7 ± 1.1 chat entries for diagnosis. No significant difference existed between professional and patient scenarios (p = 0.977). Recommendations were categorized: appropriate (48.57%), incomplete (45.71%), inappropriate (5.71%), with no significant intergroup difference (p = 0.196). ChatGPT consistently advised consulting a doctor and exhibited expertise in guiding medical consultations. Conclusion: ChatGPT demonstrates promise in providing medical insights and general advice. Its diagnostic accuracy for tonsillitis is notable, but it relies on static data and lacks individual history assessment. ChatGPT shows potential for diagnostics in simpler cases like tonsillitis, but accuracy for complex conditions needs refinement. Further research is needed for validation and broader application. © The Author(s) under exclusive licence to International Union for Physical and Engineering Sciences in Medicine (IUPESM) 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Geantă, M.
AU  - Bădescu, D.
AU  - Chirca, N.
AU  - Nechita, O.C.
AU  - Radu, C.G.
AU  - Rascu, Ș.
AU  - Rădăvoi, D.
AU  - Sima, C.
AU  - Toma, C.
AU  - Jinga, V.
TI  - The Emerging Role of Large Language Models in Improving Prostate Cancer Literacy
PY  - 2024
T2  - Bioengineering
VL  - 11
IS  - 7
C7  - 654
DO  - 10.3390/bioengineering11070654
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199630059&doi=10.3390%2fbioengineering11070654&partnerID=40&md5=3a0e71842bf8519d76ad7f2215d40580
AB  - This study assesses the effectiveness of chatbots powered by Large Language Models (LLMs)—ChatGPT 3.5, CoPilot, and Gemini—in delivering prostate cancer information, compared to the official Patient’s Guide. Using 25 expert-validated questions, we conducted a comparative analysis to evaluate accuracy, timeliness, completeness, and understandability through a Likert scale. Statistical analyses were used to quantify the performance of each model. Results indicate that ChatGPT 3.5 consistently outperformed the other models, establishing itself as a robust and reliable source of information. CoPilot also performed effectively, albeit slightly less so than ChatGPT 3.5. Despite the strengths of the Patient’s Guide, the advanced capabilities of LLMs like ChatGPT significantly enhance educational tools in healthcare. The findings underscore the need for ongoing innovation and improvement in AI applications within health sectors, especially considering the ethical implications underscored by the forthcoming EU AI Act. Future research should focus on investigating potential biases in AI-generated responses and their impact on patient outcomes. © 2024 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Abdalla, H.B.
AU  - Awlla, A.H.
AU  - Kumar, Y.
AU  - Cheraghy, M.
TI  - Big Data: Past, Present, and Future Insights
PY  - 2024
T2  - ACM International Conference Proceeding Series
SP  - 60
EP  - 70
DO  - 10.1145/3685767.3685777
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207174035&doi=10.1145%2f3685767.3685777&partnerID=40&md5=6840de607d94dd6714855d56fb15cabc
AB  - This paper presents a comprehensive analysis of the historical progression, current trends, and prospects of Big Data. It explores the technological advancements that have established Big Data as a critical element of contemporary analytics, its extensive impact across various sectors, and the ethical challenges it poses. Beginning with the early recognition of Big Data’s potential in the 2000s, the paper traces the development of foundational technologies such as Hadoop and the subsequent diversification of tools and methods. It delves into the integration of advanced analytics and machine learning, the rise of cloud-based Big Data services, and the transformative effects on sectors including healthcare, finance, agriculture, and education. The study also examines ethical considerations such as privacy, bias, transparency, and regulatory compliance, emphasizing the need for robust governance frameworks. It investigates the potential of emerging technologies like AI, IoT, and quantum computing to enhance Big Data capabilities further. It highlights future directions, including decentralized data ecosystems, advanced analytical techniques, and enhanced data privacy measures. By providing a panoramic view of Big Data’s development, this paper aims to showcase its potential to revolutionize decision-making processes, improve operational efficiency, and drive innovation across industries; it underscores the importance of balancing technological innovation with ethical responsibility to ensure positive societal advancement and global progress. To add a novelty to the discussion, an AI agent Big D was created to provide a relevant analysis of trends in Big Data. The agent uses a multimodal ChatGPT-4o Large Language Model (LLM) from OpenAI and provides its review based on uploaded files and LLM knowledge. © 2024 Copyright held by the owner/author(s).
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Currie, G.
AU  - Chandra, C.
AU  - Kiat, H.
TI  - Gender Bias in Text-to-Image Generative Artificial Intelligence When Representing Cardiologists
PY  - 2024
T2  - Information (Switzerland)
VL  - 15
IS  - 10
C7  - 594
DO  - 10.3390/info15100594
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207645224&doi=10.3390%2finfo15100594&partnerID=40&md5=42246314e99ba73b28fe8c103607162a
AB  - Introduction: While the global medical graduate and student population is approximately 50% female, only 13–15% of cardiologists and 20–27% of training fellows in cardiology are female. The potentially transformative use of text-to-image generative artificial intelligence (AI) could improve promotions and professional perceptions. In particular, DALL-E 3 offers a useful tool for promotion and education, but it could reinforce gender and ethnicity biases. Method: Responding to pre-specified prompts, DALL-E 3 via GPT-4 generated a series of individual and group images of cardiologists. Overall, 44 images were produced, including 32 images that contained individual characters and 12 group images that contained between 7 and 17 characters. All images were independently analysed by three reviewers for the characters’ apparent genders, ages, and skin tones. Results: Among all images combined, 86% (N = 123) of cardiologists were depicted as male. A light skin tone was observed in 93% (N = 133) of cardiologists. The gender distribution was not statistically different from that of actual Australian workforce data (p = 0.7342), but this represents a DALL-E 3 gender bias and the under-representation of females in the cardiology workforce. Conclusions: Gender bias associated with text-to-image generative AI when using DALL-E 3 among cardiologists limits its usefulness for promotion and education in addressing the workforce gender disparities. © 2024 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Santomartino, S.M.
AU  - Zech, J.R.
AU  - Hall, K.
AU  - Jeudy, J.
AU  - Parekh, V.
AU  - Yi, P.H.
TI  - Evaluating the Performance and Bias of Natural Language Processing Tools in Labeling Chest Radiograph Reports
PY  - 2024
T2  - Radiology
VL  - 313
IS  - 1
C7  - e232746
DO  - 10.1148/radiol.232746
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206970794&doi=10.1148%2fradiol.232746&partnerID=40&md5=c411329ead0d79647437f8cb278e4cdb
AB  - Background: Natural language processing (NLP) is commonly used to annotate radiology datasets for training deep learning (DL) models. However, the accuracy and potential biases of these NLP methods have not been thoroughly investigated, particularly across different demographic groups. Purpose: To evaluate the accuracy and demographic bias of four NLP radiology report labeling tools on two chest radiograph datasets. Materials and Methods: This retrospective study, performed between April 2022 and April 2024, evaluated chest radiograph report labeling using four NLP tools (CheXpert [rule-based], RadReportAnnotator [RRA; DL-based], OpenAI’s GPT-4 [DL-based], cTAKES [hybrid]) on a subset of the Medical Information Mart for Intensive Care (MIMIC) chest radiograph dataset balanced for representation of age, sex, and race and ethnicity (n = 692) and the entire Indiana University (IU) chest radiograph dataset (n = 3665). Three board-certified radiologists annotated the chest radiograph reports for 14 thoracic disease labels. NLP tool performance was evaluated using several metrics, including accuracy and error rate. Bias was evaluated by comparing performance between demographic subgroups using the Pearson χ2 test. Results: The IU dataset included 3665 patients (mean age, 49.7 years ± 17 [SD]; 1963 female), while the MIMIC dataset included 692 patients (mean age, 54.1 years ± 23.1; 357 female). All four NLP tools demonstrated high accuracy across findings in the IU and MIMIC datasets, as follows: CheXpert (92.6% [47 516 of 51 310], 90.2% [8742 of 9688]), RRA (82.9% [19 746 of 23 829], 92.2% [2870 of 3114]), GPT-4 (94.3% [45 586 of 48 342], 91.6% [6721 of 7336]), and cTAKES (84.7% [43 436 of 51 310], 88.7% [8597 of 9688]). RRA and cTAKES had higher accuracy (P < .001) on the MIMIC dataset, while CheXpert and GPT-4 had higher accuracy on the IU dataset. Differences (P < .001) in error rates were observed across age groups for all NLP tools except RRA on the MIMIC dataset, with the highest error rates for CheXpert, RRA, and cTAKES in patients older than 80 years (mean, 15.8% ± 5.0) and the highest error rate for GPT-4 in patients 60–80 years of age (8.3%). Conclusion: Although commonly used NLP tools for chest radiograph report annotation are accurate when evaluating reports in aggregate, demographic subanalyses showed significant bias, with poorer performance in older patients. © RSNA, 2024.
PB  - Radiological Society of North America Inc.
C2  - 39436298
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Sağtaş, E.
AU  - Ufuk, F.
AU  - Peker, H.
AU  - Yağcı, A.B.
TI  - Artificial intelligence meets medical expertise: evaluating GPT-4's proficiency in generating medical article abstracts
ST  - Yapay zeka tıbbi uzmanlıkla buluşuyor: GPT-4'ün tıbbi makale özetleri oluşturmadaki yeterliliğinin değerlendirilmesi
PY  - 2024
T2  - Pamukkale Medical Journal
VL  - 17
IS  - 4
SP  - 756
EP  - 762
DO  - 10.31362/patd.1487575
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207505677&doi=10.31362%2fpatd.1487575&partnerID=40&md5=082bc1974706d2f9116ee04c3f232b5a
AB  - Purpose: The advent of large language models like GPT-4 has opened new possibilities in natural language processing, with potential applications in medical literature. This study assesses GPT-4's ability to generate medical abstracts. It compares their quality to original abstracts written by human authors, aiming to understand the effectiveness of artificial intelligence in replicating complex, professional writing tasks. Materials and methods: A total of 250 original research articles from five prominent radiology journals published between 2021 and 2023 were selected. The body of these articles, excluding the abstracts, was fed into GPT-4, which then generated new abstracts. Three experienced radiologists blindly and independently evaluated all 500 abstracts using a five-point Likert scale for quality and understandability. Statistical analysis included mean score comparison inter-rater reliability using Fleiss' Kappa and Bland-Altman plots to assess agreement levels between raters. Results: Analysis revealed no significant difference in the mean scores between original and GPT-4 generated abstracts. The inter-rater reliability yielded kappa values indicating moderate to substantial agreement: 0.497 between Observers 1 and 2, 0.753 between Observers 1 and 3, and 0.645 between Observers 2 and 3. Bland-Altman analysis showed a slight systematic bias but was within acceptable limits of agreement. Conclusion: The study demonstrates that GPT-4 can generate medical abstracts with a quality comparable to those written by human experts. This suggests a promising role for artificial intelligence in facilitating the abstract writing process and improving its quality. © 2024, Pamukkale University. All rights reserved.
PB  - Pamukkale University
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Nolan, V.J.
AU  - Balch, J.A.
AU  - Baskaran, N.P.
AU  - Shickel, B.
AU  - Efron, P.A.
AU  - Upchurch, G.R.
AU  - Bihorac, A.
AU  - Tignanelli, C.J.
AU  - Moseley, R.E.
AU  - Loftus, T.J.
TI  - Incorporating Patient Values in Large Language Model Recommendations for Surrogate and Proxy Decisions
PY  - 2024
T2  - Critical Care Explorations
VL  - 6
IS  - 8
SP  - e1131
DO  - 10.1097/CCE.0000000000001131
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201194520&doi=10.1097%2fCCE.0000000000001131&partnerID=40&md5=64f35319a69bd15ad459c6f8fa6b93dd
AB  - Background: Surrogates, proxies, and clinicians making shared treatment decisions for patients who have lost decision-making capacity often fail to honor patients' wishes, due to stress, time pressures, misunderstanding patient values, and projecting personal biases. Advance directives intend to align care with patient values but are limited by low completion rates and application to only a subset of medical decisions. Here, we investigate the potential of large language models (LLMs) to incorporate patient values in supporting critical care clinical decision-making for incapacitated patients in a proof-of-concept study. Methods: We simulated text-based scenarios for 50 decisionally incapacitated patients for whom a medical condition required imminent clinical decisions regarding specific interventions. For each patient, we also simulated five unique value profiles captured using alternative formats: numeric ranking questionnaires, text-based questionnaires, and free-text narratives. We used pre-trained generative LLMs for two tasks: 1) text extraction of the treatments under consideration and 2) prompt-based question-answering to generate a recommendation in response to the scenario information, extracted treatment, and patient value profiles. Model outputs were compared with adjudications by three domain experts who independently evaluated each scenario and decision. Results and Conclusions: Automated extractions of the treatment in question were accurate for 88% (n = 44/50) of scenarios. LLM treatment recommendations received an average Likert score by the adjudicators of 3.92 of 5.00 (five being best) across all patients for being medically plausible and reasonable treatment recommendations, and 3.58 of 5.00 for reflecting the documented values of the patient. Scores were highest when patient values were captured as short, unstructured, and free-text narratives based on simulated patient profiles. This proof-of-concept study demonstrates the potential for LLMs to function as support tools for surrogates, proxies, and clinicians aiming to honor the wishes and values of decisionally incapacitated patients.  Copyright © 2024 The Authors.
PB  - Lippincott Williams and Wilkins
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Suleiman, A.
AU  - von Wedel, D.
AU  - Munoz-Acuna, R.
AU  - Redaelli, S.
AU  - Santarisi, A.
AU  - Seibold, E.-L.
AU  - Ratajczak, N.
AU  - Kato, S.
AU  - Said, N.
AU  - Sundar, E.
AU  - Goodspeed, V.
AU  - Schaefer, M.S.
TI  - Assessing ChatGPT's ability to emulate human reviewers in scientific research: A descriptive and qualitative approach
PY  - 2024
T2  - Computer Methods and Programs in Biomedicine
VL  - 254
C7  - 108313
DO  - 10.1016/j.cmpb.2024.108313
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197347570&doi=10.1016%2fj.cmpb.2024.108313&partnerID=40&md5=e68f4f9b49b8a381be5dd53f357a14b9
AB  - Background: ChatGPT is an AI platform whose relevance in the peer review of scientific articles is steadily growing. Nonetheless, it has sparked debates over its potential biases and inaccuracies. This study aims to assess ChatGPT's ability to qualitatively emulate human reviewers in scientific research. Methods: We included the first submitted version of the latest twenty original research articles published by the 3rd of July 2023, in a high-profile medical journal. Each article underwent evaluation by a minimum of three human reviewers during the initial review stage. Subsequently, three researchers with medical backgrounds and expertise in manuscript revision, independently and qualitatively assessed the agreement between the peer reviews generated by ChatGPT version GPT-4 and the comments provided by human reviewers for these articles. The level of agreement was categorized into complete, partial, none, or contradictory. Results: 720 human reviewers’ comments were assessed. There was a good agreement between the three assessors (Overall kappa >0.6). ChatGPT's comments demonstrated complete agreement in terms of quality and substance with 48 (6.7 %) human reviewers’ comments, partially agreed with 92 (12.8 %), identifying issues necessitating further elaboration or recommending supplementary steps to address concerns, had no agreement with a significant 565 (78.5 %), and contradicted 15 (2.1 %). ChatGPT comments on methods had the lowest proportion of complete agreement (13 comments, 3.6 %), while general comments on the manuscript displayed the highest proportion of complete agreement (17 comments, 22.1 %). Conclusion: ChatGPT version GPT-4 has a limited ability to emulate human reviewers within the peer review process of scientific research. © 2024 Elsevier B.V.
PB  - Elsevier Ireland Ltd
C2  - 38954915
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Bonnechère, B.
TI  - Unlocking the Black Box? A Comprehensive Exploration of Large Language Models in Rehabilitation
PY  - 2024
T2  - American Journal of Physical Medicine and Rehabilitation
VL  - 103
IS  - 6
SP  - 532
EP  - 537
DO  - 10.1097/PHM.0000000000002440
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193558113&doi=10.1097%2fPHM.0000000000002440&partnerID=40&md5=94b9493b76b13e353b957bd0b940c6dd
AB  - Rehabilitation is a vital component of health care, aiming to restore function and improve the well-being of individuals with disabilities or injuries. Nevertheless, the rehabilitation process is often likened to a "black box,"with complexities that pose challenges for comprehensive analysis and optimization. The emergence of large language models offers promising solutions to better understand this "black box."Large language models excel at comprehending and generating human-like text, making them valuable in the healthcare sector. In rehabilitation, healthcare professionals must integrate a wide range of data to create effective treatment plans, akin to selecting the best ingredients for the "black box."Large language models enhance data integration, communication, assessment, and prediction. This article delves into the ground-breaking use of large language models as a tool to further understand the rehabilitation process. Large language models address current rehabilitation issues, including data bias, contextual comprehension, and ethical concerns. Collaboration with healthcare experts and rigorous validation is crucial when deploying large language models. Integrating large language models into rehabilitation yields insights into this intricate process, enhancing data-driven decision making, refining clinical practices, and predicting rehabilitation outcomes. Although challenges persist, large language models represent a significant stride in rehabilitation, underscoring the importance of ethical use and collaboration. © Wolters Kluwer Health, Inc. All rights reserved.
PB  - Lippincott Williams and Wilkins
C2  - 38261757
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - du Crest, D.
AU  - Madhumita, M.
AU  - Enbiale, W.
AU  - Zink, A.
AU  - Papier, A.
AU  - Matewa, G.
AU  - Castro, H.
AU  - Perandones, H.
AU  - De Guzman, J.
AU  - Rosenbach, M.
AU  - Duong, T.-A.
AU  - Jack Li, Y.-C.
AU  - Cartier, H.
AU  - Ascher, B.
AU  - Garson, S.
AU  - Haddad, A.
AU  - Liu, D.Z.
AU  - Haykal, D.
AU  - Yoo, J.
AU  - Paul, N.
AU  - Cohen Sayag, T.
AU  - Hædersdal, M.
AU  - Freeman, E.
AU  - Garibyan, L.
TI  - Skin and Digital–The 2024 Narrative
PY  - 2024
T2  - Mayo Clinic Proceedings: Digital Health
VL  - 2
IS  - 3
SP  - 322
EP  - 330
DO  - 10.1016/j.mcpdig.2024.05.008
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206150897&doi=10.1016%2fj.mcpdig.2024.05.008&partnerID=40&md5=26b15d3bc5ba2c7ad7812fe79cc229df
AB  - The global burden of skin diseases affects over 3 billion individuals, posing important public health challenges worldwide, with profound impacts in both high-income and low-income and middle-income countries. These challenges are exacerbated by widespread disparities in access to dermatologic care and the prevalence of misinformation. This article, derived from the Skin and Digital Summit at the International Master Course on Aging Science critically evaluates how digital technologies such as artificial intelligence, teledermatology, and large language models can bridge these access gaps. It explores practical applications and case studies demonstrating the impact of these technologies in various settings, with a particular focus on adapting solutions to meet the diverse needs of low-income and middle-income countries. In addition, the narrative highlights the ongoing conversation within the dermatologic community about the role of digital advances in health care, emphasizing that this discussion is dynamic and the one that is continuously evolving. Dermatologists play an essential role in this transition, integrating digital tools into mainstream care to complement a patient-centered, culturally sensitive approach. The article advocates for a globally coordinated digital response that not only addresses current disparities in skin health care but also promotes equitable access to digital health resources, making dermatologic care more representative of all skin types and accessible worldwide. © 2024 The Authors
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Yang, H.
AU  - Zhu, D.
AU  - He, S.
AU  - Xu, Z.
AU  - Liu, Z.
AU  - Zhang, W.
AU  - Cai, J.
TI  - Enhancing psychiatric rehabilitation outcomes through a multimodal multitask learning model based on BERT and TabNet: An approach for personalized treatment and improved decision-making
PY  - 2024
T2  - Psychiatry Research
VL  - 336
C7  - 115896
DO  - 10.1016/j.psychres.2024.115896
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190335939&doi=10.1016%2fj.psychres.2024.115896&partnerID=40&md5=498580a18bf72e043d30e08d7a7ff5d5
AB  - Evaluating the rehabilitation status of individuals with serious mental illnesses (SMI) necessitates a comprehensive analysis of multimodal data, including unstructured text records and structured diagnostic data. However, progress in the effective assessment of rehabilitation status remains limited. Our study develops a deep learning model integrating Bidirectional Encoder Representations from Transformers (BERT) and TabNet through a late fusion strategy to enhance rehabilitation prediction, including referral risk, dangerous behaviors, self-awareness, and medication adherence, in patients with SMI. BERT processes unstructured textual data, such as doctor's notes, whereas TabNet manages structured diagnostic information. The model's interpretability function serves to assist healthcare professionals in understanding the model's predictive decisions, improving patient care. Our model exhibited excellent predictive performance for all four tasks, with an accuracy exceeding 0.78 and an area under the curve of 0.70. In addition, a series of tests proved the model's robustness, fairness, and interpretability. This study combines multimodal and multitask learning strategies into a model and applies it to rehabilitation assessment tasks, offering a promising new tool that can be seamlessly integrated with the clinical workflow to support the provision of optimized patient care. © 2024
PB  - Elsevier Ireland Ltd
C2  - 38626625
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Tripathi, S.
AU  - Gabriel, K.
AU  - Tripathi, P.K.
AU  - Kim, E.
TI  - Large language models reshaping molecular biology and drug development
PY  - 2024
T2  - Chemical Biology and Drug Design
VL  - 103
IS  - 6
C7  - e14568
DO  - 10.1111/cbdd.14568
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196318707&doi=10.1111%2fcbdd.14568&partnerID=40&md5=fd29b0035bc37ef276ddaa7df1cf6f58
AB  - The utilization of large language models (LLMs) has become a significant advancement in the domains of medicine and clinical informatics, providing a revolutionary potential for scientific breakthroughs and customized therapies. LLM models are trained on large datasets and exhibit the capacity to comprehend and analyze intricate biological data, encompassing genomic sequences, protein structures, and clinical health records. With the utilization of their comprehension of the language of biology, they possess the ability to reveal concealed patterns and insights that may evade human researchers. LLMs have been shown to positively impact various aspects of molecular biology, including the following: genomic analysis, drug development, precision medicine, biomarker development, experimental design, collaborative research, and accessibility to specialized expertise. However, it is imperative to acknowledge and tackle the obstacles and ethical implications involved. The careful consideration of data bias and generalization, data privacy and security, explainability and interpretability, and ethical concerns around responsible application is vital. The successful resolution of these obstacles will enable us to fully utilize the capabilities of LLMs, leading to substantial progress in the fields of molecular biology and pharmaceutical research. This progression also has the ability to bolster influential impacts for both the individual and the broader community. © 2024 John Wiley & Sons Ltd.
PB  - John Wiley and Sons Inc
C2  - 38898381
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Gimeno, A.
AU  - Krause, K.
AU  - D'Souza, S.
AU  - Walsh, C.G.
TI  - Completeness and readability of GPT-4-generated multilingual discharge instructions in the pediatric emergency department
PY  - 2024
T2  - JAMIA Open
VL  - 7
IS  - 3
C7  - ooae050
DO  - 10.1093/jamiaopen/ooae050
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197637815&doi=10.1093%2fjamiaopen%2fooae050&partnerID=40&md5=7a49a9e56b72a8b24aebf254521bb694
AB  - Objectives: The aim of this study was to assess the completeness and readability of generative pre-trained transformer-4 (GPT-4)-generated discharge instructions at prespecified reading levels for common pediatric emergency room complaints. Materials and Methods: The outputs for 6 discharge scenarios stratified by reading level (fifth or eighth grade) and language (English, Spanish) were generated fivefold using GPT-4. Specifically, 120 discharge instructions were produced and analyzed (6 scenarios: 60 in English, 60 in Spanish; 60 at a fifth-grade reading level, 60 at an eighth-grade reading level) and compared for completeness and readability (between language, between reading level, and stratified by group and reading level). Completeness was defined as the proportion of literature-derived key points included in discharge instructions. Readability was quantified using Flesch-Kincaid (English) and Fernandez-Huerta (Spanish) readability scores. Results: English-language GPT-generated discharge instructions contained a significantly higher proportion of must-include discharge instructions than those in Spanish (English: mean (standard error of the mean) = 62% (3%), Spanish: 53% (3%), P =. 02). In the fifth-grade and eighth-grade level conditions, there was no significant difference between English and Spanish outputs in completeness. Readability did not differ across languages. Discussion: GPT-4 produced readable discharge instructions in English and Spanish while modulating document reading level. Discharge instructions in English tended to have higher completeness than those in Spanish. Conclusion: Future research in prompt engineering and GPT-4 performance, both generally and in multiple languages, is needed to reduce potential for health disparities by language and reading level.  © 2024 The Author(s). Published by Oxford University Press on behalf of the American Medical Informatics Association.
PB  - Oxford University Press
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Volkmer, S.
AU  - Meyer-Lindenberg, A.
AU  - Schwarz, E.
TI  - Large language models in psychiatry: Opportunities and challenges
PY  - 2024
T2  - Psychiatry Research
VL  - 339
C7  - 116026
DO  - 10.1016/j.psychres.2024.116026
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196495175&doi=10.1016%2fj.psychres.2024.116026&partnerID=40&md5=9c7c8587798bc7e6ecf873bf672c0caf
AB  - The ability of Large Language Models (LLMs) to analyze and respond to freely written text is causing increasing excitement in the field of psychiatry; the application of such models presents unique opportunities and challenges for psychiatric applications. This review article seeks to offer a comprehensive overview of LLMs in psychiatry, their model architecture, potential use cases, and clinical considerations. LLM frameworks such as ChatGPT/GPT-4 are trained on huge amounts of text data that are sometimes fine-tuned for specific tasks. This opens up a wide range of possible psychiatric applications, such as accurately predicting individual patient risk factors for specific disorders, engaging in therapeutic intervention, and analyzing therapeutic material, to name a few. However, adoption in the psychiatric setting presents many challenges, including inherent limitations and biases in LLMs, concerns about explainability and privacy, and the potential damage resulting from produced misinformation. This review covers potential opportunities and limitations and highlights potential considerations when these models are applied in a real-world psychiatric context. © 2024
PB  - Elsevier Ireland Ltd
C2  - 38909412
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 9
ER  -

TY  - JOUR
AU  - Wu, S.
AU  - Yacub, Z.
AU  - Shasha, D.
TI  - DietNerd: A Nutrition Question-Answering System That Summarizes and Evaluates Peer-Reviewed Scientific Articles
PY  - 2024
T2  - Applied Sciences (Switzerland)
VL  - 14
IS  - 19
C7  - 9021
DO  - 10.3390/app14199021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206592874&doi=10.3390%2fapp14199021&partnerID=40&md5=77c8467f10fecdd045195128e45a28df
AB  - DietNerd is a large language model-based system designed to enhance public health education in diet and nutrition. The system responds to user questions with concise, evidence-based summaries and assesses the quality and potential biases of cited research. This paper describes the system’s workflow, back-end implementation, and the prompts used. Accuracy and quality-of-response results are presented based on an automated comparison against systematic surveys and against the responses of similar state-of-the-art systems through human feedback from registered dietitians. DietNerd is among the highest-evaluated of these systems and is unique in combining safety features with sophisticated source analysis. Thus, DietNerd could be a tool to bridge the gap between complex scientific literature and public understanding. © 2024 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Mistica, M.
AU  - Haylock, P.
AU  - Michalewicz, A.
AU  - Raad, S.
AU  - Fitzgerald, E.
AU  - Hitchcock, C.
TI  - A natural language model to automate scoring of autobiographical memories
PY  - 2024
T2  - Behavior Research Methods
VL  - 56
IS  - 7
SP  - 6707
EP  - 6720
DO  - 10.3758/s13428-024-02385-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191315539&doi=10.3758%2fs13428-024-02385-5&partnerID=40&md5=cbd1217c35016a0a2d381f5fcdae6ba8
AB  - Biases in the retrieval of personal, autobiographical memories are a core feature of multiple mental health disorders, and are associated with poor clinical prognosis. However, current assessments of memory bias are either reliant on human scoring, restricting their administration in clinical settings, or when computerized, are only able to identify one memory type. Here, we developed a natural language model able to classify text-based memories as one of five different autobiographical memory types (specific, categoric, extended, semantic associate, omission), allowing easy assessment of a wider range of memory biases, including reduced memory specificity and impaired memory flexibility. Our model was trained on 17,632 text-based, human-scored memories obtained from individuals with and without experience of memory bias and mental health challenges, which was then tested on a dataset of 5880 memories. We used 20-fold cross-validation setup, and the model was fine-tuned over BERT. Relative to benchmarking and an existing support vector model, our model achieved high accuracy (95.7%) and precision (91.0%). We provide an open-source version of the model which is able to be used without further coding, by those with no coding experience, to facilitate the assessment of autobiographical memory bias in clinical settings, and aid implementation of memory-based interventions within treatment services. © The Author(s) 2024.
PB  - Springer
C2  - 38664340
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Park, J.I.
AU  - Park, J.W.
AU  - Zhang, K.
AU  - Kim, D.
TI  - Advancing equity in breast cancer care: Natural language processing for analysing treatment outcomes in under-represented populations
PY  - 2024
T2  - BMJ Health and Care Informatics
VL  - 31
IS  - 1
C7  - e100966
DO  - 10.1136/bmjhci-2023-100966
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197700535&doi=10.1136%2fbmjhci-2023-100966&partnerID=40&md5=5098a1df3081e2aa42e94ad0c07488b3
AB  - Objective The study aimed to develop natural language processing (NLP) algorithms to automate extracting patient-centred breast cancer treatment outcomes from clinical notes in electronic health records (EHRs), particularly for women from under-represented populations. Methods The study used clinical notes from 2010 to 2021 from a tertiary hospital in the USA. The notes were processed through various NLP techniques, including vectorisation methods (term frequency-inverse document frequency (TF-IDF), Word2Vec, Doc2Vec) and classification models (support vector classification, K-nearest neighbours (KNN), random forest (RF)). Feature selection and optimisation through random search and fivefold cross-validation were also conducted. Results The study annotated 100 out of 1000 clinical notes, using 970 notes to build the text corpus. TF-IDF and Doc2Vec combined with RF showed the highest performance, while Word2Vec was less effective. RF classifier demonstrated the best performance, although with lower recall rates, suggesting more false negatives. KNN showed lower recall due to its sensitivity to data noise. Discussion The study highlights the significance of using NLP in analysing clinical notes to understand breast cancer treatment outcomes in under-represented populations. The TF-IDF and Doc2Vec models were more effective in capturing relevant information than Word2Vec. The study observed lower recall rates in RF models, attributed to the dataset's imbalanced nature and the complexity of clinical notes. Conclusion The study developed high-performing NLP pipeline to capture treatment outcomes for breast cancer in under-represented populations, demonstrating the importance of document-level vectorisation and ensemble methods in clinical notes analysis. The findings provide insights for more equitable healthcare strategies and show the potential for broader NLP applications in clinical settings.  © Author(s) (or their employer(s)) 2024. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.
PB  - BMJ Publishing Group
C2  - 38955389
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Askar, M.
AU  - Tafavvoghi, M.
AU  - Småbrekke, L.
AU  - Bongo, L.A.
AU  - Svendsen, K.
TI  - Using machine learning methods to predict all-cause somatic hospitalizations in adults: A systematic review
PY  - 2024
T2  - PLoS ONE
VL  - 19
IS  - 8
C7  - e0309175
DO  - 10.1371/journal.pone.0309175
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201888868&doi=10.1371%2fjournal.pone.0309175&partnerID=40&md5=ad54b06157420198254fb36cfcb9d033
AB  - Aim In this review, we investigated how Machine Learning (ML) was utilized to predict all-cause somatic hospital admissions and readmissions in adults. Methods We searched eight databases (PubMed, Embase, Web of Science, CINAHL, ProQuest, OpenGrey, WorldCat, and MedNar) from their inception date to October 2023, and included records that predicted all-cause somatic hospital admissions and readmissions of adults using ML methodology. We used the CHARMS checklist for data extraction, PROBAST for bias and applicability assessment, and TRIPOD for reporting quality. Results We screened 7,543 studies of which 163 full-text records were read and 116 met the review inclusion criteria. Among these, 45 predicted admission, 70 predicted readmission, and one study predicted both. There was a substantial variety in the types of datasets, algorithms, features, data preprocessing steps, evaluation, and validation methods. The most used types of features were demographics, diagnoses, vital signs, and laboratory tests. Area Under the ROC curve (AUC) was the most used evaluation metric. Models trained using boosting tree-based algorithms often performed better compared to others. ML algorithms commonly outperformed traditional regression techniques. Sixteen studies used Natural language processing (NLP) of clinical notes for prediction, all studies yielded good results. The overall adherence to reporting quality was poor in the review studies. Only five percent of models were implemented in clinical practice. The most frequently inadequately addressed methodological aspects were: providing model interpretations on the individual patient level, full code availability, performing external validation, calibrating models, and handling class imbalance. Conclusion This review has identified considerable concerns regarding methodological issues and reporting quality in studies investigating ML to predict hospitalizations. To ensure the acceptability of these models in clinical settings, it is crucial to improve the quality of future studies. © 2024 Askar et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
PB  - Public Library of Science
C2  - 39178283
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Fiorillo, L.
AU  - Mehta, V.
TI  - Accelerating editorial processes in scientific journals: Leveraging AI for rapid manuscript review
PY  - 2024
T2  - Oral Oncology Reports
VL  - 10
C7  - 100511
DO  - 10.1016/j.oor.2024.100511
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194000117&doi=10.1016%2fj.oor.2024.100511&partnerID=40&md5=40bf8d3556a2cd2fb79cebf58728de9f
AB  - Introduction: Artificial intelligence (AI) integration in business is increasingly recognized for enhancing productivity and efficiency. AI automates routine tasks and accelerates decision-making processes, optimizing operations and data analysis. This paper discusses the emergence of AI chat assistants in various sectors, focusing on models like GPT-3, Watson Assistant, Dialogflow, Alexa, Siri, Cortana, and Google Bard. These AI chat assistants exemplify the transformative impact of AI technologies in everyday life. Materials and methods: The study explores AI's role in chatbot development, examining how AI algorithms and machine learning facilitate human-like conversations in diverse industries. The method includes a detailed analysis of AI-powered chatbots' customer service, healthcare, and e-commerce applications. The manuscript also introduces an AI-assisted peer review methodology, employing ChatGPT to analyze and comment on scientific manuscripts, aiming to enhance the efficiency and quality of the academic review process. Results: The AI-assisted review of selected manuscripts demonstrates the potential of ChatGPT in providing insightful feedback and suggestions. The study showcases how AI can contribute to improved manuscript quality, increased efficiency in review processes, and enhanced accessibility. The results highlight the importance of addressing ethical considerations, including privacy and bias, in deploying AI technologies. Conclusions: This research illustrates the significant role of AI in revolutionizing business processes and academic practices. AI chat assistants and AI-assisted peer review methodologies represent pivotal advancements in their respective fields. The study underscores the need for continuous evaluation of the ethical implications of AI technologies to ensure their responsible and equitable use. © 2024 The Authors
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Ayoub, N.F.
AU  - Balakrishnan, K.
AU  - Ayoub, M.S.
AU  - Barrett, T.F.
AU  - David, A.P.
AU  - Gray, S.T.
TI  - Inherent Bias in Large Language Models: A Random Sampling Analysis
PY  - 2024
T2  - Mayo Clinic Proceedings: Digital Health
VL  - 2
IS  - 2
SP  - 186
EP  - 191
DO  - 10.1016/j.mcpdig.2024.03.003
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198118718&doi=10.1016%2fj.mcpdig.2024.03.003&partnerID=40&md5=91495dcdb55deacf718bf53e9b08e2c8
AB  - There are mounting concerns regarding inherent bias, safety, and tendency toward misinformation of large language models (LLMs), which could have significant implications in health care. This study sought to determine whether generative artificial intelligence (AI)-based simulations of physicians making life-and-death decisions in a resource-scarce environment would demonstrate bias. Thirteen questions were developed that simulated physicians treating patients in resource-limited environments. Through a random sampling of simulated physicians using OpenAI's generative pretrained transformer (GPT-4), physicians were tasked with choosing only 1 patient to save owing to limited resources. This simulation was repeated 1000 times per question, representing 1000 unique physicians and patients each. Patients and physicians spanned a variety of demographic characteristics. All patients had similar a priori likelihood of surviving the acute illness. Overall, simulated physicians consistently demonstrated racial, gender, age, political affiliation, and sexual orientation bias in clinical decision-making. Across all demographic characteristics, physicians most frequently favored patients with similar demographic characteristics as themselves, with most pairwise comparisons showing statistical significance (P<.05). Nondescript physicians favored White, male, and young demographic characteristics. The male doctor gravitated toward the male, White, and young, whereas the female doctor typically preferred female, young, and White patients. In addition to saving patients with their own political affiliation, Democratic physicians favored Black and female patients, whereas Republicans preferred White and male demographic characteristics. Heterosexual and gay/lesbian physicians frequently saved patients of similar sexual orientation. Overall, publicly available chatbot LLMs demonstrate significant biases, which may negatively impact patient outcomes if used to support clinical care decisions without appropriate precautions. © 2024 The Authors
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 9
ER  -

TY  - JOUR
AU  - Wu, W.Y.
AU  - Luke, B.
AU  - Wu, X.-C.
AU  - Lee, J.J.
AU  - Yi, Y.
AU  - Okpechi, S.C.
AU  - Gause, B.
AU  - Mehta, P.
AU  - Sherman, S.I.
AU  - Ochoa, A.
AU  - Dmitrovsky, E.
AU  - Liu, X.
TI  - Glycemic control in diabetic patients improved overall lung cancer survival across diverse populations
PY  - 2024
T2  - JNCI Cancer Spectrum
VL  - 8
IS  - 5
C7  - pkae081
DO  - 10.1093/jncics/pkae081
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206915193&doi=10.1093%2fjncics%2fpkae081&partnerID=40&md5=98f82394f1f95976fd0b9f534c47b796
AB  - Background: The consequence of diabetes on lung cancer overall survival (OS) is debated. This retrospective study used 2 large lung cancer databases to assess comprehensively diabetes effects on lung cancer OS in diverse demographic populations, including health disparity. Methods: The University of Texas MD Anderson Cancer Center database (32 643 lung cancer patients with 11 973 patients with diabetes) was extracted from electronic health records (EHRs) using natural language processing (NLP). Associations were between diabetes and lung cancer prognostic features (age, sex, race, body mass index [BMI], insurance status, smoking, stage, and histopathology). Hemoglobin A1C (HgbA1c) and glucose levels assessed glycemic control. Validation was with a Louisiana cohort (17 768 lung cancer patients with 5402 patients with diabetes) enriched for health disparity cases. Kaplan-Meier analysis, log-rank test, multivariable Cox proportional hazard models, and survival tree analyses were employed. Results: Lung cancer patients with diabetes exhibited marginally elevated OS or no statistically significant difference versus nondiabetic patients. When examining OS for 2 glycemic levels (HgbA1c > 7.0 or glucose > 154 mg/dL vs HgbA1c > 9.0 or glucose > 215 mg/dL), a statistically significant improvement in OS occurred in lung cancer patients with controlled versus uncontrolled glycemia (P < .0001). This improvement spanned sex, age, smoking status, insurance status, stage, race, BMI, histopathology, and therapy. Survival tree analysis revealed that obese and morbidly obese patients with controlled glycemia had higher lung cancer OS than comparison groups. Conclusion: These findings indicate a need for optimal glycemic control to improve lung cancer OS in diverse populations with diabetes. © 2024 Published by Oxford University Press.
PB  - Oxford University Press
C2  - 39270065
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Nan, Q.
AU  - Sheng, Q.
AU  - Cao, J.
AU  - Hu, B.
AU  - Wang, D.
AU  - Li, J.
TI  - Let Silence Speak: Enhancing Fake News Detection with Generated Comments from Large Language Models
PY  - 2024
T2  - International Conference on Information and Knowledge Management, Proceedings
SP  - 1732
EP  - 1742
DO  - 10.1145/3627673.3679519
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210035981&doi=10.1145%2f3627673.3679519&partnerID=40&md5=7b0f8da17c365339496e43f4e95a5941
AB  - Fake news detection plays a crucial role in protecting social media users and maintaining a healthy news ecosystem. Among existing works, comment-based fake news detection methods are empirically shown as promising because comments could reflect users' opinions, stances, and emotions and deepen models' understanding of fake news. Unfortunately, due to exposure bias and users' different willingness to comment, it is not easy to obtain diverse comments in reality, especially for early detection scenarios. Without obtaining the comments from the "silent'' users, the perceived opinions may be incomplete, subsequently affecting news veracity judgment. In this paper, we explore the possibility of finding an alternative source of comments to guarantee the availability of diverse comments, especially those from silent users. Specifically, we propose to adopt large language models (LLMs) as a user simulator and comment generator, and design GenFEND, a generated feedback-enhanced detection framework, which generates comments by prompting LLMs with diverse user profiles and aggregating generated comments from multiple subpopulation groups. Experiments demonstrate the effectiveness of GenFEND and further analysis shows that the generated comments cover more diverse users and could even be more effective than actual comments.  © 2024 Owner/Author.
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Franklin, G.
AU  - Stephens, R.
AU  - Piracha, M.
AU  - Tiosano, S.
AU  - Lehouillier, F.
AU  - Koppel, R.
AU  - Elkin, P.L.
TI  - The Sociodemographic Biases in Machine Learning Algorithms: A Biomedical Informatics Perspective
PY  - 2024
T2  - Life
VL  - 14
IS  - 6
C7  - 652
DO  - 10.3390/life14060652
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197252425&doi=10.3390%2flife14060652&partnerID=40&md5=465213d9a406abba1a91aa9ece86fb9a
AB  - Artificial intelligence models represented in machine learning algorithms are promising tools for risk assessment used to guide clinical and other health care decisions. Machine learning algorithms, however, may house biases that propagate stereotypes, inequities, and discrimination that contribute to socioeconomic health care disparities. The biases include those related to some sociodemographic characteristics such as race, ethnicity, gender, age, insurance, and socioeconomic status from the use of erroneous electronic health record data. Additionally, there is concern that training data and algorithmic biases in large language models pose potential drawbacks. These biases affect the lives and livelihoods of a significant percentage of the population in the United States and globally. The social and economic consequences of the associated backlash cannot be underestimated. Here, we outline some of the sociodemographic, training data, and algorithmic biases that undermine sound health care risk assessment and medical decision-making that should be addressed in the health care system. We present a perspective and overview of these biases by gender, race, ethnicity, age, historically marginalized communities, algorithmic bias, biased evaluations, implicit bias, selection/sampling bias, socioeconomic status biases, biased data distributions, cultural biases and insurance status bias, conformation bias, information bias and anchoring biases and make recommendations to improve large language model training data, including de-biasing techniques such as counterfactual role-reversed sentences during knowledge distillation, fine-tuning, prefix attachment at training time, the use of toxicity classifiers, retrieval augmented generation and algorithmic modification to mitigate the biases moving forward. © 2024 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Sallam, M.
TI  - Bibliometric top ten healthcare-related ChatGPT publications in the first ChatGPT anniversary
PY  - 2024
T2  - Narra J
VL  - 4
IS  - 2
C7  - e917
DO  - 10.52225/narra.v4i2.917
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201063447&doi=10.52225%2fnarra.v4i2.917&partnerID=40&md5=aac5aed95ea9786c824c6628254656a9
AB  - Since its public release on November 30, 2022, ChatGPT has shown promising potential in diverse healthcare applications despite ethical challenges, privacy issues, and possible biases. The aim of this study was to identify and assess the most influential publications in the field of ChatGPT utility in healthcare using bibliometric analysis. The study employed an advanced search on three databases, Scopus, Web of Science, and Google Scholar, to identify ChatGPT-related records in healthcare education, research, and practice between November 27 and 30, 2023. The ranking was based on the retrieved citation count in each database. The additional alternative metrics that were evaluated included (1) Semantic Scholar highly influential citations, (2) PlumX captures, (3) PlumX mentions, (4) PlumX social media and (5) Altmetric Attention Scores (AASs). A total of 22 unique records published in 17 different scientific journals from 14 different publishers were identified in the three databases. Only two publications were in the top 10 list across the three databases. Variable publication types were identified, with the most common being editorial/commentary publications (n=8/22, 36.4%). Nine of the 22 records had corresponding authors affiliated with institutions in the United States (40.9%). The range of citation count varied per database, with the highest range identified in Google Scholar (1019–121), followed by Scopus (242–88), and Web of Science (171–23). Google Scholar citations were correlated significantly with the following metrics: Semantic Scholar highly influential citations (Spearman’s correlation coefficient ρ=0.840, p<0.001), PlumX captures (ρ=0.831, p<0.001), PlumX mentions (ρ=0.609, p=0.004), and AASs (ρ=0.542, p=0.009). In conclusion, despite several acknowledged limitations, this study showed the evolving landscape of ChatGPT utility in healthcare. There is an urgent need for collaborative initiatives by all stakeholders involved to establish guidelines for ethical, transparent, and responsible use of ChatGPT in healthcare. The study revealed the correlation between citations and alternative metrics, highlighting its usefulness as a supplement to gauge the impact of publications, even in a rapidly growing research field. © 2024 by the authors.
PB  - Narra Sains Indonesia
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Luo, X.
AU  - Tahabi, F.M.
AU  - Marc, T.
AU  - Haunert, L.A.
AU  - Storey, S.
TI  - Zero-shot learning to extract assessment criteria and medical services from the preventive healthcare guidelines using large language models
PY  - 2024
T2  - Journal of the American Medical Informatics Association
VL  - 31
IS  - 8
SP  - 1743
EP  - 1753
DO  - 10.1093/jamia/ocae145
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199132329&doi=10.1093%2fjamia%2focae145&partnerID=40&md5=6db843a2e359b14728b0faba1c1d4219
AB  - Objectives: The integration of these preventive guidelines with Electronic Health Records (EHRs) systems, coupled with the generation of personalized preventive care recommendations, holds significant potential for improving healthcare outcomes. Our study investigates the feasibility of using Large Language Models (LLMs) to automate the assessment criteria and risk factors from the guidelines for future analysis against medical records in EHR. Materials and Methods: We annotated the criteria, risk factors, and preventive medical services described in the adult guidelines published by United States Preventive Services Taskforce and evaluated 3 state-of-the-art LLMs on extracting information in these categories from the guidelines automatically. Results: We included 24 guidelines in this study. The LLMs can automate the extraction of all criteria, risk factors, and medical services from 9 guidelines. All 3 LLMs perform well on extracting information regarding the demographic criteria or risk factors. Some LLMs perform better on extracting the social determinants of health, family history, and preventive counseling services than the others. Discussion: While LLMs demonstrate the capability to handle lengthy preventive care guidelines, several challenges persist, including constraints related to the maximum length of input tokens and the tendency to generate content rather than adhering strictly to the original input. Moreover, the utilization of LLMs in real-world clinical settings necessitates careful ethical consideration. It is imperative that healthcare professionals meticulously validate the extracted information to mitigate biases, ensure completeness, and maintain accuracy. Conclusion: We developed a data structure to store the annotated preventive guidelines and make it publicly available. Employing state-of-the-art LLMs to extract preventive care criteria, risk factors, and preventive care services paves the way for the future integration of these guidelines into the EHR.  © The Author(s) 2024.
PB  - Oxford University Press
C2  - 38900185
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Small, W.R.
AU  - Wiesenfeld, B.
AU  - Brandfield-Harvey, B.
AU  - Jonassen, Z.
AU  - Mandal, S.
AU  - Stevens, E.R.
AU  - Major, V.J.
AU  - Lostraglio, E.
AU  - Szerencsy, A.
AU  - Jones, S.
AU  - Aphinyanaphongs, Y.
AU  - Johnson, S.B.
AU  - Nov, O.
AU  - Mann, D.
TI  - Large Language Model–Based Responses to Patients’ In-Basket Messages
PY  - 2024
T2  - JAMA Network Open
VL  - 7
IS  - 7
C7  - e2422399
DO  - 10.1001/jamanetworkopen.2024.22399
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198946918&doi=10.1001%2fjamanetworkopen.2024.22399&partnerID=40&md5=011f127d9c9bb04a65544d1f731a2e5c
AB  - IMPORTANCE Virtual patient-physician communications have increased since 2020 and negatively impacted primary care physician (PCP) well-being. Generative artificial intelligence (GenAI) drafts of patient messages could potentially reduce health care professional (HCP) workload and improve communication quality, but only if the drafts are considered useful. OBJECTIVES To assess PCPs’ perceptions of GenAI drafts and to examine linguistic characteristics associated with equity and perceived empathy. DESIGN, SETTING, AND PARTICIPANTS This cross-sectional quality improvement study tested the hypothesis that PCPs’ ratings of GenAI drafts (created using the electronic health record [EHR] standard prompts) would be equivalent to HCP-generated responses on 3 dimensions. The study was conducted at NYU Langone Health using private patient-HCP communications at 3 internal medicine practices piloting GenAI. EXPOSURES Randomly assigned patient messages coupled with either an HCP message or the draft GenAI response. MAIN OUTCOMES AND MEASURES PCPs rated responses’ information content quality (eg, relevance), using a Likert scale, communication quality (eg, verbosity), using a Likert scale, and whether they would use the draft or start anew (usable vs unusable). Branching logic further probed for empathy, personalization, and professionalism of responses. Computational linguistics methods assessed content differences in HCP vs GenAI responses, focusing on equity and empathy. RESULTS A total of 16 PCPs (8 [50.0%] female) reviewed 344 messages (175 GenAI drafted; 169 HCP drafted). Both GenAI and HCP responses were rated favorably. GenAI responses were rated higher for communication style than HCP responses (mean [SD], 3.70 [1.15] vs 3.38 [1.20]; P = .01, U = 12 568.5) but were similar to HCPs on information content (mean [SD], 3.53 [1.26] vs 3.41 [1.27]; P = .37; U = 13 981.0) and usable draft proportion (mean [SD], 0.69 [0.48] vs 0.65 [0.47], P = .49, t = −0.6842). Usable GenAI responses were considered more empathetic than usable HCP responses (32 of 86 [37.2%] vs 13 of 79 [16.5%]; difference, 125.5%), possibly attributable to more subjective (mean [SD], 0.54 [0.16] vs 0.31 [0.23]; P < .001; difference, 74.2%) and positive (mean [SD] polarity, 0.21 [0.14] vs 0.13 [0.25]; P = .02; difference, 61.5%) language; they were also numerically longer (mean [SD] word count, 90.5 [32.0] vs 65.4 [62.6]; difference, 38.4%), but the difference was not statistically significant (P = .07) and more linguistically complex (mean [SD] score, 125.2 [47.8] vs 95.4 [58.8]; P = .002; difference, 31.2%). CONCLUSIONS In this cross-sectional study of PCP perceptions of an EHR-integrated GenAI chatbot, GenAI was found to communicate information better and with more empathy than HCPs, highlighting its potential to enhance patient-HCP communication. However, GenAI drafts were less readable than HCPs’, a significant concern for patients with low health or English literacy. © 2024 American Medical Association. All rights reserved.
PB  - American Medical Association
C2  - 39012633
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - CONF
AU  - Carta, S.
AU  - Giuliani, A.
AU  - Manca, M.M.
AU  - Piano, L.
AU  - Tiddia, S.G.
TI  - Towards Zero-shot Knowledge Graph building: Automated Schema Inference
PY  - 2024
T2  - UMAP 2024 - Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization
SP  - 467
EP  - 473
DO  - 10.1145/3631700.3665234
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198932122&doi=10.1145%2f3631700.3665234&partnerID=40&md5=14fe30f829817c642115f9811b1ed732
AB  - In the current Digital Transformation scenario, Knowledge Graphs are essential for comprehending, representing, and exploiting complex information in a structured form. The main paradigm in automatically generating proper Knowledge Graphs relies on predefined schemas or ontologies. Such schemas are typically manually constructed, requiring an intensive human effort, and are often sensitive to information loss due to negligence, incomplete analysis, or human subjectivity or inclination. Limiting human bias and the resulting information loss in creating proper Knowledge Graphs is paramount, particularly for user modeling in various sectors, such as education or healthcare. To this end, we propose a novel approach to automatically generating a proper entity schema. The devised methodology combines the language understanding capabilities of LLM with classical machine learning methods such as clustering to properly build an entity schema from a set of documents. This solution eliminates the need for human intervention and fosters a more efficient and comprehensive knowledge representation. The assessment of our proposal concerns adopting a state-of-the-art entity extraction model (UniNER) to estimate the relevance of the extracted entities based on the generated schema. Results confirm the potential of our approach, as we observed a negligible difference between the topic similarity score obtained with the ground truth and with the automatically generated schema (less than 1% on average on three different datasets). Such an outcome confirms that the proposed approach may be valuable in automatically creating an entity schema from a set of documents. © 2024 Owner/Author.
PB  - Association for Computing Machinery, Inc
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Kidwai-Khan, F.
AU  - Wang, R.
AU  - Skanderson, M.
AU  - Brandt, C.A.
AU  - Fodeh, S.
AU  - Womack, J.A.
TI  - A roadmap to artificial intelligence (AI): Methods for designing and building AI ready data to promote fairness
PY  - 2024
T2  - Journal of Biomedical Informatics
VL  - 154
C7  - 104654
DO  - 10.1016/j.jbi.2024.104654
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193075830&doi=10.1016%2fj.jbi.2024.104654&partnerID=40&md5=1246200b5e8d539e4292ad78c9023bfb
AB  - Objectives: We evaluated methods for preparing electronic health record data to reduce bias before applying artificial intelligence (AI). Methods: We created methods for transforming raw data into a data framework for applying machine learning and natural language processing techniques for predicting falls and fractures. Strategies such as inclusion and reporting for multiple races, mixed data sources such as outpatient, inpatient, structured codes, and unstructured notes, and addressing missingness were applied to raw data to promote a reduction in bias. The raw data was carefully curated using validated definitions to create data variables such as age, race, gender, and healthcare utilization. For the formation of these variables, clinical, statistical, and data expertise were used. The research team included a variety of experts with diverse professional and demographic backgrounds to include diverse perspectives. Results: For the prediction of falls, information extracted from radiology reports was converted to a matrix for applying machine learning. The processing of the data resulted in an input of 5,377,673 reports to the machine learning algorithm, out of which 45,304 were flagged as positive and 5,332,369 as negative for falls. Processed data resulted in lower missingness and a better representation of race and diagnosis codes. For fractures, specialized algorithms extracted snippets of text around keywork “femoral” from dual x-ray absorptiometry (DXA) scans to identify femoral neck T-scores that are important for predicting fracture risk. The natural language processing algorithms yielded 98% accuracy and 2% error rate The methods to prepare data for input to artificial intelligence processes are reproducible and can be applied to other studies. Conclusion: The life cycle of data from raw to analytic form includes data governance, cleaning, management, and analysis. When applying artificial intelligence methods, input data must be prepared optimally to reduce algorithmic bias, as biased output is harmful. Building AI-ready data frameworks that improve efficiency can contribute to transparency and reproducibility. The roadmap for the application of AI involves applying specialized techniques to input data, some of which are suggested here. This study highlights data curation aspects to be considered when preparing data for the application of artificial intelligence to reduce bias. © 2024 Elsevier Inc.
PB  - Academic Press Inc.
C2  - 38740316
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Van Buchem, M.M.
AU  - De Hond, A.A.H.
AU  - Fanconi, C.
AU  - Shah, V.
AU  - Schuessler, M.
AU  - Kant, I.M.J.
AU  - Steyerberg, E.W.
AU  - Hernandez-Boussard, T.
TI  - Applying natural language processing to patient messages to identify depression concerns in cancer patients
PY  - 2024
T2  - Journal of the American Medical Informatics Association
VL  - 31
IS  - 10
SP  - 2255
EP  - 2262
DO  - 10.1093/jamia/ocae188
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204659671&doi=10.1093%2fjamia%2focae188&partnerID=40&md5=c36849879a9ac114403477dcd8e462ac
AB  - Objective: This study aims to explore and develop tools for early identification of depression concerns among cancer patients by leveraging the novel data source of messages sent through a secure patient portal. Materials and Methods: We developed classifiers based on logistic regression (LR), support vector machines (SVMs), and 2 Bidirectional Encoder Representations from Transformers (BERT) models (original and Reddit-pretrained) on 6600 patient messages from a cancer center (2009-2022), annotated by a panel of healthcare professionals. Performance was compared using AUROC scores, and model fairness and explainability were examined. We also examined correlations between model predictions and depression diagnosis and treatment. Results: BERT and RedditBERT attained AUROC scores of 0.88 and 0.86, respectively, compared to 0.79 for LR and 0.83 for SVM. BERT showed bigger differences in performance across sex, race, and ethnicity than RedditBERT. Patients who sent messages classified as concerning had a higher chance of receiving a depression diagnosis, a prescription for antidepressants, or a referral to the psycho-oncologist. Explanations from BERT and RedditBERT differed, with no clear preference from annotators. Discussion: We show the potential of BERT and RedditBERT in identifying depression concerns in messages from cancer patients. Performance disparities across demographic groups highlight the need for careful consideration of potential biases. Further research is needed to address biases, evaluate real-world impacts, and ensure responsible integration into clinical settings. Conclusion: This work represents a significant methodological advancement in the early identification of depression concerns among cancer patients. Our work contributes to a route to reduce clinical burden while enhancing overall patient care, leveraging BERT-based models.  © 2024 The Author(s). Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved.
PB  - Oxford University Press
C2  - 39018490
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Crawford, L.M.
AU  - Hendzlik, P.
AU  - Lam, J.
AU  - Cannon, L.M.
AU  - Qi, Y.
AU  - DeCaporale-Ryan, L.
AU  - Wilson, N.A.
TI  - Digital Ink and Surgical Dreams: Perceptions of Artificial Intelligence–Generated Essays in Residency Applications
PY  - 2024
T2  - Journal of Surgical Research
VL  - 301
SP  - 504
EP  - 511
DO  - 10.1016/j.jss.2024.06.020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199159464&doi=10.1016%2fj.jss.2024.06.020&partnerID=40&md5=6352b864e6e8f83c22324224ad2642bb
AB  - Introduction: Large language models like Chat Generative Pre-Trained Transformer (ChatGPT) are increasingly used in academic writing. Faculty may consider use of artificial intelligence (AI)–generated responses a form of cheating. We sought to determine whether general surgery residency faculty could detect AI versus human-written responses to a text prompt; hypothesizing that faculty would not be able to reliably differentiate AI versus human-written responses. Methods: Ten essays were generated using a text prompt, “Tell us in 1-2 paragraphs why you are considering the University of Rochester for General Surgery residency” (Current trainees: n = 5, ChatGPT: n = 5). Ten blinded faculty reviewers rated essays (ten-point Likert scale) on the following criteria: desire to interview, relevance to the general surgery residency, overall impression, and AI- or human-generated; with scores and identification error rates compared between the groups. Results: There were no differences between groups for %total points (ChatGPT 66.0 ± 13.5%, human 70.0 ± 23.0%, P = 0.508) or identification error rates (ChatGPT 40.0 ± 35.0%, human 20.0 ± 30.0%, P = 0.175). Except for one, all essays were identified incorrectly by at least two reviewers. Essays identified as human-generated received higher overall impression scores (area under the curve: 0.82 ± 0.04, P < 0.01). Conclusions: Whether use of AI tools for academic purposes should constitute academic dishonesty is controversial. We demonstrate that human and AI-generated essays are similar in quality, but there is bias against presumed AI-generated essays. Faculty are not able to reliably differentiate human from AI-generated essays, thus bias may be misdirected. AI-tools are becoming ubiquitous and their use is not easily detected. Faculty must expect these tools to play increasing roles in medical education. © 2024 Elsevier Inc.
PB  - Academic Press Inc.
C2  - 39042979
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Voinea, Ș.-V.
AU  - Mămuleanu, M.
AU  - Teică, R.V.
AU  - Florescu, L.M.
AU  - Selișteanu, D.
AU  - Gheonea, I.A.
TI  - GPT-Driven Radiology Report Generation with Fine-Tuned Llama 3
PY  - 2024
T2  - Bioengineering
VL  - 11
IS  - 10
C7  - 1043
DO  - 10.3390/bioengineering11101043
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207676347&doi=10.3390%2fbioengineering11101043&partnerID=40&md5=1334b7f6364112b50ad76a14d5414a2f
AB  - The integration of deep learning into radiology has the potential to enhance diagnostic processes, yet its acceptance in clinical practice remains limited due to various challenges. This study aimed to develop and evaluate a fine-tuned large language model (LLM), based on Llama 3-8B, to automate the generation of accurate and concise conclusions in magnetic resonance imaging (MRI) and computed tomography (CT) radiology reports, thereby assisting radiologists and improving reporting efficiency. A dataset comprising 15,000 radiology reports was collected from the University of Medicine and Pharmacy of Craiova’s Imaging Center, covering a diverse range of MRI and CT examinations made by four experienced radiologists. The Llama 3-8B model was fine-tuned using transfer-learning techniques, incorporating parameter quantization to 4-bit precision and low-rank adaptation (LoRA) with a rank of 16 to optimize computational efficiency on consumer-grade GPUs. The model was trained over five epochs using an NVIDIA RTX 3090 GPU, with intermediary checkpoints saved for monitoring. Performance was evaluated quantitatively using Bidirectional Encoder Representations from Transformers Score (BERTScore), Recall-Oriented Understudy for Gisting Evaluation (ROUGE), Bilingual Evaluation Understudy (BLEU), and Metric for Evaluation of Translation with Explicit Ordering (METEOR) metrics on a held-out test set. Additionally, a qualitative assessment was conducted, involving 13 independent radiologists who participated in a Turing-like test and provided ratings for the AI-generated conclusions. The fine-tuned model demonstrated strong quantitative performance, achieving a BERTScore F1 of 0.8054, a ROUGE-1 F1 of 0.4998, a ROUGE-L F1 of 0.4628, and a METEOR score of 0.4282. In the human evaluation, the artificial intelligence (AI)-generated conclusions were preferred over human-written ones in approximately 21.8% of cases, indicating that the model’s outputs were competitive with those of experienced radiologists. The average rating of the AI-generated conclusions was 3.65 out of 5, reflecting a generally favorable assessment. Notably, the model maintained its consistency across various types of reports and demonstrated the ability to generalize to unseen data. The fine-tuned Llama 3-8B model effectively generates accurate and coherent conclusions for MRI and CT radiology reports. By automating the conclusion-writing process, this approach can assist radiologists in reducing their workload and enhancing report consistency, potentially addressing some barriers to the adoption of deep learning in clinical practice. The positive evaluations from independent radiologists underscore the model’s potential utility. While the model demonstrated strong performance, limitations such as dataset bias, limited sample diversity, a lack of clinical judgment, and the need for large computational resources require further refinement and real-world validation. Future work should explore the integration of such models into clinical workflows, address ethical and legal considerations, and extend this approach to generate complete radiology reports. © 2024 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Kim, J.
AU  - Wang, K.
AU  - Weng, C.
AU  - Liu, C.
TI  - Assessing the utility of large language models for phenotype-driven gene prioritization in the diagnosis of rare genetic disease
PY  - 2024
T2  - American Journal of Human Genetics
VL  - 111
IS  - 10
SP  - 2190
EP  - 2202
DO  - 10.1016/j.ajhg.2024.08.010
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206019380&doi=10.1016%2fj.ajhg.2024.08.010&partnerID=40&md5=cf928c0d61985cc2dfdeb99950724960
AB  - Phenotype-driven gene prioritization is fundamental to diagnosing rare genetic disorders. While traditional approaches rely on curated knowledge graphs with phenotype-gene relations, recent advancements in large language models (LLMs) promise a streamlined text-to-gene solution. In this study, we evaluated five LLMs, including two generative pre-trained transformers (GPT) series and three Llama2 series, assessing their performance across task completeness, gene prediction accuracy, and adherence to required output structures. We conducted experiments, exploring various combinations of models, prompts, phenotypic input types, and task difficulty levels. Our findings revealed that the best-performed LLM, GPT-4, achieved an average accuracy of 17.0% in identifying diagnosed genes within the top 50 predictions, which still falls behind traditional tools. However, accuracy increased with the model size. Consistent results were observed over time, as shown in the dataset curated after 2023. Advanced techniques such as retrieval-augmented generation (RAG) and few-shot learning did not improve the accuracy. Sophisticated prompts were more likely to enhance task completeness, especially in smaller models. Conversely, complicated prompts tended to decrease output structure compliance rate. LLMs also achieved better-than-random prediction accuracy with free-text input, though performance was slightly lower than with standardized concept input. Bias analysis showed that highly cited genes, such as BRCA1, TP53, and PTEN, are more likely to be predicted. Our study provides valuable insights into integrating LLMs with genomic analysis, contributing to the ongoing discussion on their utilization in clinical workflows. © 2024 The Authors
PB  - Cell Press
C2  - 39255797
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Pothugunta, K.
AU  - Liu, X.
AU  - Susarla, A.
AU  - Padman, R.
TI  - Assessing inclusion and representativeness on digital platforms for health education: Evidence from YouTube
PY  - 2024
T2  - Journal of Biomedical Informatics
VL  - 157
C7  - 104669
DO  - 10.1016/j.jbi.2024.104669
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199073043&doi=10.1016%2fj.jbi.2024.104669&partnerID=40&md5=3f3087eb3afca41ddcf02f2ef627a1f1
AB  - Background: Studies confirm that significant biases exist in online recommendation platforms, exacerbating pre-existing disparities and leading to less-than-optimal outcomes for underrepresented demographics. We study issues of bias in inclusion and representativeness in the context of healthcare information disseminated via videos on the YouTube social media platform, a widely used online channel for multi-media rich information. With one in three US adults using the Internet to learn about a health concern, it is critical to assess inclusivity and representativeness regarding how health information is disseminated by digital platforms such as YouTube. Methods: Leveraging methods from fair machine learning (ML), natural language processing and voice and facial recognition methods, we examine inclusivity and representativeness of video content presenters using a large corpus of videos and their metadata on a chronic condition (diabetes) extracted from the YouTube platform. Regression models are used to determine whether presenter demographics impact video popularity, measured by the video's average daily view count. A video that generates a higher view count is considered to be more popular. Results: The voice and facial recognition methods predicted the gender and race of the presenter with reasonable success. Gender is predicted through voice recognition (accuracy = 78%, AUC = 76%), while the gender and race predictions use facial recognition (accuracy = 93%, AUC = 92% and accuracy = 82%, AUC = 80%, respectively). The gender of the presenter is more significant for video views only when the face of the presenter is not visible while videos with male presenters with no face visibility have a positive relationship with view counts. Furthermore, videos with white and male presenters have a positive influence on view counts while videos with female and non − white group have high view counts. Conclusion: Presenters’ demographics do have an influence on average daily view count of videos viewed on social media platforms as shown by advanced voice and facial recognition algorithms used for assessing inclusion and representativeness of the video content. Future research can explore short videos and those at the channel level because popularity of the channel name and the number of videos associated with that channel do have an influence on view counts. © 2024 The Author(s)
PB  - Academic Press Inc.
C2  - 38880237
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Liu, M.
AU  - Okuhara, T.
AU  - Chang, X.
AU  - Okada, H.
AU  - Kiuchi, T.
TI  - Performance of ChatGPT in medical licensing examinations in countries worldwide: A systematic review and meta-analysis protocol
PY  - 2024
T2  - PLoS ONE
VL  - 19
IS  - 10
C7  - e0312771
DO  - 10.1371/journal.pone.0312771
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207438216&doi=10.1371%2fjournal.pone.0312771&partnerID=40&md5=d611f68b4c49f00348cb9c6d0a45e0cb
AB  - Introduction In November 2022, the online artificial intelligence (AI) chatbot ChatGPT was released to the public, and swiftly garnered global attention because of its ability to provide detailed answers to complex queries. In medical field, ChatGPT has shown great potential to be used in medical education and has excelled in many English-language medical licensing examinations. However, due to the variability of medical licensing examinations in different countries, and ChatGPT’s particular proficiency in English, the previous literatures showed that ChatGPT is unable to pass medical licensing examinations from non-English-speaking countries or those not administered in English. To the best of our knowledge, this is the first study to review whether ChatGPT can demonstrate consistent accuracy across diverse medical licensing examinations and be used in medical education across countries. Objective In this study protocol, we aimed to analyze and review the differences in performance of ChatGPT in medical exams in various language environments and countries, as well as its potential in medical education. Methods and analysis A systematic review and meta-analysis was conducted using PubMed, Web of Science, and Scopus to collect papers testing the performance of ChatGPT in medical licensing examinations. We imported all the collected literatures into Rayyan and screened the literatures based on the selection criteria and exclusion criteria. The risk of bias and quality of included studies was assessed by using Mixed Methods Appraisal Tool (MMAT). Data from included studies was extracted into an Excel spreadsheet. All of the above processes were completed by two reviewers independently. A third reviewer was consulted in cases of disagreement. Finally, we provided both quantitative and qualitative analysis of the findings from the included studies. © 2024 Liu et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
PB  - Public Library of Science
C2  - 39466750
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Abadir, P.M.
AU  - Battle, A.
AU  - Walston, J.D.
AU  - Chellappa, R.
TI  - Enhancing Care for Older Adults and Dementia Patients With Large Language Models: Proceedings of the National Institute on Aging-Artificial Intelligence & Technology Collaboratory for Aging Research Symposium
PY  - 2024
T2  - Journals of Gerontology - Series A Biological Sciences and Medical Sciences
VL  - 79
IS  - 9
C7  - glae176
DO  - 10.1093/gerona/glae176
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202087541&doi=10.1093%2fgerona%2fglae176&partnerID=40&md5=340af5d83562af2216e1e3df1150298d
AB  - Large Language Models (LLMs) stand on the brink of reshaping the field of aging and dementia care, challenging the one-size-fits-all paradigm with their capacity for precision medicine and individualized treatment strategies. The "Large Pre-Trained Models with a Focus on AD/ ADRD and Healthy Aging"symposium, organized by the National Institute on Aging and the Johns Hopkins Artificial Intelligence & Technology Collaboratory for Aging Research, served as a platform for exploring this potential. The symposium brought together diverse experts to discuss the integration of LLMs in aging and dementia care. They highlighted the roles LLMs can play in clinical decision support and predictive analytics, while also addressing critical ethical concerns including bias, privacy, and the responsible use of artificial intelligence (AI). The discussions focused on the need to balance technological advancement with ethical considerations in AI deployment. In conclusion, the symposium projected a future where LLMs not only revolutionize healthcare practices but also pose significant challenges that require careful navigation.  © The Author(s) 2024.
PB  - Oxford University Press
C2  - 39001657
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - McCrary, M.R.
AU  - Galambus, J.
AU  - Chen, W.-S.
TI  - Evaluating the diagnostic performance of a large language model-powered chatbot for providing immunohistochemistry recommendations in dermatopathology
PY  - 2024
T2  - Journal of Cutaneous Pathology
VL  - 51
IS  - 9
SP  - 689
EP  - 695
DO  - 10.1111/cup.14631
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193071828&doi=10.1111%2fcup.14631&partnerID=40&md5=b5f4beca387bde8b922f54fe4b4ac2a6
AB  - Background: Large language model (LLM)-powered chatbots such as ChatGPT have numerous applications. However, their effectiveness in dermatopathology has not been formally evaluated. Dermatopathological cases often require immunohistochemical workup. Here, we evaluate the performance of a chatbot in providing diagnostically useful information on immunohistochemistry relating to dermatological diseases. Methods: We queried a commonly used chatbot for the immunophenotypes of 51 cutaneous diseases, including a diverse variety of epidermal, adnexal, hematolymphoid, and soft tissue entities. We requested it to provide references for each diagnosis. All tests were repeated, compiled, quantified, and then compared with established literature standards. Results: Clustering analysis demonstrated that recommendations correlated with tumor type, suggesting chatbots can supply appropriate panels. However, a significant portion of recommendations were factually incorrect (13.9%). Citations were rarely clinically useful (24.5%). Many were confabulated (27.2%). Prompt responses for cutaneous adnexal lesions tended to be less accurate while literature references were less useful. Reference retrieval performance was associated with the number of PubMed entries per entity. Conclusions: This foundational study suggests that LLM-powered chatbots may be useful for generating immunohistochemical panels for dermatologic diagnoses. However, specific performance capabilities and biases must be considered. In addition, extreme caution is advised regarding the tendencies to fabricate material. Future models intentionally fine-tuned to augment diagnostic medicine may prove to be valuable. © 2024 John Wiley & Sons A/S. Published by John Wiley & Sons Ltd.
PB  - John Wiley and Sons Inc
C2  - 38744501
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Abreu, A.A.
AU  - Murimwa, G.Z.
AU  - Farah, E.
AU  - Stewart, J.W.
AU  - Zhang, L.
AU  - Rodriguez, J.
AU  - Sweetenham, J.
AU  - Zeh, H.J.
AU  - Wang, S.C.
AU  - Polanco, P.M.
TI  - Enhancing Readability of Online Patient-Facing Content: The Role of AI Chatbots in Improving Cancer Information Accessibility
PY  - 2024
T2  - JNCCN Journal of the National Comprehensive Cancer Network
VL  - 22
IS  - 2D
C7  - e237334
DO  - 10.6004/jnccn.2023.7334
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195284707&doi=10.6004%2fjnccn.2023.7334&partnerID=40&md5=3585a3fba5b6df9c656d7c9ffb6e37d6
AB  - Background: Internet-based health education is increasingly vital in patient care. However, the readability of online information often exceeds the average reading level of the US population, limiting accessibility and comprehension. This study investigates the use of chatbot artificial intelligence to improve the readability of cancer-related patient-facing content. Methods: We used ChatGPT 4.0 to rewrite content about breast, colon, lung, prostate, and pancreas cancer across 34 websites associated with NCCN Member Institutions. Readability was analyzed using Fry Readability Score, Flesch-Kincaid Grade Level, Gunning Fog Index, and Simple Measure of Gobbledygook. The primary outcome was the mean readability score for the original and artificial intelligence (AI)–generated content. As secondary outcomes, we assessed the accuracy, similarity, and quality using F1 scores, cosine similarity scores, and section 2 of the DISCERN instrument, respectively. Results: The mean readability level across the 34 websites was equivalent to a university freshman level (grade 1361.5). However, after ChatGPT’s intervention, the AI-generated outputs had a mean readability score equivalent to a high school freshman education level (grade 960.8). The overall F1 score for the rewritten content was 0.87, the precision score was 0.934, and the recall score was 0.814. Compared with their original counterparts, the AI-rewritten content had a cosine similarity score of 0.915 (95% CI, 0.908–0.922). The improved readability was attributed to simpler words and shorter sentences. The mean DISCERN score of the random sample of AI-generated content was equivalent to “good” (28.565), with no significant differences compared with their original counterparts. Conclusions: Our study demonstrates the potential of AI chatbots to improve the readability of patient-facing content while maintaining content quality. The decrease in requisite literacy after AI revision emphasizes the potential of this technology to reduce health care disparities caused by a mismatch between educational resources available to a patient and their health literacy. © 2024 Harborside Press. All rights reserved.
PB  - Harborside Press
C2  - 38749478
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 12
ER  -

TY  - JOUR
AU  - Fatima, A.
AU  - Shafique, M.A.
AU  - Alam, K.
AU  - Fadlalla Ahmed, T.K.
AU  - Mustafa, M.S.
TI  - ChatGPT in medicine: A cross-disciplinary systematic review of ChatGPT's (artificial intelligence) role in research, clinical practice, education, and patient interaction
PY  - 2024
T2  - Medicine (United States)
VL  - 103
IS  - 32
SP  - e39250
DO  - 10.1097/MD.0000000000039250
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201002487&doi=10.1097%2fMD.0000000000039250&partnerID=40&md5=11efff3ab9945b411cc2ab65c976ab2e
AB  - Background: ChatGPT, a powerful AI language model, has gained increasing prominence in medicine, offering potential applications in healthcare, clinical decision support, patient communication, and medical research. This systematic review aims to comprehensively assess the applications of ChatGPT in healthcare education, research, writing, patient communication, and practice while also delineating potential limitations and areas for improvement. Method: Our comprehensive database search retrieved relevant papers from PubMed, Medline and Scopus. After the screening process, 83 studies met the inclusion criteria. This review includes original studies comprising case reports, analytical studies, and editorials with original findings. Result: ChatGPT is useful for scientific research and academic writing, and assists with grammar, clarity, and coherence. This helps non-English speakers and improves accessibility by breaking down linguistic barriers. However, its limitations include probable inaccuracy and ethical issues, such as bias and plagiarism. ChatGPT streamlines workflows and offers diagnostic and educational potential in healthcare but exhibits biases and lacks emotional sensitivity. It is useful in inpatient communication, but requires up-to-date data and faces concerns about the accuracy of information and hallucinatory responses. Conclusion: Given the potential for ChatGPT to transform healthcare education, research, and practice, it is essential to approach its adoption in these areas with caution due to its inherent limitations.  © 2024 the Author(s). Published by Wolters Kluwer Health, Inc.
PB  - Lippincott Williams and Wilkins
C2  - 39121303
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - JOUR
AU  - Giorgino, R.
AU  - Alessandri-Bonetti, M.
AU  - Del Re, M.
AU  - Verdoni, F.
AU  - Peretti, G.M.
AU  - Mangiavini, L.
TI  - Google Bard and ChatGPT in Orthopedics: Which Is the Better Doctor in Sports Medicine and Pediatric Orthopedics? The Role of AI in Patient Education
PY  - 2024
T2  - Diagnostics
VL  - 14
IS  - 12
C7  - 1253
DO  - 10.3390/diagnostics14121253
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197342411&doi=10.3390%2fdiagnostics14121253&partnerID=40&md5=7178e60808ea6874f989879e9b311e4e
AB  - Background: This study evaluates the potential of ChatGPT and Google Bard as educational tools for patients in orthopedics, focusing on sports medicine and pediatric orthopedics. The aim is to compare the quality of responses provided by these natural language processing (NLP) models, addressing concerns about the potential dissemination of incorrect medical information. Methods: Ten ACL- and flat foot-related questions from a Google search were presented to ChatGPT-3.5 and Google Bard. Expert orthopedic surgeons rated the responses using the Global Quality Score (GQS). The study minimized bias by clearing chat history before each question, maintaining respondent anonymity and employing statistical analysis to compare response quality. Results: ChatGPT-3.5 and Google Bard yielded good-quality responses, with average scores of 4.1 ± 0.7 and 4 ± 0.78, respectively, for sports medicine. For pediatric orthopedics, Google Bard scored 3.5 ± 1, while the average score for responses generated by ChatGPT was 3.8 ± 0.83. In both cases, no statistically significant difference was found between the platforms (p = 0.6787, p = 0.3092). Despite ChatGPT’s responses being considered more readable, both platforms showed promise for AI-driven patient education, with no reported misinformation. Conclusions: ChatGPT and Google Bard demonstrate significant potential as supplementary patient education resources in orthopedics. However, improvements are needed for increased reliability. The study underscores the evolving role of AI in orthopedics and calls for continued research to ensure a conscientious integration of AI in healthcare education. © 2024 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - van Nuland, M.
AU  - Lobbezoo, A.-F.H.
AU  - van de Garde, E.M.W.
AU  - Herbrink, M.
AU  - van Heijl, I.
AU  - Bognàr, T.
AU  - Houwen, J.P.A.
AU  - Dekens, M.
AU  - Wannet, D.
AU  - Egberts, T.
AU  - van der Linden, P.D.
TI  - Assessing accuracy of ChatGPT in response to questions from day to day pharmaceutical care in hospitals
PY  - 2024
T2  - Exploratory Research in Clinical and Social Pharmacy
VL  - 15
C7  - 100464
DO  - 10.1016/j.rcsop.2024.100464
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197042083&doi=10.1016%2fj.rcsop.2024.100464&partnerID=40&md5=51b911ef701d32e49a3cd19633d63693
AB  - Background: The advent of Large Language Models (LLMs) such as ChatGPT introduces opportunities within the medical field. Nonetheless, use of LLM poses a risk when healthcare practitioners and patients present clinical questions to these programs without a comprehensive understanding of its suitability for clinical contexts. Objective: The objective of this study was to assess ChatGPT's ability to generate appropriate responses to clinical questions that hospital pharmacists could encounter during routine patient care. Methods: Thirty questions from 10 different domains within clinical pharmacy were collected during routine care. Questions were presented to ChatGPT in a standardized format, including patients' age, sex, drug name, dose, and indication. Subsequently, relevant information regarding specific cases were provided, and the prompt was concluded with the query “what would a hospital pharmacist do?”. The impact on accuracy was assessed for each domain by modifying personification to “what would you do?”, presenting the question in Dutch, and regenerating the primary question. All responses were independently evaluated by two senior hospital pharmacists, focusing on the availability of an advice, accuracy and concordance. Results: In 77% of questions, ChatGPT provided an advice in response to the question. For these responses, accuracy and concordance were determined. Accuracy was correct and complete for 26% of responses, correct but incomplete for 22% of responses, partially correct and partially incorrect for 30% of responses and completely incorrect for 22% of responses. The reproducibility was poor, with merely 10% of responses remaining consistent upon regeneration of the primary question. Conclusions: While concordance of responses was excellent, the accuracy and reproducibility were poor. With the described method, ChatGPT should not be used to address questions encountered by hospital pharmacists during their shifts. However, it is important to acknowledge the limitations of our methodology, including potential biases, which may have influenced the findings. © 2024 The Authors
PB  - Elsevier Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Patil, R.
AU  - Heston, T.F.
AU  - Bhuse, V.
TI  - Prompt Engineering in Healthcare
PY  - 2024
T2  - Electronics (Switzerland)
VL  - 13
IS  - 15
C7  - 2961
DO  - 10.3390/electronics13152961
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200749334&doi=10.3390%2felectronics13152961&partnerID=40&md5=8a19348471ad392f20086f390d73fa5b
AB  - The rapid advancements in artificial intelligence, particularly generative AI and large language models, have unlocked new possibilities for revolutionizing healthcare delivery. However, harnessing the full potential of these technologies requires effective prompt engineering—designing and optimizing input prompts to guide AI systems toward generating clinically relevant and accurate outputs. Despite the importance of prompt engineering, medical education has yet to fully incorporate comprehensive training on this critical skill, leading to a knowledge gap among medical clinicians. This article addresses this educational gap by providing an overview of generative AI prompt engineering, its potential applications in primary care medicine, and best practices for its effective implementation. The role of well-crafted prompts in eliciting accurate, relevant, and valuable responses from AI models is discussed, emphasizing the need for prompts grounded in medical knowledge and aligned with evidence-based guidelines. The article explores various applications of prompt engineering in primary care, including enhancing patient–provider communication, streamlining clinical documentation, supporting medical education, and facilitating personalized care and shared decision-making. Incorporating domain-specific knowledge, engaging in iterative refinement and validation of prompts, and addressing ethical considerations and potential biases are highlighted. Embracing prompt engineering as a core competency in medical education will be crucial for successfully adopting and implementing AI technologies in primary care, ultimately leading to improved patient outcomes and enhanced healthcare delivery. © 2024 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 11
ER  -

TY  - JOUR
AU  - Barcelona, V.
AU  - Scharp, D.
AU  - Idnay, B.R.
AU  - Moen, H.
AU  - Cato, K.
AU  - Topaz, M.
TI  - Identifying stigmatizing language in clinical documentation: A scoping review of emerging literature
PY  - 2024
T2  - PLoS ONE
VL  - 19
IS  - 6 June
C7  - e0303653
DO  - 10.1371/journal.pone.0303653
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197184439&doi=10.1371%2fjournal.pone.0303653&partnerID=40&md5=350abe636f86e9e0e277e05edf492ac0
AB  - Background Racism and implicit bias underlie disparities in health care access, treatment, and outcomes. An emerging area of study in examining health disparities is the use of stigmatizing language in the electronic health record (EHR). Objectives We sought to summarize the existing literature related to stigmatizing language documented in the EHR. To this end, we conducted a scoping review to identify, describe, and evaluate the current body of literature related to stigmatizing language and clinician notes. Methods We searched PubMed, Cumulative Index of Nursing and Allied Health Literature (CINAHL), and Embase databases in May 2022, and also conducted a hand search of IEEE to identify studies investigating stigmatizing language in clinical documentation. We included all studies published through April 2022. The results for each search were uploaded into EndNote X9 software, de-duplicated using the Bramer method, and then exported to Covidence software for title and abstract screening. Results Studies (N = 9) used cross-sectional (n = 3), qualitative (n = 3), mixed methods (n = 2), and retrospective cohort (n = 1) designs. Stigmatizing language was defined via content analysis of clinical documentation (n = 4), literature review (n = 2), interviews with clinicians (n = 3) and patients (n = 1), expert panel consultation, and task force guidelines (n = 1). Natural language processing was used in four studies to identify and extract stigmatizing words from clinical notes. All of the studies reviewed concluded that negative clinician attitudes and the use of stigmatizing language in documentation could negatively impact patient perception of care or health outcomes. Discussion The current literature indicates that NLP is an emerging approach to identifying stigmatizing language documented in the EHR. NLP-based solutions can be developed and integrated into routine documentation systems to screen for stigmatizing language and alert clinicians or their supervisors. Potential interventions resulting from this research could generate awareness about how implicit biases affect communication patterns and work to achieve equitable health care for diverse populations. © 2024 Barcelona et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
PB  - Public Library of Science
C2  - 38941299
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Cobert, J.
AU  - Mills, H.
AU  - Lee, A.
AU  - Gologorskaya, O.
AU  - Espejo, E.
AU  - Jeon, S.Y.
AU  - Boscardin, W.J.
AU  - Heintz, T.A.
AU  - Kennedy, C.J.
AU  - Ashana, D.C.
AU  - Chapman, A.C.
AU  - Raghunathan, K.
AU  - Smith, A.K.
AU  - Lee, S.J.
TI  - Measuring Implicit Bias in ICU Notes Using Word-Embedding Neural Network Models
PY  - 2024
T2  - Chest
VL  - 165
IS  - 6
SP  - 1481
EP  - 1490
DO  - 10.1016/j.chest.2023.12.031
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191542681&doi=10.1016%2fj.chest.2023.12.031&partnerID=40&md5=7b3542dc1d172eb963402f1536bd91dc
AB  - Background: Language in nonmedical data sets is known to transmit human-like biases when used in natural language processing (NLP) algorithms that can reinforce disparities. It is unclear if NLP algorithms of medical notes could lead to similar transmissions of biases. Research Question: Can we identify implicit bias in clinical notes, and are biases stable across time and geography? Study Design and Methods: To determine whether different racial and ethnic descriptors are similar contextually to stigmatizing language in ICU notes and whether these relationships are stable across time and geography, we identified notes on critically ill adults admitted to the University of California, San Francisco (UCSF), from 2012 through 2022 and to Beth Israel Deaconess Hospital (BIDMC) from 2001 through 2012. Because word meaning is derived largely from context, we trained unsupervised word-embedding algorithms to measure the similarity (cosine similarity) quantitatively of the context between a racial or ethnic descriptor (eg, African-American) and a stigmatizing target word (eg, nonco-operative) or group of words (violence, passivity, noncompliance, nonadherence). Results: In UCSF notes, Black descriptors were less likely to be similar contextually to violent words compared with White descriptors. Contrastingly, in BIDMC notes, Black descriptors were more likely to be similar contextually to violent words compared with White descriptors. The UCSF data set also showed that Black descriptors were more similar contextually to passivity and noncompliance words compared with Latinx descriptors. Interpretation: Implicit bias is identifiable in ICU notes. Racial and ethnic group descriptors carry different contextual relationships to stigmatizing words, depending on when and where notes were written. Because NLP models seem able to transmit implicit bias from training data, use of NLP algorithms in clinical prediction could reinforce disparities. Active debiasing strategies may be necessary to achieve algorithmic fairness when using language models in clinical research. © 2024
PB  - Elsevier Inc.
C2  - 38199323
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Gin, B.C.
AU  - ten Cate, O.
AU  - O’Sullivan, P.S.
AU  - Boscardin, C.
TI  - Assessing supervisor versus trainee viewpoints of entrustment through cognitive and affective lenses: an artificial intelligence investigation of bias in feedback
PY  - 2024
T2  - Advances in Health Sciences Education
VL  - 29
IS  - 5
SP  - 1571
EP  - 1592
DO  - 10.1007/s10459-024-10311-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185471231&doi=10.1007%2fs10459-024-10311-9&partnerID=40&md5=9d5050c9260d79bf8368311f448c8f16
AB  - The entrustment framework redirects assessment from considering only trainees’ competence to decision-making about their readiness to perform clinical tasks independently. Since trainees and supervisors both contribute to entrustment decisions, we examined the cognitive and affective factors that underly their negotiation of trust, and whether trainee demographic characteristics may bias them. Using a document analysis approach, we adapted large language models (LLMs) to examine feedback dialogs (N = 24,187, each with an associated entrustment rating) between medical student trainees and their clinical supervisors. We compared how trainees and supervisors differentially documented feedback dialogs about similar tasks by identifying qualitative themes and quantitatively assessing their correlation with entrustment ratings. Supervisors’ themes predominantly reflected skills related to patient presentations, while trainees’ themes were broader—including clinical performance and personal qualities. To examine affect, we trained an LLM to measure feedback sentiment. On average, trainees used more negative language (5.3% lower probability of positive sentiment, p < 0.05) compared to supervisors, while documenting higher entrustment ratings (+ 0.08 on a 1–4 scale, p < 0.05). We also found biases tied to demographic characteristics: trainees’ documentation reflected more positive sentiment in the case of male trainees (+ 1.3%, p < 0.05) and of trainees underrepresented in medicine (UIM) (+ 1.3%, p < 0.05). Entrustment ratings did not appear to reflect these biases, neither when documented by trainee nor supervisor. As such, bias appeared to influence the emotive language trainees used to document entrustment more than the degree of entrustment they experienced. Mitigating these biases is nonetheless important because they may affect trainees’ assimilation into their roles and formation of trusting relationships. © The Author(s) 2024.
PB  - Springer Science and Business Media B.V.
C2  - 38388855
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Rodriguez, J.A.
AU  - Alsentzer, E.
AU  - Bates, D.W.
TI  - Leveraging large language models to foster equity in healthcare
PY  - 2024
T2  - Journal of the American Medical Informatics Association
VL  - 31
IS  - 9
SP  - 2147
EP  - 2150
DO  - 10.1093/jamia/ocae055
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199138450&doi=10.1093%2fjamia%2focae055&partnerID=40&md5=213a7c696277c00a4e055a1843faa7e0
AB  - Objectives: Large language models (LLMs) are poised to change care delivery, but their impact on health equity is unclear. While marginalized populations have been historically excluded from early technology developments, LLMs present an opportunity to change our approach to developing, evaluating, and implementing new technologies. In this perspective, we describe the role of LLMs in supporting health equity. Materials and Methods: We apply the National Institute on Minority Health and Health Disparities (NIMHD) research framework to explore the use of LLMs for health equity. Results: We present opportunities for how LLMs can improve health equity across individual, family and organizational, community, and population health. We describe emerging concerns including biased data, limited technology diffusion, and privacy. Finally, we highlight recommendations focused on prompt engineering, retrieval augmentation, digital inclusion, transparency, and bias mitigation. Conclusion: The potential of LLMs to support health equity depends on making health equity a focus from the start.  © 2024 The Author(s). Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved.
PB  - Oxford University Press
C2  - 38511501
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - Mansoor, M.A.
AU  - Ansari, K.H.
TI  - Early Detection of Mental Health Crises through Artifical-Intelligence-Powered Social Media Analysis: A Prospective Observational Study
PY  - 2024
T2  - Journal of Personalized Medicine
VL  - 14
IS  - 9
C7  - 958
DO  - 10.3390/jpm14090958
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205264445&doi=10.3390%2fjpm14090958&partnerID=40&md5=ae9344004ad68a51d4f695afb107702d
AB  - Background: The early detection of mental health crises is crucial for timely interventions and improved outcomes. This study explores the potential of artificial intelligence (AI) in analyzing social media data to identify early signs of mental health crises. Methods: We developed a multimodal deep learning model integrating natural language processing and temporal analysis techniques. The model was trained on a diverse dataset of 996,452 social media posts in multiple languages (English, Spanish, Mandarin, and Arabic) collected from Twitter, Reddit, and Facebook over 12 months. Its performance was evaluated using standard metrics and validated against expert psychiatric assessments. Results: The AI model demonstrated a high level of accuracy (89.3%) in detecting early signs of mental health crises, with an average lead time of 7.2 days before human expert identification. Performance was consistent across languages (F1 scores: 0.827–0.872) and platforms (F1 scores: 0.839–0.863). Key digital markers included linguistic patterns, behavioral changes, and temporal trends. The model showed varying levels of accuracy for different crisis types: depressive episodes (91.2%), manic episodes (88.7%), suicidal ideation (93.5%), and anxiety crises (87.3%). Conclusions: AI-powered analysis of social media data shows promise for the early detection of mental health crises across diverse linguistic and cultural contexts. However, ethical challenges, including privacy concerns, potential stigmatization, and cultural biases, need careful consideration. Future research should focus on longitudinal outcome studies, ethical integration of the method with existing mental health services, and developing personalized, culturally sensitive models. © 2024 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Parente, D.J.
TI  - Generative Artificial Intelligence and Large Language Models in Primary Care Medical Education
PY  - 2024
T2  - Family Medicine
VL  - 56
IS  - 9
SP  - 534
EP  - 540
DO  - 10.22454/FamMed.2024.775525
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206408818&doi=10.22454%2fFamMed.2024.775525&partnerID=40&md5=1792dab674354741935113d86ef0cf4d
AB  - Generative artificial intelligence and large language models are the continuation of a technological revolution in information processing that began with the invention of the transistor in 1947. These technologies, driven by transformer architectures for artificial neural networks, are poised to broadly influence society. It is already apparent that these technologies will be adapted to drive innovation in education. Medical education is a high-risk activity: Information that is incorrectly taught to a student may go unrecognized for years until a relevant clinical situation appears in which that error can lead to patient harm. In this article, I discuss the principal limitations to the use of generative artificial intelligence in medical education—hallucination, bias, cost, and security—and suggest some approaches to confronting these problems. Additionally, I identify the potential applications of generative artificial intelligence to medical education, including personalized instruction, simulation, feedback, evaluation, augmentation of qualitative research, and performance of critical assessment of the existing scientific literature. © Society of Teachers of Family Medicine.
PB  - Society of Teachers of Family Medicine
C2  - 39207784
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Luo, M.-J.
AU  - Pang, J.
AU  - Bi, S.
AU  - Lai, Y.
AU  - Zhao, J.
AU  - Shang, Y.
AU  - Cui, T.
AU  - Yang, Y.
AU  - Lin, Z.
AU  - Zhao, L.
AU  - Wu, X.
AU  - Lin, D.
AU  - Chen, J.
AU  - Lin, H.
TI  - Development and Evaluation of a Retrieval-Augmented Large Language Model Framework for Ophthalmology
PY  - 2024
T2  - JAMA Ophthalmology
VL  - 142
IS  - 9
SP  - 798
EP  - 805
DO  - 10.1001/jamaophthalmol.2024.2513
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204511492&doi=10.1001%2fjamaophthalmol.2024.2513&partnerID=40&md5=7792febca524b8b787cefaaafe32aa54
AB  - Importance: Although augmenting large language models (LLMs) with knowledge bases may improve medical domain-specific performance, practical methods are needed for local implementation of LLMs that address privacy concerns and enhance accessibility for health care professionals. Objective: To develop an accurate, cost-effective local implementation of an LLM to mitigate privacy concerns and support their practical deployment in health care settings. Design, Setting, and Participants: ChatZOC (Sun Yat-Sen University Zhongshan Ophthalmology Center), a retrieval-augmented LLM framework, was developed by enhancing a baseline LLM with a comprehensive ophthalmic dataset and evaluation framework (CODE), which includes over 30000 pieces of ophthalmic knowledge. This LLM was benchmarked against 10 representative LLMs, including GPT-4 and GPT-3.5 Turbo (OpenAI), across 300 clinical questions in ophthalmology. The evaluation, involving a panel of medical experts and biomedical researchers, focused on accuracy, utility, and safety. A double-masked approach was used to try to minimize bias assessment across all models. The study used a comprehensive knowledge base derived from ophthalmic clinical practice, without directly involving clinical patients. Exposures: LLM response to clinical questions. Main Outcomes and Measures: Accuracy, utility, and safety of LLMs in responding to clinical questions. Results: The baseline model achieved a human ranking score of 0.48. The retrieval-augmented LLM had a score of 0.60, a difference of 0.12 (95% CI, 0.02-0.22; P =.02) from baseline and not different from GPT-4 with a score of 0.61 (difference = 0.01; 95% CI, -0.11 to 0.13; P =.89). For scientific consensus, the retrieval-augmented LLM was 84.0% compared with the baseline model of 46.5% (difference = 37.5%; 95% CI, 29.0%-46.0%; P <.001) and not different from GPT-4 with a value of 79.2% (difference = 4.8%; 95% CI, -0.3% to 10.0%; P =.06). Conclusions and Relevance: Results of this quality improvement study suggest that the integration of high-quality knowledge bases improved the LLM's performance in medical domains. This study highlights the transformative potential of augmented LLMs in clinical practice by providing reliable, safe, and practical clinical information. Further research is needed to explore the broader application of such frameworks in the real world. © 2024 American Medical Association. All rights reserved.
PB  - American Medical Association
C2  - 39023885
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - CONF
AU  - Zaghir, J.
AU  - Bjelogrlic, M.
AU  - Goldman, J.-P.
AU  - Bensahla, A.
AU  - Zheng, Y.
AU  - Lovis, C.
TI  - Beyond Tokens: Fair Evaluation of French Large Language Models for Clinical Named Entity Recognition
PY  - 2024
T2  - Studies in Health Technology and Informatics
VL  - 316
SP  - 666
EP  - 670
DO  - 10.3233/SHTI240502
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202006023&doi=10.3233%2fSHTI240502&partnerID=40&md5=1d446bd0a82681ccac3df06d3b1494b5
AB  - Named Entity Recognition (NER) models based on Transformers have gained prominence for their impressive performance in various languages and domains. This work delves into the often-overlooked aspect of entity-level metrics and exposes significant discrepancies between token and entity-level evaluations. The study utilizes a corpus of synthetic French oncological reports annotated with entities representing oncological morphologies. Four different French BERT-based models are fine-tuned for token classification, and their performance is rigorously assessed at both token and entity-level. In addition to fine-tuning, we evaluate ChatGPT's ability to perform NER through prompt engineering techniques. The findings reveal a notable disparity in model effectiveness when transitioning from token to entity-level metrics, highlighting the importance of comprehensive evaluation methodologies in NER tasks. Furthermore, in comparison to BERT, ChatGPT remains limited when it comes to detecting advanced entities in French. © 2024 The Authors.
PB  - IOS Press BV
C2  - 39176830
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Almeida, M.O.
AU  - Soares, A.C.G.
AU  - de Sousa, G.H.M.
AU  - Ferraz, W.R.
AU  - Trossini, G.H.G.
TI  - Investigating the Prospects of ChatGPT in Training Medicinal Chemists and the Development of Novel Drugs
PY  - 2024
T2  - Orbital
VL  - 16
IS  - 4
SP  - 319
EP  - 324
DO  - 10.17807/orbital.v16i4.21129
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216125796&doi=10.17807%2forbital.v16i4.21129&partnerID=40&md5=58d0eb24743644a4389d85f2a2fc9d50
AB  - This scientific article delves into the advantages, insights, and limitations of ChatGPT in various scientific domains. Alongside other large language models, this tool exhibits the potential to directly or indirectly assist in a range of scientific areas including Computer Science, Chemistry, Biology/Bioinformatics, and Medicine. Some of the functionalities of ChatGPT include text translation, code improvement, data visualization, and database cleaning. The model can aid in writing and translating scientific articles from mostly any language. In the field of chemoinformatics and computational chemistry, ChatGPT can provide code examples and assist in code development, by evaluating and enhancing code readability and project documentation. Furthermore, it can assist in the database cleaning process and create customized functions for performing specialized tasks. However, ChatGPT does possess some limitations, such as frequent occurrences of artificial hallucinations (a response generated by AI that comprises erroneous or misleading information presented as true), the inability to process multimodal information, and the potential for biases in its training datasets. Therefore, caution must be exercised when incorporating these technologies, considering their social impact and implications for the job market. Acknowledging limitations is crucial when using these tools. With careful and proper use, they can aid the scientific process with the potential to speed up drug discovery. © 2024, Universidade Federal de Mato Grosso do Sul, Departamento de Quimica. All rights reserved.
PB  - Universidade Federal de Mato Grosso do Sul, Departamento de Quimica
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Perlis, R.H.
AU  - Goldberg, J.F.
AU  - Ostacher, M.J.
AU  - Schneck, C.D.
TI  - Clinical decision support for bipolar depression using large language models
PY  - 2024
T2  - Neuropsychopharmacology
VL  - 49
IS  - 9
SP  - 1412
EP  - 1416
DO  - 10.1038/s41386-024-01841-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187649008&doi=10.1038%2fs41386-024-01841-2&partnerID=40&md5=4cfed64f20ca9ac7819e184f570d56a5
AB  - Management of depressive episodes in bipolar disorder remains challenging for clinicians despite the availability of treatment guidelines. In other contexts, large language models have yielded promising results for supporting clinical decisionmaking. We developed 50 sets of clinical vignettes reflecting bipolar depression and presented them to experts in bipolar disorder, who were asked to identify 5 optimal next-step pharmacotherapies and 5 poor or contraindicated choices. The same vignettes were then presented to a large language model (GPT4-turbo; gpt-4-1106-preview), with or without augmentation by prompting with recent bipolar treatment guidelines, and asked to identify the optimal next-step pharmacotherapy. Overlap between model output and gold standard was estimated. The augmented model prioritized the expert-designated optimal choice for 508/1000 vignettes (50.8%, 95% CI 47.7–53.9%; Cohen’s kappa = 0.31, 95% CI 0.28–0.35). For 120 vignettes (12.0%), at least one model choice was among the poor or contraindicated treatments. Results were not meaningfully different when gender or race of the vignette was permuted to examine risk for bias. By comparison, an un-augmented model identified the optimal treatment for 234 (23.0%, 95% CI 20.8–26.0%; McNemar’s p < 0.001 versus augmented model) of the vignettes. A sample of community clinicians scoring the same vignettes identified the optimal choice for 23.1% (95% CI 15.7–30.5%) of vignettes, on average; McNemar’s p < 0.001 versus augmented model. Large language models prompted with evidence-based guidelines represent a promising, scalable strategy for clinical decision support. In addition to prospective studies of efficacy, strategies to avoid clinician overreliance on such models, and address the possibility of bias, will be needed. © The Author(s) 2024.
PB  - Springer Nature
C2  - 38480911
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Ogletree, A.M.
AU  - Percy-Laurry, A.
AU  - Assenov, A.
AU  - Dinwiddie, G.Y.
AU  - Jones, N.L.
AU  - Marshall, V.J.
AU  - Motley, E.R.
AU  - Williams-Parry, K.
AU  - Farhat, T.
TI  - Social Determinants of Health Research at NIMHD: An Analysis of Studies Funded During 2019–2023
PY  - 2024
T2  - American Journal of Preventive Medicine
VL  - 67
IS  - 5
SP  - 713
EP  - 721
DO  - 10.1016/j.amepre.2024.06.027
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199248492&doi=10.1016%2fj.amepre.2024.06.027&partnerID=40&md5=f81c24f05c71c5549bed547a5102f057
AB  - Introduction: Social determinants of health (SDOH) contribute to differences in health outcomes and exacerbate health disparities. This study characterizes the National Institute on Minority Health and Health Disparities’ (NIMHD) portfolio of funded grants in SDOH research, identifies gaps, and provides suggestions for future research. Methods: Using the National Institutes of Health's SDOH Research, Condition, and Disease Categorization, research projects funded from 2019 to 2023 were identified and linked with NIMHD's internal coding system to extract in-depth study characteristics, including sociodemographics of study participants, disease and condition focus, and alignment with strategic priorities. Natural Language Processing methods were used to categorize projects into five Healthy People 2030 SDOH domains. Results: The resulting sample included 675 unique research projects. Most projects included racial and ethnic minority groups (89%), followed by people with lower socioeconomic status (33%), underserved rural communities (16%), and sexual and gender minority groups (13%). Most projects focused on the Etiology of health disparities (61%), followed by Interventions (54%), and Methods and Measurement (39%). Of the Healthy People 2030 domains, Social and Community Context had the greatest representation (61%) whereas Education Access and Quality had the least (6%). Variation in research project characteristics across SDOH domains is also presented. Conclusions: This study documents characteristics of SDOH research funded by NIMHD and explores how they differ across Healthy People 2030 SDOH domains. Findings highlight how study characteristics and foci align with strategic priorities and suggest opportunities for future research. © 2024
PB  - Elsevier Inc.
C2  - 38971453
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Hsieh, C.-H.
AU  - Hsieh, H.-Y.
AU  - Lin, H.-P.
TI  - Evaluating the performance of ChatGPT-3.5 and ChatGPT-4 on the Taiwan plastic surgery board examination
PY  - 2024
T2  - Heliyon
VL  - 10
IS  - 14
C7  - e34851
DO  - 10.1016/j.heliyon.2024.e34851
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198954993&doi=10.1016%2fj.heliyon.2024.e34851&partnerID=40&md5=87c2cfd91f024159f20afc83053cf07f
AB  - Background: Chat Generative Pre-Trained Transformer (ChatGPT) is a state-of-the-art large language model that has been evaluated across various medical fields, with mixed performance on licensing examinations. This study aimed to assess the performance of ChatGPT-3.5 and ChatGPT-4 in answering questions from the Taiwan Plastic Surgery Board Examination. Methods: The study evaluated the performance of ChatGPT-3.5 and ChatGPT-4 on 1375 questions from the past 8 years of the Taiwan Plastic Surgery Board Examination, including 985 single-choice and 390 multiple-choice questions. We obtained the responses between June and July 2023, launching a new chat session for each question to eliminate memory retention bias. Results: Overall, ChatGPT-4 outperformed ChatGPT-3.5, achieving a 59 % correct answer rate compared to 41 % for ChatGPT-3.5. ChatGPT-4 passed five out of eight yearly exams, whereas ChatGPT-3.5 failed all. On single-choice questions, ChatGPT-4 scored 66 % correct, compared to 48 % for ChatGPT-3.5. On multiple-choice, ChatGPT-4 achieved a 43 % correct rate, nearly double the 23 % of ChatGPT-3.5. Conclusion: As ChatGPT evolves, its performance on the Taiwan Plastic Surgery Board Examination is expected to improve further. The study suggests potential reforms, such as incorporating more problem-based scenarios, leveraging ChatGPT to refine exam questions, and integrating AI-assisted learning into candidate preparation. These advancements could enhance the assessment of candidates' critical thinking and problem-solving abilities in the field of plastic surgery. © 2024
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Ofori-Boateng, R.
AU  - Aceves-Martins, M.
AU  - Wiratunga, N.
AU  - Moreno-Garcia, C.F.
TI  - Towards the automation of systematic reviews using natural language processing, machine learning, and deep learning: a comprehensive review
PY  - 2024
T2  - Artificial Intelligence Review
VL  - 57
IS  - 8
C7  - 200
DO  - 10.1007/s10462-024-10844-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198031178&doi=10.1007%2fs10462-024-10844-w&partnerID=40&md5=39cbd8a1413e0ced5c10983d270842d3
AB  - Systematic reviews (SRs) constitute a critical foundation for evidence-based decision-making and policy formulation across various disciplines, particularly in healthcare and beyond. However, the inherently rigorous and structured nature of the SR process renders it laborious for human reviewers. Moreover, the exponential growth in daily published literature exacerbates the challenge, as SRs risk missing out on incorporating recent studies that could potentially influence research outcomes. This pressing need to streamline and enhance the efficiency of SRs has prompted significant interest in leveraging Artificial Intelligence (AI) techniques to automate various stages of the SR process. This review paper provides a comprehensive overview of the current AI methods employed for SR automation, a subject area that has not been exhaustively covered in previous literature. Through an extensive analysis of 52 related works and an original online survey, the primary AI techniques and their applications in automating key SR stages, such as search, screening, data extraction, and risk of bias assessment, are identified. The survey results offer practical insights into the current practices, experiences, opinions, and expectations of SR practitioners and researchers regarding future SR automation. Synthesis of the literature review and survey findings highlights gaps and challenges in the current landscape of SR automation using AI techniques. Based on these insights, potential future directions are discussed. This review aims to equip researchers and practitioners with a foundational understanding of the basic concepts, primary methodologies, and recent advancements in AI-driven SR automation while guiding computer scientists in exploring novel techniques to invigorate further and advance this field. © The Author(s) 2024.
PB  - Springer Nature
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Ando, K.
AU  - Sato, M.
AU  - Wakatsuki, S.
AU  - Nagai, R.
AU  - Chino, K.
AU  - Kai, H.
AU  - Sasaki, T.
AU  - Kato, R.
AU  - Nguyen, T.P.
AU  - Guo, N.
AU  - Sultan, P.
TI  - A comparative study of English and Japanese ChatGPT responses to anaesthesia-related medical questions
PY  - 2024
T2  - BJA Open
VL  - 10
C7  - 100296
DO  - 10.1016/j.bjao.2024.100296
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196002825&doi=10.1016%2fj.bjao.2024.100296&partnerID=40&md5=4695c5d20f851b00bb419b6c4516ed20
AB  - Background: The expansion of artificial intelligence (AI) within large language models (LLMs) has the potential to streamline healthcare delivery. Despite the increased use of LLMs, disparities in their performance particularly in different languages, remain underexplored. This study examines the quality of ChatGPT responses in English and Japanese, specifically to questions related to anaesthesiology. Methods: Anaesthesiologists proficient in both languages were recruited as experts in this study. Ten frequently asked questions in anaesthesia were selected and translated for evaluation. Three non-sequential responses from ChatGPT were assessed for content quality (accuracy, comprehensiveness, and safety) and communication quality (understanding, empathy/tone, and ethics) by expert evaluators. Results: Eight anaesthesiologists evaluated English and Japanese LLM responses. The overall quality for all questions combined was higher in English compared with Japanese responses. Content and communication quality were significantly higher in English compared with Japanese LLMs responses (both P<0.001) in all three responses. Comprehensiveness, safety, and understanding were higher scores in English LLM responses. In all three responses, more than half of the evaluators marked overall English responses as better than Japanese responses. Conclusions: English LLM responses to anaesthesia-related frequently asked questions were superior in quality to Japanese responses when assessed by bilingual anaesthesia experts in this report. This study highlights the potential for language-related disparities in healthcare information and the need to improve the quality of AI responses in underrepresented languages. Future studies are needed to explore these disparities in other commonly spoken languages and to compare the performance of different LLMs. © 2024 The Author(s)
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - JOUR
AU  - Chiarella, S.E.
AU  - Garcia-Guaqueta, D.P.
AU  - Drake, L.Y.
AU  - Dixon, R.E.
AU  - King, K.S.
AU  - Ryu, E.
AU  - Pongdee, T.
AU  - Park, M.A.
AU  - Kita, H.
AU  - Sagheb, E.
AU  - Kshatriya, B.S.A.
AU  - Sohn, S.
AU  - Wi, C.-I.
AU  - Sadighi Akha, A.A.
AU  - Liu, H.
AU  - Juhn, Y.J.
TI  - Sex differences in sociodemographic, clinical, and laboratory variables in childhood asthma: A birth cohort study
PY  - 2024
T2  - Annals of Allergy, Asthma and Immunology
VL  - 133
IS  - 4
SP  - 403
EP  - 412.e2
DO  - 10.1016/j.anai.2024.07.005
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200330987&doi=10.1016%2fj.anai.2024.07.005&partnerID=40&md5=561e4b0ebd64fbedaf8607c69b84fc00
AB  - Background: There are marked sex differences in the prevalence and severity of asthma, both during childhood and adulthood. There is a relative lack of comprehensive studies exploring sexdifferences in pediatric asthma cohorts. Objective: To identify the most relevant sex differences in sociodemographic, clinical, and laboratory variables in a well-characterized large pediatric asthma cohort. Methods: We performed a cross-sectional analysis of the Mayo Clinic Olmsted County Birth Cohort. In the full birth cohort, we used a natural language-processing algorithm based on the Predetermined Asthma Criteria for asthma ascertainment. In a stratified random sample of 300 children, we obtained additional pulmonary function tests and laboratory data. We identified the significant sex differences among available sociodemographic, clinical, and laboratory variables. Results: Boys were more frequently diagnosed with having asthma than girls and were younger at the time of asthma diagnosis. There were no sex differences in relation to socioeconomic status. We identified a male predominance in the presence of a tympanostomy tube and a female predominance in the history of pneumonia. A higher percentage of boys had a forced expiratory volume in 1 second/forced vital capacity ratio less than 0.85. Blood eosinophilia and atopic sensitization were also more common in boys. Finally, boys had higher levels of serum periostin than girls. Conclusion: This study described significant sex differences in a large pediatric asthma cohort. Overall, boys had earlier and more severe asthma than girls. Differences in blood eosinophilia and serum periostin provide insights into possible mechanisms of the sex bias in childhood asthma. © 2024 American College of Allergy, Asthma & Immunology
PB  - American College of Allergy, Asthma and Immunology
C2  - 39019434
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Wysocka, M.
AU  - Wysocki, O.
AU  - Delmas, M.
AU  - Mutel, V.
AU  - Freitas, A.
TI  - Large Language Models, scientific knowledge and factuality: A framework to streamline human expert evaluation
PY  - 2024
T2  - Journal of Biomedical Informatics
VL  - 158
C7  - 104724
DO  - 10.1016/j.jbi.2024.104724
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204180438&doi=10.1016%2fj.jbi.2024.104724&partnerID=40&md5=09756764e50423151d306bfa7af1341d
AB  - Objective: The paper introduces a framework for the evaluation of the encoding of factual scientific knowledge, designed to streamline the manual evaluation process typically conducted by domain experts. Inferring over and extracting information from Large Language Models (LLMs) trained on a large corpus of scientific literature can potentially define a step change in biomedical discovery, reducing the barriers for accessing and integrating existing medical evidence. This work explores the potential of LLMs for dialoguing with biomedical background knowledge, using the context of antibiotic discovery. Methods: The framework involves three evaluation steps, each assessing different aspects sequentially: fluency, prompt alignment, semantic coherence, factual knowledge, and specificity of the generated responses. By splitting these tasks between non-experts and experts, the framework reduces the effort required from the latter. The work provides a systematic assessment on the ability of eleven state-of-the-art LLMs, including ChatGPT, GPT-4 and Llama 2, in two prompting-based tasks: chemical compound definition generation and chemical compound–fungus relation determination. Results: Although recent models have improved in fluency, factual accuracy is still low and models are biased towards over-represented entities. The ability of LLMs to serve as biomedical knowledge bases is questioned, and the need for additional systematic evaluation frameworks is highlighted. Conclusion: While LLMs are currently not fit for purpose to be used as biomedical factual knowledge bases in a zero-shot setting, there is a promising emerging property in the direction of factuality as the models become domain specialised, scale up in size and level of human feedback. © 2024 The Author(s)
PB  - Academic Press Inc.
C2  - 39277154
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Xie, K.
AU  - Ojemann, W.K.S.
AU  - Gallagher, R.S.
AU  - Shinohara, R.T.
AU  - Lucas, A.
AU  - Hill, C.E.
AU  - Hamilton, R.H.
AU  - Johnson, K.B.
AU  - Roth, D.
AU  - Litt, B.
AU  - Ellis, C.A.
TI  - Disparities in seizure outcomes revealed by large language models
PY  - 2024
T2  - Journal of the American Medical Informatics Association
VL  - 31
IS  - 6
SP  - 1348
EP  - 1355
DO  - 10.1093/jamia/ocae047
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193974941&doi=10.1093%2fjamia%2focae047&partnerID=40&md5=6013d9aa876c43b30d4da03a9f58bcbe
AB  - Objective: Large-language models (LLMs) can potentially revolutionize health care delivery and research, but risk propagating existing biases or introducing new ones. In epilepsy, social determinants of health are associated with disparities in care access, but their impact on seizure outcomes among those with access remains unclear. Here we (1) evaluated our validated, epilepsy-specific LLM for intrinsic bias, and (2) used LLM-extracted seizure outcomes to determine if different demographic groups have different seizure outcomes. Materials and Methods: We tested our LLM for differences and equivalences in prediction accuracy and confidence across demographic groups defined by race, ethnicity, sex, income, and health insurance, using manually annotated notes. Next, we used LLM-classified seizure freedom at each office visit to test for demographic outcome disparities, using univariable and multivariable analyses. Results: We analyzed 84 675 clinic visits from 25 612 unique patients seen at our epilepsy center. We found little evidence of bias in the prediction accuracy or confidence of outcome classifications across demographic groups. Multivariable analysis indicated worse seizure outcomes for female patients (OR 1.33, P ≤. 001), those with public insurance (OR 1.53, P ≤. 001), and those from lower-income zip codes (OR ≥1.22, P ≤. 007). Black patients had worse outcomes than White patients in univariable but not multivariable analysis (OR 1.03, P =. 66). Conclusion: We found little evidence that our LLM was intrinsically biased against any demographic group. Seizure freedom extracted by LLM revealed disparities in seizure outcomes across several demographic groups. These findings quantify the critical need to reduce disparities in the care of people with epilepsy.  © 2024 The Author(s). Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved.
PB  - Oxford University Press
C2  - 38481027
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Kearns, W.G.
AU  - Stamoulis, G.
AU  - Glick, J.
AU  - Baisch, L.
AU  - Benner, A.
AU  - Brough, D.
AU  - Du, L.
AU  - Wilson, B.
AU  - Kearns, L.
AU  - Ng, N.
AU  - Seshan, M.
AU  - Anchan, R.
TI  - The Application of Knowledge Engineering via the Use of a Biomimetic Digital Twin Ecosystem, Phenotype-Driven Variant Analysis, and Exome Sequencing to Understand the Molecular Mechanisms of Disease
PY  - 2024
T2  - Journal of Molecular Diagnostics
VL  - 26
IS  - 7
SP  - 543
EP  - 551
DO  - 10.1016/j.jmoldx.2024.03.004
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196002701&doi=10.1016%2fj.jmoldx.2024.03.004&partnerID=40&md5=978ba10cf27e81eddeecff052348778c
AB  - Applied artificial intelligence, particularly large language models, in biomedical research is accelerating, but effective discovery and validation requires a toolset without limitations or bias. On January 30, 2023, the National Academies of Sciences, Engineering, and Medicine (NAS) appointed an ad hoc committee to identify the needs and opportunities to advance the mathematical, statistical, and computational foundations of digital twins in applications across science, medicine, engineering, and society. On December 15, 2023, the NAS released a 164-page report, “Foundational Research Gaps and Future Directions for Digital Twins.” This report described the importance of using digital twins in biomedical research. The current study was designed to develop an innovative method that incorporated phenotype-ranking algorithms with knowledge engineering via a biomimetic digital twin ecosystem. This ecosystem applied real-world reasoning principles to nonnormalized, raw data to identify hidden or “dark” data. Clinical exome sequencing study on patients with endometriosis indicated four variants of unknown clinical significance potentially associated with endometriosis-related disorders in nearly all patients analyzed. One variant of unknown clinical significance was identified in all patient samples and could be a biomarker for diagnostics. To the best of our knowledge, this is the first study to incorporate the recommendations of the NAS to biomedical research. This method can be used to understand the mechanisms of any disease, for virtual clinical trials, and to identify effective new therapies. © 2024 Association for Molecular Pathology and American Society for Investigative Pathology
PB  - Elsevier B.V.
C2  - 38556123
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Jing, J.
AU  - Zhang, Z.
AU  - Su, L.
AU  - Gao, C.
AU  - Guo, A.
AU  - Liu, X.
AU  - Wang, H.
AU  - Zhang, X.
AU  - Liu, Y.
AU  - Comi, G.
AU  - Waubant, E.
AU  - Shi, F.-D.
AU  - Tian, D.-C.
TI  - Central vein sign and trigeminal lesions of multiple sclerosis visualised by 7T MRI
PY  - 2024
T2  - Journal of Neurology, Neurosurgery and Psychiatry
VL  - 95
IS  - 8
SP  - 761
EP  - 766
DO  - 10.1136/jnnp-2023-332566
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187717545&doi=10.1136%2fjnnp-2023-332566&partnerID=40&md5=c96d67f4b10dd13e4393b724cd2ee69a
AB  - Background Although trigeminal nerve involvement is a characteristic of multiple sclerosis (MS), its prevalence across studies varies greatly due to MRI resolution and cohort selection bias. The mechanism behind the site specificity of trigeminal nerve injury is still unclear. We aim to determine the prevalence of trigeminal nerve involvement in patients with MS in a consecutive 7T brain MRI cohort. Methods This observational cohort originates from an ongoing China National Registry of Neuro-Inflammatory Diseases. Inclusion criteria were the following: age 18 years or older, diagnosis of MS according to the 2017 McDonald criteria and no clinical relapse within the preceding 3 months. Each participant underwent 7T MAGNETOM Terra scanner (Siemens, Erlangen, Germany), using a 32-channel phased array coil at Beijing Tiantan Hospital. T1-weighted magnetisation-prepared rapid acquisition gradient echoes, fluid-attenuated inversion recovery (FLAIR) and fluid and white matter suppression images were used to identify lesions. FLAIR∗ and T2∗ weighted images were used to identify central vein sign (CVS) within the trigeminal lesions. Results 120 patients underwent 7T MRI scans between December 2021 and May 2023. 19/120 (15.8%) patients had a total of 45 trigeminal lesions, of which 11/19 (57.9%) were bilateral. The linear lesions extended along the trigeminal nerve, from the root entry zone (REZ) (57.8%, 26/45) to the pontine-medullary nucleus (42.2%, 19/45). 26.9% (7/26) of the lesions in REZ showed a typical central venous sign. Conclusion In this 7T MRI cohort, the prevalence of trigeminal nerve involvement was 15.8%. Characteristic CVS was detected in 26.9% of lesions in REZ. This suggests an inflammatory demyelination mechanism of trigeminal nerve involvement in MS.  © Author(s) (or their employer(s)) 2024. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.
PB  - BMJ Publishing Group
C2  - 38453475
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Reis, M.
AU  - Reis, F.
AU  - Kunde, W.
TI  - Influence of believed AI involvement on the perception of digital medical advice
PY  - 2024
T2  - Nature Medicine
VL  - 30
IS  - 11
SP  - 3098
EP  - 3100
DO  - 10.1038/s41591-024-03180-7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199528929&doi=10.1038%2fs41591-024-03180-7&partnerID=40&md5=bb25ed6018c05a34cd2ec372774ca137
AB  - Large language models offer novel opportunities to seek digital medical advice. While previous research primarily addressed the performance of such artificial intelligence (AI)-based tools, public perception of these advancements received little attention. In two preregistered studies (n = 2,280), we presented participants with scenarios of patients obtaining medical advice. All participants received identical information, but we manipulated the putative source of this advice (‘AI’, ‘human physician’, ‘human + AI’). ‘AI’- and ‘human + AI’-labeled advice was evaluated as significantly less reliable and less empathetic compared with ‘human’-labeled advice. Moreover, participants indicated lower willingness to follow the advice when AI was believed to be involved in advice generation. Our findings point toward an anti-AI bias when receiving digital medical advice, even when AI is supposedly supervised by physicians. Given the tremendous potential of AI for medicine, elucidating ways to counteract this bias should be an important objective of future research. © The Author(s) 2024.
PB  - Nature Research
C2  - 39054373
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - JOUR
AU  - Andreadis, K.
AU  - Newman, D.R.
AU  - Twan, C.
AU  - Shunk, A.
AU  - Mann, D.M.
AU  - Stevens, E.R.
TI  - Mixed methods assessment of the influence of demographics on medical advice of ChatGPT
PY  - 2024
T2  - Journal of the American Medical Informatics Association
VL  - 31
IS  - 9
SP  - 2002
EP  - 2009
DO  - 10.1093/jamia/ocae086
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201762493&doi=10.1093%2fjamia%2focae086&partnerID=40&md5=150fdcaacb428f91823421004592a46c
AB  - Objectives: To evaluate demographic biases in diagnostic accuracy and health advice between generative artificial intelligence (AI) (ChatGPT GPT-4) and traditional symptom checkers like WebMD. Materials and Methods: Combination symptom and demographic vignettes were developed for 27 most common symptom complaints. Standardized prompts, written from a patient perspective, with varying demographic permutations of age, sex, and race/ethnicity were entered into ChatGPT (GPT-4) between July and August 2023. In total, 3 runs of 540 ChatGPT prompts were compared to the corresponding WebMD Symptom Checker output using a mixed-methods approach. In addition to diagnostic correctness, the associated text generated by ChatGPT was analyzed for readability (using Flesch-Kincaid Grade Level) and qualitative aspects like disclaimers and demographic tailoring. Results: ChatGPT matched WebMD in 91% of diagnoses, with a 24% top diagnosis match rate. Diagnostic accuracy was not significantly different across demographic groups, including age, race/ethnicity, and sex. ChatGPT's urgent care recommendations and demographic tailoring were presented significantly more to 75-year-olds versus 25-year-olds (P <. 01) but were not statistically different among race/ethnicity and sex groups. The GPT text was suitable for college students, with no significant demographic variability. Discussion: The use of non-health-tailored generative AI, like ChatGPT, for simple symptom-checking functions provides comparable diagnostic accuracy to commercially available symptom checkers and does not demonstrate significant demographic bias in this setting. The text accompanying differential diagnoses, however, suggests demographic tailoring that could potentially introduce bias. Conclusion: These results highlight the need for continued rigorous evaluation of AI-driven medical platforms, focusing on demographic biases to ensure equitable care. © 2024 The Author(s). Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved.
PB  - Oxford University Press
C2  - 38679900
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Wakunuma, K.
AU  - Eke, D.
TI  - Africa, ChatGPT, and Generative AI Systems: Ethical Benefits, Concerns, and the Need for Governance
PY  - 2024
T2  - Philosophies
VL  - 9
IS  - 3
C7  - 80
DO  - 10.3390/philosophies9030080
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197175192&doi=10.3390%2fphilosophies9030080&partnerID=40&md5=655951ac4868821a722089784d6ec63c
AB  - This paper examines the impact and implications of ChatGPT and other generative AI technologies within the African context while looking at the ethical benefits and concerns that are particularly pertinent to the continent. Through a robust analysis of ChatGPT and other generative AI systems using established approaches for analysing the ethics of emerging technologies, this paper provides unique ethical benefits and concerns for these systems in the African context. This analysis combined approaches such as anticipatory technology ethics (ATE), ethical impact assessment (EIA), and ethical issues of emerging ICT applications with AI (ETICA) with specific issues from the literature. The findings show that ChatGPT and other generative AI systems raise unique ethical concerns such as bias, intergenerational justice, exploitation of labour and cultural diversity in Africa but also have significant ethical benefits. These ethical concerns and benefits are considered crucial in shaping the design and deployment of ChatGPT and similar technologies responsibly. It further explores the potential applications of ChatGPT in critical domain areas such as education, agriculture, and healthcare, thereby demonstrating the transformative possibilities that these technologies can have on Africa. This paper underscores the critical role of AI governance as Africa increasingly adopts ChatGPT and similar AI systems. It argues that a comprehensive understanding of AI governance is essential not only for maximising the benefits of generative AI systems but also for facilitating a global dialogue. This dialogue aims to foster shared knowledge and insights between the Global North and the Global South, which is important for the development and creation of inclusive and equitable AI policies and practices that can be beneficial for all regions. © 2024 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - August, E.T.
AU  - Anderson, O.S.
AU  - Laubepin, F.A.
TI  - Brave New Words: A Framework and Process for Developing Technology-Use Guidelines for Student Writing
PY  - 2024
T2  - Pedagogy in Health Promotion
VL  - 10
IS  - 3
SP  - 187
EP  - 196
DO  - 10.1177/23733799241235119
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186863363&doi=10.1177%2f23733799241235119&partnerID=40&md5=7f54c4116fd898546a5bbd6226e81e4a
AB  - Health sciences instructors hold a wide range of opinions about generative artificial intelligence (GenAI) such as ChatGPT, Bing, and Bard; however, many are uncertain about guiding students on how to use technology for assigned writing. Our survey of 62 public health instructors at a single institution highlighted their perceived benefits, limitations, and concerns about student use of GenAI for assigned writing. Perceived benefits included the completion of tasks unrelated to relevant learning such as spellchecking and reference formatting, as well as for certain writing activities such as brainstorming. Several identified the preparation for future workplace activities as a meaningful benefit. Important limitations and concerns included the worry that GenAI would inhibit learning, as well as ethical and equity-related concerns. Nearly half of instructors expressed concerns about whether using GenAI tools constitutes plagiarism or violates academic integrity. Nearly half of instructors also indicated concern about being able to detect whether a student completed an assignment with GenAI tools. Developing thoughtful guidance on technology use for assigned writing is important as it sets standards for academic integrity and supports learning. We used the survey data and applied backward design principles to develop the Brave New Words framework and three-step process described in this paper. This framework is intended to help instructors think through and ultimately develop guidelines for students on whether and how they should use technology for assigned writing. An example assignment and activity are used to demonstrate the framework. © 2024 The Author(s).
PB  - SAGE Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Patel, M.A.
AU  - Villalobos, F.
AU  - Shan, K.
AU  - Tardo, L.M.
AU  - Horton, L.A.
AU  - Sguigna, P.V.
AU  - Blackburn, K.M.
AU  - Munoz, S.B.
AU  - Moog, T.M.
AU  - Smith, A.D.
AU  - Burgess, K.W.
AU  - McCreary, M.
AU  - Okuda, D.T.
TI  - Generative artificial intelligence versus clinicians: Who diagnoses multiple sclerosis faster and with greater accuracy?
PY  - 2024
T2  - Multiple Sclerosis and Related Disorders
VL  - 90
C7  - 105791
DO  - 10.1016/j.msard.2024.105791
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201157509&doi=10.1016%2fj.msard.2024.105791&partnerID=40&md5=e2ee132351efe548223aa7b911630766
AB  - Background: Those receiving the diagnosis of multiple sclerosis (MS) over the next ten years will predominantly be part of Generation Z (Gen Z). Recent observations within our clinic suggest that younger people with MS utilize online generative artificial intelligence (AI) platforms for personalized medical advice prior to their first visit with a specialist in neuroimmunology. The use of such platforms is anticipated to increase given the technology driven nature, desire for instant communication, and cost-conscious nature of Gen Z. Our objective was to determine if ChatGPT (Generative Pre-trained Transformer) could diagnose MS in individuals earlier than their clinical timeline, and to assess if the accuracy differed based on age, sex, and race/ethnicity. Methods: People with MS between 18 and 59 years of age were studied. The clinical timeline for people diagnosed with MS was retrospectively identified and simulated using ChatGPT-3.5 (GPT-3.5). Chats were conducted using both actual and derivatives of their age, sex, and race/ethnicity to test diagnostic accuracy. A Kaplan-Meier survival curve was estimated for time to diagnosis, clustered by subject. The p-value testing for differences in time to diagnosis was accomplished using a general Wilcoxon test. Logistic regression (subject-specific intercept) was used to capture intra-subject correlation to test the accuracy prior to and after the inclusion of MRI data. Results: The study cohort included 100 unique people with MS. Of those, 50 were members of Gen Z (38 female; 22 White; mean age at first symptom was 20.6 years (y) (standard deviation (SD)=2.2y)), and 50 were non-Gen Z (34 female; 27 White; mean age at first symptom was 37.0y (SD=10.4y)). In addition, a total of 529 people that represented digital simulations of the original cohort of 100 people (333 female; 166 White; 136 Black/African American; 107 Asian; 120 Hispanic, mean age at first symptom was 31.6y (SD=12.4y)) were generated allowing for 629 scripted conversations to be analyzed. The estimated median time to diagnosis in clinic was significantly longer at 0.35y (95% CI=[0.28, 0.48]) versus that by ChatGPT at 0.08y (95% CI=[0.04, 0.24]) (p<0.0001). There was no difference in the diagnostic accuracy between ages and by race/ethnicity prior to the inclusion of MRI data. However, prior to including the MRI data, males had a 47% less likely chance of a correct diagnosis relative to females (p=0.05). Post-MRI data inclusion within GPT-3.5, the odds of an accurate diagnosis was 4.0-fold greater for Gen Z participants, relative to non-Gen Z participants (p=0.01) with the diagnostic accuracy being 68% less in males relative to females (p=0.009), and 75% less for White subjects, relative to non-White subjects (p=0.0004). Conclusion: Although generative AI platforms enable rapid information access and are not principally designed for use in healthcare, an increase in use by Gen Z is anticipated. However, the obtained responses may not be generalizable to all users and bias may exist in select groups. © 2024 Elsevier B.V.
PB  - Elsevier B.V.
C2  - 39146892
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Antoniak, M.
AU  - Naik, A.
AU  - Alvarado, C.S.
AU  - Wang, L.L.
AU  - Chen, I.Y.
TI  - NLP for Maternal Healthcare: Perspectives and Guiding Principles in the Age of LLMs
PY  - 2024
T2  - 2024 ACM Conference on Fairness, Accountability, and Transparency, FAccT 2024
SP  - 1446
EP  - 1463
DO  - 10.1145/3630106.3658982
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196661713&doi=10.1145%2f3630106.3658982&partnerID=40&md5=d8b3820f8d1616c8ed7e38ef56e0c833
AB  - Ethical frameworks for the use of natural language processing (NLP) are urgently needed to shape how large language models (LLMs) and similar tools are used for healthcare applications. Healthcare faces existing challenges including the balance of power in clinician-patient relationships, systemic health disparities, historical injustices, and economic constraints. Drawing directly from the voices of those most affected, and focusing on a case study of a specific healthcare setting, we propose a set of guiding principles for the use of NLP in maternal healthcare. We led an interactive session centered on an LLM-based chatbot demonstration during a full-day workshop with 39 participants, and additionally surveyed 30 healthcare workers and 30 birthing people about their values, needs, and perceptions of NLP tools in the context of maternal health. We conducted quantitative and qualitative analyses of the survey results and interactive discussions to consolidate our findings into a set of guiding principles. We propose nine principles for ethical use of NLP for maternal healthcare, grouped into three themes: (i) recognizing contextual significance (ii) holistic measurements, and (iii) who/what is valued. For each principle, we describe its underlying rationale and provide practical advice. This set of principles can provide a methodological pattern for other researchers and serve as a resource to practitioners working on maternal health and other healthcare fields to emphasize the importance of technical nuance, historical context, and inclusive design when developing NLP technologies for clinical use.  © 2024 Owner/Author.
PB  - Association for Computing Machinery, Inc
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Chen, A.
AU  - Chen, D.O.
AU  - Tian, L.
TI  - Benchmarking the symptom-checking capabilities of ChatGPT for a broad range of diseases
PY  - 2024
T2  - Journal of the American Medical Informatics Association
VL  - 31
IS  - 9
SP  - 2084
EP  - 2088
DO  - 10.1093/jamia/ocad245
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202054910&doi=10.1093%2fjamia%2focad245&partnerID=40&md5=f0bbffa73ccfdb6d371cc034669f2194
AB  - Objective: This study evaluates ChatGPT's symptom-checking accuracy across a broad range of diseases using the Mayo Clinic Symptom Checker patient service as a benchmark. Methods: We prompted ChatGPT with symptoms of 194 distinct diseases. By comparing its predictions with expectations, we calculated a relative comparative score (RCS) to gauge accuracy. Results: ChatGPT's GPT-4 model achieved an average RCS of 78.8%, outperforming the GPT-3.5-turbo by 10.5%. Some specialties scored above 90%. Discussion: The test set, although extensive, was not exhaustive. Future studies should include a more comprehensive disease spectrum. Conclusion: ChatGPT exhibits high accuracy in symptom checking for a broad range of diseases, showcasing its potential as a medical training tool in learning health systems to enhance care quality and address health disparities.  © 2023 The Author(s). Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved.
PB  - Oxford University Press
C2  - 38109889
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 9
ER  -

TY  - JOUR
AU  - Breazu, P.
AU  - Katsos, N.
TI  - ChatGPT-4 as a journalist: Whose perspectives is it reproducing?
PY  - 2024
T2  - Discourse and Society
VL  - 35
IS  - 6
SP  - 687
EP  - 707
DO  - 10.1177/09579265241251479
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193833672&doi=10.1177%2f09579265241251479&partnerID=40&md5=6711e6dfff15991ddace0be1727839ec
AB  - The rapid emergence of generative AI models in the media sector demands a critical examination of the narratives these models produce, particularly in relation to sensitive topics, such as politics, racism, immigration, public health, gender and violence, among others. The ease with which generative AI can produce narratives on sensitive topics raises concerns about potential harms, such as amplifying biases or spreading misinformation. Our study juxtaposes the content generated by a state-of-the-art generative AI, specifically ChatGPT-4, with actual articles from leading UK media outlets on the topic of immigration. Our specific case study focusses on the representation of Eastern European Roma migrants in the context of the 2016 UK Referendum on EU membership. Through a comparative critical discourse analysis, we uncover patterns of representation, inherent biases and potential discrepancies in representation between AI-generated narratives and mainstream media discourse with different political views. Preliminary findings suggest that ChatGPT-4 exhibits a remarkable degree of objectivity in its reporting and demonstrates heightened racial awareness in the content it produces. Moreover, it appears to consistently prioritise factual accuracy over sensationalism. All these features set it apart from right-wing media articles in our sample. This is further evidenced by the fact that, in most instances, ChatGPT-4 refrains from generating text or does so only after considerable adjustments when prompted with headlines that the model deems inflammatory. While these features can be attributed to the model’s diverse training data and model architecture, the findings invite further examination to determine the full scope of ChatGPT-4’s capabilities and its potential shortcomings in representing the full spectrum of social and political perspectives prevalent in society. © The Author(s) 2024.
PB  - SAGE Publications Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - Giorgi, S.
AU  - Isman, K.
AU  - Liu, T.
AU  - Fried, Z.
AU  - Sedoc, J.
AU  - Curtis, B.
TI  - Evaluating generative AI responses to real-world drug-related questions
PY  - 2024
T2  - Psychiatry Research
VL  - 339
C7  - 116058
DO  - 10.1016/j.psychres.2024.116058
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199437087&doi=10.1016%2fj.psychres.2024.116058&partnerID=40&md5=a583b5b575ca82babe3ed22f6a973cc5
AB  - Generative Artificial Intelligence (AI) systems such as OpenAI's ChatGPT, capable of an unprecedented ability to generate human-like text and converse in real time, hold potential for large-scale deployment in clinical settings such as substance use treatment. Treatment for substance use disorders (SUDs) is particularly high stakes, requiring evidence-based clinical treatment, mental health expertise, and peer support. Thus, promises of AI systems addressing deficient healthcare resources and structural bias are relevant within this domain, especially in an anonymous setting. This study explores the effectiveness of generative AI in answering real-world substance use and recovery questions. We collect questions from online recovery forums, use ChatGPT and Meta's LLaMA-2 for responses, and have SUD clinicians rate these AI responses. While clinicians rated the AI-generated responses as high quality, we discovered instances of dangerous disinformation, including disregard for suicidal ideation, incorrect emergency helplines, and endorsement of home detox. Moreover, the AI systems produced inconsistent advice depending on question phrasing. These findings indicate a risky mix of seemingly high-quality, accurate responses upon initial inspection that contain inaccurate and potentially deadly medical advice. Consequently, while generative AI shows promise, its real-world application in sensitive healthcare domains necessitates further safeguards and clinical validation. © 2024
PB  - Elsevier Ireland Ltd
C2  - 39059040
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Albuck, A.L.
AU  - Becnel, C.M.
AU  - Sirna, D.J.
AU  - Turner, J.
TI  - Precision of Chatbot Generative Pretrained Transformer Version 4-Generated References for Colon and Rectal Surgical Literature
PY  - 2024
T2  - Journal of Surgical Research
VL  - 302
SP  - 324
EP  - 328
DO  - 10.1016/j.jss.2024.07.021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200768590&doi=10.1016%2fj.jss.2024.07.021&partnerID=40&md5=3129ab54bdb4eb50c08588ae3c8e08aa
AB  - Introduction: The TUSOM ONR STEM-IMPRESS Program sees a future where everyone is given opportunities and resources to become a physician regardless of race, gender, sexual orientation, social economic status, or any other differences in the shared learning environment. We strive to achieve this vision by EMPOWERING all college freshmen and sophomores to make the decision to enter the medical field; ENRICHING them with a well-rounded program that includes clinical rotations, research projects, MCAT preparations, interview tips, and panel sessions; and ENGAGING with them by establishing career-long mentorship. Program values: Equity—IMPRESS offers services to those who would otherwise not have access to opportunities and exposure to healthcare careers. Diversity—IMPRESS recognizes the importance of diversity in medicine including race, gender, sexual orientation, social economic status, ideology, or any other differences. Inclusion—IMPRESS values the uniqueness of each individual viewpoint, experience, and their combined contribution to a more inclusive environment in medicine. Culture—IMPRESS promotes a culture in medicine that reflects the richness and vitality in the culture of New Orleans and the surrounding area. The objective is to assess the precision of references generated by Chatbot Generative Pretrained Transformer version 4 (ChatGPT-4) in scientific literature pertaining to colon and rectal surgery. Methods: Ten frequently studied keywords pertaining to colon and rectal surgery were chosen: colon cancer, rectal cancer, anal cancer, total neoadjuvant therapy, diverticulitis, low anterior resection, transanal minimally invasive surgery, ileal pouch anal anastomosis, abdominoperineal resection, and hemorrhoidectomy. ChatGPT-4 was prompted to search for the most representative citations for all keywords. After this, two separate evaluators meticulously examined the outcomes each key element, awarding full accuracy to generated citations in which there was no discrepancies in any of the fields when cross-referenced with the Scopus, Google, and PubMed databases. References from ChatGPT-4 underwent a thorough review process, which involved careful examination of key elements such as the article title, authors, journal name, publication year, and Digital Object Identifier (DOI). Results: Forty-one of the 100 references generated by were fully accurate; however, but none included a DOI. Partial accuracy was observed in 67 of the references, which were identifiable by title and journal. Performance varied across specific keywords; for example, references for colon and rectal cancer were 100% identifiable by title and journal, but no term had 100% accuracy across all categories. Notably, none of the generated references correctly listed all authors. Conducted within a short timeframe during which ChatGPT4 is rapidly evolving and updating its knowledge base. Conclusions: While ChatGPT-4 offers improvements over its predecessors and shows potential for use in academic literature, its inconsistent performance across categories, lack of DOIs, and irregularities in authorship listings raise concerns about its readiness for application in the field of colon and rectal surgery research. © 2024 Elsevier Inc.
PB  - Academic Press Inc.
C2  - 39121800
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Hadar-Shoval, D.
AU  - Asraf, K.
AU  - Shinan-Altman, S.
AU  - Elyoseph, Z.
AU  - Levkovich, I.
TI  - Embedded values-like shape ethical reasoning of large language models on primary care ethical dilemmas
PY  - 2024
T2  - Heliyon
VL  - 10
IS  - 18
C7  - e38056
DO  - 10.1016/j.heliyon.2024.e38056
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204450486&doi=10.1016%2fj.heliyon.2024.e38056&partnerID=40&md5=ab0681427c02c2280d950ad20bca43a7
AB  - Objective: This article uses the framework of Schwartz's values theory to examine whether the embedded values-like profile within large language models (LLMs) impact ethical decision-making dilemmas faced by primary care. It specifically aims to evaluate whether each LLM exhibits a distinct values-like profile, assess its alignment with general population values, and determine whether latent values influence clinical recommendations. Methods: The Portrait Values Questionnaire-Revised (PVQ-RR) was submitted to each LLM (Claude, Bard, GPT-3.5, and GPT-4) 20 times to ensure reliable and valid responses. Their responses were compared to a benchmark derived from a diverse international sample consisting of over 53,000 culturally diverse respondents who completed the PVQ-RR. Four vignettes depicting prototypical professional quandaries involving conflicts between competing values were presented to the LLMs. The option selected by each LLM and the strength of its recommendation were evaluated to determine if underlying values-like impact output. Results: Each LLM demonstrated a unique values-like profile. Universalism and self-direction were prioritized, while power and tradition were assigned less importance than population benchmarks, suggesting potential Western-centric biases. Four clinical vignettes involving value conflicts were presented to the LLMs. Preliminary indications suggested that embedded values-like influence recommendations. Significant variances in confidence strength regarding chosen recommendations materialized between models, proposing that further vetting is required before the LLMs can be relied on as judgment aids. However, the overall selection of preferences aligned with intrinsic value hierarchies. Conclusion: The distinct intrinsic values-like embedded within LLMs shape ethical decision-making, which carries implications for their integration in primary care settings serving diverse populations. For context-appropriate, equitable delivery of AI-assisted healthcare globally it is essential that LLMs are tailored to align with cultural outlooks. © 2024
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Fiori, M.
AU  - Civitarese, G.
AU  - Bettini, C.
TI  - Using Large Language Models to Compare Explainable Models for Smart Home Human Activity Recognition
PY  - 2024
T2  - UbiComp Companion 2024 - Companion of the 2024 ACM International Joint Conference on Pervasive and Ubiquitous Computing
SP  - 881
EP  - 884
DO  - 10.1145/3675094.3679000
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206141987&doi=10.1145%2f3675094.3679000&partnerID=40&md5=d31b605f3720b9f0b906fcc45005ac7a
AB  - Recognizing daily activities with unobtrusive sensors in smart environments enables various healthcare applications. Monitoring how subjects perform activities at home and their changes over time can reveal early symptoms of health issues, such as cognitive decline. Most approaches in this field use deep learning models, which are often seen as black boxes mapping sensor data to activities. However, non-expert users like clinicians need to trust and understand these models' outputs. Thus, eXplainable AI (XAI) methods for Human Activity Recognition have emerged to provide intuitive natural language explanations from these models. Different XAI methods generate different explanations, and their effectiveness is typically evaluated through user surveys, that are often challenging in terms of costs and fairness. This paper proposes an automatic evaluation method using Large Language Models (LLMs) to identify, in a pool of candidates, the best XAI approach for non-expert users. Our preliminary results suggest that LLM evaluation aligns with user surveys. © 2024 Copyright held by the owner/author(s).
PB  - Association for Computing Machinery, Inc
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Wolfe, R.
AU  - Mitra, T.
TI  - The Impact and Opportunities of Generative AI in Fact-Checking
PY  - 2024
T2  - 2024 ACM Conference on Fairness, Accountability, and Transparency, FAccT 2024
SP  - 1531
EP  - 1543
DO  - 10.1145/3630106.3658987
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196616049&doi=10.1145%2f3630106.3658987&partnerID=40&md5=e82e515931338ee26bd6d5e9251e2d62
AB  - Generative AI appears poised to transform white collar professions, with more than 90% of Fortune 500 companies using OpenAI's flagship GPT models, which have been characterized as "general purpose technologies"capable of effecting epochal changes in the economy. But how will such technologies impact organizations whose job is to verify and report factual information, and to ensure the health of the information ecosystem? To investigate this question, we conducted 30 interviews with N=38 participants working at 29 fact-checking organizations across six continents, asking about how they use generative AI and the opportunities and challenges they see in the technology. We found that uses of generative AI envisioned by fact-checkers differ based on organizational infrastructure, with applications for quality assurance in Editing, for trend analysis in Investigation, and for information literacy in Advocacy. We used the TOE framework to describe participant concerns ranging from the Technological (lack of transparency), to the Organizational (resource constraints), to the Environmental (uncertain and evolving policy). Building on the insights of our participants, we describe value tensions between fact-checking and generative AI, and propose a novel Verification dimension to the design space of generative models for information verification work. Finally, we outline an agenda for fairness, accountability, and transparency research to support the responsible use of generative AI in fact-checking. Throughout, we highlight the importance of human infrastructure and labor in producing verified information in collaboration with AI. We expect that this work will inform not only the scientific literature on fact-checking, but also contribute to understanding of organizational adaptation to a powerful but unreliable new technology.  © 2024 Owner/Author.
PB  - Association for Computing Machinery, Inc
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Ott, K.
AU  - Cepeda, S.
AU  - Hartmann, D.
AU  - Kramer, F.
AU  - Müller, D.
TI  - Predicting Overall Survival of Glioblastoma Patients Using Deep Learning Classification Based on MRIs
PY  - 2024
T2  - Studies in Health Technology and Informatics
VL  - 317
SP  - 356
EP  - 365
DO  - 10.3233/SHTI240878
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203334370&doi=10.3233%2fSHTI240878&partnerID=40&md5=ef42bea6c7bf30b8a0aec2271234ce5b
AB  - Introduction Glioblastoma (GB) is one of the most aggressive tumors of the brain. Despite intensive treatment, the average overall survival (OS) is 15-18 months. Therefore, it is helpful to be able to assess a patient's OS to tailor treatment more specifically to the course of the disease. Automated analysis of routinely generated MRI sequences (FLAIR, T1, T1CE, and T2) using deep learning-based image classification has the potential to enable accurate OS predictions. Methods In this work, a method was developed and evaluated that classifies the OS into three classes - “short”, “medium” and “long”. For this purpose, the four MRI sequences of a person were corrected using bias-field correction and merged into one image. The pipeline was realized by a bagging model using 5-fold cross-validation and the ResNet50 architecture. Results The best model was able to achieve an F1-score of 0.51 and an accuracy of 0.67. In addition, this work enabled a largely clear differentiation of the “short” and “long” classes, which offers high clinical significance as decision support. Conclusion Automated analysis of MRI scans using deep learning-based image classification has the potential to enable accurate OS prediction in glioblastomas. © 2024 The Authors.
PB  - IOS Press BV
C2  - 39234740
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Araújo, R.
AU  - Ramalhete, L.
AU  - Viegas, A.
AU  - Von Rekowski, C.P.
AU  - Fonseca, T.A.H.
AU  - Calado, C.R.C.
AU  - Bento, L.
TI  - Simplifying Data Analysis in Biomedical Research: An Automated, User-Friendly Tool
PY  - 2024
T2  - Methods and Protocols
VL  - 7
IS  - 3
C7  - 36
DO  - 10.3390/mps7030036
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197176729&doi=10.3390%2fmps7030036&partnerID=40&md5=ff986c5a8016b0fb690f51df4ecd5355
AB  - Robust data normalization and analysis are pivotal in biomedical research to ensure that observed differences in populations are directly attributable to the target variable, rather than disparities between control and study groups. ArsHive addresses this challenge using advanced algorithms to normalize populations (e.g., control and study groups) and perform statistical evaluations between demographic, clinical, and other variables within biomedical datasets, resulting in more balanced and unbiased analyses. The tool’s functionality extends to comprehensive data reporting, which elucidates the effects of data processing, while maintaining dataset integrity. Additionally, ArsHive is complemented by A.D.A. (Autonomous Digital Assistant), which employs OpenAI’s GPT-4 model to assist researchers with inquiries, enhancing the decision-making process. In this proof-of-concept study, we tested ArsHive on three different datasets derived from proprietary data, demonstrating its effectiveness in managing complex clinical and therapeutic information and highlighting its versatility for diverse research fields. © 2024 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - King, R.C.
AU  - Samaan, J.S.
AU  - Yeo, Y.H.
AU  - Mody, B.
AU  - Lombardo, D.M.
AU  - Ghashghaei, R.
TI  - Appropriateness of ChatGPT in Answering Heart Failure Related Questions
PY  - 2024
T2  - Heart Lung and Circulation
VL  - 33
IS  - 9
SP  - 1314
EP  - 1318
DO  - 10.1016/j.hlc.2024.03.005
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194708028&doi=10.1016%2fj.hlc.2024.03.005&partnerID=40&md5=b06f2f220f5210d84926398d77efae79
AB  - Background: Heart failure requires complex management, and increased patient knowledge has been shown to improve outcomes. This study assessed the knowledge of Chat Generative Pre-trained Transformer (ChatGPT) and its appropriateness as a supplemental resource of information for patients with heart failure. Method: A total of 107 frequently asked heart failure-related questions were included in 3 categories: “basic knowledge” (49), “management” (41) and “other” (17). Two responses per question were generated using both GPT-3.5 and GPT-4 (i.e., two responses per question per model). The accuracy and reproducibility of responses were graded by two reviewers, board-certified in cardiology, with differences resolved by a third reviewer, board-certified in cardiology and advanced heart failure. Accuracy was graded using a four-point scale: (1) comprehensive, (2) correct but inadequate, (3) some correct and some incorrect, and (4) completely incorrect. Results: GPT-4 provided 107/107 (100%) responses with correct information. Further, GPT-4 displayed a greater proportion of comprehensive knowledge for the categories of “basic knowledge” and “management” (89.8% and 82.9%, respectively). For GPT-3, there were two total responses (1.9%) graded as “some correct and incorrect” for GPT-3.5, while no “completely incorrect” responses were produced. With respect to comprehensive knowledge, GPT-3.5 performed best in the “management” category and “other” category (prognosis, procedures, and support) (78.1%, 94.1%). The models also provided highly reproducible responses, with GPT-3.5 scoring above 94% in every category and GPT-4 with 100% for all answers. Conclusions: GPT-3.5 and GPT-4 answered the majority of heart failure-related questions accurately and reliably. If validated in future studies, ChatGPT may serve as a useful tool in the future by providing accessible health-related information and education to patients living with heart failure. In its current state, ChatGPT necessitates further rigorous testing and validation to ensure patient safety and equity across all patient demographics. © 2024 The Author(s)
PB  - Elsevier Ltd
C2  - 38821760
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 13
ER  -

TY  - CONF
AU  - Devi, A.
AU  - Uttrani, S.
AU  - Singla, A.
AU  - Jha, S.
AU  - Dasgupta, N.
AU  - Natarajan, S.
AU  - Punekar, R.S.
AU  - Pickett, L.A.
AU  - Dutt, V.
TI  - Automating Clinical Trial Eligibility Screening: Quantitative Analysis of GPT Models versus Human Expertise
PY  - 2024
T2  - ACM International Conference Proceeding Series
SP  - 626
EP  - 632
DO  - 10.1145/3652037.3663922
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198053910&doi=10.1145%2f3652037.3663922&partnerID=40&md5=aafdf6707c1031f98e079dc032ee129b
AB  - Objective: This study quantitatively assesses the performance of GPT model in classifying patient eligibility for clinical trials, aiming to minimize the need for expert clinical judgment and to cut down on related expenses. Data: Ten US NSCLC drug-only interventional clinical trials were selected from clinicaltrials.gov. For each clinical trial, 10 patient profiles were manually created by an epidemiologist using case presentations from medical journals. The dataset included two sets of adult patient profiles (50 eligible patients and 50 non-eligible patients for 100 patients) with a range of complexities, from complex to simple cases. The 100-case dataset was then analyzed in a GPT-3.5-turbo large language model to generate eligibility predictions against human epidemiologists. Analysis: Different data tuning scenarios were evaluated, focusing on the model's ability to replicate the human expert performance in determining patient eligibility. The tuning scenarios included no cases (zero-shot), 50 cases, and 80 cases. Results: GPT-3.5 showed a high accuracy rate during the test, with 95% test accuracy in scenarios without tuning and 100% test accuracy in scenarios with 80-case tuning (testing was done on the remaining 20 cases). GPT-3.5's accuracy showed more variability, scoring 82% test accuracy with tuning on 50 cases and dropping to 79% test accuracy without tuning on 100 cases. Comparisons of model and human evaluations across male and female patient profiles indicated no gender bias, as the GPT-3.5 model performed equivalently to human assessments. Conclusion: The GPT-3.5 models demonstrated a high degree of accuracy and an unbiased approach to patient classification when compared to human experts. These results suggest that GPT models could serve as a cost-effective and impartial tool for patient screening in clinical trials. Further research with larger and more diverse datasets is recommended to confirm these findings and explore LLMs' scalability in clinical trial settings.  © 2024 ACM.
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Wolfe, R.
AU  - Slaughter, I.
AU  - Han, B.
AU  - Wen, B.
AU  - Yang, Y.
AU  - Rosenblatt, L.
AU  - Herman, B.
AU  - Brown, E.
AU  - Qu, Z.
AU  - Weber, N.
AU  - Howe, B.
TI  - Laboratory-Scale AI: Open-Weight Models are Competitive with ChatGPT Even in Low-Resource Settings
PY  - 2024
T2  - 2024 ACM Conference on Fairness, Accountability, and Transparency, FAccT 2024
SP  - 1199
EP  - 1210
DO  - 10.1145/3630106.3658966
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196652584&doi=10.1145%2f3630106.3658966&partnerID=40&md5=b9e143cdb75c512e198e22b5e46d5f5a
AB  - The rapid proliferation of generative AI has raised questions about the competitiveness of lower-parameter, locally tunable, open-weight models relative to high-parameter, API-guarded, closed-weight models in terms of performance, domain adaptation, cost, and generalization. Centering under-resourced yet risk-intolerant settings in government, research, and healthcare, we see for-profit closed-weight models as incompatible with requirements for transparency, privacy, adaptability, and standards of evidence. Yet the performance penalty in using open-weight models, especially in low-data and low-resource settings, is unclear. We assess the feasibility of using smaller, open-weight models to replace GPT-4-Turbo in zero-shot, few-shot, and fine-tuned regimes, assuming access to only a single, low-cost GPU. We assess value-sensitive issues around bias, privacy, and abstention on three additional tasks relevant to those topics. We find that with relatively low effort, very low absolute monetary cost, and relatively little data for fine-tuning, small open-weight models can achieve competitive performance in domain-adapted tasks without sacrificing generality. We then run experiments considering practical issues in bias, privacy, and hallucination risk, finding that open models offer several benefits over closed models. We intend this work as a case study in understanding the opportunity cost of reproducibility and transparency over for-profit state-of-the-art zero shot performance, finding this cost to be marginal under realistic settings.  © 2024 Owner/Author.
PB  - Association for Computing Machinery, Inc
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Elsamahy, E.A.
AU  - Ahmed, A.E.
AU  - Shoala, T.
AU  - Maghraby, F.A.
TI  - Deep-GenMut: Automated genetic mutation classification in oncology: A deep learning comparative study
PY  - 2024
T2  - Heliyon
VL  - 10
IS  - 11
C7  - e32279
DO  - 10.1016/j.heliyon.2024.e32279
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194876227&doi=10.1016%2fj.heliyon.2024.e32279&partnerID=40&md5=ec5ef32987ccd69c9f76cc34afe49d0c
AB  - Early cancer detection and treatment depend on the discovery of specific genes that cause cancer. The classification of genetic mutations was initially done manually. However, this process relies on pathologists and can be a time-consuming task. Therefore, to improve the precision of clinical interpretation, researchers have developed computational algorithms that leverage next-generation sequencing technologies for automated mutation analysis. This paper utilized four deep learning classification models with training collections of biomedical texts. These models comprise bidirectional encoder representations from transformers for Biomedical text mining (BioBERT), a specialized language model implemented for biological contexts. Impressive results in multiple tasks, including text classification, language inference, and question answering, can be obtained by simply adding an extra layer to the BioBERT model. Moreover, bidirectional encoder representations from transformers (BERT), long short-term memory (LSTM), and bidirectional LSTM (BiLSTM) have been leveraged to produce very good results in categorizing genetic mutations based on textual evidence. The dataset used in the work was created by Memorial Sloan Kettering Cancer Center (MSKCC), which contains several mutations. Furthermore, this dataset poses a major classification challenge in the Kaggle research prediction competitions. In carrying out the work, three challenges were identified: enormous text length, biased representation of the data, and repeated data instances. Based on the commonly used evaluation metrics, the experimental results show that the BioBERT model outperforms other models with an F1 score of 0.87 and 0.850 MCC, which can be considered as improved performance compared to similar results in the literature that have an F1 score of 0.70 achieved with the BERT model. © 2024 The Authors
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Luo, S.
AU  - Canavese, F.
AU  - Aroojis, A.
AU  - Andreacchio, A.
AU  - Anticevic, D.
AU  - Bouchard, M.
AU  - Castaneda, P.
AU  - De Rosa, V.
AU  - Fiogbe, M.A.
AU  - Frick, S.L.
AU  - Hui, J.H.
AU  - Johari, A.N.
AU  - Loro, A.
AU  - Lyu, X.
AU  - Matsushita, M.
AU  - Omeroglu, H.
AU  - Roye, D.P.
AU  - Shah, M.M.
AU  - Yong, B.
AU  - Li, L.
TI  - Are Generative Pretrained Transformer 4 Responses to Developmental Dysplasia of the Hip Clinical Scenarios Universal? An International Review
PY  - 2024
T2  - Journal of Pediatric Orthopaedics
VL  - 44
IS  - 6
SP  - e504
EP  - e511
DO  - 10.1097/BPO.0000000000002682
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195327974&doi=10.1097%2fBPO.0000000000002682&partnerID=40&md5=bc8659108a6858ee0c5d11f60e38b379
AB  - Objective: There is increasing interest in applying artificial intelligence chatbots like generative pretrained transformer 4 (GPT-4) in the medical field. This study aimed to explore the universality of GPT-4 responses to simulated clinical scenarios of developmental dysplasia of the hip (DDH) across diverse global settings. Methods: Seventeen international experts with more than 15 years of experience in pediatric orthopaedics were selected for the evaluation panel. Eight simulated DDH clinical scenarios were created, covering 4 key areas: (1) initial evaluation and diagnosis, (2) initial examination and treatment, (3) nursing care and follow-up, and (4) prognosis and rehabilitation planning. Each scenario was completed independently in a new GPT-4 session. Interrater reliability was assessed using Fleiss kappa, and the quality, relevance, and applicability of GPT-4 responses were analyzed using median scores and interquartile ranges. Following scoring, experts met in ZOOM sessions to generate Regional Consensus Assessment Scores, which were intended to represent a consistent regional assessment of the use of the GPT-4 in pediatric orthopaedic care. Results: GPT-4's responses to the 8 clinical DDH scenarios received performance scores ranging from 44.3% to 98.9% of the 88-point maximum. The Fleiss kappa statistic of 0.113 (P = 0.001) indicated low agreement among experts in their ratings. When assessing the responses' quality, relevance, and applicability, the median scores were 3, with interquartile ranges of 3 to 4, 3 to 4, and 2 to 3, respectively. Significant differences were noted in the prognosis and rehabilitation domain scores (P < 0.05 for all). Regional consensus scores were 75 for Africa, 74 for Asia, 73 for India, 80 for Europe, and 65 for North America, with the Kruskal-Wallis test highlighting significant disparities between these regions (P = 0.034). Conclusions: This study demonstrates the promise of GPT-4 in pediatric orthopaedic care, particularly in supporting preliminary DDH assessments and guiding treatment strategies for specialist care. However, effective integration of GPT-4 into clinical practice will require adaptation to specific regional health care contexts, highlighting the importance of a nuanced approach to health technology adaptation. Level of Evidence: Level IV. © 2024 Lippincott Williams and Wilkins. All rights reserved.
PB  - Lippincott Williams and Wilkins
C2  - 38597198
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Zhang, L.
AU  - Liu, M.
AU  - Wang, L.
AU  - Zhang, Y.
AU  - Xu, X.
AU  - Pan, Z.
AU  - Feng, Y.
AU  - Zhao, J.
AU  - Zhang, L.
AU  - Yao, G.
AU  - Chen, X.
AU  - Xie, X.
TI  - Constructing a Large Language Model to Generate Impressions from Findings in Radiology Reports
PY  - 2024
T2  - Radiology
VL  - 312
IS  - 3
C7  - 240885
DO  - 10.1148/radiol.240885
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204418899&doi=10.1148%2fradiol.240885&partnerID=40&md5=01de64de0496657911ef10e91c04bc89
AB  - Background: The specialization and complexity of radiology makes the automatic generation of radiologic impressions (ie, a diagnosis with differential diagnosis and management recommendations) challenging. Purpose: To develop a large language model (LLM) that generates impressions based on imaging findings and to evaluate its performance in professional and linguistic dimensions. Materials and Methods: Six radiologists recorded imaging examination findings from August 2 to 31, 2023, at Shanghai General Hospital and used the developed LLM before routinely writing report impressions for multiple radiologic modalities (CT, MRI, radiography, mammography) and anatomic sites (cranium and face, neck, chest, upper abdomen, lower abdomen, vessels, bone and joint, spine, breast), making necessary corrections and completing the radiologic impression. A subset was defined to investigate cases where the LLM-generated impressions differed from the final radiologist impressions by excluding identical and highly similar cases. An expert panel scored the LLM-generated impressions on a five-point Likert scale (5 = strongly agree) based on scientific terminology, coherence, specific diagnosis, differential diagnosis, management recommendations, correctness, comprehensiveness, harmlessness, and lack of bias. Results: In this retrospective study, an LLM was pretrained using 20 GB of medical and general-purpose text data. The fine-tuning data set comprised 1.5 GB of data, including 800 radiology reports with paired instructions (describing the output task in natural language) and outputs. Test set 2 included data from 3988 patients (median age, 56 years [IQR, 40-68 years]; 2159 male). The median recall, precision, and F1 score of LLM-generated impressions were 0.775 (IQR, 0.56-1), 0.84 (IQR, 0.611-1), and 0.772 (IQR, 0.578-0.957), respectively, using the final impressions as the reference standard. In a subset of 1014 patients (median age, 57 years [IQR, 42-69 years]; 528 male), the overall median expert panel score for LLM-generated impressions was 5 (IQR, 5-5), ranging from 4 (IQR, 3-5) to 5 (IQR, 5-5). Conclusion: The developed LLM generated radiologic impressions that were professionally and linguistically appropriate for a full spectrum of radiology examinations.  © RSNA, 2024.
PB  - Radiological Society of North America Inc.
C2  - 39287525
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Wu, J.
AU  - Wu, X.
AU  - Qiu, Z.
AU  - Li, M.
AU  - Lin, S.
AU  - Zhang, Y.
AU  - Zheng, Y.
AU  - Yuan, C.
AU  - Yang, J.
TI  - Large language models leverage external knowledge to extend clinical insight beyond language boundaries
PY  - 2024
T2  - Journal of the American Medical Informatics Association
VL  - 31
IS  - 9
SP  - 2054
EP  - 2064
DO  - 10.1093/jamia/ocae079
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202016169&doi=10.1093%2fjamia%2focae079&partnerID=40&md5=943dda24016db1a2e32bfd3077737925
AB  - Objectives: Large Language Models (LLMs) such as ChatGPT and Med-PaLM have excelled in various medical question-answering tasks. However, these English-centric models encounter challenges in non-English clinical settings, primarily due to limited clinical knowledge in respective languages, a consequence of imbalanced training corpora. We systematically evaluate LLMs in the Chinese medical context and develop a novel in-context learning framework to enhance their performance. Materials and Methods: The latest China National Medical Licensing Examination (CNMLE-2022) served as the benchmark. We collected 53 medical books and 381 149 medical questions to construct the medical knowledge base and question bank. The proposed Knowledge and Few-shot Enhancement In-context Learning (KFE) framework leverages the in-context learning ability of LLMs to integrate diverse external clinical knowledge sources. We evaluated KFE with ChatGPT (GPT-3.5), GPT-4, Baichuan2-7B, Baichuan2-13B, and QWEN-72B in CNMLE-2022 and further investigated the effectiveness of different pathways for incorporating LLMs with medical knowledge from 7 distinct perspectives. Results: Directly applying ChatGPT failed to qualify for the CNMLE-2022 at a score of 51. Cooperated with the KFE framework, the LLMs with varying sizes yielded consistent and significant improvements. The ChatGPT's performance surged to 70.04 and GPT-4 achieved the highest score of 82.59. This surpasses the qualification threshold (60) and exceeds the average human score of 68.70, affirming the effectiveness and robustness of the framework. It also enabled a smaller Baichuan2-13B to pass the examination, showcasing the great potential in low-resource settings. Discussion and Conclusion: This study shed light on the optimal practices to enhance the capabilities of LLMs in non-English medical scenarios. By synergizing medical knowledge through in-context learning, LLMs can extend clinical insight beyond language barriers in healthcare, significantly reducing language-related disparities of LLM applications and ensuring global benefit in this field. © 2024 The Author(s). Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved.
PB  - Oxford University Press
C2  - 38684792
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Gabriel, R.A.
AU  - Litake, O.
AU  - Simpson, S.
AU  - Burton, B.N.
AU  - Waterman, R.S.
AU  - Macias, A.A.
TI  - On the development and validation of large language model-based classifiers for identifying social determinants of health
PY  - 2024
T2  - Proceedings of the National Academy of Sciences of the United States of America
VL  - 121
IS  - 39
C7  - e2320716121
DO  - 10.1073/pnas.2320716121
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204418498&doi=10.1073%2fpnas.2320716121&partnerID=40&md5=328799cd144a7c447f46f1467054e4e4
AB  - The assessment of social determinants of health (SDoH) within healthcare systems is crucial for comprehensive patient care and addressing health disparities. Current challenges arise from the limited inclusion of structured SDoH information within electronic health record (EHR) systems, often due to the lack of standardized diagnosis codes. This study delves into the transformative potential of large language models (LLM) to overcome these challenges. LLM-based classifiers—using Bidirectional Encoder Representations from Transformers (BERT) and A Robustly Optimized BERT Pretraining Approach (RoBERTa)—were developed for SDoH concepts, including homelessness, food insecurity, and domestic violence, using synthetic training datasets generated by generative pre-trained transformers combined with authentic clinical notes. Models were then validated on separate datasets: Medical Information Mart for Intensive Care-III and our institutional EHR data. When training the model with a combination of synthetic and authentic notes, validation on our institutional dataset yielded an area under the receiver operating characteristics curve of 0.78 for detecting homelessness, 0.72 for detecting food insecurity, and 0.83 for detecting domestic violence. This study underscores the potential of LLMs in extracting SDoH information from clinical text. Automated detection of SDoH may be instrumental for healthcare providers in identifying at-risk patients, guiding targeted interventions, and contributing to population health initiatives aimed at mitigating disparities. Copyright © 2024 the Author(s). Published by PNAS. This open access article is distributed under Creative Commons Attribution License 4.0 (CC BY).
PB  - National Academy of Sciences
C2  - 39284061
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Farmer, H.
AU  - Kreiner, K.
AU  - Schütz, T.
AU  - Pölzl, G.
AU  - Puelacher, C.
AU  - Schreier, G.
TI  - The Evolution of Telehealth in Heart Failure Management: The Role of Large Language Models and HerzMobil as a Potential Use Case
PY  - 2024
T2  - Studies in Health Technology and Informatics
VL  - 313
SP  - 228
EP  - 233
DO  - 10.3233/SHTI240042
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191718543&doi=10.3233%2fSHTI240042&partnerID=40&md5=29b3d429352a6fea20ec5523414dcd2e
AB  - The burgeoning domain of telehealth has witnessed substantial transformation through the advent of advanced technologies such as Large Language Models (LLMs). This study examines the integration of LLMs in heart failure management, with a focus on HerzMobil as a pioneering telehealth program. The technical underpinnings of LLMs, their current applications in the medical field, and their potential to enhance telehealth services, have been explored. The paper highlights the benefits of LLMs in patient interaction, clinical documentation, and decision-making processes. Through the HerzMobil case study, improvements in patient self-management and reductions in hospital readmission rates have been observed, showcasing the successful application of telehealth in chronic disease management. The paper also delves into the challenges and ethical considerations of LLM integration, such as data privacy, potential biases, and regulatory compliance, underscoring the need for a balanced approach that prioritizes patient safety and ethical standards.  © 2024 The Authors.
PB  - IOS Press BV
C2  - 38682535
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Lee, S.-Y.
AU  - Alzeen, M.
AU  - Ahmed, A.
TI  - Estimation of racial and language disparities in pediatric emergency department triage using statistical modeling and natural language processing
PY  - 2024
T2  - Journal of the American Medical Informatics Association
VL  - 31
IS  - 4
SP  - 958
EP  - 967
DO  - 10.1093/jamia/ocae018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189763128&doi=10.1093%2fjamia%2focae018&partnerID=40&md5=dce367a544f285f95d3f1be282c5f1fe
AB  - Objectives: The study aims to assess racial and language disparities in pediatric emergency department (ED) triage using analytical techniques and provide insights into the extent and nature of the disparities in the ED setting. Materials and Methods: The study analyzed a cross-sectional dataset encompassing ED visits from January 2019 to April 2021. The study utilized analytical techniques, including K-mean clustering (KNN), multivariate adaptive regression splines (MARS), and natural language processing (NLP) embedding. NLP embedding and KNN were employed to handle the chief complaints and categorize them into clusters, while the MARS was used to identify significant interactions among the clinical features. The study also explored important variables, including age-adjusted vital signs. Multiple logistic regression models with varying specifications were developed to assess the robustness of analysis results. Results: The study consistently found that non-White children, especially African American (AA) and Hispanic, were often under-triaged, with AA children having >2 times higher odds of receiving lower acuity scores compared to White children. While the results are generally consistent, incorporating relevant variables modified the results for specific patient groups (eg, Asians). Discussion: By employing a comprehensive analysis methodology, the study checked the robustness of the analysis results on racial and language disparities in pediatric ED triage. The study also recognized the significance of analytical techniques in assessing pediatric health conditions and analyzing disparities. Conclusion: The study's findings highlight the significant need for equal and fair assessment and treatment in the pediatric ED, regardless of their patients' race and language.  © 2024 The Author(s). Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved.
PB  - Oxford University Press
C2  - 38349846
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Victor, B.G.
AU  - McNally, K.
AU  - Qi, Z.
AU  - Perron, B.E.
TI  - Construct-Irrelevant Variance on the ASWB Clinical Social Work Licensing Exam: A Replication of Prior Validity Concerns
PY  - 2024
T2  - Research on Social Work Practice
VL  - 34
IS  - 2
SP  - 217
EP  - 221
DO  - 10.1177/10497315231188305
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165304952&doi=10.1177%2f10497315231188305&partnerID=40&md5=b6761b41b0b1aa77326a70389cdb632b
AB  - Purpose: This study sought to replicate a previous investigation of construct-irrelevant variance on the Association of Social Work Boards (ASWB) clinical licensing exam completed by Albright and Thyer over a decade ago. Method: The performance of ChatGPT was assessed on a modified version of 50 newly developed clinical exam questions currently distributed by the ASWB, where only the four multiple-choice options for each item were presented without the question. Results: ChatGPT achieved an average accuracy rate of 73.3% across three rounds of testing, providing strong evidence of construct-irrelevant variance. Discussion: These results raise concerns about the construct validity of the clinical exam and emphasize the need for reassessment of its structure and content to ensure fairness and accuracy. Based on the findings, state legislators and regulators are encouraged to temporarily discontinue the use of the ASWB exam in the clinical licensure process until its validity flaws are resolved. © The Author(s) 2023.
PB  - SAGE Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Patias, I.
AU  - Miteva, D.
AU  - Peltekova, E.
AU  - Wright, M.
AU  - Gasteiger-Klicpera, B.
TI  - Leveraging Large Language Models to Enhance Mental Health Literacy and Diversity Awareness in Adolescents: The me_HeLi-D Project
PY  - 2024
T2  - 8th International Symposium on Innovative Approaches in Smart Technologies, ISAS 2024 - Proceedings
DO  - 10.1109/ISAS64331.2024.10845582
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217992498&doi=10.1109%2fISAS64331.2024.10845582&partnerID=40&md5=3b6c92add072a878e2f4f9430ebbdfca
AB  - The Erasmus+ project me_HeLi-D (mental health literacy and diversity) aims to promote mental health literacy (MHL) and diversity awareness among adolescents aged 12-15 through a digital program designed to enhance well-being, resilience, and emotional regulation. This paper explores the integration of Large Language Models (LLMs) within the me_HeLi-D project, highlighting their potential to support mental health education and intervention. LLMs, such as GPT4, offer scalable, personalized, and contextually relevant support, making them valuable tools in digital mental health initiatives. The paper discusses the benefits and challenges of using LLMs, including their ability to provide immediate, evidence-based responses and the ethical considerations surrounding data privacy and bias. LLMs can deliver tailored feedback and resources that resonate with the diverse experiences of adolescents, thus fostering a more inclusive and supportive environment for mental health learning. Preliminary findings suggest that LLMs can effectively enhance mental health literacy and support adolescents in managing stress and preventing mental health issues. Furthermore, the interactive nature of these models encourages continuous engagement and learning, making complex mental health concepts more accessible to young audiences. The integration of LLMs also highlights their potential to adapt to different cultural contexts, thus promoting diversity awareness more effectively. The paper concludes with recommendations for future research and the responsible development of LLMs in mental health projects, emphasizing the need for ongoing evaluation and adaptation to meet the evolving needs of adolescents. Ultimately, the integration of LLMs within the me_HeLi-D project underscores their potential as powerful allies in the pursuit of better mental health outcomes and greater diversity awareness among youth.  © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Browne, R.
AU  - Gull, K.
AU  - Hurley, C.M.
AU  - Sugrue, R.M.
AU  - O'Sullivan, J.B.
TI  - ChatGPT-4 Can Help Hand Surgeons Communicate Better With Patients
PY  - 2024
T2  - Journal of Hand Surgery Global Online
VL  - 6
IS  - 3
SP  - 436
EP  - 438
DO  - 10.1016/j.jhsg.2024.03.008
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189655057&doi=10.1016%2fj.jhsg.2024.03.008&partnerID=40&md5=8d0ae665bb54d3e2256059c90fa8ee31
AB  - The American Society for Surgery of the Hand and British Society for Surgery of the Hand produce patient-focused information above the sixth-grade readability recommended by the American Medical Association. To promote health equity, patient-focused content should be aimed at an appropriate level of health literacy. Artificial intelligence–driven large language models may be able to assist hand surgery societies in improving the readability of the information provided to patients. The readability was calculated for all the articles written in English on the American Society for Surgery of the Hand and British Society for Surgery of the Hand websites, in terms of seven of the commonest readability formulas. Chat Generative Pre-Trained Transformer version 4 (ChatGPT-4) was then asked to rewrite each article at a sixth-grade readability level. The readability for each response was calculated and compared with the unedited articles. Chat Generative Pre-Trained Transformer version 4 was able to improve the readability across all chosen readability formulas and was successful in achieving a mean sixth-grade readability level in terms of the Flesch Kincaid Grade Level and Simple Measure of Gobbledygook calculations. It increased the mean Flesch Reading Ease score, with higher scores representing more readable material. This study demonstrated that ChatGPT-4 can be used to improve the readability of patient-focused material in hand surgery. However, ChatGPT-4 is interested primarily in sounding natural, and not in seeking truth, and hence, each response must be evaluated by the surgeon to ensure that information accuracy is not being sacrificed for the sake of readability by this powerful tool. © 2024 The Authors
PB  - Elsevier Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Szczesniewski, J.J.
AU  - Ramoso Alba, A.
AU  - Rodríguez Castro, P.M.
AU  - Lorenzo Gómez, M.F.
AU  - Sainz González, J.
AU  - Llanes González, L.
TI  - Quality of information about urologic pathology in English and Spanish from ChatGPT, BARD, and Copilot
ST  - Calidad de información de ChatGPT, BARD y Copilot acerca de patología urológica en inglés y en español
PY  - 2024
T2  - Actas Urologicas Espanolas
VL  - 48
IS  - 5
SP  - 398
EP  - 403
DO  - 10.1016/j.acuro.2023.12.002
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184054145&doi=10.1016%2fj.acuro.2023.12.002&partnerID=40&md5=168b2a9ba96beb1808e7c1600e364994
AB  - Introduction and objective: Generative artificial intelligence makes it possible to ask about medical pathologies in dialog boxes. Our objective was to analyze the quality of information about the most common urological pathologies provided by ChatGPT (OpenIA), BARD (Google), and Copilot (Microsoft). Methods: We analyzed information on the following pathologies and their treatments as provided by AI: prostate cancer, kidney cancer, bladder cancer, urinary lithiasis, and benign prostatic hypertrophy (BPH). Questions in English and Spanish were posed in dialog boxes; the answers were collected and analyzed with DISCERN questionnaires and the overall appropriateness of the response. Surgical procedures were performed with an informed consent questionnaire. Results: The responses from the three chatbots explained the pathology, detailed risk factors, and described treatments. The difference is that BARD and Copilot provide external information citations, which ChatGPT does not. The highest DISCERN scores, in absolute numbers, were obtained in Copilot; however, on the appropriacy scale it was noted that their responses were not the most appropriate. The best surgical treatment scores were obtained by BARD, followed by ChatGPT, and finally Copilot. Conclusions: The answers obtained from generative AI on urological diseases depended on the formulation of the question. The information provided had significant biases, depending on pathology, language, and above all, the dialog box consulted. © 2024 AEU
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - JOUR
AU  - Stonestrom, A.J.
AU  - Menghrajani, K.N.
AU  - Devlin, S.M.
AU  - Franch-Expósito, S.
AU  - Ptashkin, R.N.
AU  - Patel, S.Y.
AU  - Spitzer, B.
AU  - Wu, X.
AU  - Jee, J.
AU  - Vela, P.S.
AU  - Milbank, J.H.
AU  - Shah, R.H.
AU  - Mohanty, A.S.
AU  - Brannon, A.R.
AU  - Xiao, W.
AU  - Berger, M.F.
AU  - Mantha, S.
AU  - Levine, R.L.
TI  - High-risk and silent clonal hematopoietic genotypes in patients with nonhematologic cancer
PY  - 2024
T2  - Blood Advances
VL  - 8
IS  - 4
SP  - 846
EP  - 856
DO  - 10.1182/bloodadvances.2023011262
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186749469&doi=10.1182%2fbloodadvances.2023011262&partnerID=40&md5=b25b4b635dd28d7dfb0a89bb13a22eef
AB  - Clonal hematopoiesis (CH) identified by somatic gene variants with variant allele fraction (VAF) ≥ 2% is associated with an increased risk of hematologic malignancy. However, CH defined by a broader set of genotypes and lower VAFs is ubiquitous in older individuals. To improve our understanding of the relationship between CH genotype and risk of hematologic malignancy, we analyzed data from 42 714 patients who underwent blood sequencing as a normal comparator for nonhematologic tumor testing using a large cancerrelated gene panel. We cataloged hematologic malignancies in this cohort using natural language processing and manual curation of medical records. We found that some CH genotypes including JAK2, RUNX1, and XPO1 variants were associated with high hematologic malignancy risk. Chronic disease was predicted better than acute disease suggesting the influence of length bias. To better understand the implications of hematopoietic clonality independent of mutational function, we evaluated a set of silent synonymous and noncoding mutations. We found that silent CH, particularly when multiple variants were present or VAF was high, was associated with increased risk of hematologic malignancy. We tracked expansion of CH mutations in 26 hematologic malignancies sequenced with the same platform. JAK2 and TP53 VAF consistently expanded at disease onset, whereas DNMT3A and silent CH VAFs mostly decreased. These data inform the clinical and biological interpretation of CH in the context of nonhematologic cancer.  © 2024 by The American Society of Hematology.
PB  - American Society of Hematology
C2  - 38147626
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - CONF
AU  - Wu, Y.
AU  - Hughes, J.A.
AU  - Lyrstedt, A.-L.
AU  - Hazelwood, S.
AU  - Brown, N.J.
AU  - Jones, L.
AU  - Douglas, C.
AU  - Jarugula, R.
AU  - Chu, K.
AU  - Nguyen, A.
TI  - Developing Robust Clinical Text Deep Learning Models - A "Painless" Approach
PY  - 2024
T2  - Studies in Health Technology and Informatics
VL  - 310
SP  - 705
EP  - 709
DO  - 10.3233/SHTI231056
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183579669&doi=10.3233%2fSHTI231056&partnerID=40&md5=2561621a03d5d45993ce7e2684548441
AB  - The success of deep learning in natural language processing relies on ample labelled training data. However, models in the health domain often face data inadequacy due to the high cost and difficulty of acquiring training data. Developing such models thus requires robustness and performance on new data. A generalised incremental multiphase framework is proposed for developing robust and performant clinical text deep learning classifiers. It incorporates incremental multiphases for training data size assessments, cross-validation setup to avoid test data bias, and robustness testing through inter/intra-model significance analysis. The framework's effectiveness and generalisation were confirmed by the task of identifying patients presenting in 'pain' to the emergency department.  © 2024 International Medical Informatics Association (IMIA) and IOS Press.
PB  - IOS Press BV
C2  - 38269900
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Alkamli, S.
AU  - Al-Yahya, M.
AU  - Alyahya, K.
TI  - Ethical and Legal Considerations of Large Language Models: A Systematic Review of the Literature
PY  - 2024
T2  - 2024 2nd International Conference on Foundation and Large Language Models, FLLM 2024
SP  - 576
EP  - 586
DO  - 10.1109/FLLM63129.2024.10852451
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218351952&doi=10.1109%2fFLLM63129.2024.10852451&partnerID=40&md5=31f220c70e9b8a4e660fe6d6bc59a7c3
AB  - Large Language Models (LLMs) such as OpenAI's GPT-4 and Google's Gemini have rapidly emerged as influential tools across various sectors, including healthcare, education, research, and law. Despite their benefits, LLMs present significant ethical and legal challenges that necessitate thorough examination. This systematic review aims to synthesize existing literature on these considerations, focusing on issues such as bias, privacy, transparency, misinformation, plagiarism, accountability, and security. By analyzing 43 peer-reviewed studies published over the last five years, we identify the primary ethical and legal concerns associated with LLMs, categorize these concerns, and report the research methods and recommendations proposed to mitigate potential harms. Our findings reveal that while significant progress has been made in understanding these issues, there remains a need for more cohesive and comprehensive approaches to address the ethical and legal implications of LLMs. This review provides valuable insights for researchers, policymakers, and developers, emphasizing the critical importance of addressing ethical and legal considerations to maintain public trust and ensure the beneficial impact of LLM technologies. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Bakdash, L.
AU  - Abid, A.
AU  - Gourisankar, A.
AU  - Henry, T.L.
TI  - Chatting Beyond ChatGPT: Advancing Equity Through AI-Driven Language Interpretation
PY  - 2024
T2  - Journal of General Internal Medicine
VL  - 39
IS  - 3
SP  - 492
EP  - 495
DO  - 10.1007/s11606-023-08497-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175255184&doi=10.1007%2fs11606-023-08497-6&partnerID=40&md5=7034517c10161d7500164d9c1fd68b58
AB  - Medical interpretation is an underutilized resource, despite its legal mandate and proven efficacy in improving health outcomes for populations with low English proficiency. This disconnect can often be attributed to the costs and wait-times associated with traditional means of interpretation, making the service inaccessible and burdensome. Technology has improved access to translation through phone and video interpretation; with the acceleration of artificial intelligence (AI) large language models, we have an opportunity to further improve interpreter access through real-time, automated translation. The impetus to utilize this burgeoning tool for improved health equity must be combined with a critical view of the safety, privacy, and clinical decision-making risks involved. Physicians must be active participants and collaborators in both the mobilization of AI tools to improve clinical care and the development of regulations to mitigate harm. © The Author(s), under exclusive licence to Society of General Internal Medicine 2023.
PB  - Springer
C2  - 37904073
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 13
ER  -

TY  - CONF
AU  - Ali, U.S.
TI  - Ask your Transcript: LLM Driven Insights for Academic Advising
PY  - 2024
T2  - 2024 2nd International Conference on Computing and Data Analytics, ICCDA 2024 - Proceedings
DO  - 10.1109/ICCDA64887.2024.10867349
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219126272&doi=10.1109%2fICCDA64887.2024.10867349&partnerID=40&md5=35771b21a2506b8d1947f3a32981fc59
AB  - Recent advances in Artificial Intelligence have witnessed an unprecedented penetration of AI tools into our day-to-day life like never before. Especially, a generative AI tool such as ChatGPT that is driven by Large Language Model (LLM), has been proven successful and effectively adapted across many domains such as business, health care, education and much more. The ability of LLM models in understating natural language text and generating human-fashioned responses is very astonishing that paved the way for wide usage of such tools among the population. The students in education domain face many hurdles in their academic advising due to the lack of adequate upfront information available to them. In this work, we explore a more viable and feasible solution to alleviate this problem in term of LLM driven tool and provide a student community with concise and informative responses by retrieving the information given in their academic transcripts. The proposed work crafts a possible list of questions that student community seeks answers based on their past academic performance and experience in navigating the transcripts for their information needs. The tool then injects these questions as prompts to the RAG models to generate relevant automated responses in natural language as if generated by human adviser. The proposed tool may compliment both the student and human advisers in term of equipping them with academic equity and comfort.  © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Brozović, J.
AU  - Mikulić, B.
AU  - Tomas, M.
AU  - Juzbašić, M.
AU  - Blašković, M.
TI  - Assessing the performance of Bing Chat artificial intelligence: Dental exams, clinical guidelines, and patients’ frequent questions
PY  - 2024
T2  - Journal of Dentistry
VL  - 144
C7  - 104927
DO  - 10.1016/j.jdent.2024.104927
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187938035&doi=10.1016%2fj.jdent.2024.104927&partnerID=40&md5=431f1ea5bdc46939329ea4c352609251
AB  - Objectives: Bing Chat is a large language model artificial intelligence (AI) with online search and text generating capabilities. This study assessed its performance within the scope of dentistry in: (a) tackling exam questions for dental students, (ii) providing guidelines for dental practitioners, and (iii) answering patients’ frequently asked questions. We discuss the potential of clinical tutoring, common patient communication and impact on academia. Methods: With the aim of assessing AI's performance in dental exams, Bing Chat was presented with 532 multiple-choice questions and awarded scores based on its answers. In evaluating guidelines for clinicians, a further set of 15 questions, each with 2 follow-up questions on clinical protocols, was presented to the AI. The answers were assessed by 4 reviewers using electronic visual analog scale. In evaluating answers to patients’ frequently asked questions, another list of 15 common questions was included in the session, with respective outputs assessed. Results: Bing Chat correctly answered 383 out of 532 multiple-choice questions in dental exam part, achieving a score of 71.99 %. As for outlining clinical protocols for practitioners, the overall assessment score was 81.05 %. In answering patients’ frequently asked questions, Bing Chat achieved an overall mean score of 83.8 %. The assessments demonstrated low inter-rater reliability. CONCLUSIONS: The overall performance of Bing Chat was above the regularly adopted passing scores, particularly in answering patient's frequently asked questions. The generated content may have biased sources. These results suggest the importance of raising clinicians’ awareness of AI's benefits and risks, as well as timely adaptations of dental education curricula, and safeguarding its use in dentistry and healthcare in general. Clinical significance: Bing Chat AI performed above the passing threshold in three categories, and thus demonstrated potential for educational assistance, clinical tutoring, and answering patients’ questions. We recommend popularizing its benefits and risks among students and clinicians, while maintaining awareness of possible false information. © 2024 Elsevier Ltd
PB  - Elsevier Ltd
C2  - 38458379
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Elbattah, M.
AU  - Arnaud, E.
AU  - Ghazali, D.A.
AU  - Dequen, G.
TI  - Exploring the Ethical Challenges of Large Language Models in Emergency Medicine: A Comparative International Review
PY  - 2024
T2  - Proceedings - 2024 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2024
SP  - 5750
EP  - 5755
DO  - 10.1109/BIBM62325.2024.10822376
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217276119&doi=10.1109%2fBIBM62325.2024.10822376&partnerID=40&md5=897f6868889f82d7cd8d90a0eb720254
AB  - Large Language Models (LLMs) hold promise for advancing Emergency Medicine by enhancing operational efficiency and supporting decision-making. This scoping review explores the ethical, legal, and global considerations influencing LLM deployment in emergency care. Key ethical concerns, including patient safety, data privacy, and transparency, emphasise the need for explainable AI (XAI) to build trust and prevent biased outputs. Legal challenges highlight the importance of regulatory compliance, especially regarding data protection laws like the GDPR. Significant international variability in LLM adoption further underscores the need for harmonised guidelines to ensure safe and equitable AI integration across diverse healthcare systems. To advance the responsible use of LLMs, future research should prioritise model transparency, consider resource-limited settings, and focus on establishing robust regulatory frameworks. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Madsen, J.
AU  - Vila, C.
AU  - Anand, P.
AU  - Lau, K.H.V.
TI  - Social Work in Outpatient Neurology at a Safety-Net Hospital: A 200-Hour Profile
PY  - 2024
T2  - Journal of Immigrant and Minority Health
VL  - 26
IS  - 1
SP  - 247
EP  - 252
DO  - 10.1007/s10903-023-01533-x
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169899295&doi=10.1007%2fs10903-023-01533-x&partnerID=40&md5=56e1c43e9f17034e7bcfea999d3c0b73
AB  - Social work plays a critical role in preventive health and mitigation of healthcare disparities, but few studies focus on its role in multi-specialty clinics serving marginalized populations. We aimed to characterize the role of outpatient neurology social work at an urban, safety-net hospital. In December 2021, we introduced a dedicated social worker to a neurology clinic primarily caring for an underserved patient population. We logged and characterized the first 200 consecutive hours of patient encounters, classifying interventions based on a recently popularized 10-category scheme in social work literature derived from natural language processing and machine learning algorithms. We characterized 125 encounters with neurology patients referred to social work. The neurology social worker spent the greatest amount of time on care coordination (40%), followed by housing insecurity (14%) and applications and reporting (11%). Interventions that required the most time per case included housing (129 min), applications and reporting (120 min), care coordination (96 min). The majority of interventions were directly related to the patient’s underlying neurologic disorder, highlighting the importance of a neurology-specific social worker. Embedding a social worker in a multi-specialty neurology clinic may address many of the root causes of neurologic health disparities. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
PB  - Springer
C2  - 37676447
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Kopitar, L.
AU  - Fister, I.
AU  - Stiglic, G.
TI  - Using Generative AI to Improve the Performance and Interpretability of Rule-Based Diagnosis of Type 2 Diabetes Mellitus
PY  - 2024
T2  - Information (Switzerland)
VL  - 15
IS  - 3
C7  - 162
DO  - 10.3390/info15030162
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188881309&doi=10.3390%2finfo15030162&partnerID=40&md5=7658881e7894ea85de7bf773befedca3
AB  - Introduction: Type 2 diabetes mellitus is a major global health concern, but interpreting machine learning models for diagnosis remains challenging. This study investigates combining association rule mining with advanced natural language processing to improve both diagnostic accuracy and interpretability. This novel approach has not been explored before in using pretrained transformers for diabetes classification on tabular data. Methods: The study used the Pima Indians Diabetes dataset to investigate Type 2 diabetes mellitus. Python and Jupyter Notebook were employed for analysis, with the NiaARM framework for association rule mining. LightGBM and the dalex package were used for performance comparison and feature importance analysis, respectively. SHAP was used for local interpretability. OpenAI GPT version 3.5 was utilized for outcome prediction and interpretation. The source code is available on GitHub. Results: NiaARM generated 350 rules to predict diabetes. LightGBM performed better than the GPT-based model. A comparison of GPT and NiaARM rules showed disparities, prompting a similarity score analysis. LightGBM’s decision making leaned heavily on glucose, age, and BMI, as highlighted in feature importance rankings. Beeswarm plots demonstrated how feature values correlate with their influence on diagnosis outcomes. Discussion: Combining association rule mining with GPT for Type 2 diabetes mellitus classification yields limited effectiveness. Enhancements like preprocessing and hyperparameter tuning are required. Interpretation challenges and GPT’s dependency on provided rules indicate the necessity for prompt engineering and similarity score methods. Variations in feature importance rankings underscore the complexity of T2DM. Concerns regarding GPT’s reliability emphasize the importance of iterative approaches for improving prediction accuracy. © 2024 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Sarma, G.
AU  - Kashyap, H.
AU  - Medhi, P.P.
TI  - ChatGPT in Head and Neck Oncology-Opportunities and Challenges
PY  - 2024
T2  - Indian Journal of Otolaryngology and Head and Neck Surgery
VL  - 76
IS  - 1
SP  - 1425
EP  - 1429
DO  - 10.1007/s12070-023-04201-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169326667&doi=10.1007%2fs12070-023-04201-6&partnerID=40&md5=7465692363b5aec48fce0feb5efc2271
AB  - Head and neck oncology represents a complex and challenging field, encompassing the diagnosis, treatment and management of various malignancies affecting the intricate anatomical structures of the head and neck region. With advancements in artificial intelligence (AI), chatbot applications have emerged as a promising tool to revolutionize the field of Head and Neck oncology. ChatGPT is a cutting-edge language model developed by OpenAI that can help the oncologist in the clinic in scheduling appointments, establishing a clinical diagnosis, making a treatment plan and follow-up. ChatGPT also plays an essential role in telemedicine consultations, medical documentation, scientific writing and research. ChatGPT carries its inherent drawbacks too. ChatGPT raises significant ethical concerns related to authorship, accountability, transparency, bias, and the potential for misinformation. ChatGPT’s training data is limited to September 2021; thus, regular updates are required to keep pace with the rapidly evolving medical research and advancements. Therefore, a judicial approach to using ChatGPT is of utmost importance. Head and Neck Oncologists can reap the maximum benefit of this technology in terms of patient care, education and research to improve clinical outcomes. © Association of Otolaryngologists of India 2023.
PB  - Springer
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Gabriel, S.
AU  - Puri, I.
AU  - Xu, X.
AU  - Malgaroli, M.
AU  - Ghassemi, M.
TI  - Can AI Relate: Testing Large Language Model Response for Mental Health Support
PY  - 2024
T2  - EMNLP 2024 - 2024 Conference on Empirical Methods in Natural Language Processing, Findings of EMNLP 2024
SP  - 2206
EP  - 2221
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216134970&partnerID=40&md5=67ac5e4ac9518acccb94de1273655e68
AB  - Large language models (LLMs) are already being piloted for clinical use in hospital systems like NYU Langone, Dana-Farber and the NHS. A proposed deployment use case is psychotherapy, where a LLM-powered chatbot can treat a patient undergoing a mental health crisis. Deployment of LLMs for mental health response could hypothetically broaden access to psychotherapy and provide new possibilities for personalizing care. However, recent high-profile failures, like damaging dieting advice offered by the Tessa chatbot to patients with eating disorders, have led to doubt about their reliability in high-stakes and safety-critical settings. In this work, we develop an evaluation framework for determining whether LLM response is a viable and ethical path forward for the automation of mental health treatment. Our framework measures equity in empathy and adherence of LLM responses to motivational interviewing theory. Using human evaluation with trained clinicians and automatic quality-of-care metrics grounded in psychology research, we compare the responses provided by peer-to-peer responders to those provided by a state-of-the-art LLM. We show that LLMs like GPT-4 use implicit and explicit cues to infer patient demographics like race. We then show that there are statistically significant discrepancies between patient subgroups: Responses to Black posters consistently have lower empathy than for any other demographic group (2%-13% lower than the control group). Promisingly, we do find that the manner in which responses are generated significantly impacts the quality of the response. We conclude by proposing safety guidelines for the potential deployment of LLMs for mental health response. © 2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ahmed, M.
AU  - Stone, M.L.
AU  - Stidham, R.W.
TI  - Artificial Intelligence and IBD: Where are We Now and Where Will We Be in the Future?
PY  - 2024
T2  - Current Gastroenterology Reports
VL  - 26
IS  - 5
SP  - 137
EP  - 144
DO  - 10.1007/s11894-024-00918-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186210997&doi=10.1007%2fs11894-024-00918-8&partnerID=40&md5=5cb2182590582ecd4561744d8b0fb748
AB  - Purpose of Review: Artificial intelligence (AI) is quickly demonstrating the ability to address problems and challenges in the care of IBD. This review with commentary will highlight today’s advancements in AI applications for IBD in image analysis, understanding text, and replicating clinical knowledge and experience. Recent Findings: Advancements in machine learning methods, availability of high-performance computing, and increasing digitization of medical data are providing opportunities for AI to assist in IBD care. Multiple groups have demonstrated the ability of AI to replicate expert endoscopic scoring in IBD, with expansion into automated capsule endoscopy, enterography, and histologic interpretations. Further, AI image analysis is being used to develop new endoscopic scoring with more granularity and detail than is possible using conventional methods. Advancements in natural language processing are proving to reduce laborious tasks required in the care of IBD, including documentation, information searches, and chart review. Finally, large language models and chatbots that can understand language and generate human-like replies are beginning to exhibit clinical intelligence that will revolutionize how we deliver IBD care. Summary: Today, AI is being deployed to replicate expert judgement in specific tasks where disagreement, subjectivity, and bias are common. However, the near future will herald contributions of AI doing what we cannot, including new detailed measures of IBD, enhanced analysis of images, and perhaps even fully automating care. As we speculate on future technologic capabilities that may improve how we care for IBD, this review will also consider how we will implement and fairly use AI in practice. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.
PB  - Springer
C2  - 38411898
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Guerra-Adames, A.
AU  - Avalos-Fernandez, M.
AU  - Doremus, O.
AU  - Gil-Jardiné, C.
AU  - Lagarde, E.
TI  - Uncovering Judgment Biases in Emergency Triage: A Public Health Approach Based on Large Language Models
PY  - 2024
T2  - Proceedings of Machine Learning Research
VL  - 259
SP  - 420
EP  - 439
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219199408&partnerID=40&md5=98f4d9a324f76191f8ce77a6cfb15d54
AB  - Judgment biases in emergency triage can adversely affect patient outcomes. This study examines sex/gender biases using four advanced language models fine-tuned on real-world emergency department data. We introduce a novel approach based on the testing method, commonly used in hiring bias detection, by automatically altering triage notes to change patient sex references. Results indicate a significant bias: female patients are assigned lower severity ratings than male patients with identical clinical conditions. This bias is more pronounced with female nurses or when patients report higher pain levels but diminishes with increased nurse experience. Identifying these biases can inform interventions such as enhanced training, protocol updates, and machine learning tools to support clinical decision-making. © 2024 A. Guerra-Adames, M. Avalos-Fernandez, O. Doremus, C. Gil-Jardiné & E. Lagarde.
PB  - ML Research Press
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Cao, Q.
AU  - Xu, Z.
AU  - Chen, Y.
AU  - Ma, C.
AU  - Yang, X.
TI  - Domain-Controlled Prompt Learning
PY  - 2024
T2  - Proceedings of the AAAI Conference on Artificial Intelligence
VL  - 38
IS  - 2
SP  - 936
EP  - 944
DO  - 10.1609/aaai.v38i2.27853
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189520515&doi=10.1609%2faaai.v38i2.27853&partnerID=40&md5=d81724d2a167ceef86f663fd29939b3d
AB  - Large pre-trained vision-language models, such as CLIP, have shown remarkable generalization capabilities across various tasks when appropriate text prompts are provided. However, adapting these models to specific domains, like remote sensing images (RSIs), medical images, etc, remains unexplored and challenging. Existing prompt learning methods often lack domain-awareness or domain-transfer mechanisms, leading to suboptimal performance due to the misinterpretation of specific images in natural image patterns. To tackle this dilemma, we proposed a Domain-Controlled Prompt Learning for the specific domains. Specifically, the large-scale specific domain foundation model (LSDM) is first introduced to provide essential specific domain knowledge. Using lightweight neural networks, we transfer this knowledge into domain biases, which control both the visual and language branches to obtain domain-adaptive prompts in a directly incorporating manner. Simultaneously, to overcome the existing overfitting challenge, we propose a novel noisy-adding strategy, without extra trainable parameters, to help the model escape the suboptimal solution in a global domain oscillation manner. Experimental results show our method achieves state-of-the-art performance in specific domain image recognition datasets. Our code is available at https://github.com/caoql98/DCPL. © 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
PB  - Association for the Advancement of Artificial Intelligence
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Decoupes, R.
AU  - Roche, M.
AU  - Teisseire, M.
TI  - GeoNLPlify: A spatial data augmentation enhancing text classification for crisis monitoring
PY  - 2024
T2  - Intelligent Data Analysis
VL  - 28
IS  - 2
SP  - 507
EP  - 531
DO  - 10.3233/IDA-230040
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192362831&doi=10.3233%2fIDA-230040&partnerID=40&md5=993869ffb1741cabfb02e31f57b29eec
AB  - Crises such as natural disasters and public health emergencies generate vast amounts of text data, making it challenging to classify the information into relevant categories. Acquiring expert-labeled data for such scenarios can be difficult, leading to limited training datasets for text classification by fine-tuning BERT-like models. Unfortunately, traditional data augmentation techniques only slightly improve F1-scores. How can data augmentation be used to obtain better results in this applied domain? In this paper, using neural network explicability methods, we aim to highlight that fine-tuned BERT-like models on crisis corpora give too much importance to spatial information to make their predictions. This overfitting of spatial information limits their ability to generalize especially when the event which occurs in a place has evolved and changed since the training dataset has been built. To reduce this bias, we propose GeoNLPlify,1 a novel data augmentation technique that leverages spatial information to generate new labeled data for text classification related to crises. Our approach aims to address overfitting without necessitating modifications to the underlying model architecture, distinguishing it from other prevalent methods employed to combat overfitting. Our results show that GeoNLPlify significantly improves F1-scores, demonstrating the potential of the spatial information for data augmentation for crisis-related text classification tasks. In order to evaluate the contribution of our method, GeoNLPlify is applied to three public datasets (PADI-web, CrisisNLP and SST2) and compared with classical natural language processing data augmentations. © 2024 – The authors. Published by IOS Press.
PB  - IOS Press BV
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Kour, H.
AU  - Gupta, M.K.
TI  - Hybrid evolutionary intelligent network for sentiment analysis using Twitter data during COVID-19 pandemic
PY  - 2024
T2  - Expert Systems
VL  - 41
IS  - 3
C7  - e13489
DO  - 10.1111/exsy.13489
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175373600&doi=10.1111%2fexsy.13489&partnerID=40&md5=937d6c928b783b669d8dce9d31c2f20b
AB  - COVID-19 pandemic has impacted many nations, causing physical as well as mental health concerns globally. In most countries, governments enforced strict lockdowns and social distancing, thus affecting people's daily lives. People usually tweet their views on online platforms that is unstructured text with implicit meaning. With the evolution of artificial intelligence in the natural language processing domain, the prediction of sentiments accurately has become a challenge. To contribute as a solution to this, a hybrid approach is proposed for sentiment prediction with the use of an evolutionary-based approach, transfer-based learning and machine learning. The proposed approach uses bidirectional encoder representations from transformers (BERT) with genetic algorithm (GA) and support vector machine (SVM), namely, hybrid evolutionary intelligent model (GA-BERT-SVM). These approaches aid in extracting important features considering semantics and context present in the text. To avoid the limitations of the backpropagation approach, such as trapping in local minima and overfitting the data, the initial parameters (weights and biases) of the dense layers has been optimized using GA. Additionally, the pretrained BERT layers are utilized without any modification, following a standard transfer learning approach. The BERT embeddings are concatenated with the SVM for training and classification. GridSearchCV and GeneticSearchCV is used for obtaining optimal parameters of SVM. A multi-classification problem is tackled using a benchmark COVID-19 dataset, which comprises of Twitter data and is categorized into COVIDSENTI-A, COVIDSENTI-B, COVIDSENTI-C and a combined dataset called COVIDSENTI. Experimental evaluation demonstrates promising results of the proposed model in terms of accuracy, F1-score, precision and recall, surpassing state-of-the-art approaches. © 2023 John Wiley & Sons Ltd.
PB  - John Wiley and Sons Inc
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Xiao, H.
AU  - Wu, X.
AU  - Tong, J.
AU  - Li, B.
AU  - Sun, Y.
TI  - Chinese Elderly Healthcare-Oriented Conversation: CareQA Dataset and Its Knowledge Distillation Based Generation Framework
PY  - 2024
T2  - Proceedings - 2024 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2024
SP  - 3866
EP  - 3871
DO  - 10.1109/BIBM62325.2024.10822408
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217280912&doi=10.1109%2fBIBM62325.2024.10822408&partnerID=40&md5=dbb4330532bd4ae4971ecde2d3e44c57
AB  - The increasing global aging brings the substantial demand for healthcare knowledge among the elderly. Large Language Models (LLMs) based Conversation Agents (CAs) hold significant promise for addressing the elderly's healthcare knowledge inquiries. Yet, general LLMs often fall short in providing professional and practically usable healthcare conversations due to the lack of specific knowledge, possible hallucination issues and contextual comprehension biases. To address these challenges, we first propose a cost-effective, domain-specific questioning-answering (QA) generation framework based on knowledge distillation (KD). Based on this framework, we then built CareQA, the first Chinese healthcare QA dataset specifically for the elderly, with 41,694 QA pairs spanning geriatric diseases covering multiple categories. A comprehensive benchmarking experiment, including both automated and human evaluation, is conducted to examine the usability of CareQA. The results demonstrate that the LLMs fine-tuned on CareQA perform better in answering elderly healthcare-related questions. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Rivière, J.G.
AU  - Soler Palacín, P.
AU  - Butte, M.J.
TI  - Proceedings from the inaugural Artificial Intelligence in Primary Immune Deficiencies (AIPID) conference
PY  - 2024
T2  - Journal of Allergy and Clinical Immunology
VL  - 153
IS  - 3
SP  - 637
EP  - 642
DO  - 10.1016/j.jaci.2024.01.002
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185775549&doi=10.1016%2fj.jaci.2024.01.002&partnerID=40&md5=13a550434e9ffeb43c812886277ca12e
AB  - Here, we summarize the proceedings of the inaugural Artificial Intelligence in Primary Immune Deficiencies conference, during which experts and advocates gathered to advance research into the applications of artificial intelligence (AI), machine learning, and other computational tools in the diagnosis and management of inborn errors of immunity (IEIs). The conference focused on the key themes of expediting IEI diagnoses, challenges in data collection, roles of natural language processing and large language models in interpreting electronic health records, and ethical considerations in implementation. Innovative AI-based tools trained on electronic health records and claims databases have discovered new patterns of warning signs for IEIs, facilitating faster diagnoses and enhancing patient outcomes. Challenges in training AIs persist on account of data limitations, especially in cases of rare diseases, overlapping phenotypes, and biases inherent in current data sets. Furthermore, experts highlighted the significance of ethical considerations, data protection, and the necessity for open science principles. The conference delved into regulatory frameworks, equity in access, and the imperative for collaborative efforts to overcome these obstacles and harness the transformative potential of AI. Concerted efforts to successfully integrate AI into daily clinical immunology practice are still needed. © 2024 American Academy of Allergy, Asthma & Immunology
PB  - Elsevier Inc.
C2  - 38224784
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Chandler, R.D.
AU  - Warner, S.
AU  - Aidoo-Frimpong, G.
AU  - Wells, J.
TI  - “What Did You Say, ChatGPT?” The Use of AI in Black Women’s HIV Self-Education: An Inductive Qualitative Data Analysis
PY  - 2024
T2  - Journal of the Association of Nurses in AIDS Care
VL  - 35
IS  - 3
SP  - 294
EP  - 302
DO  - 10.1097/JNC.0000000000000468
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192382594&doi=10.1097%2fJNC.0000000000000468&partnerID=40&md5=8f1aba70413645b1f72d73c5cdf29f68
AB  - The emergence of widely accessible artificial intelligence (AI) chatbots such as ChatGPT presents unique opportunities and challenges in public health self-education. This study examined simulations with ChatGPT for its use in public education of sexual health of Black women, specifically in HIV prevention and/or HIV PrEP use. The research questions guiding the study are as follows: (a) does the information ChatGPT offers about HIV prevention and HIV PrEP differ based on stated race? and (b) how could this relatively new platform inform public health education of Black women educating themselves about sexual health behaviors, diagnoses, and treatments? In addressing these questions, this study also uncovered notable differences in ChatGPT’s tone when responding to users based on race. This study described valuable insights that can inform health care professionals, educators, and policymakers, ultimately advancing the cause of sexual health equity for Black women and underscoring the paradigm-shifting potential of AI in the field of public health education. Copyright © 2024 Association of Nurses in AIDS Care.
PB  - Lippincott Williams and Wilkins
C2  - 38949904
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Molica, S.
AU  - Shanafelt, T.D.
AU  - Allsup, D.
AU  - Giannarelli, D.
TI  - Impact of Targeted Agents on Survival of Chronic Lymphocytic Leukemia Patients Fit for Fludarabine, Cyclophosphamide, and Rituximab (FCR) Relative to Age- and Sex-Matched Population
PY  - 2024
T2  - Cancers
VL  - 16
IS  - 6
C7  - 1085
DO  - 10.3390/cancers16061085
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189159741&doi=10.3390%2fcancers16061085&partnerID=40&md5=d219954c3f1117a78736eed7f4e1f238
AB  - To assess the impact of first-line treatment with targeted agents (TAs) or fludarabine, cyclophosphamide, and rituximab (FCR)-based chemo-immunotherapy (CIT) on overall survival (OS) compared to age- and sex-matched individuals in the general population, we conducted an aggregated analysis of phase 3 clinical trials, including the two FLAIR sub-studies, ECOG1912, and CLL13 trials. The restricted mean survival time (RMST), an alternative measure in outcome analyses capturing OS changes over the entire history of the disease, was used to minimize biases associated with the short follow-up time of trials. Patients treated with TAs demonstrated a higher 5-year RMST (58.1 months; 95% CI: 57.4 to 58.8) compared to those treated with CIT (5-year RMST, 56.9 months; 95% CI: 56.7–58.2). Furthermore, the OS comparison of treatment groups with the AGMGP suggests that TAs may mitigate the impact of CLL on OS during the first five years post-treatment initiation. In summary, the 5-year RMST difference was −0.4 months (95% CI: −0.8 to 0.2; p = 0.10) when comparing CLL patients treated with TAs to the Italian age- and gender-matched general population (AGMGP). A similar trend was observed when CLL patients treated with TAs were compared to the US AGMGP (5-year RMST difference, 0.3 months; 95% CI: −0.1 to 0.9; p = 0.12). In contrast, CLL patients treated with FCR exhibited sustained OS differences when compared to both the Italian cohort (5-year RMST difference: −1.6 months; 95% CI: −2.4 to −0.9; p < 0.0001) and the US AGMGP cohort (5-year RMST difference: −0.9 months; 95% CI: −1.7 to −0.2; p = 0.015). Although these results support TAs as the preferred first-line treatment for younger CLL patients, it is crucial to acknowledge that variations in patient selection criteria and clinical profiles across clinical trials necessitate a cautious interpretation of these findings that should be viewed as directional and hypothesis-generating. A longer follow-up is needed to assess the survival improvement of younger CLL patients treated with TAs relative to the AGMGP. © 2024 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Kalam, K.T.
AU  - Rahman, J.M.
AU  - Islam, M.R.
AU  - Dewan, S.M.R.
TI  - ChatGPT and mental health: Friends or foes?
PY  - 2024
T2  - Health Science Reports
VL  - 7
IS  - 2
C7  - e1912
DO  - 10.1002/hsr2.1912
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185480456&doi=10.1002%2fhsr2.1912&partnerID=40&md5=739f8ee424cd88bc0e24e032426f6675
AB  - Background: ChatGPT is an artificial intelligence (AI) language model that has gained popularity as a virtual assistant because of its exceptional capacity to solve problems and make decisions. However, there are some ways in which technological misuse and incorrect interpretations can have potentially hazardous consequences for a user's mental health. Discussion: Because it lacks real-time fact-checking capabilities, ChatGPT may create misleading or erroneous information. Considering AI technology has the potential to influence a person's thinking, we anticipate ChatGPT's future repercussions on mental health by considering instances in which inappropriate usage may lead to mental disorders. While several studies have demonstrated how the AI model may transform mental health care and therapy, certain drawbacks, including bias and privacy violations, have also been identified. Conclusion: Educating people and organizing workshops on AI technology usage, strengthening privacy measures, and updating ethical standards are crucial initiatives to prevent misuse and resultant dire impacts on mental health. Longitudinal research on the potential of these platforms to impact a variety of mental health problems is recommended in the future. © 2024 The Authors. Health Science Reports published by Wiley Periodicals LLC.
PB  - John Wiley and Sons Inc
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - Amin, K.S.
AU  - Forman, H.P.
AU  - Davis, M.A.
TI  - Even with ChatGPT, race matters
PY  - 2024
T2  - Clinical Imaging
VL  - 109
C7  - 110113
DO  - 10.1016/j.clinimag.2024.110113
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188823680&doi=10.1016%2fj.clinimag.2024.110113&partnerID=40&md5=b33b53ae5d3ff26231e85b65c9473f32
AB  - Background: Applications of large language models such as ChatGPT are increasingly being studied. Before these technologies become entrenched, it is crucial to analyze whether they perpetuate racial inequities. Methods: We asked Open AI's ChatGPT-3.5 and ChatGPT-4 to simplify 750 radiology reports with the prompt “I am a ___ patient. Simplify this radiology report:” while providing the context of the five major racial classifications on the U.S. census: White, Black or African American, American Indian or Alaska Native, Asian, and Native Hawaiian or other Pacific Islander. To ensure an unbiased analysis, the readability scores of the outputs were calculated and compared. Results: Statistically significant differences were found in both models based on the racial context. For ChatGPT-3.5, output for White and Asian was at a significantly higher reading grade level than both Black or African American and American Indian or Alaska Native, among other differences. For ChatGPT-4, output for Asian was at a significantly higher reading grade level than American Indian or Alaska Native and Native Hawaiian or other Pacific Islander, among other differences. Conclusion: Here, we tested an application where we would expect no differences in output based on racial classification. Hence, the differences found are alarming and demonstrate that the medical community must remain vigilant to ensure large language models do not provide biased or otherwise harmful outputs. © 2024 Elsevier Inc.
PB  - Elsevier Inc.
C2  - 38552383
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 16
ER  -

TY  - JOUR
AU  - Jačisko, J.
AU  - Veselý, V.
AU  - Chang, K.-V.
AU  - Özçakar, L.
TI  - (How) ChatGPT—Artificial Intelligence Thinks It Can Help/Harm Physiatry
PY  - 2024
T2  - American Journal of Physical Medicine and Rehabilitation
VL  - 103
IS  - 4
SP  - 346
EP  - 349
DO  - 10.1097/PHM.0000000000002370
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187955191&doi=10.1097%2fPHM.0000000000002370&partnerID=40&md5=fa3c63fd3615d80d7eeafc19c18e6f95
AB  - ChatGPT is a chatbot that is based on the generative pretrained transformer architecture as an artificial inteligence-based large language model. Its widespread use in healthcare practice, research, and education seems to be (increasingly) inevitable. Also considering the relevant limitations regarding privacy, ethics, bias, legal, and validity, in this article, its use as a supplement (for sure not as a substitute for physicians) is discussed in light of the recent literature. Particularly, the “opinion” of ChatGPT about how it can help/harm physiatry is exemplified. © 2024 Lippincott Williams and Wilkins. All rights reserved.
PB  - Lippincott Williams and Wilkins
C2  - 38112589
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Bhuiyan, M.I.
AU  - Kamarudin, N.S.
AU  - Ismail, N.H.
TI  - Understanding Mental Health Content on Social Media and It’s Effect Towards Suicidal Ideation
PY  - 2024
T2  - International Journal of Advanced Computer Science and Applications
VL  - 15
IS  - 11
SP  - 342
EP  - 356
DO  - 10.14569/IJACSA.2024.0151133
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000674117&doi=10.14569%2fIJACSA.2024.0151133&partnerID=40&md5=bc479637abf2c3a9a43ffcf244c4f555
AB  - The study “Understanding Mental Health Content on Social Media and Its Effect Towards Suicidal Ideation” aims to detail the recognition of suicidal intent through social media, with a focus on the improvement and part of the machine learning (ML), deep learning (DL), and natural language processing (NLP). This review underscores the critical need for effective strategies to identify and support individuals with suicidal ideation, exploiting technological innovations in ML and DL to further suicide prevention efforts. The study details the application of these technologies in analyzing vast amounts of unstructured social media data to detect linguistic patterns, keywords, phrases, tones, and contextual cues associated with suicidal thoughts. It explores various ML and DL models like SVMs, CNNs, LSTM, neural networks, and their effectiveness in interpreting complex data patterns and emotional nuances within text data. The review discusses the potential of these technologies to serve as a life-saving tool by identifying at-risk individuals through their digital traces. Furthermore, it evaluates the real-world effectiveness, limitations, and ethical considerations of employing these technologies for suicide prevention, stressing the importance of responsible development and usage. The study aims to fill critical knowledge gaps by analyzing recent studies, methodologies, tools, and techniques in this field. It highlights the importance of synthesizing current literature to inform practical tools and suicide prevention efforts, guiding innovation in reliable, ethical systems for early intervention. This research synthesis evaluates the intersection of technology and mental health, advocating for the ethical and responsible application of ML, DL, and NLP to offer life-saving potential worldwide while addressing challenges like generalizability, biases, privacy, and the need for further research to ensure these technologies do not exacerbate existing inequities and harms. © (2024), (Science and Information Organization). All Rights Reserved.
PB  - Science and Information Organization
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Lai, H.
AU  - Ge, L.
AU  - Sun, M.
AU  - Pan, B.
AU  - Huang, J.
AU  - Hou, L.
AU  - Yang, Q.
AU  - Liu, J.
AU  - Liu, J.
AU  - Ye, Z.
AU  - Xia, D.
AU  - Zhao, W.
AU  - Wang, X.
AU  - Liu, M.
AU  - Talukdar, J.R.
AU  - Tian, J.
AU  - Yang, K.
AU  - Estill, J.
TI  - Assessing the Risk of Bias in Randomized Clinical Trials With Large Language Models
PY  - 2024
T2  - JAMA Network Open
VL  - 7
IS  - 5
SP  - E2412687
DO  - 10.1001/jamanetworkopen.2024.12687
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194016374&doi=10.1001%2fjamanetworkopen.2024.12687&partnerID=40&md5=c257bd719ccb02cb5b5d45d3ee2afdc4
AB  - Importance: Large language models (LLMs) may facilitate the labor-intensive process of systematic reviews. However, the exact methods and reliability remain uncertain. Objective: To explore the feasibility and reliability of using LLMs to assess risk of bias (ROB) in randomized clinical trials (RCTs). Design, Setting, and Participants: A survey study was conducted between August 10, 2023, and October 30, 2023. Thirty RCTs were selected from published systematic reviews. Main Outcomes and Measures: A structured prompt was developed to guide ChatGPT (LLM 1) and Claude (LLM 2) in assessing the ROB in these RCTs using a modified version of the Cochrane ROB tool developed by the CLARITY group at McMaster University. Each RCT was assessed twice by both models, and the results were documented. The results were compared with an assessment by 3 experts, which was considered a criterion standard. Correct assessment rates, sensitivity, specificity, and F1 scores were calculated to reflect accuracy, both overall and for each domain of the Cochrane ROB tool; consistent assessment rates and Cohen κ were calculated to gauge consistency; and assessment time was calculated to measure efficiency. Performance between the 2 models was compared using risk differences. Results: Both models demonstrated high correct assessment rates. LLM 1 reached a mean correct assessment rate of 84.5% (95% CI, 81.5%-87.3%), and LLM 2 reached a significantly higher rate of 89.5% (95% CI, 87.0%-91.8%). The risk difference between the 2 models was 0.05 (95% CI, 0.01-0.09). In most domains, domain-specific correct rates were around 80% to 90%; however, sensitivity below 0.80 was observed in domains 1 (random sequence generation), 2 (allocation concealment), and 6 (other concerns). Domains 4 (missing outcome data), 5 (selective outcome reporting), and 6 had F1 scores below 0.50. The consistent rates between the 2 assessments were 84.0% for LLM 1 and 87.3% for LLM 2. LLM 1's κ exceeded 0.80 in 7 and LLM 2's in 8 domains. The mean (SD) time needed for assessment was 77 (16) seconds for LLM 1 and 53 (12) seconds for LLM 2. Conclusions: In this survey study of applying LLMs for ROB assessment, LLM 1 and LLM 2 demonstrated substantial accuracy and consistency in evaluating RCTs, suggesting their potential as supportive tools in systematic review processes.. © 2024 American Medical Association. All rights reserved.
PB  - American Medical Association
C2  - 38776081
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 11
ER  -

TY  - JOUR
AU  - Alter, I.L.
AU  - Chan, K.
AU  - Lechien, J.
AU  - Rameau, A.
TI  - An introduction to machine learning and generative artificial intelligence for otolaryngologists—head and neck surgeons: a narrative review
PY  - 2024
T2  - European Archives of Oto-Rhino-Laryngology
VL  - 281
IS  - 5
SP  - 2723
EP  - 2731
DO  - 10.1007/s00405-024-08512-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185947101&doi=10.1007%2fs00405-024-08512-4&partnerID=40&md5=acf512940579fcb4ed459cd9de604f79
AB  - Purpose: Despite the robust expansion of research surrounding artificial intelligence (AI) and machine learning (ML) and their applications to medicine, these methodologies often remain opaque and inaccessible to many otolaryngologists. Especially, with the increasing ubiquity of large-language models (LLMs), such as ChatGPT and their potential implementation in clinical practice, clinicians may benefit from a baseline understanding of some aspects of AI. In this narrative review, we seek to clarify underlying concepts, illustrate applications to otolaryngology, and highlight future directions and limitations of these tools. Methods: Recent literature regarding AI principles and otolaryngologic applications of ML and LLMs was reviewed via search in PubMed and Google Scholar. Results: Significant recent strides have been made in otolaryngology research utilizing AI and ML, across all subspecialties, including neurotology, head and neck oncology, laryngology, rhinology, and sleep surgery. Potential applications suggested by recent publications include screening and diagnosis, predictive tools, clinical decision support, and clinical workflow improvement via LLMs. Ongoing concerns regarding AI in medicine include ethical concerns around bias and data sharing, as well as the “black box” problem and limitations in explainability. Conclusions: Potential implementations of AI in otolaryngology are rapidly expanding. While implementation in clinical practice remains theoretical for most of these tools, their potential power to influence the practice of otolaryngology is substantial. Level of evidence: 4. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.
PB  - Springer Science and Business Media Deutschland GmbH
C2  - 38393353
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 13
ER  -

TY  - CONF
AU  - Sharma, A.
AU  - Rushton, K.
AU  - Lin, I.W.
AU  - Nguyen, T.
AU  - Althoff, T.
TI  - Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring
PY  - 2024
T2  - Conference on Human Factors in Computing Systems - Proceedings
C7  - 700
DO  - 10.1145/3613904.3642761
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194869350&doi=10.1145%2f3613904.3642761&partnerID=40&md5=b2f7d8cbf244d0beaf0c250345762e6f
AB  - Self-guided mental health interventions, such as “do-it-yourself” tools to learn and practice coping strategies, show great promise to improve access to mental health care. However, these interventions are often cognitively demanding and emotionally triggering, creating accessibility barriers that limit their wide-scale implementation and adoption. In this paper, we study how human-language model interaction can support self-guided mental health interventions. We take cognitive restructuring, an evidence-based therapeutic technique to overcome negative thinking, as a case study. In an IRB-approved randomized field study on a large mental health website with 15,531 participants, we design and evaluate a system that uses language models to support people through various steps of cognitive restructuring. Our findings reveal that our system positively impacts emotional intensity for 67% of participants and helps 65% overcome negative thoughts. Although adolescents report relatively worse outcomes, we find that tailored interventions that simplify language model generations improve overall effectiveness and equity. © 2024 Copyright held by the owner/author(s)
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Ying, L.
AU  - Li, S.
AU  - Chen, C.
AU  - Yang, F.
AU  - Li, X.
AU  - Chen, Y.
AU  - Ding, Y.
AU  - Chang, G.
AU  - Li, J.
AU  - Wang, X.
TI  - Screening/diagnosis of pediatric endocrine disorders through the artificial intelligence model in different language settings
PY  - 2024
T2  - European Journal of Pediatrics
VL  - 183
IS  - 6
SP  - 2655
EP  - 2661
DO  - 10.1007/s00431-024-05527-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188070131&doi=10.1007%2fs00431-024-05527-1&partnerID=40&md5=0a83fc3e60377ad281a12ecc51a3aa91
AB  - This study is aimed at examining the impact of ChatGPT on pediatric endocrine and metabolic conditions, particularly in the areas of screening and diagnosis, in both Chinese and English modes. A 40-question questionnaire covering the four most common pediatric endocrine and metabolic conditions was posed to ChatGPT in both Chinese and English three times each. Six pediatric endocrinologists evaluated the responses. ChatGPT performed better when responding to questions in English, with an unreliable rate of 7.5% compared to 27.5% for Chinese questions, indicating a more consistent response pattern in English. Among the reliable questions, the answers were more comprehensive and satisfactory in the English mode. We also found disparities in ChatGPT’s performance when interacting with different target groups and diseases, with improved performance for questions posed by clinicians in English and better performance for questions related to diabetes and overweight/obesity in Chinese for both clinicians and patients. Language comprehension, providing incomprehensive answers, and errors in key data were the main contributors to the low scores, according to reviewer feedback. Conclusion: Despite these limitations, as ChatGPT continues to evolve and expand its network, it has significant potential as a practical and effective tool for clinical diagnosis and treatment. (Table presented.) © The Author(s) 2024.
PB  - Springer Science and Business Media Deutschland GmbH
C2  - 38502320
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Moura, L.
AU  - Jones, D.T.
AU  - Sheikh, I.S.
AU  - Murphy, S.
AU  - Kalfin, M.
AU  - Kummer, B.R.
AU  - Weathers, A.L.
AU  - Grinspan, Z.M.
AU  - Silsbee, H.M.
AU  - Jones, L.K.
AU  - Patel, A.D.
TI  - Implications of Large Language Models for Quality and Efficiency of Neurologic Care Emerging Issues in Neurology
PY  - 2024
T2  - Neurology
VL  - 102
IS  - 11
C7  - e209497
DO  - 10.1212/WNL.0000000000209497
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193620810&doi=10.1212%2fWNL.0000000000209497&partnerID=40&md5=28da2c4e3e920b68b6a4f3e029b4565a
AB  - Large language models (LLMs) are advanced artificial intelligence (AI) systems that excel in recognizing and generating human-like language, possibly serving as valuable tools for neurology-related information tasks. Although LLMs have shown remarkable potential in various areas, their performance in the dynamic environment of daily clinical practice remains uncertain. This article outlines multiple limitations and challenges of using LLMs in clinical settings that need to be addressed, including limited clinical reasoning, variable reliability and accuracy, reproducibility bias, self-serving bias, sponsorship bias, and potential for exacerbating health care disparities. These challenges are further compounded by practical business considerations and infrastructure requirements, including associated costs. To overcome these hurdles and harness the potential of LLMs effectively, this article includes considerations for health care organizations, researchers, and neurologists contemplating the use of LLMs in clinical practice. It is essential for health care organizations to cultivate a culture that welcomes AI solutions and aligns them seamlessly with health care operations. Clear objectives and business plans should guide the selection of AI solutions, ensuring they meet organizational needs and budget considerations. Engaging both clinical and nonclinical stakeholders can help secure necessary resources, foster trust, and ensure the long-term sustainability of AI implementations. Testing, validation, training, and ongoing monitoring are pivotal for successful integration. For neurologists, safeguarding patient data privacy is paramount. Seeking guidance from institutional information technology resources for informed, compliant decisions, and remaining vigilant against biases in LLM outputs are essential practices in responsible and unbiased utilization of AI tools. In research, obtaining institutional review board approval is crucial when dealing with patient data, even if deidentified, to ensure ethical use. Compliance with established guidelines like SPIRIT-AI, MI-CLAIM, and CONSORT-AI is necessary to maintain consistency and mitigate biases in AI research. In summary, the integration of LLMs into clinical neurology offers immense promise while presenting formidable challenges. Awareness of these considerations is vital for harnessing the potential of AI in neurologic care effectively and enhancing patient care quality and safety. The article serves as a guide for health care organizations, researchers, and neurologists navigating this transformative landscape. © 2024 American Academy of Neurology.
PB  - Lippincott Williams and Wilkins
C2  - 38759131
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - CONF
AU  - Wang, Z.
AU  - Wu, Z.
AU  - Guan, X.
AU  - Thaler, M.
AU  - Koshiyama, A.
AU  - Lu, Q.
AU  - Beepath, S.
AU  - Ertekin, E.
AU  - Perez-Ortiz, M.
TI  - JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models
PY  - 2024
T2  - EMNLP 2024 - 2024 Conference on Empirical Methods in Natural Language Processing, Findings of EMNLP 2024
SP  - 3227
EP  - 3246
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217623082&partnerID=40&md5=f6bd269d1e2e12308d3fd90dd6e3efb4
AB  - The use of Large Language Models (LLMs) in hiring has led to legislative actions to protect vulnerable demographic groups. This paper presents a novel framework for benchmarking hierarchical gender hiring bias in Large Language Models (LLMs) for resume scoring, revealing significant issues of reverse gender hiring bias and overdebiasing. Our contributions are fourfold: Firstly, we introduce a new construct grounded in labour economics, legal principles, and critiques of current bias benchmarks: hiring bias can be categorized into two types: Level bias (difference in the average outcomes between demographic counterfactual groups) and Spread bias (difference in the variance of outcomes between demographic counterfactual groups); Level bias can be further subdivided into statistical bias (i.e. changing with non-demographic content) and taste-based bias (i.e. consistent regardless of non-demographic content). Secondly, the framework includes rigorous statistical and computational hiring bias metrics, such as Rank After Scoring (RAS), Rank-based Impact Ratio, Permutation Test, and Fixed Effects Model. Thirdly, we analyze gender hiring biases in ten state-of-the-art LLMs. Seven out of ten LLMs show significant biases against males in at least one industry. An industry-effect regression reveals that the healthcare industry is the most biased against males. Moreover, we found that the bias performance remains invariant with resume content for eight out of ten LLMs. This indicates that the bias performance measured in this paper might apply to other resume datasets with different resume qualities. Fourthly, we provide a user-friendly demo and resume dataset to support the adoption and practical use of the framework, which can be generalized to other social traits and tasks. © 2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Riina, N.
AU  - Patlolla, L.
AU  - Joya, C.H.
AU  - Bautista, R.
AU  - Olivar-Villanueva, M.
AU  - Kumar, A.
TI  - An Evaluation of English to Spanish Medical Translation by Large Language Models: A Quantitative and Qualitative Analysis
PY  - 2024
T2  - AMTA 2024 - 16th Conference of the Association for Machine Translation in the Americas
VL  - 2
SP  - 237
EP  - 254
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217364847&partnerID=40&md5=e535514276d8dd7d97efc152b3bf3a40
AB  - Medical translation is a critical tool for overcoming the barriers of discordant cultural backgrounds and languages within the healthcare field. Large Language Models (LLMs) that advertise translation and multilingual capabilities, like ChatGPT, pose a newfound solution that could include unique abilities that a typical machine translation (MT) system does not exhibit (e.g. catering a translation for a specific patient, such as a child). This work compares the English to Spanish translation of three LLMs: ChatGPT3.5 Turbo, ChatGPT4o, and Aguila with the performance of Google Translate. Medical Translations were provided by MedlinePlus, a parallel dataset developed by the National Library of Medicine that consists of four categories of information for patients in English and Spanish: health topics, patient instructions, lab tests, and drug information. Each model translated 15,816 sentences which were scored by three automated metrics: BLEU, BERTscore, and METEOR. 100 sentences were also graded by three Spanish interpreters using metrics defined in this paper: Fluency (is the translation correct Spanish), Adequacy (does the translation convey the original meaning), and Patient-friendliness (is the translation written in language that a patient can easily understand). The human evaluated translations were then subject to qualitative analysis that examined frequent errors and word choice. Automated results indicated that Chat-GPT4o performed equivalently to Google Translate, with ChatGPT3.5 not far behind. Human rated scores found both Chat-GPT models to perform statistically similar to Google Translate in all three metrics. Aguila, the only model intended for primarily Spanish and Catalan use, surprisingly performed much worse than the other models. However, qualitative analysis of Aguila's translations reveal the use of terms that may reach a broader audience, rendering the Spanish used more accessible than the other models. It is important, as MT systems are applied to the medical field, that the translations provided by these models are not only factually correct and patient safe, but accessible by vulnerable populations. This work provides an evaluation of the most recent ChatGPT model's medical translations with a comparison to a well-researched system, Google Translate, using verified metrics. Our work also highlights small, yet important disparities between the Spanish use of LLMs with English as a primary language and other LLMs that are intended for Spanish use. © 2024 AMTA 2024 - 16th Conference of the Association for Machine Translation in the Americas. All rights reserved.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Wang, A.
AU  - Kim, E.
AU  - Oleru, O.
AU  - Seyidova, N.
AU  - Taub, P.J.
TI  - Artificial Intelligence in Plastic Surgery: ChatGPT as a Tool to Address Disparities in Health Literacy
PY  - 2024
T2  - Plastic and reconstructive surgery
VL  - 153
IS  - 6
SP  - 1232e
EP  - 1234e
DO  - 10.1097/PRS.0000000000011202
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185300220&doi=10.1097%2fPRS.0000000000011202&partnerID=40&md5=363b41e3efd2799283c071fc3eef7481
C2  - 37983817
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - CONF
AU  - Baeza-Yates, R.
TI  - Introduction to Responsible AI
PY  - 2024
T2  - WSDM 2024 - Proceedings of the 17th ACM International Conference on Web Search and Data Mining
SP  - 1114
EP  - 1117
DO  - 10.1145/3616855.3636455
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191698391&doi=10.1145%2f3616855.3636455&partnerID=40&md5=8a6c5cad77622507806dd9fbe02d097c
AB  - In the first part of this tutorial we define responsible AI and we discuss the problems embedded in terms like ethical or trustworthy AI. In the second part, to set the stage, we cover irresponsible AI: discrimination (e.g., the impact of human biases); pseudo-science (e.g., biometric based behavioral predictions); human limitations (e.g., human incompetence, cognitive biases); technical limitations (data as a proxy of reality, wrong evaluation); social impact (e.g., unfair digital markets or mental health and disinformation issues created by large language models); environmental impact (e.g., indiscriminate use of computing resources). These examples do have a personal bias but set the context for the third part where we cover the current challenges: ethical principles, governance and regulation. We finish by discussing our responsible AI initiatives, many recommendations, and some philosophical issues. © 2024 ACM.
PB  - Association for Computing Machinery, Inc
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Rini, P.L.
AU  - Gayathri, K.S.
TI  - Cognitive decline assessment using semantic linguistic content and transformer deep learning architecture
PY  - 2024
T2  - International Journal of Language and Communication Disorders
VL  - 59
IS  - 3
SP  - 1110
EP  - 1127
DO  - 10.1111/1460-6984.12973
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176920401&doi=10.1111%2f1460-6984.12973&partnerID=40&md5=c53ee029c986a277907fdafde44933da
AB  - Background: Dementia is a cognitive decline that leads to the progressive deterioration of an individual's ability to perform daily activities independently. As a result, a considerable amount of time and resources are spent on caretaking. Early detection of dementia can significantly reduce the effort and resources needed for caretaking. Aims: This research proposes an approach for assessing cognitive decline by analysing speech data, specifically focusing on speech relevance as a crucial indicator for memory recall. Methods & Procedures: This is a cross-sectional, online, self-administered. The proposed method used deep learning architecture based on transformers, with BERT (Bidirectional Encoder Representations from Transformers) and Sentence-Transformer to derive encoded representations of speech transcripts. These representations provide contextually descriptive information that is used to analyse the relevance of sentences in their respective contexts. The encoded information is then compared using cosine similarity metrics to measure the relevance of uttered sequences of sentences. The study uses the Pitt Corpus Dementia dataset for experimentation, which consists of speech data from individuals with and without dementia. The accuracy of the proposed multi-QA-MPNet (Multi-Query Maximum Inner Product Search Pretraining) model is compared with other pretrained transformer models of Sentence-Transformer. Outcomes & Results: The results show that the proposed approach outperforms the other models in capturing context level information, particularly semantic memory. Additionally, the study explores the suitability of different similarity measures to evaluate the relevance of uttered sequences of sentences. The experimentation reveals that cosine similarity is the most appropriate measure for this task. Conclusions & Implications: This finding has significant implications for the early warning signs of dementia, as it suggests that cosine similarity metrics can effectively capture the semantic relevance of spoken language. The persistent cognitive decline over time acts as one of the indicators for prevalence of dementia. Additionally early dementia could be recognised by analysis on other modalities like speech and brain images. WHAT THIS PAPER ADDS: What is already known on this subject It is already known that speech- and language-based detection methods can be useful for dementia diagnosis, as language difficulties are often early signs of the disease. Additionally, deep learning algorithms have shown promise in detecting and diagnosing dementia through analysing large datasets, particularly in speech- and language-based detection methods. However, further research is needed to validate the performance of these algorithms on larger and more diverse datasets and to address potential biases and limitations. What this paper adds to existing knowledge This study presents a unique and effective approach for cognitive decline assessment through analysing speech data. The study provides valuable insights into the importance of context and semantic memory in accurately detecting the potential in dementia and demonstrates the applicability of deep learning models for this purpose. The findings of this study have important clinical implications and can inform future research and development in the field of dementia detection and care. What are the potential or actual clinical implications of this work? The proposed approach for cognitive decline assessment using speech data and deep learning models has significant clinical implications. It has the potential to improve the accuracy and efficiency of dementia diagnosis, leading to earlier detection and more effective treatments, which can improve patient outcomes and quality of life. © 2023 Royal College of Speech and Language Therapists.
PB  - John Wiley and Sons Inc
C2  - 37971395
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Howard, E.C.
AU  - Chong, N.Y.K.
AU  - Carnino, J.M.
AU  - Levi, J.R.
TI  - Comparison of ChatGPT knowledge against 2020 consensus statement on ankyloglossia in children
PY  - 2024
T2  - International Journal of Pediatric Otorhinolaryngology
VL  - 180
C7  - 111957
DO  - 10.1016/j.ijporl.2024.111957
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190423018&doi=10.1016%2fj.ijporl.2024.111957&partnerID=40&md5=44dc59921e965585db3abce84e6ccee5
AB  - Objective: This paper evaluates ChatGPT's accuracy and consistency in providing information on ankyloglossia, a congenital oral condition. Assessing alignment with expert consensus, the study explores potential implications for patients relying on AI for medical information. Methods: Statements from the 2020 clinical consensus statement on ankyloglossia were presented to ChatGPT, and its responses were scored using a 9-point Likert scale. The study analyzed the mean and standard deviation of ChatGPT scores for each statement. Statistical analysis was conducted using Excel. Results: Among the 63 statements assessed, 67 % of ChatGPT responses closely aligned with expert consensus mean scores. However, 17 % (11/63) were statements in which the ChatGPT mean response was different from the CCS mean by 2.0 or greater, raising concerns about ChatGPT's potential influence in disseminating uncertain or debated medical information. Variations in mean scores highlighted discrepancies, with some statements showing significant deviations from expert opinions. Conclusion: While ChatGPT mirrored medical viewpoints on ankyloglossia, alignment with non-consensus statements raises caution in relying on it for medical advice. Future research should refine AI models, address inaccuracies, and explore diverse user queries for safe integration into medical decision-making. Despite potential benefits, ongoing examination of ChatGPT's power and limitations is crucial, considering its impact on health equity and information access. © 2024 Elsevier B.V.
PB  - Elsevier Ireland Ltd
C2  - 38640573
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Hake, J.
AU  - Crowley, M.
AU  - Coy, A.
AU  - Shanks, D.
AU  - Eoff, A.
AU  - Kirmer-Voss, K.
AU  - Dhanda, G.
AU  - Parente, D.J.
TI  - Quality, Accuracy, and Bias in ChatGPT-Based Summarization of Medical Abstracts
PY  - 2024
T2  - Annals of Family Medicine
VL  - 22
IS  - 2
SP  - 113
EP  - 120
DO  - 10.1370/afm.3075
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188913837&doi=10.1370%2fafm.3075&partnerID=40&md5=7bfbfcf73d754633036232fd62e44025
AB  - PURPOSE Worldwide clinical knowledge is expanding rapidly, but physicians have sparse time to review scientific literature. Large language models (eg, Chat Generative Pretrained Transformer [ChatGPT]), might help summarize and prioritize research articles to review. However, large language models sometimes “hallucinate” incorrect information. METHODS We evaluated ChatGPT’s ability to summarize 140 peer-reviewed abstracts from 14 journals. Physicians rated the quality, accuracy, and bias of the ChatGPT summaries. We also compared human ratings of relevance to various areas of medicine to ChatGPT relevance ratings. RESULTS ChatGPT produced summaries that were 70% shorter (mean abstract length of 2,438 characters decreased to 739 characters). Summaries were nevertheless rated as high quality (median score 90, interquartile range [IQR] 87.0-92.5; scale 0-100), high accuracy (median 92.5, IQR 89.0-95.0), and low bias (median 0, IQR 0-7.5). Serious inaccuracies and hallucinations were uncommon. Classification of the relevance of entire journals to various fields of medicine closely mirrored physician classifications (nonlinear standard error of the regression [SER] 8.6 on a scale of 0-100). However, relevance classification for individual articles was much more modest (SER 22.3). CONCLUSIONS Summaries generated by ChatGPT were 70% shorter than mean abstract length and were characterized by high quality, high accuracy, and low bias. Conversely, ChatGPT had modest ability to classify the relevance of articles to medical specialties. We suggest that ChatGPT can help family physicians accelerate review of the scientific literature and have developed software (pyJournalWatch) to support this application. Life-critical medical decisions should remain based on full, critical, and thoughtful evaluation of the full text of research articles in context with clinical guidelines. © 2024, Annals of Family Medicine, Inc. All rights reserved.
PB  - Annals of Family Medicine, Inc
C2  - 38527823
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 14
ER  -

TY  - CONF
AU  - Rawat, R.
AU  - McBride, H.
AU  - Ghosh, R.
AU  - Nirmal, D.
AU  - Moon, J.
AU  - Alamuri, D.
AU  - O’Brien, S.
AU  - Zhu, K.
TI  - DiversityMedQA: A Benchmark for Assessing Demographic Biases in Medical Diagnosis using Large Language Models
PY  - 2024
T2  - NLP4PI 2024 - 3rd Workshop on NLP for Positive Impact, Proceedings of the Workshop
SP  - 334
EP  - 348
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216923537&partnerID=40&md5=8bc68caff877e13652a3f2e3e6e476cd
AB  - As large language models (LLMs) gain traction in healthcare, concerns about their susceptibility to demographic biases are growing. We introduce DiversityMedQA1, a novel benchmark designed to assess LLM responses to medical queries across diverse patient demographics, such as gender and ethnicity. By perturbing questions from the MedQA dataset, which comprises of medical board exam questions, we created a benchmark that captures the nuanced differences in medical diagnosis across varying patient profiles. To ensure that our perturbations did not alter the clinical outcomes, we implemented a filtering strategy to validate each perturbation, so that any performance discrepancies would be indicative of bias. Our findings reveal notable discrepancies in model performance when tested against these demographic variations. By releasing DiversityMedQA, we provide a resource for evaluating and mitigating demographic bias in LLM medical diagnoses. © 2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Farkhani, S.
AU  - Demnitz, N.
AU  - Boraxbekk, C.-J.
AU  - Lundell, H.
AU  - Siebner, H.R.
AU  - Petersen, E.T.
AU  - Madsen, K.H.
TI  - End-to-end volumetric segmentation of white matter hyperintensities using deep learning
PY  - 2024
T2  - Computer Methods and Programs in Biomedicine
VL  - 245
C7  - 108008
DO  - 10.1016/j.cmpb.2024.108008
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184998077&doi=10.1016%2fj.cmpb.2024.108008&partnerID=40&md5=bab8ad1eb4176ea74c68ad91255fd5ab
AB  - Background and objectives: Reliable detection of white matter hyperintensities (WMH) is crucial for studying the impact of diffuse white-matter pathology on brain health and monitoring changes in WMH load over time. However, manual annotation of 3D high-dimensional neuroimages is laborious and can be prone to biases and errors in the annotation procedure. In this study, we evaluate the performance of deep learning (DL) segmentation tools and propose a novel volumetric segmentation model incorporating self-attention via a transformer-based architecture. Ultimately, we aim to evaluate diverse factors that influence WMH segmentation, aiming for a comprehensive analysis of the state-of-the-art algorithms in a broader context. Methods: We trained state-of-the-art DL algorithms, and incorporated advanced attention mechanisms, using structural fluid-attenuated inversion recovery (FLAIR) image acquisitions. The anatomical MRI data utilized for model training was obtained from healthy individuals aged 62–70 years in the Live active Successful Aging (LISA) project. Given the potential sparsity of lesion volume among healthy aging individuals, we explored the impact of incorporating a weighted loss function and ensemble models. To assess the generalizability of the studied DL models, we applied the trained algorithm to an independent subset of data sourced from the MICCAI WMH challenge (MWSC). Notably, this subset had vastly different acquisition parameters compared to the LISA dataset used for training. Results: Consistently, DL approaches exhibited commendable segmentation performance, achieving the level of inter-rater agreement comparable to expert performance, ensuring superior quality segmentation outcomes. On the out of sample dataset, the ensemble models exhibited the most outstanding performance. Conclusions: DL methods generally surpassed conventional approaches in our study. While all DL methods performed comparably, incorporating attention mechanisms could prove advantageous in future applications with a wider availability of training data. As expected, our experiments indicate that the use of ensemble-based models enables the superior generalization in out-of-distribution settings. We believe that introducing DL methods in the WHM annotation workflow in heathy aging cohorts is promising, not only for reducing the annotation time required, but also for eventually improving accuracy and robustness via incorporating the automatic segmentations in the evaluation procedure. © 2024
PB  - Elsevier Ireland Ltd
C2  - 38290291
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Agarwal, A.
AU  - Banerjee, T.
AU  - Romine, W.L.
AU  - Cajita, M.
TI  - Debias-CLR: A Contrastive Learning Based Debiasing Method for Algorithmic Fairness in Healthcare Applications
PY  - 2024
T2  - Proceedings - 2024 IEEE International Conference on Big Data, BigData 2024
SP  - 6411
EP  - 6419
DO  - 10.1109/BigData62323.2024.10825827
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218058674&doi=10.1109%2fBigData62323.2024.10825827&partnerID=40&md5=d28160e2992645c6e5456e8fdc9e1df8
AB  - Artificial intelligence based predictive models trained on the clinical notes of patients can be demographically biased, often influenced by the demographic distribution of the training data. This could lead to adverse healthcare disparities in predicting outcomes like length of stay of the patients. To avoid such possibilities, it is necessary to mitigate the demographic biases within these models so that the model predicts outcomes for individual patients in a fair manner. We proposed an implicit in-processing debiasing method to combat disparate treatment which occurs when the machine learning model predict different outcomes for individuals based on the sensitive attributes like gender, ethnicity, race, and likewise. For this purpose, we used clinical notes of heart failure patients and used diagnostic codes, procedure reports and physiological vitals of the patients. We used Clinical Bidirectional Encoder Representations from Transformers (Clinical BERT) to obtain feature embeddings within the diagnostic codes and procedure reports, and Long Short-Term Memory (LSTM) autoencoders to obtain feature embeddings within the physiological vitals. Then, we trained two separate deep learning contrastive learning frameworks, one for gender and the other for ethnicity to obtain debiased representations within those demographic traits. We called this debiasing framework as Debias-CLR. We leveraged clinical phenotypes of the patients identified in the diagnostic codes and procedure reports in the previous study to measure the fairness statistically. We found that Debias-CLR was able to reduce the Single-Category Word Embedding Association Test (SC-WEAT) effect size score when debiasing for gender from 0.8 to 0.3 and from 0.4 to 0.2 while using clinical phenotypes in the diagnostic codes and procedure reports respectively as targets. Similarly, after debiasing for ethnicity, the SC-WEAT effect size score reduced from 1 to 0.5 and from -1 to 0.3 in an opposite bias direction while using clinical phenotypes in the diagnostic codes and procedure reports respectively as targets. We further found that in order to obtain fair representations in the embedding space using Debias-CLR, the accuracy of the predictive models on downstream tasks like predicting length of stay of the patients did not get reduced as compared to using the un-debiased counterparts for training the predictive models. Hence, we conclude that our proposed approach, Debias-CLR is fair and representative in mitigating demographic biases and can reduce health disparities by making fair predictions for the underrepresented populations. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Lukic, S.
AU  - Fan, Z.
AU  - García, A.M.
AU  - Welch, A.E.
AU  - Ratnasiri, B.M.
AU  - Wilson, S.M.
AU  - Henry, M.L.
AU  - Vonk, J.
AU  - Deleon, J.
AU  - Miller, B.L.
AU  - Miller, Z.
AU  - Mandelli, M.L.
AU  - Gorno-Tempini, M.L.
TI  - Discriminating nonfluent/agrammatic and logopenic PPA variants with automatically extracted morphosyntactic measures from connected speech
PY  - 2024
T2  - Cortex
VL  - 173
SP  - 34
EP  - 48
DO  - 10.1016/j.cortex.2023.12.013
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185721908&doi=10.1016%2fj.cortex.2023.12.013&partnerID=40&md5=ace0a99c7d843a1b233cd45bbdcdf76a
AB  - Morphosyntactic assessments are important for characterizing individuals with nonfluent/agrammatic variant primary progressive aphasia (nfvPPA). Yet, standard tests are subject to examiner bias and often fail to differentiate between nfvPPA and logopenic variant PPA (lvPPA). Moreover, relevant neural signatures remain underexplored. Here, we leverage natural language processing tools to automatically capture morphosyntactic disturbances and their neuroanatomical correlates in 35 individuals with nfvPPA relative to 10 healthy controls (HC) and 26 individuals with lvPPA. Participants described a picture, and ensuing transcripts were analyzed via part-of-speech tagging to extract sentence-related features (e.g., subordinating and coordinating conjunctions), verbal-related features (e.g., tense markers), and nominal-related features (e.g., subjective and possessive pronouns). Gradient boosting machines were used to classify between groups using all features. We identified the most discriminant morphosyntactic marker via a feature importance algorithm and examined its neural correlates via voxel-based morphometry. Individuals with nfvPPA produced fewer morphosyntactic elements than the other two groups. Such features robustly discriminated them from both individuals with lvPPA and HCs with an AUC of .95 and .82, respectively. The most discriminatory feature corresponded to subordinating conjunctions was correlated with cortical atrophy within the left posterior inferior frontal gyrus across groups (pFWE < .05). Automated morphosyntactic analysis can efficiently differentiate nfvPPA from lvPPA. Also, the most sensitive morphosyntactic markers correlate with a core atrophy region of nfvPPA. Our approach, thus, can contribute to a key challenge in PPA diagnosis. © 2024
PB  - Masson SpA
C2  - 38359511
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Bakkum, M.J.
AU  - Hartjes, M.G.
AU  - Piët, J.D.
AU  - Donker, E.M.
AU  - Likic, R.
AU  - Sanz, E.
AU  - de Ponti, F.
AU  - Verdonk, P.
AU  - Richir, M.C.
AU  - van Agtmael, M.A.
AU  - Tichelaar, J.
TI  - Using artificial intelligence to create diverse and inclusive medical case vignettes for education
PY  - 2024
T2  - British Journal of Clinical Pharmacology
VL  - 90
IS  - 3
SP  - 640
EP  - 648
DO  - 10.1111/bcp.15977
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181682461&doi=10.1111%2fbcp.15977&partnerID=40&md5=ec0fd4eee1519302e0f568db767022d0
AB  - Aims: Medical case vignettes play a crucial role in medical education, yet they often fail to authentically represent diverse patients. Moreover, these vignettes tend to oversimplify the complex relationship between patient characteristics and medical conditions, leading to biased and potentially harmful perspectives among students. Displaying aspects of patient diversity, such as ethnicity, in written cases proves challenging. Additionally, creating these cases places a significant burden on teachers in terms of labour and time. Our objective is to explore the potential of artificial intelligence (AI)-assisted computer-generated clinical cases to expedite case creation and enhance diversity, along with AI-generated patient photographs for more lifelike portrayal. Methods: In this study, we employed ChatGPT (OpenAI, GPT 3.5) to develop diverse and inclusive medical case vignettes. We evaluated various approaches and identified a set of eight consecutive prompts that can be readily customized to accommodate local contexts and specific assignments. To enhance visual representation, we utilized Adobe Firefly beta for image generation. Results: Using the described prompts, we consistently generated cases for various assignments, producing sets of 30 cases at a time. We ensured the inclusion of mandatory checks and formatting, completing the process within approximately 60 min per set. Conclusions: Our approach significantly accelerated case creation and improved diversity, although prioritizing maximum diversity compromised representativeness to some extent. While the optimized prompts are easily reusable, the process itself demands computer skills not all educators possess. To address this, we aim to share all created patients as open educational resources, empowering educators to create cases independently. © 2023 The Authors. British Journal of Clinical Pharmacology published by John Wiley & Sons Ltd on behalf of British Pharmacological Society.
PB  - John Wiley and Sons Inc
C2  - 38016816
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 14
ER  -

TY  - JOUR
AU  - Baker, H.P.
AU  - Dwyer, E.
AU  - Kalidoss, S.
AU  - Hynes, K.
AU  - Wolf, J.
AU  - Strelzow, J.A.
TI  - ChatGPT's Ability to Assist with Clinical Documentation: A Randomized Controlled Trial
PY  - 2024
T2  - Journal of the American Academy of Orthopaedic Surgeons
VL  - 32
IS  - 3
SP  - 123
EP  - 129
DO  - 10.5435/JAAOS-D-23-00474
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182957139&doi=10.5435%2fJAAOS-D-23-00474&partnerID=40&md5=e4ad1320bf4a252c2e08196374ae04be
AB  - Introduction:Clinical documentation is a critical aspect of health care that enables healthcare providers to communicate effectively with each other and maintain accurate patient care records. Artificial intelligence tools, such as chatbots and virtual assistants, have the potential to assist healthcare providers in clinical documentation. ChatGPT is an artificial intelligence conversational model that generates human-like responses to text-based prompts. In this study, we sought to investigate ChatGPT's ability to assist with writing a history of present illness based on standardized patient histories.Methods:A blinded, randomized controlled study was conducted to compare the use of typing, dictation, and ChatGPT as tools to document history of present illness (HPI) of standardized patient histories. Eleven study participants, consisting of medical students, orthopaedic surgery residents, and attending surgeons, completed three HPIs using a different documentation technique for each one. Participants were randomized into cohorts based on the type of documentation technique. Participants were asked to interview standardized patients and document the patient's history of present illness using their assigned method.Results:ChatGPT was found to be intermediate for speed; dictation was fastest, but produced markedly longer and higher quality patient histories based on Physician Documentation Quality Instrument score compared with dictation and typing. However, ChatGPT included erroneous information in 36% of the documents. Poor agreement existed on the quality of patient histories between reviewers.Discussion:Our study suggests that ChatGPT has the potential to improve clinical documentation by producing more comprehensive and organized HPIs. ChatGPT can generate longer and more detailed documentation compared with typing or dictation documentation methods. However, additional studies are needed to investigate and address concerns regarding privacy, bias, and accuracy of information.  © American Academy of Orthopaedic Surgeons.
PB  - Lippincott Williams and Wilkins
C2  - 37976385
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 24
ER  -

TY  - CONF
AU  - Gabriel, S.
AU  - Lyu, L.
AU  - Siderius, J.
AU  - Ghassemi, M.
AU  - Andreas, J.
AU  - Ozdaglar, A.
TI  - MisinfoEval: Generative AI in the Era of “Alternative Facts”
PY  - 2024
T2  - EMNLP 2024 - 2024 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference
SP  - 8566
EP  - 8578
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217780611&partnerID=40&md5=a7f10293564545a9eb5d6cea2493a525
AB  - The spread of misinformation on social media platforms threatens democratic processes, contributes to massive economic losses, and endangers public health. Many efforts to address misinformation focus on a knowledge deficit model and propose interventions for improving users' critical thinking through access to facts. Such efforts are often hampered by challenges with scalability, and by platform users' personal biases. The emergence of generative AI presents promising opportunities for countering misinformation at scale across ideological barriers. In this paper, we introduce a framework (MisinfoEval) for generating and comprehensively evaluating large language model (LLM) based misinformation interventions. We present (1) an experiment with a simulated social media environment to measure effectiveness of misinformation interventions, and (2) a second experiment with personalized explanations tailored to the demographics and beliefs of users with the goal of countering misinformation by appealing to their pre-existing values. Our findings confirm that LLM-based interventions are highly effective at correcting user behavior (improving overall user accuracy at reliability labeling by up to 41.72%). Furthermore, we find that users favor more personalized interventions when making decisions about news reliability and users shown personalized interventions have significantly higher accuracy at identifying misinformation. © 2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Grote, T.
AU  - Berens, P.
TI  - A paradigm shift?—On the ethics of medical large language models
PY  - 2024
T2  - Bioethics
VL  - 38
IS  - 5
SP  - 383
EP  - 390
DO  - 10.1111/bioe.13283
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189500258&doi=10.1111%2fbioe.13283&partnerID=40&md5=99a33d22c004b619baca164bcb2be094
AB  - After a wave of breakthroughs in image-based medical diagnostics and risk prediction models, machine learning (ML) has turned into a normal science. However, prominent researchers are claiming that another paradigm shift in medical ML is imminent—due to most recent staggering successes of large language models—from single-purpose applications toward generalist models, driven by natural language. This article investigates the implications of this paradigm shift for the ethical debate. Focusing on issues like trust, transparency, threats of patient autonomy, responsibility issues in the collaboration of clinicians and ML models, fairness, and privacy, it will be argued that the main problems will be continuous with the current debate. However, due to functioning of large language models, the complexity of all these problems increases. In addition, the article discusses some profound challenges for the clinical evaluation of large language models and threats to the reproducibility and replicability of studies about large language models in medicine due to corporate interests. © 2024 The Authors. Bioethics published by John Wiley & Sons Ltd.
PB  - John Wiley and Sons Inc
C2  - 38523587
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - JOUR
AU  - Nazir, A.
AU  - Chakravarthy, T.K.
AU  - Cecchini, D.A.
AU  - Khajuria, R.
AU  - Sharma, P.
AU  - Mirik, A.T.
AU  - Kocaman, V.
AU  - Talby, D.
TI  - LangTest: A comprehensive evaluation library for custom LLM and NLP models[Formula presented]
PY  - 2024
T2  - Software Impacts
VL  - 19
C7  - 100619
DO  - 10.1016/j.simpa.2024.100619
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184904107&doi=10.1016%2fj.simpa.2024.100619&partnerID=40&md5=5a0055394edbcfbf1767932105532fbe
AB  - The use of natural language processing (NLP) models, including the more recent large language models (LLM) in real-world applications obtained relevant success in the past years. To measure the performance of these systems, traditional performance metrics such as accuracy, precision, recall, and f1-score are used. Although it is important to measure the performance of the models in those terms, natural language often requires an holistic evaluation that consider other important aspects such as robustness, bias, accuracy, toxicity, fairness, safety, efficiency, clinical relevance, security, representation, disinformation, political orientation, sensitivity, factuality, legal concerns, and vulnerabilities. To address the gap, we introduce LangTest, an open source Python toolkit, aimed at reshaping the evaluation of LLMs and NLP models in real-world applications. The project aims to empower data scientists, enabling them to meet high standards in the ever-evolving landscape of AI model development. Specifically, it provides a comprehensive suite of more than 60 test types, ensuring a more comprehensive understanding of a model's behavior and responsible AI use. In this experiment, a Named Entity Recognition (NER) clinical model showed significant improvement in its capabilities to identify clinical entities in text after applying data augmentation for robustness. © 2024 John Snow Labs Inc
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 9
ER  -

TY  - CONF
AU  - Ong, Y.J.
AU  - Gala, J.
AU  - An, S.
AU  - Moore, R.
AU  - Jadav, D.
TI  - Adversarially Exploring Vulnerabilities in LLMs to Evaluate Social Biases
PY  - 2024
T2  - Proceedings - 2024 IEEE International Conference on Big Data, BigData 2024
SP  - 5289
EP  - 5297
DO  - 10.1109/BigData62323.2024.10825408
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218036347&doi=10.1109%2fBigData62323.2024.10825408&partnerID=40&md5=49a9b19e85a26b7702aa49ec6b555763
AB  - Generative AI has caused a paradigm shift in the area of Artificial Intelligence (AI) and as such has inspired much new research, especially on Large Language Models (LLMs). LLMs are transforming how people interact with computers in service-oriented fields in both the consumer (for example: retail, travel, education, healthcare) and enterprise (customer care, field service, sales, marketing, etc.) spaces. One barrier to widespread adoption is the current unpredictability of LLM behavior: users must trust that LLM-based services and systems are accurate, fair, and unbiased. Model responses that exhibit biases related to race, social status, and other sensitive topics can have serious consequences, ranging from lack of trust in the model to adverse social implications for consumers, all the way to damage to the reputations of the corporations that provide them. This study explores how to uncover biases related to social stigmas in LLM output, by using an adversarial prompt-based approach. Discovering model vulnerabilities of this type is a nontrivial task due to the large search space, making it resource-intensive. We present an evaluation framework for probing and analyzing the behaviors of multiple LLMs systematically. We use a curated set of adversarial prompts with a focus on uncovering biased responses to prompts associated with social attributes. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Lee, J.-H.
AU  - Choi, E.
AU  - McDougal, R.
AU  - Lytton, W.W.
TI  - GPT-4 Performance for Neurologic Localization
PY  - 2024
T2  - Neurology: Clinical Practice
VL  - 14
IS  - 3
C7  - e200293
DO  - 10.1212/CPJ.0000000000200293
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206472793&doi=10.1212%2fCPJ.0000000000200293&partnerID=40&md5=88d46b40ae96a3dbcd20e4e187a05273
AB  - Background and Objectives In health care, large language models such as Generative Pretrained Transformers (GPTs), trained on extensive text datasets, have potential applications in reducing health care disparities across regions and populations. Previous software developed for lesion localization has been limited in scope. This study aims to evaluate the capability of GPT-4 for lesion localization based on clinical presentation. Methods GPT-4 was prompted using history and neurologic physical examination (H&P) from published cases of acute stroke followed by questions for clinical reasoning with answering for “single or multiple lesions,” “side,” and “brain region” using Zero-Shot Chain-of-Thought and Text Classification prompting. GPT-4 output on 3 separate trials for each of 46 cases was compared with imaging-based localization. Results GPT-4 successfully processed raw text from H&P to generate accurate neuroanatomical localization and detailed clinical reasoning. Performance metrics across trial-based analysis for specificity, sensitivity, precision, and F1-score were 0.87, 0.74, 0.75, and 0.74, respectively, for side; 0.94, 0.85, 0.84, and 0.85, respectively, for brain region. Class labels within the brain region were similarly high for all regions except the cerebellum and were also similar when considering all 3 trials to examine metrics by case. Errors were due to extrinsic causes—inadequate information in the published cases, and intrinsic causes—failures of logic or inadequate knowledge base. Discussion This study reveals capabilities of GPT-4 in the localization of acute stroke lesions, showing a potential future role as a clinical tool in neurology. Copyright © 2024 The Author(s).
PB  - Lippincott Williams and Wilkins
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Warbhe, M.K.
AU  - Verma, P.
TI  - A Review on the Applications of Natural Language Processing in Healthcare
PY  - 2024
T2  - Proceedings of the 4th International Conference on Ubiquitous Computing and Intelligent Information Systems, ICUIS 2024
SP  - 1317
EP  - 1323
DO  - 10.1109/ICUIS64676.2024.10866525
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000258219&doi=10.1109%2fICUIS64676.2024.10866525&partnerID=40&md5=80f2ebdcc814e8c8d710ae3e487494cf
AB  - In healthcare, Natural Language Processing (NLP) is a technique to improve clinical decision support, patient care, and medical research. By automating clinical recording, NLP enhances the calibre of medical records and professional communication. It makes Electronic Health Records (EHRs) easier to use by facilitating quick and easy data entry, retrieval, and access to vital patient data that helps with precise diagnosis and efficient treatment planning. Through the analysis of patient-specific data, NLP supports clinical decision support by delivering prompt recommendations. Chatbots and virtual assistants managing repetitive duties like booking appointments also improve patient connection. NLP makes it easier to analyse vast amounts of literature for medical research, facilitating the extraction and summary of important discoveries. Healthcare data processing requires text preparation, sentiment analysis, Named Entity Recognition (NER), semantic parsing, and Natural Language Generation (NLG). However, there are obstacles to overcome, like bias, ethical concerns, system integration, privacy, and data quality. The healthcare industry can use NLP to enhance patient outcomes, promote personalized medication, and streamline healthcare delivery by guaranteeing interoperability and setting standards. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Hanai, A.
AU  - Ishikawa, T.
AU  - Kawauchi, S.
AU  - Iida, Y.
AU  - Kawakami, E.
TI  - Generative artificial intelligence and non-pharmacological bias: an experimental study on cancer patient sexual health communications
PY  - 2024
T2  - BMJ Health and Care Informatics
VL  - 31
IS  - 1
C7  - e100924
DO  - 10.1136/bmjhci-2023-100924
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190220384&doi=10.1136%2fbmjhci-2023-100924&partnerID=40&md5=6333ef67bff4252243a9a78b15503aa5
AB  - Objectives The objective of this study was to explore the feature of generative artificial intelligence (AI) in asking sexual health among cancer survivors, which are often challenging for patients to discuss. Methods We employed the Generative Pre-trained Transformer-3.5 (GPT) as the generative AI platform and used DocsBot for citation retrieval (June 2023). A structured prompt was devised to generate 100 questions from the AI, based on epidemiological survey data regarding sexual difficulties among cancer survivors. These questions were submitted to Bot1 (standard GPT) and Bot2 (sourced from two clinical guidelines). Results No censorship of sexual expressions or medical terms occurred. Despite the lack of reflection on guideline recommendations, ‘consultation’ was significantly more prevalent in both bots’ responses compared with pharmacological interventions, with ORs of 47.3 (p<0.001) in Bot1 and 97.2 (p<0.001) in Bot2. Discussion Generative AI can serve to provide health information on sensitive topics such as sexual health, despite the potential for policy-restricted content. Responses were biased towards non-pharmacological interventions, which is probably due to a GPT model designed with the’s prohibition policy on replying to medical topics. This shift warrants attention as it could potentially trigger patients’ expectations for non-pharmacological interventions. © Author(s) (or their employer(s)) 2024.
PB  - BMJ Publishing Group
C2  - 38575326
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Ma, Z.
AU  - Mei, Y.
AU  - Long, Y.
AU  - Su, Z.
AU  - Gajos, K.Z.
TI  - Evaluating the Experience of LGBTQ+ People Using Large Language Model Based Chatbots for Mental Health Support
PY  - 2024
T2  - Conference on Human Factors in Computing Systems - Proceedings
C7  - 872
DO  - 10.1145/3613904.3642482
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194847199&doi=10.1145%2f3613904.3642482&partnerID=40&md5=d20b22621197868e3798bd4c9d494d47
AB  - LGBTQ+ individuals are increasingly turning to chatbots powered by large language models (LLMs) to meet their mental health needs. However, little research has explored whether these chatbots can adequately and safely provide tailored support for this demographic. We interviewed 18 LGBTQ+ and 13 non-LGBTQ+ participants about their experiences with LLM-based chatbots for mental health needs. LGBTQ+ participants relied on these chatbots for mental health support, likely due to an absence of support in real life. Notably, while LLMs offer prompt support, they frequently fall short in grasping the nuances of LGBTQ-specific challenges. Although fine-tuning LLMs to address LGBTQ+ needs can be a step in the right direction, it isn't the panacea. The deeper issue is entrenched in societal discrimination. Consequently, we call on future researchers and designers to look beyond mere technical refinements and advocate for holistic strategies that confront and counteract the societal biases burdening the LGBTQ+ community. © 2024 Copyright held by the owner/author(s)
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - McGowan, M.
AU  - Correia Martins, F.
AU  - Keen, J.-L.
AU  - Whitehead, A.
AU  - Davis, E.
AU  - Pathiraja, P.
AU  - Bolton, H.
AU  - Baldwin, P.
TI  - Can natural language processing be effectively applied for audit data analysis in gynaecological oncology at a UK cancer centre?
PY  - 2024
T2  - International Journal of Medical Informatics
VL  - 182
C7  - 105306
DO  - 10.1016/j.ijmedinf.2023.105306
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178996566&doi=10.1016%2fj.ijmedinf.2023.105306&partnerID=40&md5=9e4a5b9acd2cc3bfddce1e70ebe4289f
AB  - Background: The British Gynaecological Cancer Society (BGCS) has highlighted the disparity of ovarian cancer outcomes in the UK compared to other European countries. Therefore, cancer quality assurance audits and subspecialty training are important in improving the UK standard of care for these patients. The current workforce crisis afflicting the NHS creates difficulty in dedicating teams of clinicians to these audits. We present a single institution study to evaluate if NLP-generated code can improve the efficiency of ovarian cancer and subspeciality reaccreditations audits. We used the chat bot Google Bard to write Visual Basic Applications algorithms that utilise Excel files from electronic health records. Methods: Primary ovarian cancer data from 2019 to 2022 was retrospectively collected from the Cambridge University Hospital electronic health records. The surgical subspecialty reaccreditation audit analysed the 2022 surgical database. A modular coding approach with Google Bard was applied to generate audit algorithms. The time to complete these current audits was compared against the 2016 ovarian cancer and 2020 subspeciality reaccreditation audits. Results: The previous ovarian cancer audit conducted in 2016 required 3 clinicians for the 135 cases and data collection required 1800 min. Data analysis was completed in 300 min. The current ovarian cancer audit allocated 2 clinicians to the 600 surgical cases. Data collection was completed in 3120 min, 3360 min for code development and 720 min for testing. The 2020 subspecialty reaccreditation audit was completed in 360 min. The 2022 subspecialty reaccreditation audit was completed in 1680 min, with 960 min for code development, 240 for debugging and 480 min for testing. Conclusion: We have demonstrated that NLP-generated code can significantly increase the efficiency of surgical quality assurance audits by eliminating the need for manual data analysis. With the current trajectory of NLP development, increasingly complex algorithms can be developed with minimal programming knowledge. © 2023 Elsevier B.V.
PB  - Elsevier Ireland Ltd
C2  - 38065003
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Zhao, Y.
AU  - Wang, H.
AU  - Liu, Y.
AU  - Suhuang, W.
AU  - Wu, X.
AU  - Zheng, Y.
TI  - Can LLMs Replace Clinical Doctors? Exploring Bias in Disease Diagnosis by Large Language Models
PY  - 2024
T2  - EMNLP 2024 - 2024 Conference on Empirical Methods in Natural Language Processing, Findings of EMNLP 2024
SP  - 13914
EP  - 13935
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217616679&partnerID=40&md5=a9f3c232989c1947702a8c7691f63c87
AB  - The bias of disease prediction in Large Language Models (LLMs) is a critical yet underexplored issue, with potential implications for healthcare outcomes and equity. As LLMs increasingly find applications in healthcare, understanding and addressing their biases becomes paramount. This study focuses on this crucial topic, investigating the bias of disease prediction in models such as GPT-4, ChatGPT, and Qwen1.5-72b across gender, age range, and disease judgment behaviors.1 Utilizing a comprehensive real-clinical health record dataset of over 330,000 entries, we uncover that all three models exhibit distinct biases, indicating a pervasive issue of unfairness. To measure this, we introduce a novel metric-the diagnosis bias score, which reflects the ratio of prediction numbers to label numbers. Our in-depth analysis, based on this score, sheds light on the inherent biases in these models. In response to these findings, we propose a simple yet effective prompt-based solution to alleviate the observed bias in disease prediction with LLMs. This research underscores the importance of fairness in AI, particularly in healthcare applications, and offers a practical approach to enhance the equity of disease prediction models. © 2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Barcelona, V.
AU  - Scharp, D.
AU  - Moen, H.
AU  - Davoudi, A.
AU  - Idnay, B.R.
AU  - Cato, K.
AU  - Topaz, M.
TI  - Using Natural Language Processing to Identify Stigmatizing Language in Labor and Birth Clinical Notes
PY  - 2024
T2  - Maternal and Child Health Journal
VL  - 28
IS  - 3
SP  - 578
EP  - 586
DO  - 10.1007/s10995-023-03857-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180437558&doi=10.1007%2fs10995-023-03857-4&partnerID=40&md5=9e2fcb1396b322e7567c0767a638161a
AB  - Introduction: Stigma and bias related to race and other minoritized statuses may underlie disparities in pregnancy and birth outcomes. One emerging method to identify bias is the study of stigmatizing language in the electronic health record. The objective of our study was to develop automated natural language processing (NLP) methods to identify two types of stigmatizing language: marginalizing language and its complement, power/privilege language, accurately and automatically in labor and birth notes. Methods: We analyzed notes for all birthing people > 20 weeks’ gestation admitted for labor and birth at two hospitals during 2017. We then employed text preprocessing techniques, specifically using TF-IDF values as inputs, and tested machine learning classification algorithms to identify stigmatizing and power/privilege language in clinical notes. The algorithms assessed included Decision Trees, Random Forest, and Support Vector Machines. Additionally, we applied a feature importance evaluation method (InfoGain) to discern words that are highly correlated with these language categories. Results: For marginalizing language, Decision Trees yielded the best classification with an F-score of 0.73. For power/privilege language, Support Vector Machines performed optimally, achieving an F-score of 0.91. These results demonstrate the effectiveness of the selected machine learning methods in classifying language categories in clinical notes. Conclusion: We identified well-performing machine learning methods to automatically detect stigmatizing language in clinical notes. To our knowledge, this is the first study to use NLP performance metrics to evaluate the performance of machine learning methods in discerning stigmatizing language. Future studies should delve deeper into refining and evaluating NLP methods, incorporating the latest algorithms rooted in deep learning. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.
PB  - Springer
C2  - 38147277
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Chaturvedi, J.
AU  - Stewart, R.
AU  - Ashworth, M.
AU  - Roberts, A.
TI  - Distributions of recorded pain in mental health records: a natural language processing based study
PY  - 2024
T2  - BMJ Open
VL  - 14
IS  - 4
C7  - e079923
DO  - 10.1136/bmjopen-2023-079923
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191105477&doi=10.1136%2fbmjopen-2023-079923&partnerID=40&md5=4ef30f2ee184660d60d04ea1677feb17
AB  - Objective The objective of this study is to determine demographic and diagnostic distributions of physical pain recorded in clinical notes of a mental health electronic health records database by using natural language processing and examine the overlap in recorded physical pain between primary and secondary care. Design, setting and participants The data were extracted from an anonymised version of the electronic health records of a large secondary mental healthcare provider serving a catchment of 1.3 million residents in south London. These included patients under active referral, aged 18+ at the index date of 1 July 2018 and having at least one clinical document (≥30 characters) between 1 July 2017 and 1 July 2019. This cohort was compared with linked primary care records from one of the four local government areas. Outcome The primary outcome of interest was the presence of recorded physical pain within the clinical notes of the patients, not including psychological or metaphorical pain. Results A total of 27 211 patients were retrieved. Of these, 52% (14,202) had narrative text containing relevant mentions of physical pain. Older patients (OR 1.17, 95% CI 1.15 to 1.19), females (OR 1.42, 95% CI 1.35 to 1.49), Asians (OR 1.30, 95% CI 1.16 to 1.45) or black (OR 1.49, 95% CI 1.40 to 1.59) ethnicities, living in deprived neighbourhoods (OR 1.64, 95% CI 1.55 to 1.73) showed higher odds of recorded pain. Patients with severe mental illnesses were found to be less likely to report pain (OR 0.43, 95% CI 0.41 to 0.46, p<0.001). 17% of the cohort from secondary care also had records from primary care. Conclusion The findings of this study show sociodemographic and diagnostic differences in recorded pain. Specifically, lower documentation across certain groups indicates the need for better screening protocols and training on recognising varied pain presentations. Additionally, targeting improved detection of pain for minority and disadvantaged groups by care providers can promote health equity. © Author(s) (or their employer(s)) 2024.
PB  - BMJ Publishing Group
C2  - 38642997
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Yang, W.
AU  - Yi, D.
AU  - Zhou, X.
AU  - Leng, Y.
TI  - Translational analysis of data science and causal learning in real-world clinical evaluation of traditional Chinese medicine
PY  - 2024
T2  - Science of Traditional Chinese Medicine
VL  - 2
IS  - 1
SP  - 57
EP  - 65
DO  - 10.1097/st9.0000000000000025
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204157750&doi=10.1097%2fst9.0000000000000025&partnerID=40&md5=801795b673a2008903925b3d8c26f713
AB  - Real-world clinical evaluation of traditional Chinese medicine (RWCE-TCM) is a method for comprehensively evaluating the clinical effects of TCM, with the aim of delving into the causality between TCM intervention and clinical outcomes. The study explored data science and causal learning methods to transform RWD into reliable real-world evidence, aiming to provide an innovative approach for RWCE-TCM. This study proposes a 10-step data science methodology to address the challenges posed by diverse and complex data in RWCE-TCM. The methodology involves several key steps, including data integration and warehouse building, high-dimensional feature selection, the use of interpretable statistical machine learning algorithms, complex networks, and graph network analysis, knowledge mining techniques such as natural language processing and machine learning, observational study design, and the application of artificial intelligence tools to build an intelligent engine for translational analysis. The goal is to establish a method for clinical positioning, applicable population screening, and mining the structural association of TCM characteristic therapies. In addition, the study adopts the principle of real-world research and a causal learning method for TCM clinical data. We constructed a multidimensional clinical knowledge map of "disease-syndrome-symptom-prescription-medicine"to enhance our understanding of the diagnosis and treatment laws of TCM, clarify the unique therapies, and explore information conducive to individualized treatment. The causal inference process of observational data can address confounding bias and reduce individual heterogeneity, promoting the transformation of TCM RWD into reliable clinical evidence. Intelligent data science improves efficiency and accuracy for implementing RWCE-TCM. The proposed data science methodology for TCM can handle complex data, ensure high-quality RWD acquisition and analysis, and provide in-depth insights into clinical benefits of TCM. This method supports the intelligent translation and demonstration of RWD in TCM, leads the data-driven translational analysis of causal learning, and innovates the path of RWCE-TCM.  © Wolters Kluwer Health, Inc. All rights reserved.
PB  - Wolters Kluwer Health
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Raza, S.
AU  - Garg, M.
AU  - Reji, D.J.
AU  - Bashir, S.R.
AU  - Ding, C.
TI  - NBIAS: A natural language processing framework for BIAS identification in text
PY  - 2024
T2  - Expert Systems with Applications
VL  - 237
C7  - 121542
DO  - 10.1016/j.eswa.2023.121542
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171615875&doi=10.1016%2fj.eswa.2023.121542&partnerID=40&md5=8c791a083fa6781b235693c92398346c
AB  - Bias in textual data can lead to skewed interpretations and outcomes when the data is used. These biases could perpetuate stereotypes, discrimination, or other forms of unfair treatment. An algorithm trained on biased data may end up making decisions that disproportionately impact a certain group of people. Therefore, it is crucial to detect and remove these biases to ensure the fair and ethical use of data. To this end, we develop a comprehensive and robust framework NBIAS that consists of four main layers: data, corpus construction, model development and an evaluation layer. The dataset is constructed by collecting diverse data from various domains, including social media, healthcare, and job hiring portals. As such, we applied a transformer-based token classification model that is able to identify bias words/phrases through a unique named entity BIAS. In the evaluation procedure, we incorporate a blend of quantitative and qualitative measures to gauge the effectiveness of our models. We achieve accuracy improvements ranging from 1% to 8% compared to baselines. We are also able to generate a robust understanding of the model functioning. The proposed approach is applicable to a variety of biases and contributes to the fair and ethical use of textual data. © 2023 Elsevier Ltd
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 26
ER  -

TY  - CONF
AU  - Goyal, S.
AU  - Rastogi, E.
AU  - Rajagopal, S.P.
AU  - Yuan, D.
AU  - Zhao, F.
AU  - Chintagunta, J.
AU  - Naik, G.
AU  - Ward, J.
TI  - HealAI: A Healthcare LLM for Effective Medical Documentation
PY  - 2024
T2  - WSDM 2024 - Proceedings of the 17th ACM International Conference on Web Search and Data Mining
SP  - 1167
EP  - 1168
DO  - 10.1145/3616855.3635739
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191548394&doi=10.1145%2f3616855.3635739&partnerID=40&md5=f7489957a0152120eb9a85f60fbd383d
AB  - Since the advent of LLM's like GPT4 everyone in various industries has been trying to harness their power. Healthcare is an industry where this is a specifically challenging problem due to the high accuracy requirements. Prompt Engineering is a common technique used to design instructions for model responses, however, its challenges lie in the fact that the generic models may not be trained to accurately execute these specific tasks. We will present our journey of developing a cost-effective medical LLM, surpassing GPT4 in medical note-writing tasks. We'll touch upon our trials with medical prompt engineering, GPT4's limitations, and training an optimized LLM for specific medical tasks. We'll showcase multiple comparisons on model sizes, training data, and pipeline designs that enabled us to outperform GPT4 with smaller models, maintaining precision, reducing biases, preventing hallucinations, and enhancing note-writing style. © 2024 Owner/Author.
PB  - Association for Computing Machinery, Inc
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - Chen, Y.
AU  - Yang, X.
AU  - Yue, X.
AU  - Lin, X.
AU  - Zhang, Q.
AU  - Fujita, H.
TI  - A general variation-driven network for medical image synthesis
PY  - 2024
T2  - Applied Intelligence
VL  - 54
IS  - 4
SP  - 3295
EP  - 3307
DO  - 10.1007/s10489-023-05017-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187251371&doi=10.1007%2fs10489-023-05017-1&partnerID=40&md5=cae911359b6a685dc7ba7212f8923b19
AB  - The significance of medical image synthesis has exponentially grown due to constrained medical resources, making it a critical component in numerous clinical applications. This process facilitates the generation of high-quality, multi-modal medical images, ultimately enhancing medical image diagnostics. Currently, prevailing medical image synthesis methodologies primarily rely on voxel-based or GAN-based strategies to address substantial challenges arising from disparities in various imaging principles and significant noise. However, these methodologies rarely consider the intensity distribution difference among multi-modal or multi-parameters medical images, which generates unstable and unexplainable results. In response to these limitations, we propose a novel approach-a general variation-driven neural network for medical image synthesis that considers explicit data distribution. Within this method, we introduce the concept of a variation-based distance metric, providing a quantitative framework for capturing distribution disparities between medical images originating from both the source and target domains. Subsequently, guided by this variation-based distance metric, we introduce a robust end-to-end neural network architecture carefully designed to synthesize target medical images. Our proposed method has undergone extensive experimentation across various medical image synthesis tasks, including cross-modality transformations between CT and MRI, high-dose CT synthesis from low-dose CT, and the conversion of multi-parameters in MRI, including T1, T2, T1ce, and Flair sequences. In comparative assessments against existing methods, our approach consistently outperforms them across three publicly available datasets: Gold Atlas, LDCT, and BraTS2018. Additionally, we have successfully applied our model to generate high-quality Micro-CT images in dental clinics from CBCT data, significantly enhancing diagnostic capabilities in this clinical setting. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.
PB  - Springer
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Namburi, S.
AU  - Hopkins, G.
TI  - Beyond Content: A Trauma-Informed Framework for Academic Writing Evaluation
PY  - 2024
T2  - Proceedings of the International Conference on Education Research, ICER 2024
SP  - 236
EP  - 246
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216107850&partnerID=40&md5=92382fd8289e60ad76ef271f2997f643
AB  - The paper proposes a novel framework for assessing student academic writing integrating an understanding of trauma into the evaluation process. This framework emphasises the importance of recognising linguistic markers in student writing that may indicate underlying psychological distress, such as post-traumatic stress disorder (PTSD). Traditional higher education academic assessments often overlook these markers, focusing on content quality and adherence to formal writing standards. This oversight could lead to missed opportunities for early intervention, particularly in educational settings where students do not openly disclose their mental health challenges. Building on trauma-informed pedagogy, this study explores how Natural Language Processing (NLP) models can be leveraged to identify linguistic markers associated with poor mental health, such as PTSD, anxiety, and depression, within students' academic writing. By analysing writing patterns like non-linear narrative structures, obsessive thoughts, and disjointed syntax, we argue that NLP can offer an essential tool for early detection of trauma-related challenges. Such markers are often overlooked in traditional grading systems, which prioritize form and rhetoric. A case study using student writing samples demonstrates how changes in rhetorical fluency and writing quality can correlate with a documented decline in mental health. The results of NLP analysis reveal a progressive decline in coherence, lexical diversity, and thematic focus, which align with known linguistic markers of trauma. These findings underscore the potential of NLP to serve as an early-warning system, alerting educators to the need for intervention and support. Despite the promise of these methods, current NLP models face limitations in linguistic diversity, reproducibility, and population bias. Therefore, we advocate for the development of more inclusive models built on ethical frameworks that consider the socio-rhetorical contexts of student writing. Additionally, large and secure datasets are required to ensure representativeness, with attention to student privacy concerns. Ultimately, this paper calls for higher education institutions to adopt trauma-sensitive evaluation frameworks that integrate academic and emotional well-being, ensuring more equitable and compassionate assessments. © 2024 Proceedings of the International Conference on Education Research, ICER 2024. All rights reserved.
PB  - Academic Conferences International Limited
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Maaz, S.
AU  - Palaganas, J.C.
AU  - Palaganas, G.
AU  - Bajwa, M.
TI  - A guide to prompt design: foundations and applications for healthcare simulationists
PY  - 2024
T2  - Frontiers in Medicine
VL  - 11
C7  - 1504532
DO  - 10.3389/fmed.2024.1504532
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219533103&doi=10.3389%2ffmed.2024.1504532&partnerID=40&md5=f08970c735b5dbe92fde96a12ca9d16c
AB  - Large Language Models (LLMs) like ChatGPT, Gemini, and Claude gain traction in healthcare simulation; this paper offers simulationists a practical guide to effective prompt design. Grounded in a structured literature review and iterative prompt testing, this paper proposes best practices for developing calibrated prompts, explores various prompt types and techniques with use cases, and addresses the challenges, including ethical considerations for using LLMs in healthcare simulation. This guide helps bridge the knowledge gap for simulationists on LLM use in simulation-based education, offering tailored guidance on prompt design. Examples were created through iterative testing to ensure alignment with simulation objectives, covering use cases such as clinical scenario development, OSCE station creation, simulated person scripting, and debriefing facilitation. These use cases provide easy-to-apply methods to enhance realism, engagement, and educational alignment in simulations. Key challenges associated with LLM integration, including bias, privacy concerns, hallucinations, lack of transparency, and the need for robust oversight and evaluation, are discussed alongside ethical considerations unique to healthcare education. Recommendations are provided to help simulationists craft prompts that align with educational objectives while mitigating these challenges. By offering these insights, this paper contributes valuable, timely knowledge for simulationists seeking to leverage generative AI’s capabilities in healthcare education responsibly. Copyright © 2025 Maaz, Palaganas, Palaganas and Bajwa.
PB  - Frontiers Media SA
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Sallam, M.
AU  - Al-Farajat, A.
AU  - Egger, J.
TI  - Envisioning the Future of ChatGPT in Healthcare: Insights and Recommendations from a Systematic Identification of Influential Research and a Call for Papers
PY  - 2024
T2  - Jordan Medical Journal
VL  - 58
IS  - 1
SP  - 95
EP  - 108
DO  - 10.35516/jmj.v58i1.2285
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186998859&doi=10.35516%2fjmj.v58i1.2285&partnerID=40&md5=2786982aba25b61896120078e151cbac
AB  - Background and Aims: ChatGPT represents the most popular and widely used generative artificial intelligence (AI) model that received significant attention in healthcare research. The aim of the current study was to assess the future trajectory of the needed research in this domain based on the recommendations of the top influential published records. Materials and Methods: A systematic search was conducted on Scopus, Web of Science, and Google Scholar (27–30 November 2023) to identify the top ten ChatGPT-related published records in healthcare across the three databases. Classification of the records as “top” denoting high influence in the field was based on citation counts. Results: A total of 22 unique records from 17 different journals representing 14 different publishers were identified as the top ChatGPT-related publications in healthcare subject. Based on the identified records’ recommendations, the following themes appeared as important areas to consider in future ChatGPT research in healthcare: improving healthcare education, improved efficiency of clinical processes (e.g., documentation), addressing ethical concerns (e.g., patient privacy and consent), supporting research tasks (e.g., data analysis, manuscript preparation), mitigating ChatGPT output biases, improving patient education and engagement, and developing standardized assessment protocols for ChatGPT utility in healthcare. Conclusions: The current review highlighted key areas to be prioritized in assessment of ChatGPT utility in healthcare. Interdisciplinary collaborations and standardizing methodologies are needed to synthesize robust evidence in these studies. Based on these recommendations and the promising potential of ChatGPT on healthcare, JMJ launched a call for papers for a special issue entitled “Evaluating Generative AI-Based Models in Healthcare”. © 2024 DSR Publishers ∕ The University of Jordan. All Rights Reserved.
PB  - University of Jordan,Deanship of Scientific Research
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 11
ER  -

TY  - JOUR
AU  - Andrew, A.
TI  - Potential applications and implications of large language models in primary care
PY  - 2024
T2  - Family Medicine and Community Health
VL  - 12
IS  - Suppl 1
C7  - e002602
DO  - 10.1136/fmch-2023-002602
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183695151&doi=10.1136%2ffmch-2023-002602&partnerID=40&md5=3e484de141b0059468f9139733427c68
AB  - The recent release of highly advanced generative artificial intelligence (AI) chatbots, including ChatGPT and Bard, which are powered by large language models (LLMs), has attracted growing mainstream interest over its diverse applications in clinical practice, including in health and healthcare. The potential applications of LLM-based programmes in the medical field range from assisting medical practitioners in improving their clinical decision-making and streamlining administrative paperwork to empowering patients to take charge of their own health. However, despite the broad range of benefits, the use of such AI tools also comes with several limitations and ethical concerns that warrant further consideration, encompassing issues related to privacy, data bias, and the accuracy and reliability of information generated by AI. The focus of prior research has primarily centred on the broad applications of LLMs in medicine. To the author’s knowledge, this is, the first article that consolidates current and pertinent literature on LLMs to examine its potential in primary care. The objectives of this paper are not only to summarise the potential benefits, risks and challenges of using LLMs in primary care, but also to offer insights into considerations that primary care clinicians should take into account when deciding to adopt and integrate such technologies into their clinical practice. © Author(s)
PB  - BMJ Publishing Group
C2  - 38290759
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 14
ER  -

TY  - JOUR
AU  - Goktas, P.
AU  - Kucukkaya, A.
AU  - Karacay, P.
TI  - Utilizing GPT 4.0 with prompt learning in nursing education: A case study approach based on Benner's theory
PY  - 2024
T2  - Teaching and Learning in Nursing
VL  - 19
IS  - 2
SP  - e358
EP  - e367
DO  - 10.1016/j.teln.2023.12.014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182375602&doi=10.1016%2fj.teln.2023.12.014&partnerID=40&md5=83176339f0b4ce4c60794c393536bca9
AB  - Background: Artificial intelligence (AI) and large language models, such as ChatGPT, have the potential to enhance nursing education by serving as a virtual mentor who can provide real-time guidance and resources. However, integrating AI chatbots into nursing necessitates a detailed understanding of how to align these technologies with established nursing theories. Aim: To develop guidelines on how to employ GPT 4.0 using the prompt learning method. Methods: In the method of prompt learning, users give specific prompts, and the AI model reacts according to its training, specifically aligned with the progression of knowledge and skills in nursing education as outlined by Benner's theory. This study used a case study methodology. Results: The customization of a conversational AI chatbot is shown to support the development of nursing knowledge and skills. This paper outlines how to integrate Benner's theory with ChatGPT's capabilities, addresses bias issues, and establishes best practices for the safe and effective use of AI in nursing. Conclusion: The findings have important implications for the advancement of nursing education and the safe and responsible use of AI tools in clinical environments. © 2023 Organization for Associate Degree Nursing
PB  - Elsevier Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 14
ER  -

TY  - CONF
AU  - Fonner, D.F.
AU  - Coyle, F.P.
TI  - Responsible AI for Government Program Evaluation and Performance Audits
PY  - 2024
T2  - Proceedings - 2024 IEEE International Conference on Big Data, BigData 2024
SP  - 8222
EP  - 8224
DO  - 10.1109/BigData62323.2024.10825518
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217983325&doi=10.1109%2fBigData62323.2024.10825518&partnerID=40&md5=6553e3396502d73540d80c1e766b4ba4
AB  - With the proliferation of user-friendly generative-AI tools, many government agencies, such as the National Science Foundation and the National Institutes of Health, have started exploring how these tools could improve one of their primary functions: managing grant programs to distribute funds for research. Government bans on the use of generative-AI for initial grant application evaluation limits the use of this technology. However, this research shows that a post hoc evaluative approach within the domain of Responsible AI provides a means by which government agencies can evaluate the big data collected through their grant programs. This proposed framework, Responsible AI for Evaluation (RAI-E), achieves this goal by curating big governmental administrative data using grant evaluation rubrics followed by model fine-tuning for agency-specific evaluation tasks. Convergence between model- and human-generated evaluations then enables robust diagnosis of grant program equity and fairness via Responsible AI and algorithm auditing methods. Preliminary experiments show improved model performance through the instruction-tuning of open source language models for evaluating government grant applications. Further research will explore the auditing of the underlying algorithms, devising a method for algorithm-in-the-loop frameworks to aid government administrators in evaluating their human-driven program performance. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Avalos-Fernandez, M.
AU  - Cohen, D.
AU  - Russon, D.
AU  - Davids, M.
AU  - Doremus, O.
AU  - Chenais, G.
AU  - Tellier, E.
AU  - Gil-Jardiné, C.
AU  - Lagarde, E.
TI  - Detecting Human Bias in Emergency Triage Using LLMs: Literature Review, Preliminary Study, and Experimental Plan
PY  - 2024
T2  - Proceedings of the International Florida Artificial Intelligence Research Society Conference, FLAIRS
VL  - 37
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200490563&partnerID=40&md5=51c816b005015e6b7240afc078ed2a79
AB  - The surge in AI-based research for emergency healthcare poses challenges such as data protection compliance and the risk of exacerbating health inequalities. Human biases in demographic data used to train AI systems may indeed be replicated. Yet, AI also offers a chance for a paradigm shift, acting as a tool to counteract human biases. Our study focuses on emergency triage, rapidly categorizing patients by severity upon arrival. Objectives include conducting a literature review to identify potential human biases in triage and presenting a preliminary study. This involves a qualitative survey to complement the review on factors influencing triage scores. Moreover, we analyze triage data descriptively and pilot AI-driven triage using an LLM with data from the local hospital. Finally, assembling these pieces, we outline an experimental plan to assess AI’s effectiveness in detecting human biases in triage data. Copyright © 2024 by the authors.
PB  - Florida Online Journals, University of Florida
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Putalpattu, M.P.
AU  - Bhargavi, K.
AU  - Mayani, M.B.
AU  - Srinivas, P.
AU  - Siddiqa, A.
AU  - Kunkulagunta, M.
TI  - Advancing Predictive Modeling in Healthcare A Data Science Approach Utilizing AI-Driven Algorithms
PY  - 2024
T2  - Intelligent Computing and Emerging Communication Technologies, ICEC 2024
DO  - 10.1109/ICEC59683.2024.10837024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217985464&doi=10.1109%2fICEC59683.2024.10837024&partnerID=40&md5=79b9dd3e4260d4d880f95a040fc11bd9
AB  - Artificial intelligence (AI) is based on the premise that machines may learn from data collected from different sources to mimic human intellect in order to carry out tasks, identify patterns, or anticipate outcomes. Many areas of technology have made extensive use of AI and ML algorithms, including: autonomous vehicles, recommendation systems in e-commerce and social media, financial technology, question answering systems, and natural language processing. In a similar vein, AI is quietly revolutionising healthcare research. Half a century ago, there was a lot of interest in using a rule-based method for illness diagnosis and clinical decision assistance. When it comes to diagnosing diseases and developing individualized treatment programs, AI systems show remarkable accuracy in analyzing medical imagery. Streamlining operations using AI-powered solutions improves efficiency and the patient experience, while predictive analytics help find patients at high risk so they can get preventative treatments. By automating monotonous operations, particularly in the domains of surgery and rehabilitation, robots powered by artificial intelligence also improve healthcare delivery. Data quality, interpretability, bias, and regulatory frameworks are all important concerns that must be resolved before AI can be used responsibly. Ethical and successful AI integration into healthcare requires robust regulatory frameworks, education, safety validation, human-AI collaboration, and education.  © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Hussain, T.
AU  - Wang, D.
AU  - Li, B.
TI  - The influence of the COVID-19 pandemic on the adoption and impact of AI ChatGPT: Challenges, applications, and ethical considerations
PY  - 2024
T2  - Acta Psychologica
VL  - 246
C7  - 104264
DO  - 10.1016/j.actpsy.2024.104264
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190290816&doi=10.1016%2fj.actpsy.2024.104264&partnerID=40&md5=ef7f5e54975c17e6b7e2708c70a96650
AB  - Design/methodology/approach. This article employs qualitative thematic modeling to gather insights from 30 informants. The study explores various aspects related to the impact of the COVID-19 pandemic on AI ChatGPT technologies. Purpose: The purpose of this research is to examine how the COVID-19 pandemic has influenced the increased usage and adoption of AI ChatGPT. It aims to explore the pandemic's impact on AI ChatGPT and its applications in specific domains, as well as the challenges and opportunities it presents. Findings: The findings highlight that the pandemic has led to a surge in online activities, resulting in a heightened demand for AI ChatGPT. It has been widely used in areas such as healthcare, mental health support, remote collaboration, and personalized customer experiences. The article showcases examples of AI ChatGPT's application during the pandemic. Strength of study: This qualitative framework enables the study to delve deeply into the multifaceted dimensions of AI ChatGPT's role during the pandemic, capturing the diverse experiences and insights of users, practitioners, and experts. By embracing the qualitative nature of inquiry and this research offers a comprehensive understanding of the challenges, opportunities, and ethical considerations associated with the adoption and utilization of AI ChatGPT in crisis contexts. Practical implications: The insights from this research have practical implications for policymakers, developers, and researchers. This reserach emphasize the need for responsible and ethical implementation of AI ChatGPT to fully harness its potential in addressing societal needs during and beyond the pandemic. Social implications: The increased reliance on AI ChatGPT during the pandemic has led to changes in user behavior, expectations, and interactions. However, it has also unveiled ethical considerations and potential risks. Addressing societal and ethical concerns, such as user impact and autonomy, privacy and security, bias and fairness, and transparency and accountability, is crucial for the responsible deployment of AI ChatGPT. Originality/value: This research contributes to the understanding of the novel role of AI ChatGPT in times of crisis, particularly in the era of COVID-19 pandemic. It highlights the necessity of responsible and ethical implementation of AI ChatGPT and provides valuable insights for the development and application of AI technology in the future. © 2024
PB  - Elsevier B.V.
C2  - 38626597
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - JOUR
AU  - Xu, X.
AU  - Yao, B.
AU  - Dong, Y.
AU  - Gabriel, S.
AU  - Yu, H.
AU  - Hendler, J.
AU  - Ghassemi, M.
AU  - Dey, A.K.
AU  - Wang, D.
TI  - Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data
PY  - 2024
T2  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 8
IS  - 1
C7  - 31
DO  - 10.1145/3643540
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193764147&doi=10.1145%2f3643540&partnerID=40&md5=1c565e6e7af789720e9014c73fa30e77
AB  - Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present a comprehensive evaluation of multiple LLMs on various mental health prediction tasks via online text data, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs’ capability on mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs’ capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
PB  - Association for Computing Machinery
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 51
ER  -

TY  - JOUR
AU  - Naik, A.
AU  - Stein, A.
AU  - Wu, Y.
AU  - Naik, M.
AU  - Wong, E.
TI  - TorchQL: A Programming Framework for Integrity Constraints in Machine Learning
PY  - 2024
T2  - Proceedings of the ACM on Programming Languages
VL  - 8
IS  - OOPSLA1
C7  - 124
DO  - 10.1145/3649841
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195780679&doi=10.1145%2f3649841&partnerID=40&md5=20950b87a1f5a3695a2645db3e4c88e7
AB  - Finding errors in machine learning applications requires a thorough exploration of their behavior over data. Existing approaches used by practitioners are often ad-hoc and lack the abstractions needed to scale this process. We present TorchQL, a programming framework to evaluate and improve the correctness of machine learning applications. TorchQL allows users to write queries to specify and check integrity constraints over machine learning models and datasets. It seamlessly integrates relational algebra with functional programming to allow for highly expressive queries using only eight intuitive operators. We evaluate TorchQL on diverse use-cases including finding critical temporal inconsistencies in objects detected across video frames in autonomous driving, finding data imputation errors in time-series medical records, finding data labeling errors in real-world images, and evaluating biases and constraining outputs of language models. Our experiments show that TorchQL enables up to 13x faster query executions than baselines like Pandas and MongoDB, and up to 40% shorter queries than native Python. We also conduct a user study and find that TorchQL is natural enough for developers familiar with Python to specify complex integrity constraints.  © 2024 Owner/Author.
PB  - Association for Computing Machinery
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Grippaudo, F.R.
AU  - Nigrelli, S.
AU  - Patrignani, A.
AU  - Ribuffo, D.
TI  - Quality of the Information provided by ChatGPT for Patients in Breast Plastic Surgery: Are we already in the future?
PY  - 2024
T2  - JPRAS Open
VL  - 40
SP  - 99
EP  - 105
DO  - 10.1016/j.jpra.2024.02.001
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186750353&doi=10.1016%2fj.jpra.2024.02.001&partnerID=40&md5=b8f5746458ef55826fc2a03c041cfd50
AB  - Introduction: In recent years, artificial intelligence (AI) has gained popularity, even in the field of plastic surgery. It is increasingly common for patients to use the internet to gather information about plastic surgery, and AI-based chatbots, such as ChatGPT, could be employed to answer patients' questions. The aim of this study was to evaluate the quality of medical information provided by ChatGPT regarding three of the most common procedures in breast plastic surgery: breast reconstruction, breast reduction, and augmentation mammaplasty. Methods: The quality of information was evaluated through the expanded EQIP scale. Responses were collected from a pool made by ten resident doctors in plastic surgery and then processed by SPSS software ver. 28.0. Results: The analysis of the contents provided by ChatGPT revealed sufficient quality of information across all selected topics, with a high bias in terms of distribution of the score between the different items. There was a critical lack in the “Information data field” (0/6 score in all the 3 investigations) but a very high overall evaluation concerning the “Structure data” (>7/11 in all the 3 investigations). Conclusion: Currently, AI serves as a valuable tool for patients; however, engineers and developers must address certain critical issues. It is possible that models like ChatGPT will play an important role in improving patient's consciousness about medical procedures and surgical interventions in the future, but their role must be considered ancillary to that of surgeons. © 2024 The Author(s)
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - Obasa, A.E.
TI  - Large language models through the lens of ubuntu for health research in sub-Saharan Africa
PY  - 2024
T2  - South African Journal of Science
VL  - 120
IS  - 5/6
C7  - 16814
DO  - 10.17159/sajs.2024/16814
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196805636&doi=10.17159%2fsajs.2024%2f16814&partnerID=40&md5=89f1bf479274a39a3ff38fab351b7657
AB  - Ubuntu provides a distinct philosophy that could be useful in addressing the cultural and geographical nuances within sub-Saharan Africa. This philosophy offers a unique framework that could prove valuable in navigating these nuances in the sub-Saharan region. At its core, ubuntu emphasises interconnectedness, community-driven engagement, and sustainability. This perspective underscores the need for culturally sensitive technology solutions that honour and safeguard local traditions while promoting individual liberties and communal welfare. Ubuntu’s approach offers an intriguing balance between the individual and the collective. Marginalised groups must be included in a comprehensive approach. Bias in sub-Saharan Africa has deep roots in historical injustice and is further reinforced by cultural norms, religious beliefs, and practices. In this article, I elaborate on ethical concerns in the context of sub-Saharan Africa. © 2024. The Author(s).
PB  - Academy of Science of South Africa
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Goodman-Meza, D.
AU  - Goto, M.
AU  - Salimian, A.
AU  - Shoptaw, S.
AU  - Bui, A.A.T.
AU  - Gordon, A.J.
AU  - Goetz, M.B.
TI  - Impact of Potential Case Misclassification by Administrative Diagnostic Codes on Outcome Assessment of Observational Study for People Who Inject Drugs
PY  - 2024
T2  - Open Forum Infectious Diseases
VL  - 11
IS  - 2
C7  - ofae030
DO  - 10.1093/ofid/ofae030
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185933819&doi=10.1093%2fofid%2fofae030&partnerID=40&md5=53fbb4077b7f77bd782485501c9b584f
AB  - Introduction. Initiation of medications for opioid use disorder (MOUD) within the hospital setting may improve outcomes for people who inject drugs (PWID) hospitalized because of an infection. Many studies used International Classification of Diseases (ICD) codes to identify PWID, although these may be misclassified and thus, inaccurate. We hypothesized that bias from misclassification of PWID using ICD codes may impact analyses of MOUD outcomes. Methods. We analyzed a cohort of 36 868 cases of patients diagnosed with Staphylococcus aureus bacteremia at 124 US Veterans Health Administration hospitals between 2003 and 2014. To identify PWID, we implemented an ICD code–based algorithm and a natural language processing (NLP) algorithm for classification of admission notes. We analyzed outcomes of prescribing MOUD as an inpatient using both approaches. Our primary outcome was 365-day all-cause mortality. We fit mixed-effects Cox regression models with receipt or not of MOUD during the index hospitalization as the primary predictor and 365-day mortality as the outcome. Results. NLP identified 2389 cases as PWID, whereas ICD codes identified 6804 cases as PWID. In the cohort identified by NLP, receipt of inpatient MOUD was associated with a protective effect on 365-day survival (adjusted hazard ratio, 0.48; 95% confidence interval, .29–.81; P < .01) compared with those not receiving MOUD. There was no significant effect of MOUD receipt in the cohort identified by ICD codes (adjusted hazard ratio, 1.00; 95% confidence interval, .77–1.30; P = .99). Conclusions. MOUD was protective of all-cause mortality when NLP was used to identify PWID, but not significant when ICD codes were used to identify the analytic subjects. © The Author(s) 2024.
PB  - Oxford University Press
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Frankenberger, W.D.
AU  - Zorc, J.J.
AU  - Ten Have, E.D.
AU  - Brodecki, D.
AU  - Faig, W.G.
TI  - Triage Accuracy in Pediatrics Using the Emergency Severity Index
PY  - 2024
T2  - Journal of Emergency Nursing
VL  - 50
IS  - 2
SP  - 207
EP  - 214
DO  - 10.1016/j.jen.2023.11.009
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182435699&doi=10.1016%2fj.jen.2023.11.009&partnerID=40&md5=35e2ed27901d592a3e3c1555b3c29e22
AB  - Introduction: Although the Emergency Severity Index is the most widely used tool in the United States to prioritize care for patients who seek emergency care, including children, there are significant deficiencies in the tool's performance. Inaccurate triage has been associated with delayed treatment, unnecessary diagnostic testing, and bias in clinical care. We evaluated the accuracy of the Emergency Severity Index to stratify patient priority based on predicted resource utilization in pediatric emergency department patients and identified covariates influencing performance. Methods: This cross-sectional, retrospective study used a data platform that links clinical and research data sets from a single freestanding pediatric hospital in the United States. Chi-square analysis was used to describes rates of over- and undertriage. Mixed effects ordinal logistic regression identified associations between Emergency Severity Index categories assigned at triage and key emergency department resources using discrete data elements and natural language processing of text notes. Results: We analyzed 304,422 emergency department visits by 153,984 unique individuals in the final analysis; 80% of visits were triaged as lower acuity Emergency Severity Index levels 3 to 5, with the most common level being Emergency Severity Index 4 (43%). Emergency department visits scored Emergency Severity Index levels 3 and 4 were triaged accurately 46% and 38%, respectively. We noted racial differences in overall triage accuracy. Discussion: Although the plurality of patients was scored as Emergency Severity Index 4, 50% were mistriaged, and there were disparities based on race indicating Emergency Severity Index mistriages pediatric patients. Further study is needed to elucidate the application of the Emergency Severity Indices in pediatrics using a multicenter emergency department population with diverse clinical and demographic characteristics. © 2023 Emergency Nurses Association
PB  - Elsevier Inc.
C2  - 38099907
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Ramesh, K.
AU  - Gandhi, N.
AU  - Madaan, P.
AU  - Bauer, L.
AU  - Peris, C.
AU  - Field, A.
TI  - Evaluating Differentially Private Synthetic Data Generation in High-Stakes Domains
PY  - 2024
T2  - EMNLP 2024 - 2024 Conference on Empirical Methods in Natural Language Processing, Findings of EMNLP 2024
SP  - 15254
EP  - 15269
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217615725&partnerID=40&md5=c902eb7c768fea45cf0bee9e5b4b631f
AB  - The difficulty of anonymizing text data hinders the development and deployment of NLP in high-stakes domains that involve private data, such as healthcare and social services. Poorly anonymized sensitive data cannot be easily shared with annotators or external researchers, nor can it be used to train public models. In this work, we explore the feasibility of using synthetic data generated from differentially private language models in place of real data to facilitate the development of NLP in these domains without compromising privacy. In contrast to prior work, we generate synthetic data for real high-stakes domains, and we propose and conduct use-inspired evaluations to assess data quality. Our results show that prior simplistic evaluations have failed to highlight utility, privacy, and fairness issues in the synthetic data. Overall, our work underscores the need for further improvements to synthetic data generation for it to be a viable way to enable privacy-preserving data sharing. © 2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Anadria, D.
AU  - Giachanou, A.
AU  - Kernahan, J.
AU  - Dobbe, R.
AU  - Oberski, D.
TI  - Algorithmic Fairness in Clinical Natural Language Processing: Challenges and Opportunities
PY  - 2024
T2  - CEUR Workshop Proceedings
VL  - 3908
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219514173&partnerID=40&md5=a2701334b5677ec2ec92006bacd36e67
AB  - The surge in research and development of clinical natural language processing (NLP) has prompted inquiries into the algorithmic fairness of the proposed and deployed technical solutions. In spite of the proliferation of research, limited work has synthesized reflected on the state of algorithmic fairness in clinical NLP. In this short paper, we summarize the findings of our scoping review of literature and present challenges and opportunities in the domain. We identify challenges and opportunities related to studying and measuring protected groups, selecting appropriate methodology, data sharing and privacy, as well as generalizability. The goal of this article is to start a discussion and raise awareness about the gaps encountered within algorithmic fairness in clinical NLP and pave the way for future research. © 2024 Copyright for this paper by its authors.
PB  - CEUR-WS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Weber, M.T.
AU  - Schaaf, J.
AU  - Storf, H.
AU  - Wagner, T.O.F.
AU  - Berger, A.
AU  - Noll, R.
TI  - Editing Physicians' Responses Using GPT-4 for Academic Research
PY  - 2024
T2  - Studies in Health Technology and Informatics
VL  - 313
SP  - 101
EP  - 106
DO  - 10.3233/SHTI240019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191758341&doi=10.3233%2fSHTI240019&partnerID=40&md5=0b4e8507ae9f5230d374fd4f6bb0d964
AB  - The integration of Artificial Intelligence (AI) into digital healthcare, particularly in the anonymisation and processing of health information, holds considerable potential. Objectives: To develop a methodology using Generative Pre-trained Transformer (GPT) models to preserve the essence of medical advice in doctors' responses, while editing them for use in scientific studies. Methods: German and English responses from EXABO, a rare respiratory disease platform, were processed using iterative refinement and other prompt engineering techniques, with a focus on removing identifiable and irrelevant content. Results: Of 40 responses tested, 31 were accurately modified according to the developed guidelines. Challenges included misclassification and incomplete removal, with incremental prompting proving more accurate than combined prompting. Conclusion: GPT-4 models show promise in medical response editing, but face challenges in accuracy and consistency. Precision in prompt engineering is essential in medical contexts to minimise bias and retain relevant information.  © 2024 The Authors.
PB  - IOS Press BV
C2  - 38682512
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Anibal, J.
AU  - Huth, H.
AU  - Li, M.
AU  - Hazen, L.
AU  - Daoud, V.
AU  - Ebedes, D.
AU  - Lam, Y.M.
AU  - Nguyen, H.
AU  - Hong, P.V.
AU  - Kleinman, M.
AU  - Ost, S.
AU  - Jackson, C.
AU  - Sprabery, L.
AU  - Elangovan, C.
AU  - Krishnaiah, B.
AU  - Akst, L.
AU  - Lina, I.
AU  - Elyazar, I.
AU  - Ekawati, L.
AU  - Jansen, S.
AU  - Nduwayezu, R.
AU  - Garcia, C.
AU  - Plum, J.
AU  - Brenner, J.
AU  - Song, M.
AU  - Ricotta, E.
AU  - Clifton, D.
AU  - Thwaites, C.L.
AU  - Bensoussan, Y.
AU  - Wood, B.
TI  - Voice EHR: introducing multimodal audio data for health
PY  - 2024
T2  - Frontiers in Digital Health
VL  - 6
C7  - 1448351
DO  - 10.3389/fdgth.2024.1448351
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217691601&doi=10.3389%2ffdgth.2024.1448351&partnerID=40&md5=4c55606955f093f58899f20e53ae978e
AB  - Introduction: Artificial intelligence (AI) models trained on audio data may have the potential to rapidly perform clinical tasks, enhancing medical decision-making and potentially improving outcomes through early detection. Existing technologies depend on limited datasets collected with expensive recording equipment in high-income countries, which challenges deployment in resource-constrained, high-volume settings where audio data may have a profound impact on health equity. Methods: This report introduces a novel protocol for audio data collection and a corresponding application that captures health information through guided questions. Results: To demonstrate the potential of Voice EHR as a biomarker of health, initial experiments on data quality and multiple case studies are presented in this report. Large language models (LLMs) were used to compare transcribed Voice EHR data with data (from the same patients) collected through conventional techniques like multiple choice questions. Information contained in the Voice EHR samples was consistently rated as equally or more relevant to a health evaluation. Discussion: The HEAR application facilitates the collection of an audio electronic health record (“Voice EHR”) that may contain complex biomarkers of health from conventional voice/respiratory features, speech patterns, and spoken language with semantic meaning and longitudinal context–potentially compensating for the typical limitations of unimodal clinical datasets. 2025 Anibal, Huth, Li, Hazen, Daoud, Ebedes, Lam, Nguyen, Hong, Kleinman, Ost, Jackson, Sprabery, Elangovan, Krishnaiah, Akst, Lina, Elyazar, Ekawati, Jansen, Nduwayezu, Garcia, Plum, Brenner, Song, Ricotta, Clifton, Thwaites, Bensoussan and Wood.
PB  - Frontiers Media SA
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Carson, N.J.
AU  - Yang, X.
AU  - Mullin, B.
AU  - Stettenbauer, E.
AU  - Waddington, M.
AU  - Zhang, A.
AU  - Williams, P.
AU  - Rios Perez, G.E.
AU  - Cook, B.L.
TI  - Predicting adolescent suicidal behavior following inpatient discharge using structured and unstructured data
PY  - 2024
T2  - Journal of Affective Disorders
VL  - 350
SP  - 382
EP  - 387
DO  - 10.1016/j.jad.2023.12.059
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184755932&doi=10.1016%2fj.jad.2023.12.059&partnerID=40&md5=a637a98517755c2bce7f551576af412d
AB  - Background: The objective was to develop and assess performance of an algorithm predicting suicide-related ICD codes within three months of psychiatric discharge. Methods: This prognostic study used a retrospective cohort of EHR data from 2789 youth (12 to 20 years old) hospitalized in a safety net institution in the Northeastern United States. The dataset combined structured data with unstructured data obtained through natural language processing of clinical notes. Machine learning approaches compared gradient boosting to random forest analyses. Results: Area under the ROC and precision-recall curve were 0.88 and 0.17, respectively, for the final Gradient Boosting model. The cutoff point of the model-generated predicted probabilities of suicide that optimally classified the individual as high risk or not was 0.009. When applying the chosen cutoff (0.009) to the hold-out testing set, the model correctly identified 8 positive cases out of 10, and 418 negative cases out 548. The corresponding performance metrics showed 80 % sensitivity, 76 % specificity, 6 % PPV, 99 % NPV, F-1 score of 0.11, and an accuracy of 76 %. Limitations: The data in this study comes from a single health system, possibly introducing bias in the model's algorithm. Thus, the model may have underestimated the incidence of suicidal behavior in the study population. Further research should include multiple system EHRs. Conclusions: These performance metrics suggest a benefit to including both unstructured and structured data in design of predictive algorithms for suicidal behavior, which can be integrated into psychiatric services to help assess risk. © 2024 Elsevier B.V.
PB  - Elsevier B.V.
C2  - 38158050
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Zhang, D.
AU  - Hu, R.
AU  - Tao, D.
AU  - Feng, H.
AU  - Rundensteiner, E.
TI  - LLM-based Hierarchical Label Annotation for Foodborne Illness Detection on Social Media
PY  - 2024
T2  - Proceedings - 2024 IEEE International Conference on Big Data, BigData 2024
SP  - 7272
EP  - 7281
DO  - 10.1109/BigData62323.2024.10825369
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218007239&doi=10.1109%2fBigData62323.2024.10825369&partnerID=40&md5=3231d61df03ac026dea62a89f300ee4d
AB  - Foodborne illnesses pose a threat to public health, leading to morbidity, mortality, and economic burden annually. Social media, while providing a rich timely source for training AI models for surveillance, requires effective tools for annotation. While Large Language Models (LLMs) have shown promise for generating simple labels, here hierarchical labels composed of entity types like food type and symptom (at individual word level) and the foodborne illness event (at complete post level) are required. For this, we introduce ICL2FID, the first LLM-based hierarchical labeling framework designed to annotate social media posts for foodborne illness detection at two levels using only a few demonstration examples. To utilize the interconnection between post and word levels, ICL2FID instructs the LLM to leverage information from one level when predicting the other level. To combat model hallucination and cyclic dependencies, a verification step improves evidence propagation between interconnected word and post-level labeling tasks. Strategies for custom selection of demonstration examples are designed reducing biases and increasing representation. We compare ICL2FID against traditional supervised learning and other LLM methods, demonstrating that it not only achieves superior accuracy but does so at a fraction of the cost and time. These findings highlight ICL2FID's potential as a viable alternative for hierarchical label generation in scenarios with limited resources and huge data sets. Code is available at https://github.com/zdy93/ICL2FID. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Tao, B.K.
AU  - Handzic, A.
AU  - Hua, N.J.
AU  - Vosoughi, A.R.
AU  - Margolin, E.A.
AU  - Micieli, J.A.
TI  - Utility of ChatGPT for Automated Creation of Patient Education Handouts: An Application in Neuro-Ophthalmology
PY  - 2024
T2  - Journal of Neuro-Ophthalmology
VL  - 44
IS  - 1
SP  - 119
EP  - 124
DO  - 10.1097/WNO.0000000000002074
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185346962&doi=10.1097%2fWNO.0000000000002074&partnerID=40&md5=51c8095ccf6eef7ac75f228ac14d4eec
AB  - Background:Patient education in ophthalmology poses a challenge for physicians because of time and resource limitations. ChatGPT (OpenAI, San Francisco) may assist with automating production of patient handouts on common neuro-ophthalmic diseases.Methods:We queried ChatGPT-3.5 to generate 51 patient education handouts across 17 conditions. We devised the "Quality of Generated Language Outputs for Patients" (QGLOP) tool to assess handouts on the domains of accuracy/comprehensiveness, bias, currency, and tone, each scored out of 4 for a total of 16. A fellowship-trained neuro-ophthalmologist scored each passage. Handout readability was assessed using the Simple Measure of Gobbledygook (SMOG), which estimates years of education required to understand a text.Results:The QGLOP scores for accuracy, bias, currency, and tone were found to be 2.43, 3, 3.43, and 3.02 respectively. The mean QGLOP score was 11.9 [95% CI 8.98, 14.8] out of 16 points, indicating a performance of 74.4% [95% CI 56.1%, 92.5%]. The mean SMOG across responses as 10.9 [95% CI 9.36, 12.4] years of education.Conclusions:The mean QGLOP score suggests that a fellowship-trained ophthalmologist may have at-least a moderate level of satisfaction with the write-up quality conferred by ChatGPT. This still requires a final review and editing before dissemination. Comparatively, the rarer 5% of responses collectively on either extreme would require very mild or extensive revision. Also, the mean SMOG score exceeded the accepted upper limits of grade 8 reading level for health-related patient handouts. In its current iteration, ChatGPT should be used as an efficiency tool to generate an initial draft for the neuro-ophthalmologist, who may then refine the accuracy and readability for a lay readership. © 2024 Lippincott Williams and Wilkins. All rights reserved.
PB  - Lippincott Williams and Wilkins
C2  - 38175720
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 11
ER  -

TY  - JOUR
AU  - Dachraoui, C.
AU  - Mouelhi, A.
AU  - Mosbeh, A.
AU  - Sliti, W.
AU  - Drissi, C.
AU  - Solaiman, B.
AU  - Labidi, S.
TI  - A machine learning approach for multiple sclerosis diagnosis through Detecron Architecture
PY  - 2024
T2  - Multimedia Tools and Applications
VL  - 83
IS  - 14
SP  - 42837
EP  - 42859
DO  - 10.1007/s11042-023-17055-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173869234&doi=10.1007%2fs11042-023-17055-5&partnerID=40&md5=01684e036562130363f054adb6f00d5d
AB  - Multiple sclerosis is a prevalent inflammatory disease affecting the central nervous system, leading to demyelination. Neuroradiology relies on accurate analysis of white matter lesions for diagnosis and prognosis. Automated methods for segmenting lesions in MRI scans offer crucial benefits of objectivity and efficiency, making them particularly valuable for analyzing large datasets. In contrast, manual delineation of MRI lesions is both time-consuming and prone to subjective bias. To overcome these issues, this paper proposes and develops an automated diagnosis approach using the Detecron-2 architecture. The method utilizes a fully modified Convolutional Neural Network on 3D FLAIR-weighted Magnetic Resonance Images.The approach is trained and validated on a dataset of 3000 images acquired from a Siemens 3Tesla MRI machine at the National Institute of Neurology Mongi Ben Hmida in Tunisia, using technical metrics. Comparisons with recent achievements demonstrate promising results. By addressing challenges in data augmentation and deep learning configurations, the proposed model effectively mitigates issues as overfitting. Notably, it achieves an impressive average detection accuracy of 87%, specificity (= 80,19%), precision (= 80%), sensitivity (= 76,1%) and intersection over Union (= 87,9%) when assessing healthy and pathological images. Additionally, the study recognizes the manual monitoring of multiple sclerosis plaques as a time-consuming and challenging task for clinicians. It highlights the importance of lesion segmentation for quantitative analysis of disease progression. As a second focus, the research aims to develop an automated segmentation to enhance the accuracy and efficiency of lesion identification, addressing the inconsistencies and variations observed among different observers. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.
PB  - Springer
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Liu, J.
AU  - Hu, T.
AU  - Xiong, H.
AU  - Du, J.
AU  - Feng, Y.
AU  - Wu, J.
AU  - Zhou, J.T.
AU  - Liu, Z.
TI  - VPL: Visual Proxy Learning Framework for Zero-Shot Medical Image Diagnosis
PY  - 2024
T2  - EMNLP 2024 - 2024 Conference on Empirical Methods in Natural Language Processing, Findings of EMNLP 2024
SP  - 9978
EP  - 9992
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216820137&partnerID=40&md5=57ad671ae2aa4ffe6d067f2ff2d2146f
AB  - Vision-language models like CLIP, utilizing class proxies derived from class name text features, have shown a notable capability in zero-shot medical image diagnosis which is vital in scenarios with limited disease databases or labeled samples.However, insufficient medical text precision and the modal disparity between text and vision spaces pose challenges for such paradigm.We show analytically and experimentally that enriching medical texts with detailed descriptions can markedly enhance the diagnosis performance, with the granularity and phrasing of these enhancements having a crucial impact on CLIP's understanding of medical images; and learning proxies within the vision domain can effectively circumvent the modal gap issue.Based on our analysis, we propose a medical visual proxy learning framework comprising two key components: a text refinement module that creates high-quality medical text descriptions, and a stable Sinkhorn algorithm for an efficient generation of pseudo labels which further guide the visual proxy learning.Our method elevates the Vanilla CLIP inference by supplying meticulously crafted clues to leverage CLIP's existing interpretive power and using the feature of refined texts to bridge the vision-text gap.The effectiveness and robustness of our method are clearly demonstrated through extensive experiments.Notably, our method outperforms the state-of-the-art zero-shot medical image diagnosis by a significant margin, ranging from 1.69% to 15.31% on five datasets covering various diseases, confirming its immense potential in zero-shot diagnosis across diverse medical applications. © 2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Yu, Z.
AU  - Peng, C.
AU  - Yang, X.
AU  - Dang, C.
AU  - Adekkanattu, P.
AU  - Gopal Patra, B.
AU  - Peng, Y.
AU  - Pathak, J.
AU  - Wilson, D.L.
AU  - Chang, C.-Y.
AU  - Lo-Ciganic, W.-H.
AU  - George, T.J.
AU  - Hogan, W.R.
AU  - Guo, Y.
AU  - Bian, J.
AU  - Wu, Y.
TI  - Identifying social determinants of health from clinical narratives: A study of performance, documentation ratio, and potential bias
PY  - 2024
T2  - Journal of Biomedical Informatics
VL  - 153
C7  - 104642
DO  - 10.1016/j.jbi.2024.104642
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190333397&doi=10.1016%2fj.jbi.2024.104642&partnerID=40&md5=4d6984178bc10fc46833a54d6d45aec8
AB  - Objective: To develop a natural language processing (NLP) package to extract social determinants of health (SDoH) from clinical narratives, examine the bias among race and gender groups, test the generalizability of extracting SDoH for different disease groups, and examine population-level extraction ratio. Methods: We developed SDoH corpora using clinical notes identified at the University of Florida (UF) Health. We systematically compared 7 transformer-based large language models (LLMs) and developed an open-source package – SODA (i.e., SOcial DeterminAnts) to facilitate SDoH extraction from clinical narratives. We examined the performance and potential bias of SODA for different race and gender groups, tested the generalizability of SODA using two disease domains including cancer and opioid use, and explored strategies for improvement. We applied SODA to extract 19 categories of SDoH from the breast (n = 7,971), lung (n = 11,804), and colorectal cancer (n = 6,240) cohorts to assess patient-level extraction ratio and examine the differences among race and gender groups. Results: We developed an SDoH corpus using 629 clinical notes of cancer patients with annotations of 13,193 SDoH concepts/attributes from 19 categories of SDoH, and another cross-disease validation corpus using 200 notes from opioid use patients with 4,342 SDoH concepts/attributes. We compared 7 transformer models and the GatorTron model achieved the best mean average strict/lenient F1 scores of 0.9122 and 0.9367 for SDoH concept extraction and 0.9584 and 0.9593 for linking attributes to SDoH concepts. There is a small performance gap (∼4%) between Males and Females, but a large performance gap (>16 %) among race groups. The performance dropped when we applied the cancer SDoH model to the opioid cohort; fine-tuning using a smaller opioid SDoH corpus improved the performance. The extraction ratio varied in the three cancer cohorts, in which 10 SDoH could be extracted from over 70 % of cancer patients, but 9 SDoH could be extracted from less than 70 % of cancer patients. Individuals from the White and Black groups have a higher extraction ratio than other minority race groups. Conclusions: Our SODA package achieved good performance in extracting 19 categories of SDoH from clinical narratives. The SODA package with pre-trained transformer models is available at https://github.com/uf-hobi-informatics-lab/SODA_Docker. © 2024 Elsevier Inc.
PB  - Academic Press Inc.
C2  - 38621641
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Jin, Y.
AU  - Chandra, M.
AU  - Verma, G.
AU  - Hu, Y.
AU  - De Choudhury, M.
AU  - Kumar, S.
TI  - Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries
PY  - 2024
T2  - WWW 2024 - Proceedings of the ACM Web Conference
SP  - 2627
EP  - 2638
DO  - 10.1145/3589334.3645643
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194097866&doi=10.1145%2f3589334.3645643&partnerID=40&md5=aa9865c131199d475475065b608064c8
AB  - Large language models (LLMs) are transforming the ways the general public accesses and consumes information. Their influence is particularly pronounced in pivotal sectors like healthcare, where lay individuals are increasingly appropriating LLMs as conversational agents for everyday queries. While LLMs demonstrate impressive language understanding and generation proficiencies, concerns regarding their safety remain paramount in these high-stake domains. Moreover, the development of LLMs is disproportionately focused on English. It remains unclear how these LLMs perform in the context of non-English languages, a gap that is critical for ensuring equity in the real-world use of these systems. This paper provides a framework to investigate the effectiveness of LLMs as multi-lingual dialogue systems for healthcare queries. Our empirically-derived framework XlingEval focuses on three fundamental criteria for evaluating LLM responses to naturalistic human-authored health-related questions: correctness, consistency, and verifiability. Through extensive experiments on four major global languages, including English, Spanish, Chinese, and Hindi, spanning three expert-annotated large health Q&A datasets, and through an amalgamation of algorithmic and human-evaluation strategies, we found a pronounced disparity in LLM responses across these languages, indicating a need for enhanced cross-lingual capabilities. We further propose XLingHealth, a cross-lingual benchmark for examining the multilingual capabilities of LLMs in the healthcare context. Our findings underscore the pressing need to bolster the cross-lingual capacities of these models, and to provide an equitable information ecosystem accessible to all. © 2024 Owner/Author.
PB  - Association for Computing Machinery, Inc
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 9
ER  -

TY  - JOUR
AU  - Civettini, I.
AU  - Zappaterra, A.
AU  - Granelli, B.M.
AU  - Rindone, G.
AU  - Aroldi, A.
AU  - Bonfanti, S.
AU  - Colombo, F.
AU  - Fedele, M.
AU  - Grillo, G.
AU  - Parma, M.
AU  - Perfetti, P.
AU  - Terruzzi, E.
AU  - Gambacorti-Passerini, C.
AU  - Ramazzotti, D.
AU  - Cavalca, F.
TI  - Evaluating the performance of large language models in haematopoietic stem cell transplantation decision-making
PY  - 2024
T2  - British Journal of Haematology
VL  - 204
IS  - 4
SP  - 1523
EP  - 1528
DO  - 10.1111/bjh.19200
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178962668&doi=10.1111%2fbjh.19200&partnerID=40&md5=15e995c461e45c448c74c3d1ddf7349a
AB  - In a first-of-its-kind study, we assessed the capabilities of large language models (LLMs) in making complex decisions in haematopoietic stem cell transplantation. The evaluation was conducted not only for Generative Pre-trained Transformer 4 (GPT-4) but also conducted on other artificial intelligence models: PaLm 2 and Llama-2. Using detailed haematological histories that include both clinical, molecular and donor data, we conducted a triple-blind survey to compare LLMs to haematology residents. We found that residents significantly outperformed LLMs (p = 0.02), particularly in transplant eligibility assessment (p = 0.01). Our triple-blind methodology aimed to mitigate potential biases in evaluating LLMs and revealed both their promise and limitations in deciphering complex haematological clinical scenarios. © 2023 The Authors. British Journal of Haematology published by British Society for Haematology and John Wiley & Sons Ltd.
PB  - John Wiley and Sons Inc
C2  - 38070128
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Gongane, V.U.
AU  - Munot, M.V.
AU  - Anuse, A.D.
TI  - A survey of explainable AI techniques for detection of fake news and hate speech on social media platforms
PY  - 2024
T2  - Journal of Computational Social Science
VL  - 7
IS  - 1
SP  - 587
EP  - 623
DO  - 10.1007/s42001-024-00248-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187890375&doi=10.1007%2fs42001-024-00248-9&partnerID=40&md5=f3620b993b268edbffc33cb86f0729ea
AB  - Artificial intelligence (AI) is a computing field that has played a pivotal role in delivering technological revolutions in various sectors like business, healthcare, finance, social networking, entertainment, and news. With an inimitable ability of AI to process and analyze any form of data (image, text, audio, and video) with the help of high-power computing machines, it is considered as an integral part of Industry 4.0. Social media and internet are another form of technology advancement in digital communication that has created a tremendous impact in the society. Social networking sites like Facebook, Twitter, YouTube, and Instagram provide a platform for people to freely express their thoughts and views. The past decade is witnessing an awful side of social media through the dissemination of online fake news and hate speech content. Social networking sites make use of AI tools to tackle with the increasing hate speech and fake news content. Natural language processing (NLP), a field of AI, include techniques that process vast amount of online content accompanied with machine learning (ML) and deep learning (DL) algorithms that learn the representations of data for detection, classification, and prediction tasks. AI algorithms are considered as “black box” where the decisions made by the algorithms are sometimes biased and lack in transparency. Many state-of-art AI algorithms show low recall and low F1-score metric for diverse forms of hate speech and fake news. The inadequacy of explanations about the decisions made by AI for classification and prediction task is a crucial challenge that needs to be considered. Explainable AI (XAI) is an upcoming research field that has added a new dimension to AI which is “Explainability”. XAI shows a unique ability of interpreting and explaining the decisions made by ML models. This feature of XAI is deployed in various applications like autonomous vehicles and medical diagnostics. In context of social media content, XAI plays an important role in interpreting the diverse forms of hate speech and fake news. Literature studies have reported various XAI models like SHAP (SHapley Additive exPlanations) and Local Interpretable Model-agnostic Explanations (LIME) for detection of hate speech and fake news content. The paper aims to explore XAI models for detection and classification of hate speech and fake news on social media platforms as reported in the research literature. This paper provides a review of evaluation metrics that quantify the XAI technique used in hate speech and fake news detection. The paper leaps into the technical and ethical challenges involved when using XAI models to handle the nuance of online text published on social media platforms. © The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd. 2024.
PB  - Springer
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 9
ER  -

TY  - JOUR
AU  - Shi, Y.
AU  - Ji, J.
AU  - Zhang, X.
AU  - Liu, Y.
AU  - Wang, Z.
AU  - Xu, H.
TI  - Prior tissue knowledge-driven contrastive learning for brain CT report generation
PY  - 2024
T2  - Multimedia Systems
VL  - 30
IS  - 2
C7  - 98
DO  - 10.1007/s00530-024-01289-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189155049&doi=10.1007%2fs00530-024-01289-w&partnerID=40&md5=b27569095105aca6f808f2a5c6d7baf2
AB  - Writing medical reports for brain computed tomography (CT) is essential for radiologists to diagnose cerebrovascular diseases. Recent advances in medical report generation have driven significant progress in producing accurate descriptions of radiology imaging, especially for chest X-rays. Different from the mainstream chest X-ray report generation task, producing a brain CT report faces extreme challenges for language models: (1) Severe visual data bias led by multiple serialized images and sparse lesions, and (2) serious textual data bias led by unbalanced distributions of pathological words. To alleviate the significant visual and textual data bias, we propose a prior tissue knowledge-driven contrastive learning model to improve brain CT report generation. Specifically, we first summarize prior tissue knowledge from the perspectives of visual and textual modalities, including Scan-Tissue and Report-Tissue labels, to depict the clinical experience of brain specialists and enhance the feature representations. Then, driven by prior tissue knowledge, a multi-label retrieval-based contrastive learning module is proposed to effectively separate positive and negative imaging-report pairs by decreasing the disturbance made by hard-negative samples. In this way, the model can learn the essential and generalized consistency between visual and textual features, which is able to relieve the multimodal data bias and boost the generation of high-quality reports. We comprehensively compare the model with previous state-of-the-art methods on the BCT-CHR dataset. The remarkable performance of our model demonstrates that our knowledge-aware contrastive learning paradigm can effectively benefit the brain CT report generation. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Okeh, E.
TI  - Transforming Healthcare: A Comprehensive Approach to Mitigating Bias and Fostering Empathy through AI-Driven Augmented Reality
PY  - 2024
T2  - Proceedings of the AAAI Conference on Artificial Intelligence
VL  - 38
IS  - 21
SP  - 23753
EP  - 23754
DO  - 10.1609/aaai.v38i21.30553
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189617794&doi=10.1609%2faaai.v38i21.30553&partnerID=40&md5=591c7dec4bb2ba72d0bd6a65e03ffd40
AB  - The integration of Artificial Intelligence (AI) into Augmented Reality (AR) for medical applications is propelled by the aim to address evident healthcare disparities. Certain communities have encountered disparities in medical diagnoses, exemplified by Black individuals exhibiting a 2.4 times higher likelihood of schizophrenia diagnosis compared to their white counterparts (Faber et al., 2023). These disparities often arise from structured interview assessments overlooking cultural nuances, resulting in increased misdiagnosis rates. This study leverages AI and AR to develop unbiased diagnostic tools and enhance empathy in healthcare professionals' training. Uniquely prioritizing the reduction of biased language and the fostering of empathy through AI-driven Natural Language Processing (NLP) and AI-driven virtual patients, the research aims to enhance diagnostic accuracy while promoting cultural sensitivity among healthcare professionals. Aligned with broader goals of achieving equitable healthcare and reducing disparities, the evaluation involves pre- and post-training assessments to measure language improvements and empathy enhancements. Successful implementation could lead to a more equitable healthcare landscape, fostering trust in AI-driven systems and ensuring fairer medical care for diverse communities. Copyright © 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
PB  - Association for the Advancement of Artificial Intelligence
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Klivans, A.
AU  - Dimakis, A.G.
AU  - Grauman, K.
AU  - Tamir, J.I.
AU  - Diaz, D.J.
AU  - Davidson, K.
TI  - Institute for Foundations of Machine Learning (IFML): Advancing AI systems that will transform our world
PY  - 2024
T2  - AI Magazine
VL  - 45
IS  - 1
SP  - 35
EP  - 41
DO  - 10.1002/aaai.12163
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188287637&doi=10.1002%2faaai.12163&partnerID=40&md5=1f286054f64efaa719764b9fa59314d0
AB  - The Institute for Foundations of Machine Learning (IFML) focuses on core foundational tools to power the next generation of machine learning models. Its research underpins the algorithms and data sets that make generative artificial intelligence (AI) more accurate and reliable. Headquartered at The University of Texas at Austin, IFML researchers collaborate across an ecosystem that spans University of Washington, Stanford, UCLA, Microsoft Research, the Santa Fe Institute, and Wichita State University. Over the past year, we have witnessed incredible breakthroughs in AI on topics that are at the heart of IFML's agenda, such as foundation models, LLMs, fine-tuning, and diffusion with game-changing applications influencing almost every area of science and technology. In this article, we seek to highlight seek to highlight the application of foundational machine learning research on key use-inspired topics: Fairness in Imaging with Deep Learning: designing the correct metrics and algorithms to make deep networks less biased. Deep proteins: using foundational machine learning techniques to advance protein engineering and launch a biomanufacturing revolution. Sounds and Space for Audio-Visual Learning: building agents capable of audio-visual navigation in complex 3D environments via new data augmentations. Improving Speed and Robustness of Magnetic Resonance Imaging: using deep learning algorithms to develop fast and robust MRI methods for clinical diagnostic imaging. IFML is also responding to explosive industry demand for an AI-capable workforce. We have launched an accessible, affordable, and scalable new degree program—the MSAI—that looks to wholly reshape the AI/ML workforce pipeline. © 2024 UT Austin. AI Magazine published by John Wiley & Sons Ltd on behalf of Association for the Advancement of Artificial Intelligence.
PB  - John Wiley and Sons Inc
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Surapaneni, K.M.
AU  - Rajajagadeesan, A.
AU  - Goudhaman, L.
AU  - Lakshmanan, S.
AU  - Sundaramoorthi, S.
AU  - Ravi, D.
AU  - Rajendiran, K.
AU  - Swaminathan, P.
TI  - Evaluating ChatGPT as a self-learning tool in medical biochemistry: A performance assessment in undergraduate medical university examination
PY  - 2024
T2  - Biochemistry and Molecular Biology Education
VL  - 52
IS  - 2
SP  - 237
EP  - 248
DO  - 10.1002/bmb.21808
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180219788&doi=10.1002%2fbmb.21808&partnerID=40&md5=f56a25168928c59949eb6803f3ea5130
AB  - The emergence of ChatGPT as one of the most advanced chatbots and its ability to generate diverse data has given room for numerous discussions worldwide regarding its utility, particularly in advancing medical education and research. This study seeks to assess the performance of ChatGPT in medical biochemistry to evaluate its potential as an effective self-learning tool for medical students. This evaluation was carried out using the university examination question papers of both parts 1 and 2 of medical biochemistry which comprised theory and multiple choice questions (MCQs) accounting for a total of 100 in each part. The questions were used to interact with ChatGPT, and three raters independently reviewed and scored the answers to prevent bias in scoring. We conducted the inter-item correlation matrix and the interclass correlation between raters 1, 2, and 3. For MCQs, symmetric measures in the form of kappa value (a measure of agreement) were performed between raters 1, 2, and 3. ChatGPT generated relevant and appropriate answers to all questions along with explanations for MCQs. ChatGPT has “passed” the medical biochemistry university examination with an average score of 117 out of 200 (58%) in both papers. In Paper 1, ChatGPT has secured 60 ± 2.29 and 57 ± 4.36 in Paper 2. The kappa value for all the cross-analysis of Rater 1, Rater 2, and Rater 3 scores in MCQ was 1.000. The evaluation of ChatGPT as a self-learning tool in medical biochemistry has yielded important insights. While it is encouraging that ChatGPT has demonstrated proficiency in this area, the overall score of 58% indicates that there is work to be done. To unlock its full potential as a self-learning tool, ChatGPT must focus on generating not only accurate but also comprehensive and contextually relevant content. © 2023 International Union of Biochemistry and Molecular Biology.
PB  - John Wiley and Sons Inc
C2  - 38112255
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - JOUR
AU  - Huang, T.
AU  - Socrates, V.
AU  - Gilson, A.
AU  - Safranek, C.
AU  - Chi, L.
AU  - Wang, E.A.
AU  - Puglisi, L.B.
AU  - Brandt, C.
AU  - Taylor, R.A.
AU  - Wang, K.
TI  - Identifying incarceration status in the electronic health record using large language models in emergency department settings
PY  - 2024
T2  - Journal of Clinical and Translational Science
VL  - 8
IS  - 1
C7  - e53
DO  - 10.1017/cts.2024.496
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187706822&doi=10.1017%2fcts.2024.496&partnerID=40&md5=af2fce2d7b3b36a8da063e9a22aa376d
AB  - Background: Incarceration is a significant social determinant of health, contributing to high morbidity, mortality, and racialized health inequities. However, incarceration status is largely invisible to health services research due to inadequate clinical electronic health record (EHR) capture. This study aims to develop, train, and validate natural language processing (NLP) techniques to more effectively identify incarceration status in the EHR. Methods: The study population consisted of adult patients (≥ 18 y.o.) who presented to the emergency department between June 2013 and August 2021. The EHR database was filtered for notes for specific incarceration-related terms, and then a random selection of 1,000 notes was annotated for incarceration and further stratified into specific statuses of prior history, recent, and current incarceration. For NLP model development, 80% of the notes were used to train the Longformer-based and RoBERTa algorithms. The remaining 20% of the notes underwent analysis with GPT-4. Results: There were 849 unique patients across 989 visits in the 1000 annotated notes. Manual annotation revealed that 559 of 1000 notes (55.9%) contained evidence of incarceration history. ICD-10 code (sensitivity: 4.8%, specificity: 99.1%, F1-score: 0.09) demonstrated inferior performance to RoBERTa NLP (sensitivity: 78.6%, specificity: 73.3%, F1-score: 0.79), Longformer NLP (sensitivity: 94.6%, specificity: 87.5%, F1-score: 0.93), and GPT-4 (sensitivity: 100%, specificity: 61.1%, F1-score: 0.86). Conclusions: Our advanced NLP models demonstrate a high degree of accuracy in identifying incarceration status from clinical notes. Further research is needed to explore their scaled implementation in population health initiatives and assess their potential to mitigate health disparities through tailored system interventions.  © The Author(s), 2024. Published by Cambridge University Press on behalf of Association for Clinical and Translational Science.
PB  - Cambridge University Press
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Rani, M.
AU  - Mishra, B.K.
AU  - Thakker, D.
AU  - Babar, M.
AU  - Jones, W.
AU  - Din, A.
TI  - Biases and Trustworthiness Challenges with Mitigation Strategies for Large Language Models in Healthcare
PY  - 2024
T2  - 2024 International Conference on IT and Industrial Technologies, ICIT 2024
DO  - 10.1109/ICIT63607.2024.10859641
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218216818&doi=10.1109%2fICIT63607.2024.10859641&partnerID=40&md5=8e62f7f59472196b37f0368d21850086
AB  - Rapid innovations in Large Language Models (LLMs) have resulted in remarkably efficient decision-making and learning capacities, specifically in critical sectors such as healthcare. Domain-specific LLMs are progressively being designed for medical pre-screening and diagnostic procedures in healthcare. Despite these advancements, LLMs persist as opaque systems lacking the capacity to offer fair decisions and trustworthy explanations. Though various techniques are proposed to address challenges associated with LLMs, further research is considered essential to adopt LLMs in high-risk sectors. This article explores challenges related to LLMs along with suggested strategies for mitigation. Among several challenges, this study presents a comprehensive overview of biases and trustworthiness in healthcare LLMs. It presents an overview of clinical, cognitive, and demographic bias mitigation approaches at the Data, Model, and Inference levels. Further, it provides a detailed critical analysis of existing bias quantification metrics and healthcare benchmarks to assess trustworthiness in clinical LLMs. This research is supported by an empirical study, where existing patient records are extracted to fine-tune Llama2. The responses from fine-tuned Llama 2 are analyzed for various biases, and existing bias mitigation strategies are applied. However, the results indicate existing bias mitigation approaches need to be revised, highlighting the need for advanced techniques. This study concludes by explaining the essential research areas in bias mitigation and trustworthiness necessary to ensure the practical deployment of LLMs in clinical decision-making. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Pillai, M.
AU  - Posada, J.
AU  - Gardner, R.M.
AU  - Hernandez-Boussard, T.
AU  - Bannett, Y.
TI  - Measuring quality-of-care in treatment of young children with attention-deficit/hyperactivity disorder using pre-trained language models
PY  - 2024
T2  - Journal of the American Medical Informatics Association
VL  - 31
IS  - 4
SP  - 949
EP  - 957
DO  - 10.1093/jamia/ocae001
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189671384&doi=10.1093%2fjamia%2focae001&partnerID=40&md5=04f7a6546414143c4c64e545e3ac4d0d
AB  - Objective: To measure pediatrician adherence to evidence-based guidelines in the treatment of young children with attention-deficit/hyperactivity disorder (ADHD) in a diverse healthcare system using natural language processing (NLP) techniques. Materials and Methods: We extracted structured and free-text data from electronic health records (EHRs) of all office visits (2015-2019) of children aged 4-6 years in a community-based primary healthcare network in California, who had ≥1 visits with an ICD-10 diagnosis of ADHD. Two pediatricians annotated clinical notes of the first ADHD visit for 423 patients. Inter-annotator agreement (IAA) was assessed for the recommendation for the first-line behavioral treatment (F-measure = 0.89). Four pre-trained language models, including BioClinical Bidirectional Encoder Representations from Transformers (BioClinicalBERT), were used to identify behavioral treatment recommendations using a 70/30 train/test split. For temporal validation, we deployed BioClinicalBERT on 1,020 unannotated notes from other ADHD visits and well-care visits; all positively classified notes (n = 53) and 5% of negatively classified notes (n = 50) were manually reviewed. Results: Of 423 patients, 313 (74%) were male; 298 (70%) were privately insured; 138 (33%) were White; 61 (14%) were Hispanic. The BioClinicalBERT model trained on the first ADHD visits achieved F1 = 0.76, precision = 0.81, recall = 0.72, and AUC = 0.81 [0.72-0.89]. Temporal validation achieved F1 = 0.77, precision = 0.68, and recall = 0.88. Fairness analysis revealed low model performance in publicly insured patients (F1 = 0.53). Conclusion: Deploying pre-trained language models on a variable set of clinical notes accurately captured pediatrician adherence to guidelines in the treatment of children with ADHD. Validating this approach in other patient populations is needed to achieve equitable measurement of quality of care at scale and improve clinical care for mental health conditions.  © 2024 The Author(s). Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved.
PB  - Oxford University Press
C2  - 38244997
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Rahman, C.R.
AU  - Wong, L.
TI  - How much can ChatGPT really help computational biologists in programming?
PY  - 2024
T2  - Journal of Bioinformatics and Computational Biology
VL  - 22
IS  - 2
C7  - 2471001
DO  - 10.1142/S021972002471001X
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194062678&doi=10.1142%2fS021972002471001X&partnerID=40&md5=d6a298b60e39defe5a368ca2db527e8d
AB  - ChatGPT, a recently developed product by openAI, is successfully leaving its mark as a multi-purpose natural language based chatbot. In this paper, we are more interested in analyzing its potential in the field of computational biology. A major share of work done by computational biologists these days involve coding up bioinformatics algorithms, analyzing data, creating pipelining scripts and even machine learning modeling and feature extraction. This paper focuses on the potential influence (both positive and negative) of ChatGPT in the mentioned aspects with illustrative examples from different perspectives. Compared to other fields of computer science, computational biology has (1) less coding resources, (2) more sensitivity and bias issues (deals with medical data), and (3) more necessity of coding assistance (people from diverse background come to this field). Keeping such issues in mind, we cover use cases such as code writing, reviewing, debugging, converting, refactoring, and pipelining using ChatGPT from the perspective of computational biologists in this paper. © 2024 World Scientific Publishing Europe Ltd.
PB  - World Scientific
C2  - 38779779
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Hassan, S.N.
TI  - Response of Broiler Chickens to Marjoram (Origanum majorana) Medical Plant Challenged with E.coli
PY  - 2024
T2  - Pakistan Journal of Life and Social Sciences
VL  - 22
IS  - 2
SP  - 12943
EP  - 12954
DO  - 10.57239/PJLSS-2024-22.2.00925
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209743890&doi=10.57239%2fPJLSS-2024-22.2.00925&partnerID=40&md5=dc764001af4646a2c4f37ac665df1289
AB  - This investigation aims to evaluate the beneficial effects of Marjoram (Origanum majorana) as a substitute for the antibiotic zinc bacitracin in enhancing the efficiency and gut wellness of broilers subjected to an E. coli (Escherichia coli) challenging paradigm. A feeding trial took place with 460 day old chicks Ross 308, and they were randomly placed in four treatments, each consisting of six repeats, and housed in two distinct areas. Moreover, Four treatments, comprising a positive control, an antibiotic, Marjoram at 0.1%, and Marjoram at 0.2%, were subjected to an E. coli challenge and placed in room one; in contrast, the fifth treatment functioned as a negative control (non-challenged) and was located in the next room. Broiler birds were exposed to E. coli on days eight and nine of their age. Following that, On days 24 and 35 of the chickens’ age, the increase in body weight and feed conversion ratio were significantly greater (P < 0.01) in the negative control group than in the other experimental groups. Furthermore, each of the concentrations for Marjoram markedly enhanced the live body weight compared to other experimental treatments. In comparison to the positive control, birds subjected to negative control, antibiotic, Marjoram 0.1%, and Marjoram 0.2% exhibited an increase in villus height (P > 0.03). Also, the result from the experiment showed an enhanced villus height/crypt depth ratio (P > 0.04). Furthermore, the serum levels of Glutamic Pyruvic Transaminase (GPT) was significantly decreased (P < 0.001) in the negative control relative to the positive control group. Additionally, broilers administered with E. coli challenge, levels of GPT were significantly reduced (p < 0.001) with antibiotic, Marjoram 0.1%, and also 0.2% compared to broilers with no challenge treatment. Notable disparities in NDV levels among broiler chicks were reported. In this study, Marjoram demonstrated efficacy comparable to antibiotics in mitigating the adverse effects of E. coli on broiler chickens’ performance and gut health. The results obtained by utilizing Marjoram in this investigation provided positive indicators for the possible control of E. coli in broiler chicken production. © (2024), (Elite Scientific Publications). All rights reserved.
PB  - Elite Scientific Publications
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Miao, J.
AU  - Thongprayoon, C.
AU  - Valencia, O.G.
AU  - Craici, I.M.
AU  - Cheungpasitporn, W.
TI  - Navigating Nephrology's Decline Through a GPT-4 Analysis of Internal Medicine Specialties in the United States: Qualitative Study
PY  - 2024
T2  - JMIR Medical Education
VL  - 10
C7  - e57157
DO  - 10.2196/57157
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208268305&doi=10.2196%2f57157&partnerID=40&md5=22aff96b35d1046004de14e8293e8f4c
AB  - Background: The 2024 Nephrology fellowship match data show the declining interest in nephrology in the United States, with an 11% drop in candidates and a mere 66% (321/488) of positions filled. Objective: The study aims to discern the factors influencing this trend using ChatGPT, a leading chatbot model, for insights into the comparative appeal of nephrology versus other internal medicine specialties. Methods: Using the GPT-4 model, the study compared nephrology with 13 other internal medicine specialties, evaluating each on 7 criteria including intellectual complexity, work-life balance, procedural involvement, research opportunities, patient relationships, career demand, and financial compensation. Each criterion was assigned scores from 1 to 10, with the cumulative score determining the ranking. The approach included counteracting potential bias by instructing GPT-4 to favor other specialties over nephrology in reverse scenarios. Results: GPT-4 ranked nephrology only above sleep medicine. While nephrology scored higher than hospice and palliative medicine, it fell short in key criteria such as work-life balance, patient relationships, and career demand. When examining the percentage of filled positions in the 2024 appointment year match, nephrology’s filled rate was 66%, only higher than the 45% (155/348) filled rate of geriatric medicine. Nephrology’s score decreased by 4%‐14% in 5 criteria including intellectual challenge and complexity, procedural involvement, career opportunity and demand, research and academic opportunities, and financial compensation. Conclusions: ChatGPT does not favor nephrology over most internal medicine specialties, highlighting its diminishing appeal as a career choice. This trend raises significant concerns, especially considering the overall physician shortage, and prompts a reevaluation of factors affecting specialty choice among medical residents. © Jing Miao, Charat Thongprayoon, Oscar Garcia Valencia, Iasmina M Craici, Wisit Cheungpasitporn.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - El Gharib, K.
AU  - Jundi, B.
AU  - Furfaro, D.
AU  - Abdulnour, R.-E.E.
TI  - AI-assisted human clinical reasoning in the ICU: beyond “to err is human”
PY  - 2024
T2  - Frontiers in Artificial Intelligence
VL  - 7
C7  - 1506676
DO  - 10.3389/frai.2024.1506676
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212698556&doi=10.3389%2ffrai.2024.1506676&partnerID=40&md5=431eefc8461a8a350c89401b48a33c49
AB  - Diagnostic errors pose a significant public health challenge, affecting nearly 800,000 Americans annually, with even higher rates globally. In the ICU, these errors are particularly prevalent, leading to substantial morbidity and mortality. The clinical reasoning process aims to reduce diagnostic uncertainty and establish a plausible differential diagnosis but is often hindered by cognitive load, patient complexity, and clinician burnout. These factors contribute to cognitive biases that compromise diagnostic accuracy. Emerging technologies like large language models (LLMs) offer potential solutions to enhance clinical reasoning and improve diagnostic precision. In this perspective article, we explore the roles of LLMs, such as GPT-4, in addressing diagnostic challenges in critical care settings through a case study of a critically ill patient managed with LLM assistance. Copyright © 2024 El Gharib, Jundi, Furfaro and Abdulnour.
PB  - Frontiers Media SA
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Pittman, J.
TI  - Effective Continuous Quantitative Measures for End-to-End AI Guardrails
PY  - 2024
T2  - Proceedings of the 4th International Conference on AI Research, ICAIR 2024
SP  - 355
EP  - 363
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215695720&partnerID=40&md5=7cdcf8c8506e9a666afe4537c863fe62
AB  - Large Language Models such as ChatGPT have brought cutting-edge AI systems into the cultural zeitgeist. As a result, AI is no longer an isolated fief of academia or forward-leaning businesses. There are more than 35 million visits to open-source models in public repositories monthly. Clearly, the general technology community has caught onto the power of such systems and is keen to harness the promise of efficiency, productivity, and enhanced capability. Concurrent to this uptrend, AI systems are understood to be potentially vulnerable to various ethical issues. Such issues range from bias and fairness, to explainability and trustworthiness. More than mere theory, such vulnerabilities have manifested in mainstream settings such as politics, medicine, and law. The ethical implementation and operation of AI systems is, therefore, of critical interest as the democratization of such systems gains accelerates. However, there is an ongoing challenge insofar as there is little consensus on what constitutes quantitative ethical and responsible AI guardrails. This leaves AI practitioners without sufficient guidance to implement systems reasonably free from societal level harm. Accordingly, this work presents a structured taxonomy and concept matrix consisting of 39 discrete guardrails arrayed across a three-phased AI system lifecycle. Measure families further organize measures in terms such as bias mitigation, adversarial robustness, and anomaly monitoring. Then, I provide specific quantitative metrics for each measure construct. The intended takeaway is for AI practitioners to have the means to select appropriate and effective metrics for assuring ethical and responsible guardrails. © Proceedings of the 4th International Conference on AI Research, ICAIR 2024.
PB  - Academic Conferences International Limited
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Tian, Y.
AU  - Gan, R.
AU  - Song, Y.
AU  - Zhang, J.
AU  - Zhang, Y.
TI  - CHIMED-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences
PY  - 2024
T2  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
VL  - 1
SP  - 7156
EP  - 7173
DO  - 10.18653/v1/2024.acl-long.386
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204460754&doi=10.18653%2fv1%2f2024.acl-long.386&partnerID=40&md5=6dc9ee7e3fea15efc252463f59d89506
AB  - Recently, the increasing demand for superior medical services has highlighted the discrepancies in the medical infrastructure. With big data, especially texts, forming the foundation of medical services, there is an exigent need for effective natural language processing (NLP) solutions tailored to the healthcare domain. Conventional approaches leveraging pre-trained models present promising results in this domain and current large language models (LLMs) offer advanced foundation for medical text processing. However, most medical LLMs are trained only with supervised fine-tuning (SFT), even though it efficiently empowers LLMs to understand and respond to medical instructions but is ineffective in learning domain knowledge and aligning with human preference. In this work, we propose CHIMED-GPT, a new benchmark LLM designed explicitly for Chinese medical domain, and undergoes a comprehensive training regime with pre-training, SFT, and RLHF. Evaluations on tasks including information extraction, question answering, and dialogue generation demonstrate CHIMED-GPT's superior performance over general domain LLMs. Furthermore, we analyze possible biases through prompting CHIMED-GPT to perform attitude scales regarding discrimination of patients, so as to contribute to further responsible development of LLMs in the medical domain. © 2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Wen, B.
AU  - Norel, R.
AU  - Liu, J.
AU  - Stappenbeck, T.
AU  - Zulkernine, F.
AU  - Chen, H.
TI  - Leveraging Large Language Models for Patient Engagement: The Power of Conversational AI in Digital Health
PY  - 2024
T2  - Proceedings - 2024 IEEE International Conference on Digital Health, ICDH 2024
SP  - 104
EP  - 113
DO  - 10.1109/ICDH62654.2024.00027
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203820422&doi=10.1109%2fICDH62654.2024.00027&partnerID=40&md5=3d0b7ed983763466332b9ad6bd136b02
AB  - The rapid advancements in large language models (LLMs) have opened up new opportunities for transforming patient engagement in healthcare through conversational AI. This paper presents an overview of the current landscape of LLMs in healthcare, specifically focusing on their applications in analyzing and generating conversations for improved patient engagement. We showcase the power of LLMs in handling unstructured conversational data through four case studies: (1) analyzing mental health discussions on Reddit, (2) developing a personalized chatbot for cognitive engagement in seniors, (3) summarizing medical conversation datasets, and (4) designing an AI-powered patient engagement system. These case studies demonstrate how LLMs can effectively extract insights and summarizations from unstructured dialogues and engage patients in guided, goal-oriented conversations. Leveraging LLMs for conversational analysis and generation opens new doors for many patient-centered outcomes research opportunities. However, integrating LLMs into healthcare raises important ethical considerations regarding data privacy, bias, transparency, and regulatory compliance. We discuss best practices and guidelines for the responsible development and deployment of LLMs in healthcare settings. Realizing the full potential of LLMs in digital health will require close collaboration between the AI and healthcare professionals communities to address technical challenges and ensure these powerful tools' safety, efficacy, and equity.  © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Gupta, S.
AU  - Shrivastava, V.
AU  - Deshpande, A.
AU  - Kalyan, A.
AU  - Clark, P.
AU  - Sabharwal, A.
AU  - Khot, T.
TI  - BIAS RUNS DEEP: IMPLICIT REASONING BIASES IN PERSONA-ASSIGNED LLMS
PY  - 2024
T2  - 12th International Conference on Learning Representations, ICLR 2024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200581128&partnerID=40&md5=7089d5b8a338b1117037805ea2e0dd20
AB  - Recent works have showcased the ability of large-scale language models (LLMs) to embody diverse personas in their responses, exemplified by prompts like 'You are Yoda. Explain the Theory of Relativity.' While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs' capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to perform basic reasoning tasks. Our study covers 24 reasoning datasets (spanning mathematics, law, medicine, morals, and more), 4 LLMs (2 versions of ChatGPT-3.5, GPT-4-Turbo, and Llama-2-70b-chat), and 19 diverse personas (e.g., 'an Asian person') spanning 5 socio-demographic groups: race, gender, religion, disability, and political affiliation. Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. While they overtly reject stereotypes when explicitly asked ('Are Black people less skilled at mathematics?'), they manifest stereotypical and often erroneous presumptions when prompted to answer questions while adopting a persona. These can be observed as abstentions in the model's response, e.g., 'As a Black person, I am unable to answer this question as it requires math knowledge', and generally result in a substantial drop in performance on reasoning tasks. Our experiments with ChatGPT-3.5 show that this bias is ubiquitous-80% of our personas demonstrate bias; it is significant-some datasets show performance drops of 70%+; and can be especially harmful for certain groups-some personas suffer statistically significant drops on 80%+ of the datasets. Overall, all four LLMs exhibit persona-induced bias to varying extents, with GPT-4-Turbo showing the least but still a problematic amount of bias (evident in 42% of the personas). Further analysis shows that these persona-induced errors can be hard-to-discern as they do not always manifest as explicit abstentions, and can also be hard-to-avoid-we find de-biasing prompts to have minimal to no effect. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs-a trend on the rise-can surface their deep-rooted biases and have unforeseeable and detrimental side-effects. © 2024 12th International Conference on Learning Representations, ICLR 2024. All rights reserved.
PB  - International Conference on Learning Representations, ICLR
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Çoban, E.
AU  - Altay, B.
TI  - ChatGPT May Help Inform Patients in Dental Implantology
PY  - 2024
T2  - International Journal of Oral and Maxillofacial Implants
VL  - 39
IS  - 5
SP  - e203
EP  - e208
DO  - 10.11607/jomi.10777
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206595126&doi=10.11607%2fjomi.10777&partnerID=40&md5=8cd6698845b9b66aea7ad080840cd332
AB  - Purpose: Patients may have high expectations regarding dental implants based on the source of their information, which can lead to challenges in clinical communication. This study aims to evaluate the quality of responses provided by Chat Generative Pretrained Transformer (ChatGPT, OpenAI), an artificial intelligence (AI) program, to patient questions in the field of dental implantology. Materials and Methods: This study was prospectively designed as a cross-sectional study. Frequently asked questions by patients about general information on dental implantology (Part 1) and dental implant brands (Part 2) were posed to the ChatGPT program. Responses were independently assessed by oral and maxillofacial surgeons (Group 1; n = 10), periodontologists (Group 2; n = 10), prosthodontists (Group 3; n = 10), and general dentists (Group 4; n = 10) using the Global Quality Scale (GQS, scored from 1 [low quality] to 5 [high quality]). Results: There was a total of 60 questions, with 30 questions in each part. Participants in the study were evenly distributed by gender (50% female, 50% male) with a mean age of 32.6 ± 4.07 years. The mean years of experience were 8.5 ± 3.12 years. There were no significant differences in mean age, gender, and years of experience among the groups (P >.05). The overall mean GQS score was 3.87 ± 0.29. Part 1 had a mean score of 3.9 ± 0.35, and Part 2 had a mean score of 3.85 ± 0.29, with no statistically significant difference (P?>.05). Conclusions: The AI platform may contribute to the additional education of patients in the field of dental implantology and aid in understanding treatment procedures. However, it is concerning that ChatGPT may exhibit bias regarding dental implant brands, which could impact patient guidance. Int J Oral Maxillofac Implants 2024;39:e203–e208. doi: 10.11607/jomi.10777 © (2024), (Quintessence Publishing Co. Inc.). All rights reserved.
PB  - Quintessence Publishing Co. Inc.
C2  - 38728144
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Gupta, S.
AU  - Kundu, R.
AU  - Aditya Deo, A.K.
AU  - Patnaik, M.
AU  - Kundu, T.
AU  - Dehury, M.K.
TI  - Enhancing Transparency and Mitigating Bias in Large Language Models' Responses with Sophistication
PY  - 2024
T2  - 2024 IEEE International Conference on Information Technology, Electronics and Intelligent Communication Systems, ICITEICS 2024
DO  - 10.1109/ICITEICS61368.2024.10625096
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203712518&doi=10.1109%2fICITEICS61368.2024.10625096&partnerID=40&md5=8d3d52826e5542b12bcfe9b2dcd81a9d
AB  - Algorithmic bias, woven into the fabric of today's world, skews opportunities and fuels discrimination. This can have amplifying consequences in the healthcare sector, image generation, and many others. Existing techniques to mitigate bias in AI primarily focus on data cleansing and model retraining, often proving ineffective due to the inherent complexity and hidden nature of biases. The main problem lies in the algorithm used for the cleaning of the data and not being effective enough to mitigate the bias present in the data. In contrast, the proposed approach is an architectural fusion of two of the algorithms for debiasing the data namely reweighing and adversarial debiasing. This architectural fusion promises significant advancements in transparent and unbiased chatbot interactions, compared to existing methods, it is going to pave the way for fairer and more equitable communication in the age of AI.  © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Abdalrazzaq, A.S.
AU  - Kadhim Meftin, N.
AU  - Hussein Al Fawadi, D.M.
AU  - Hameed Merzah, H.
AU  - Baban, O.
AU  - Akymenko, A.
AU  - Jameel Al-Obaidi, M.I.
TI  - Optimizing Robotic Interaction and Decision Making Using Advanced NLP and AI Technologies
PY  - 2024
T2  - Conference of Open Innovation Association, FRUCT
SP  - 692
EP  - 701
DO  - 10.23919/FRUCT64283.2024.10749908
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210869313&doi=10.23919%2fFRUCT64283.2024.10749908&partnerID=40&md5=11b41b452ec730f688917756eb36c944
AB  - Background: Incorporating Natural Language Processing (NLP) and Artificial Intelligence (AI) algorithms into robotic assistants can further improve their capacity to independently execute more sophisticated tasks across various sectors, such as healthcare, manufacturing, or retail. However, there have been numerous roadblocks to making them practical from an end-user perspective and addressing ethical issues.Objective: The article delves into the role of NLP and AI assimilation in extending the functional capabilities of robotic assistants, specifically improving language comprehension, decision-making accuracy, and adaptive learning. Additionally, explores the ethical consequences of increased robot independence.Methods: The methodology uses two publicly available datasets that facilitated the training and testing of these language models: 1) Multi-Domain Dialogue Dataset (MDDD), which has half a million labeled conversations with tasks as complex as open-domain multi-turn dialogues, and 2) OpenAI Language Interaction Data set (OLID), an interaction data collected from over one million human-robot interactions. The robots' performance was tested in simulated and real-world settings, including healthcare, manufacturing, and retail.Results: Healthcare exhibited an increase in language comprehension accuracy of 16% (84% post-integration), while autonomous decision-making and manufacturing improved by around 25% (83% post-integration), as well as customer services response rate and retail reaching around upto15% (80% post-integration) improvements. Adaptive learning effectiveness improved by 27% in manufacturing, indicating the robots obtained better performance as they worked.Conclusion: Combining NLP with AI allows robotic assistants to do more complicated, contextually aware activities, expanding their usefulness across many disciplines. While acknowledging the many advances, continual study and investigation are essential to navigate robotic helper technology and its many uses. Though these advancements show potential, upcoming studies need to concentrate on reducing ethical risks such as bias and privacy issues to guarantee the responsible implementation of AI-powered robots. © 2024 FRUCT Oy.
PB  - IEEE Computer Society
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Chen, C.
AU  - Wu, Y.
AU  - Dai, Q.
AU  - Zhou, H.-Y.
AU  - Xu, M.
AU  - Yang, S.
AU  - Han, X.
AU  - Yu, Y.
TI  - A Survey on Graph Neural Networks and Graph Transformers in Computer Vision: A Task-Oriented Perspective
PY  - 2024
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
VL  - 46
IS  - 12
SP  - 10297
EP  - 10318
DO  - 10.1109/TPAMI.2024.3445463
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201759935&doi=10.1109%2fTPAMI.2024.3445463&partnerID=40&md5=208f154fb9192e367f29c3b32e628515
AB  - Graph Neural Networks (GNNs) have gained momentum in graph representation learning and boosted the state of the art in a variety of areas, such as data mining (e.g., social network analysis and recommender systems), computer vision (e.g., object detection and point cloud learning), and natural language processing (e.g., relation extraction and sequence learning), to name a few. With the emergence of Transformers in natural language processing and computer vision, graph Transformers embed a graph structure into the Transformer architecture to overcome the limitations of local neighborhood aggregation while avoiding strict structural inductive biases. In this paper, we present a comprehensive review of GNNs and graph Transformers in computer vision from a task-oriented perspective. Specifically, we divide their applications in computer vision into five categories according to the modality of input data, i.e., 2D natural images, videos, 3D data, vision + language, and medical images. In each category, we further divide the applications according to a set of vision tasks. Such a task-oriented taxonomy allows us to examine how each task is tackled by different GNN-based approaches and how well these approaches perform. Based on the necessary preliminaries, we provide the definitions and challenges of the tasks, in-depth coverage of the representative approaches, as well as discussions regarding insights, limitations, and future directions. © 1979-2012 IEEE.
PB  - IEEE Computer Society
C2  - 39159038
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 14
ER  -

TY  - JOUR
AU  - Afjal, M.
TI  - Evolving trends, limitations, and ethical considerations in AI-driven conversational interfaces: assessing ChatGPT's impact on healthcare, financial services, and educational sectors
PY  - 2024
T2  - Technology Analysis and Strategic Management
DO  - 10.1080/09537325.2024.2420617
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209894343&doi=10.1080%2f09537325.2024.2420617&partnerID=40&md5=145175a67483ce4c4599ecf0b55d38a6
AB  - This study presents a comprehensive bibliometric analysis of ChatGPT's impact on three critical sectors: healthcare, financial services, and education, covering the period from 2022 to 2024. The research aimed to explore the evolving trends, limitations, and ethical considerations associated with the deployment of ChatGPT and similar AI-driven conversational interfaces. The findings reveal that while ChatGPT offers significant potential in enhancing efficiency and decision-making processes, it also faces substantial challenges in data privacy, integration with existing systems, and the need for improved emotional intelligence. The study highlights the inherent biases in AI models, raising concerns about the accuracy and fairness of AI-driven outputs. Ethical considerations, including data privacy, transparency, accountability, and equity in AI applications, are identified as crucial areas requiring urgent attention and development. Additionally, the research underscores the importance of developing robust regulatory frameworks and ethical guidelines to govern AI deployment in these sectors. Successful integration of ChatGPT and similar technologies depends on addressing these challenges and ethical considerations through collaborative efforts among AI developers, industry professionals, policymakers, and ethicists. Future research should focus on improving AI's data security, emotional intelligence, bias reduction, human-AI collaboration, and assessing long-term impacts across diverse settings. © 2024 Informa UK Limited, trading as Taylor & Francis Group.
PB  - Routledge
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Sathe, G.
AU  - Choudhary, V.
AU  - Bhagat, D.
TI  - Comprehensive Review on Large Language Models (LLMs)
PY  - 2024
T2  - 15th International Conference on Advances in Computing, Control, and Telecommunication Technologies, ACT 2024
VL  - 2
SP  - 3097
EP  - 3102
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209074454&partnerID=40&md5=e66cd6a7b194e341469140ab827a5bc7
AB  - This review examines the evolving universe of Large Language Models (LLMs), exploring their multifaceted nature. Beyond their definition and key features, we examine their diverse applications and profound impact on the modern world. LLMs stand out for their fluency, adaptability, and continuous learning, enabling them to generate creative text formats, answer complex questions, and hold natural conversations. We dissect their value across sectors like education, healthcare, creative industries, and business, showcasing their potential to personalize learning, revolutionize medical analysis, fuel creative endeavors, and streamline business processes. As LLMs evolve, they hold the key to reshaping human-computer interaction and defining the future of communication, content creation, and innovation. However, ethical considerations around bias, misinformation, and job displacement demand careful attention. Only by prioritizing responsible development that upholds human values can we harness the power of LLMs to build a future where language technology empowers individuals and transforms industries. © Grenze Scientific Society, 2024.
PB  - Grenze Scientific Society
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Lewis, M.
AU  - Hayhoe, B.
TI  - The digital Balint: using AI in reflective practice
PY  - 2024
T2  - Education for Primary Care
VL  - 35
IS  - 6
SP  - 198
EP  - 202
DO  - 10.1080/14739879.2024.2372606
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201976329&doi=10.1080%2f14739879.2024.2372606&partnerID=40&md5=a5ca2c21e0a1fd19e98975abd445f965
AB  - Reflective practice is fundamental to postgraduate general practitioner (GP) training and ongoing professional development. However, real-world challenges like time constraints and professional isolation often limit meaningful engagement with this critical skill. This article proposes that large language models (LLMs), sophisticated artificial intelligence systems, may have potential for enhancing reflective practice. We present three case studies, in which we explore the ability of LLMs to generate thought-provoking questions, which could prompt GPs to consider new angles, address underlying factors, and bridge the gap between theory and practice. Our findings suggest that LLMs could help reframe experiences and foster deeper self reflection, particularly for isolated practitioners. While ethical concerns regarding privacy, over reliance, and potential biases exist, we consider the possibility of responsibly integrating LLMs into reflective practice. For trainees, AI-generated questions might complement personal reflection under guidance. For GPs working in isolation, LLMs present an opportunity to enhance reflective practice, challenging us to consider a place for this technological innovation without diminishing the human aspects essential to medical practice. © 2024 Informa UK Limited, trading as Taylor & Francis Group.
PB  - Taylor and Francis Ltd.
C2  - 39178303
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Ivy, Z.K.
AU  - Hwee, S.
AU  - Kimball, B.C.
AU  - Evans, M.D.
AU  - Marka, N.
AU  - Bendel, C.
AU  - Boucher, A.A.
TI  - Disparities in Documentation: Evidence of Race-Based Biases in the Electronic Medical Record
PY  - 2024
T2  - Journal of Racial and Ethnic Health Disparities
DO  - 10.1007/s40615-024-02132-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201563920&doi=10.1007%2fs40615-024-02132-8&partnerID=40&md5=029bd47988815e846623bf5e6b89ca0e
AB  - Personal implicit biases may contribute to inequitable health outcomes, but the mechanisms of these effects are unclear at a system level. This study aimed to determine whether stigmatizing subjective terms in electronic medical records (EMR) reflect larger societal racial biases. A cross-sectional study was conducted using natural language processing software of all documentation where one or more predefined stigmatizing words were used between January 1, 2019 and June 30, 2021. EMR from emergency care and inpatient encounters in a metropolitan healthcare system were analyzed, focused on the presence or absence of race-based differences in word usage, either by specific terms or by groupings of negative or positive terms based on the common perceptions of the words. The persistence (“stickiness”) of negative and/or positive characterizations in subsequent encounters for an individual was also evaluated. Final analyses included 12,238 encounters for 9135 patients, ranging from newborn to 104 years old. White (68%) vs Black/African American (17%) were the analyzed groups. Several negative terms (e.g., noncompliant, disrespectful, and curse words) were significantly more frequent in encounters with Black/African American patients. In contrast, positive terms (e.g., compliant, polite) were statistically more likely to be in White patients’ documentation. Independent of race, negative characterizations were twice as likely to persist compared with positive ones in subsequent encounters. The use of stigmatizing language in documentation mirrors the same race-based inequities seen in medical outcomes and larger sociodemographic trends. This may contribute to observed healthcare outcome differences by disseminating one’s implicit biases to unknown future healthcare providers. © W. Montague Cobb-NMA Health Institute 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Maghsoudi, A.
AU  - Sada, Y.H.
AU  - Nowakowski, S.
AU  - Guffey, D.
AU  - Zhu, H.
AU  - Yarlagadda, S.R.
AU  - Li, A.
AU  - Razjouyan, J.
TI  - A Multi-Institutional Natural Language Processing Pipeline to Extract Performance Status From Electronic Health Records
PY  - 2024
T2  - Cancer Control
VL  - 31
DO  - 10.1177/10732748241279518
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203110481&doi=10.1177%2f10732748241279518&partnerID=40&md5=09ac82e8ae05a97f56d2a48af0279b3e
AB  - Purpose: Performance status (PS), an essential indicator of patients’ functional abilities, is often documented in clinical notes of patients with cancer. The use of natural language processing (NLP) in extracting PS from electronic medical records (EMRs) has shown promise in enhancing clinical decision-making, patient monitoring, and research studies. We designed and validated a multi-institute NLP pipeline to automatically extract performance status from free-text patient notes. Patients and Methods: We collected data from 19,481 patients in Harris Health System (HHS) and 333,862 patients from veteran affair’s corporate data warehouse (VA-CDW) and randomly selected 400 patients from each data source to train and validate (50%) and test (50%) the proposed pipeline. We designed an NLP pipeline using an expert-derived rule-based approach in conjunction with extensive post-processing to solidify its proficiency. To demonstrate the pipeline’s application, we tested the compliance of PS documentation suggested by the American Society of Clinical Oncology (ASCO) Quality Metric and investigated the potential disparity in PS reporting for stage IV non-small cell lung cancer (NSCLC). We used a logistic regression test, considering patients in terms of race/ethnicity, conversing language, marital status, and gender. Results: The test results on the HHS cohort showed 92% accuracy, and on VA data demonstrated 98.5% accuracy. For stage IV NSCLC patients, the proposed pipeline achieved an accuracy of 98.5%. Furthermore, our analysis revealed a documentation rate of over 85% for PS among NSCLC patients, surpassing the ASCO Quality Metrics. No disparities were observed in the documentation of PS. Conclusion: Our proposed NLP pipeline shows promising results in extracting PS from free-text notes from various health institutions. It may be used in longitudinal cancer data registries. © The Author(s) 2024.
PB  - SAGE Publications Ltd
C2  - 39222957
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Lawrence, H.R.
AU  - Schneider, R.A.
AU  - Rubin, S.B.
AU  - Mataric, M.J.
AU  - McDuff, D.J.
AU  - Bell, M.J.
TI  - The Opportunities and Risks of Large Language Models in Mental Health
PY  - 2024
T2  - JMIR Mental Health
VL  - 11
C7  - e59479
DO  - 10.2196/59479
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201771814&doi=10.2196%2f59479&partnerID=40&md5=348b576ea188c1c57781e5ffbcb5ccc2
AB  - Global rates of mental health concerns are rising, and there is increasing realization that existing models of mental health care will not adequately expand to meet the demand. With the emergence of large language models (LLMs) has come great optimism regarding their promise to create novel, large-scale solutions to support mental health. Despite their nascence, LLMs have already been applied to mental health-related tasks. In this paper, we summarize the extant literature on efforts to use LLMs to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with LLMs' application to mental health and encourage the adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health LLMs. It is especially critical to ensure that mental health LLMs are fine-tuned for mental health, enhance mental health equity, and adhere to ethical standards and that people, including those with lived experience with mental health concerns, are involved in all stages from development through deployment. Prioritizing these efforts will minimize potential harms to mental health and maximize the likelihood that LLMs will positively impact mental health globally. © 2024 JMIR Publications Inc.. All rights reserved.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - JOUR
AU  - Zhang, C.
AU  - Wharton, M.
AU  - Liu, Y.
TI  - Ameliorating Racial Disparities in HIV Prevention via a Nurse-Led, AI-Enhanced Program for Pre-Exposure Prophylaxis Utilization Among Black Cisgender Women: Protocol for a Mixed Methods Study
PY  - 2024
T2  - JMIR Research Protocols
VL  - 13
C7  - e59975
DO  - 10.2196/59975
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201085199&doi=10.2196%2f59975&partnerID=40&md5=18379d8f0bf5759b3a4329760f964158
AB  - Background: HIV pre-exposure prophylaxis (PrEP) is a critical biomedical strategy to prevent HIV transmission among cisgender women. Despite its proven effectiveness, Black cisgender women remain significantly underrepresented throughout the PrEP care continuum, facing barriers such as limited access to care, medical mistrust, and intersectional racial or HIV stigma. Addressing these disparities is vital to improving HIV prevention outcomes within this community. On the other hand, nurse practitioners (NPs) play a pivotal role in PrEP utilization but are underrepresented due to a lack of awareness, a lack of human resources, and insufficient support. Equipped with the rapid evolution of artificial intelligence (AI) and advanced large language models, chatbots effectively facilitate health care communication and linkage to care in various domains, including HIV prevention and PrEP care. Objective: Our study harnesses NPs’ holistic care capabilities and the power of AI through natural language processing algorithms, providing targeted, patient-centered facilitation for PrEP care. Our overarching goal is to create a nurse-led, stakeholder-inclusive, and AI-powered program to facilitate PrEP utilization among Black cisgender women, ultimately enhancing HIV prevention efforts in this vulnerable group in 3 phases. This project aims to mitigate health disparities and advance innovative, technology-based solutions. Methods: The study uses a mixed methods design involving semistructured interviews with key stakeholders, including 50 PrEP-eligible Black women, 10 NPs, and a community advisory board representing various socioeconomic backgrounds. The AI-powered chatbot is developed using HumanX technology and SmartBot360’s Health Insurance Portability and Accountability Act–compliant framework to ensure data privacy and security. The study spans 18 months and consists of 3 phases: exploration, development, and evaluation. Results: As of May 2024, the institutional review board protocol for phase 1 has been approved. We plan to start recruitment for Black cisgender women and NPs in September 2024, with the aim to collect information to understand their preferences regarding chatbot development. While institutional review board approval for phases 2 and 3 is still in progress, we have made significant strides in networking for participant recruitment. We plan to conduct data collection soon, and further updates on the recruitment and data collection progress will be provided as the study advances. Conclusions: The AI-powered chatbot offers a novel approach to improving PrEP care utilization among Black cisgender women, with opportunities to reduce barriers to care and facilitate a stigma-free environment. However, challenges remain regarding health equity and the digital divide, emphasizing the need for culturally competent design and robust data privacy protocols. The implications of this study extend beyond PrEP care, presenting a scalable model that can address broader health disparities. © 2024 JMIR Publications Inc.. All rights reserved.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - de Arriba-Pérez, F.
AU  - García-Méndez, S.
TI  - Leveraging large language models through natural language processing to provide interpretable machine learning predictions of mental deterioration in real time
PY  - 2024
T2  - Arabian Journal for Science and Engineering
DO  - 10.1007/s13369-024-09508-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202166777&doi=10.1007%2fs13369-024-09508-2&partnerID=40&md5=83073c15354bf9a5e941abc86be09559
AB  - Based on official estimates, 50 million people worldwide are affected by dementia, and this number increases by 10 million new patients every year. Without a cure, clinical prognostication and early intervention represent the most effective ways to delay its progression. To this end, artificial intelligence and computational linguistics can be exploited for natural language analysis, personalized assessment, monitoring, and treatment. However, traditional approaches need more semantic knowledge management and explicability capabilities. Moreover, using large language models (llms) for cognitive decline diagnosis is still scarce, even though these models represent the most advanced way for clinical–patient communication using intelligent systems. Consequently, we leverage an llm using the latest natural language processing (nlp) techniques in a chatbot solution to provide interpretable machine learning prediction of cognitive decline in real-time. Linguistic-conceptual features are exploited for appropriate natural language analysis. Through explainability, we aim to fight potential biases of the models and improve their potential to help clinical workers in their diagnosis decisions. More in detail, the proposed pipeline is composed of (i) data extraction employing nlp-based prompt engineering; (ii) stream-based data processing including feature engineering, analysis, and selection; (iii) real-time classification; and (iv) the explainability dashboard to provide visual and natural language descriptions of the prediction outcome. Classification results exceed 80% in all evaluation metrics, with a recall value for the mental deterioration class about 85%. To sum up, we contribute with an affordable, flexible, non-invasive, personalized diagnostic system to this work. © The Author(s) 2024.
PB  - Springer Nature
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Khan, Z.
AU  - Ambadekar, S.
TI  - AI-Powered Collective Decision-Making Systems and the Future Trends
PY  - 2024
T2  - 2024 15th International Conference on Computing Communication and Networking Technologies, ICCCNT 2024
DO  - 10.1109/ICCCNT61001.2024.10725853
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212922880&doi=10.1109%2fICCCNT61001.2024.10725853&partnerID=40&md5=24f633dc53a14a6bf81e5686efd7ad05
AB  - This paper presents an in-depth survey of the current state and future trends of artificial intelligence (AI) in augmenting collective decision-making processes. At the intersection of AI and group dynamics, this research explores how advanced AI technologies such as machine learning, reinforcement learning, natural language processing, predictive analytics, and swarm intelligence, revolutionize the way collective decisions are made in various sectors, including business, healthcare, governance, and finance. We delve into the theoretical foundations of collective decision-making, highlighting the pivotal role of AI in enhancing human judgment, overcoming cognitive limitations, and facilitating consensus in group settings. The paper examines the application of specific AI methodologies like decision trees, ensemble methods, Bayesian approaches, optimization algorithms, and simulation models in diverse decision-making scenarios. Real-world case studies are presented to illustrate the practical implications and transformative impact of AI in fields such as business strategy, healthcare decision support, financial market predictions, and public policy formulation. We address the challenges and limitations inherent in AI-driven decision systems, such as data quality, ethical concerns, biases, transparency, and integration with human processes. The abstract concludes by discussing emerging trends and future directions in AI collective decision-making, including advancements in explainable AI, ethical AI governance, integration of quantum computing, democratisation of decision-making, continuous learning in AI systems, and cross-disciplinary applications, underscoring the potential of AI to significantly enhance the efficacy and equity of collective decision-making across various domains. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Lee, G.G.
AU  - Goodman, D.
AU  - Chang, T.C.P.
TI  - Impact of Demographic Modifiers on Readabilit of Myopia Education Materials Generated by Large Language Models
PY  - 2024
T2  - Clinical Ophthalmology
VL  - 18
SP  - 3591
EP  - 3604
DO  - 10.2147/OPTH.S483024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211781934&doi=10.2147%2fOPTH.S483024&partnerID=40&md5=8e8253831cf37d03d3f2e58b0cde5de9
AB  - Background: The rise of large language models (LLM) promises to widely impact healthcare providers and patients alike. As these tools reflect the biases of currently available data on the internet, there is a risk that increasing LLM use will proliferate these biases and affect information quality. This study aims to characterize the effects of different race, ethnicity, and gender modifiers in question prompts presented to three large language models (LLM) on the length and readability of patient education materials about myopia. Methods: ChatGPT, Gemini, and Copilot were provided a standardized prompt incorporating demographic modifiers to inquire about myopia. The races and ethnicities evaluated were Asian, Black, Hispanic, Native American, and White. Gender was limited to male or female. The prompt was inserted five times into new chat windows. Responses were analyzed for readability by word count, Simple Measure of Gobbledygook (SMOG) index, Flesch-Kincaid Grade Level, and Flesch Reading Ease score. Significant differences were analyzed using two-way ANOVA on SPSS. Results: A total of 150 responses were analyzed. There were no differences in SMOG index, Flesch-Kincaid Grade Level, or Flesch Reading Ease scores between responses generated with prompts containing different gender, race, or ethnicity modifiers using ChatGPT or Copilot. Gemini-generated responses differed significantly in their SMOG Index, Flesch-Kincaid Grade Level, and Flesch Reading Ease based on the race mentioned in the prompt (p<0.05). Conclusion: Patient demographic information impacts the reading level of educational material generated by Gemini but not by ChatGPT or Copilot. As patients use LLMs to understand ophthalmologic diagnoses like myopia, clinicians and users should be aware of demographic influences on readability. Patient gender, race, and ethnicity may be overlooked variables affecting the readability of LLM-generated education materials, which can impact patient care. Future research could focus on the accuracy of generated information to identify potential risks of misinformation. © 2024 Lee et al.
PB  - Dove Medical Press Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Armbruster, J.
AU  - Bussmann, F.
AU  - Rothhaas, C.
AU  - Titze, N.
AU  - Grützner, P.A.
AU  - Freischmidt, H.
TI  - “Doctor ChatGPT, Can You Help Me?” The Patient’s Perspective: Cross-Sectional Study
PY  - 2024
T2  - Journal of Medical Internet Research
VL  - 26
C7  - e58831
DO  - 10.2196/58831
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205446439&doi=10.2196%2f58831&partnerID=40&md5=bb7b8508a3353e34532832d0441df205
AB  - Background: Artificial intelligence and the language models derived from it, such as ChatGPT, offer immense possibilities, particularly in the field of medicine. It is already evident that ChatGPT can provide adequate and, in some cases, expert-level responses to health-related queries and advice for patients. However, it is currently unknown how patients perceive these capabilities, whether they can derive benefit from them, and whether potential risks, such as harmful suggestions, are detected by patients. Objective: This study aims to clarify whether patients can get useful and safe health care advice from an artificial intelligence chatbot assistant. Methods: This cross-sectional study was conducted using 100 publicly available health-related questions from 5 medical specialties (trauma, general surgery, otolaryngology, pediatrics, and internal medicine) from a web-based platform for patients. Responses generated by ChatGPT-4.0 and by an expert panel (EP) of experienced physicians from the aforementioned web-based platform were packed into 10 sets consisting of 10 questions each. The blinded evaluation was carried out by patients regarding empathy and usefulness (assessed through the question: “Would this answer have helped you?”) on a scale from 1 to 5. As a control, evaluation was also performed by 3 physicians in each respective medical specialty, who were additionally asked about the potential harm of the response and its correctness. Results: In total, 200 sets of questions were submitted by 64 patients (mean 45.7, SD 15.9 years; 29/64, 45.3% male), resulting in 2000 evaluated answers of ChatGPT and the EP each. ChatGPT scored higher in terms of empathy (4.18 vs 2.7; P<.001) and usefulness (4.04 vs 2.98; P<.001). Subanalysis revealed a small bias in terms of levels of empathy given by women in comparison with men (4.46 vs 4.14; P=.049). Ratings of ChatGPT were high regardless of the participant’s age. The same highly significant results were observed in the evaluation of the respective specialist physicians. ChatGPT outperformed significantly in correctness (4.51 vs 3.55; P<.001). Specialists rated the usefulness (3.93 vs 4.59) and correctness (4.62 vs 3.84) significantly lower in potentially harmful responses from ChatGPT (P<.001). This was not the case among patients. Conclusions: The results indicate that ChatGPT is capable of supporting patients in health-related queries better than physicians, at least in terms of written advice through a web-based platform. In this study, ChatGPT’s responses had a lower percentage of potentially harmful advice than the web-based EP. However, it is crucial to note that this finding is based on a specific study design and may not generalize to all health care settings. Alarmingly, patients are not able to independently recognize these potential dangers. ©Jonas Armbruster, Florian Bussmann, Catharina Rothhaas, Nadine Titze, Paul Alfred Grützner, Holger Freischmidt.
PB  - JMIR Publications Inc.
C2  - 39352738
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Akyon, S.H.
AU  - Akyon, F.C.
AU  - Camyar, A.S.
AU  - Hızlı, F.
AU  - Sari, T.
AU  - Hızlı, Ş.
TI  - Evaluating the Capabilities of Generative AI Tools in Understanding Medical Papers: Qualitative Study
PY  - 2024
T2  - JMIR Medical Informatics
VL  - 12
C7  - e59258
DO  - 10.2196/59258
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204381860&doi=10.2196%2f59258&partnerID=40&md5=2f1295a914a00f4b5bfab3bdda97ba64
AB  - Background: Reading medical papers is a challenging and time-consuming task for doctors, especially when the papers are long and complex. A tool that can help doctors efficiently process and understand medical papers is needed. Objective: This study aims to critically assess and compare the comprehension capabilities of large language models (LLMs) in accurately and efficiently understanding medical research papers using the STROBE (Strengthening the Reporting of Observational Studies in Epidemiology) checklist, which provides a standardized framework for evaluating key elements of observational study. Methods: The study is a methodological type of research. The study aims to evaluate the understanding capabilities of new generative artificial intelligence tools in medical papers. A novel benchmark pipeline processed 50 medical research papers from PubMed, comparing the answers of 6 LLMs (GPT-3.5-Turbo, GPT-4-0613, GPT-4-1106, PaLM 2, Claude v1, and Gemini Pro) to the benchmark established by expert medical professors. Fifteen questions, derived from the STROBE checklist, assessed LLMs’ understanding of different sections of a research paper. Results: LLMs exhibited varying performance, with GPT-3.5-Turbo achieving the highest percentage of correct answers (n=3916, 66.9%), followed by GPT-4-1106 (n=3837, 65.6%), PaLM 2 (n=3632, 62.1%), Claude v1 (n=2887, 58.3%), Gemini Pro (n=2878, 49.2%), and GPT-4-0613 (n=2580, 44.1%). Statistical analysis revealed statistically significant differences between LLMs (P<.001), with older models showing inconsistent performance compared to newer versions. LLMs showcased distinct performances for each question across different parts of a scholarly paper—with certain models like PaLM 2 and GPT-3.5 showing remarkable versatility and depth in understanding. Conclusions: This study is the first to evaluate the performance of different LLMs in understanding medical papers using the retrieval augmented generation method. The findings highlight the potential of LLMs to enhance medical research by improving efficiency and facilitating evidence-based decision-making. Further research is needed to address limitations such as the influence of question formats, potential biases, and the rapid evolution of LLM models. ©Seyma Handan Akyon, Fatih Cagatay Akyon, Ahmet Sefa Camyar, Fatih Hızlı, Talha Sari, Şamil Hızlı.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Green, H.L.
AU  - Gu, Y.
TI  - TWO ‘USE CASES’ OF CHATGPT IN MEDICAL EDUCATION IDENTIFIED IN THE LITERATURE: INTERACTIVE DIALOGUE AND CONTENT GENERATION
PY  - 2024
T2  - International Conferences e-Society 2024 and Mobile Learning 2024
SP  - 369
EP  - 372
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203582990&partnerID=40&md5=2b1aa19703092023bd4042ea2f3647e5
AB  - Major shifts are occurring in higher education since the inception of Artificial Intelligence (AI) tools such as ChatGPT. With AI’s growing integration into academia, understanding its usage and impact is imperative. We reviewed literature on ChatGPT use in medical education and identified two ‘use cases’ as: (1) an interactive dialogue tool to enrich student learning, and (2) a content generation tool to assist educators in course content creation. Literature also reported concerns around information accuracy, academic integrity, and potential bias. We call for more research to inform the development of pedagogical activities using ChatGPT, and to examine its impact and the effectiveness of risk mitigation strategies. © 2024 International Conferences e-Society 2024 and Mobile Learning 2024. All rights reserved.
PB  - IADIS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Bergomi, L.
TI  - Fostering Human-AI interaction: development of a Clinical Decision Support System enhanced by eXplainable AI and Natural Language Processing
PY  - 2024
T2  - CEUR Workshop Proceedings
VL  - 3793
SP  - 321
EP  - 328
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208286837&partnerID=40&md5=71f74ced0cdd204f075646a5d0344846
AB  - Artificial Intelligence (AI) is increasingly integrated into Decision Support Systems (DSS). The explainability of AI-based systems becomes crucial in sensitive and critical domains, such as healthcare, where ethical considerations and reliability are paramount concerns. In the clinical setting, it is important to evaluate how humans and AI can collaborate on cognitive tasks. Collaboration protocols (HAI-CP) allow for the investigation of the usefulness of AI models and their impact on users (both positive and negative). Although research on the application of these methods is blooming, there is little understanding of the impact on clinical decision-making, especially for eXplainable AI (XAI) systems, due to the lack of user studies. Therefore, the goal of this proposal is to develop a clinical DSS enhanced by XAI and Natural Language Processing (NLP): their synergy can add value to the interaction between users and AI, fostering a more linguistically natural, comprehensible, trustworthy, and supporting interfacing, that blends into the existing workflows. This proposal explores potential solutions to tailor natural language explanations and data visualizations to the end-user, improving the comprehensibility of the reasons behind a decision, and increasing the user’s confidence in the decision; investigates and tests possible strategies to “get the patient-in-the-loop”; explores uncertainty quantification and counterfactual approaches, and finally assesses the impact on naturalistic (i.e., real-world) decision-making and long-term effects and biases. © 2024 Copyright for this paper by its authors.
PB  - CEUR-WS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Levy, S.
AU  - Karver, T.S.
AU  - Adler, W.D.
AU  - Kaufman, M.R.
AU  - Dredze, M.
TI  - Evaluating Biases in Context-Dependent Sexual and Reproductive Health Questions
PY  - 2024
T2  - EMNLP 2024 - 2024 Conference on Empirical Methods in Natural Language Processing, Findings of EMNLP 2024
SP  - 5801
EP  - 5812
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214147972&partnerID=40&md5=a3ef7507c9b7e038b6de74f0095f422c
AB  - Chat-based large language models have the opportunity to empower individuals lacking high-quality healthcare access to receive personalized information across a variety of topics. However, users may ask underspecified questions that require additional context for a model to correctly answer. We study how large language model biases are exhibited through these contextual questions in the healthcare domain. To accomplish this, we curate a dataset of sexual and reproductive healthcare questions (CONTEXTSRH) that are dependent on age, sex, and location attributes. We compare models' outputs with and without demographic context to determine answer alignment among our contextual questions. Our experiments reveal biases in each of these attributes, where young adult female users are favored. © 2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Tian, Y.
TI  - Empowering college physical education: AI-driven training, teaching, and intelligent information processing
PY  - 2024
T2  - MCB Molecular and Cellular Biomechanics
VL  - 21
IS  - 1
C7  - 327
DO  - 10.62617/mcb.v21i1.327
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207100201&doi=10.62617%2fmcb.v21i1.327&partnerID=40&md5=d1d3b65825edf55c908f214357bb02c4
AB  - The current status, methods utilized for Physical Education Training and Teaching System for College Students, and difficulties during information processing are all investigated in this comprehensive study. We compiled 130 empirical research on Artificial Intelligence-based Physical Education (AIPE) using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses are analysed. Research shows that AI may improve health tracking, individualized training, and analysis of sporting performance. There is a lot of promise in AIPE for individualized lessons, immediate feedback, varied classroom settings, and evaluation. The problems arise when dealing with technological dependability, privacy concerns, and the need for instructor assistance. These results give light on important questions for future AIPE. This research delves into the process of creating and launching an all-encompassing educational platform that makes use of AI and data processing methods. Our system's goals include improving the quality of instruction, tailoring feedback to each student, and enhancing the overall learning experience. AI Algorithms powered by artificial intelligence help us shift through student test scores, identify knowledge gaps, and modify lessons appropriately. This makes sure that the curriculum meets each student's needs. Practical exercises, quizzes, and assignments get immediate feedback through the system. It uses natural language processing (NLP) to analyse student answers, find misunderstandings, and provide help for fixing them. The system personalizes learning routes according to students' choices, learning styles, and progress. It suggests further reading, interactive games, and group assignments. Through the automation of administrative activities, generation of analytics reports, and suggestion of pedagogical changes, the system aids instructors. It makes it easier for students and instructors to talk to one another. Data protection, overcoming AI biases, and getting teachers on board with tech-enhanced lessons are all obstacles. Future studies should aim to improve the system, confirm its efficacy, and encourage its implementation across educational institutions. Finally, there is great potential for improving higher education with an AI-based training and teaching system with strong data processing skills. Students and teachers may reap the advantages of a technologically enhanced, ever-changing learning environment. © 2024 by author(s).
PB  - Sin-Chn Scientific Press
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Ballard, D.H.
AU  - Antigua-Made, A.
AU  - Barre, E.
AU  - Edney, E.
AU  - Gordon, E.B.
AU  - Kelahan, L.
AU  - Lodhi, T.
AU  - Martin, J.G.
AU  - Ozkan, M.
AU  - Serdynski, K.
AU  - Spieler, B.
AU  - Zhu, D.
AU  - Adams, S.J.
TI  - Impact of ChatGPT and Large Language Models on Radiology Education: Association of Academic Radiology—Radiology Research Alliance Task Force White Paper
PY  - 2024
T2  - Academic Radiology
DO  - 10.1016/j.acra.2024.10.023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210748225&doi=10.1016%2fj.acra.2024.10.023&partnerID=40&md5=6e76cc5489cae87267fd5fe93b028da3
AB  - Generative artificial intelligence, including large language models (LLMs), holds immense potential to enhance healthcare, medical education, and health research. Recognizing the transformative opportunities and potential risks afforded by LLMs, the Association of Academic Radiology—Radiology Research Alliance convened a task force to explore the promise and pitfalls of using LLMs such as ChatGPT in radiology. This white paper explores the impact of LLMs on radiology education, highlighting their potential to enrich curriculum development, teaching and learning, and learner assessment. Despite these advantages, the implementation of LLMs presents challenges, including limits on accuracy and transparency, the risk of misinformation, data privacy issues, and potential biases, which must be carefully considered. We provide recommendations for the successful integration of LLMs and LLM-based educational tools into radiology education programs, emphasizing assessment of the technological readiness of LLMs for specific use cases, structured planning, regular evaluation, faculty development, increased training opportunities, academic-industry collaboration, and research on best practices for employing LLMs in education. © 2024 The Association of University Radiologists
PB  - Elsevier Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Ahmad, N.
AU  - Mamatjan, E.
AU  - Wali, T.
AU  - Mamatjan, Y.
TI  - The Development of CanPrompt Strategy in Large Language Models for Cancer Care
PY  - 2024
T2  - 21st IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology, CIBCB 2024
DO  - 10.1109/CIBCB58642.2024.10702147
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207474816&doi=10.1109%2fCIBCB58642.2024.10702147&partnerID=40&md5=c886835464e7352cfbd42743c6fde0de
AB  - Background: The recent revolution in Large Language Models (LLMs) is transforming industries, enhancing communication, and reshaping research methodologies. LLMs have found significant applications across various sectors, notably in finance for stock market predictions, and in healthcare, where complex medical data is analyzed for diagnosis at an early stage, improving diagnostic procedures, and personalized treatment planning. In healthcare, where complex medical data is analyzed for diagnosis at an early stage. Despite the immense potential, challenges such as overwhelming Big Data, model hallucinations, and ethical concerns about patient privacy and bias persist. Method: We implemented novel strategies like CanPrompt to mitigate the accuracy and hallucination concerns to ensure responsible deployment. The CanPrompt strategy utilizes prompt engineering combined with few-shot and in-context learning to significantly enhance model accuracy by generating more relevant answers. The models were tested against a specialized dataset from MedQuAD, focusing on cancer, and evaluated using metrics like ROUGE and BERTScore to assess the semantic and syntactic accuracy of generated responses against validated "Gold Answers". Through this approach, the study seeks to outline the potential and limitations of LLMs in improving cancer care. Result: After applying CanPrompt with models Mistral 7x8b, Falcon 40b, and Llama 3-8b, BERTScore results showed Mistral leading with an accuracy around 84%, Falcon slightly lower, and Llama the least, with respective precision scores also reflecting a similar trend. Conclusion: The study demonstrates the promise of LLMs in cancer care through the introduction of CanPrompt. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Priya, E.
AU  - Dinesh Kumar, E.
AU  - Jayachandiran, K.
AU  - Shamprakash, R.
TI  - Smart Healthcare Assistant with Epidemiological Modelling
PY  - 2024
T2  - 4th International Conference on Power, Energy, Control and Transmission Systems: Harnessing Power and Energy for an Affordable Electrification of India, ICPECTS 2024
DO  - 10.1109/ICPECTS62210.2024.10780252
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215507164&doi=10.1109%2fICPECTS62210.2024.10780252&partnerID=40&md5=c34e96940ba6003920f86ad573dec2b7
AB  - The perpetual threat posed by infectious disease outbreaks necessitates continuous advancements in epidemic modeling and public health interventions. Traditional approaches, while valuable, often face challenges in accurately predicting disease transmission dynamics and informing timely containment strategies. In response, this paper proposes a novel hybrid system that integrates established epidemiological models with cutting-edge Artificial Intelligence (AI) techniques, including ensemble methods and Generative Al for Natural Language Processing (NLP). This extends beyond epidemic modeling to address healthcare accessibility disparities through the development of a chatbot powered by Language Model (LLM) technology. The chatbot, capable of conversing in native languages, offers medical assistance and addresses health-related queries effectively, thereby bridging gaps in healthcare access. Additionally, sophisticated disease dynamic models and endemic simulators are employed to forecast disease spread accurately, ensuring the availability of healthcare information even in remote areas. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Mentink, L.J.
AU  - van Osch, M.J.P.
AU  - Bakker, L.J.
AU  - Olde Rikkert, M.G.M.
AU  - Beckmann, C.F.
AU  - Claassen, J.A.H.R.
AU  - Haak, K.V.
TI  - Functional and vascular neuroimaging in maritime pilots with long-term sleep disruption
PY  - 2024
T2  - GeroScience
DO  - 10.1007/s11357-024-01417-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208986807&doi=10.1007%2fs11357-024-01417-4&partnerID=40&md5=18c232e5b5da9bf652ad7d1e9d7f2140
AB  - The mechanism underlying the possible causal association between long-term sleep disruption and Alzheimer’s disease remains unclear Musiek et al. 2015. A hypothesised pathway through increased brain amyloid load was not confirmed in previous work in our cohort of maritime pilots with long-term work-related sleep disruption Thomas et al. Alzheimer’s Res Ther 2020;12:101. Here, using functional MRI, T2-FLAIR, and arterial spin labeling MRI scans, we explored alternative neuroimaging biomarkers related to both sleep disruption and AD: resting-state network co-activation and between-network connectivity of the default mode network (DMN), salience network (SAL) and frontoparietal network (FPN), vascular damage and cerebral blood flow (CBF). We acquired data of 16 maritime pilots (56 ± 2.3 years old) with work-related long-term sleep disruption (23 ± 4.8 working years) and 16 healthy controls (59 ± 3.3 years old), with normal sleep patterns (Pittsburgh Sleep Quality Index ≤ 5). Maritime pilots did not show altered co-activation in either the DMN, FPN, or SAL and no differences in between-network connectivity. We did not detect increased markers of vascular damage in maritime pilots, and additionally, maritime pilots did not show altered CBF-patterns compared to healthy controls. In summary, maritime pilots with long-term sleep disruption did not show neuroimaging markers indicative of preclinical AD compared to healthy controls. These findings do not resemble those of short-term sleep deprivation studies. This could be due to resiliency to sleep disruption or selection bias, as participants have already been exposed to and were able to deal with sleep disruption for multiple years, or to compensatory mechanisms Mentink et al. PLoS ONE. 2021;15(12):e0237622. This suggests the relationship between sleep disruption and AD is not as strong as previously implied in studies on short-term sleep deprivation, which would be beneficial for all shift workers suffering from work-related sleep disruptions. © The Author(s) 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Fei, X.
AU  - Tang, Y.
AU  - Zhang, J.
AU  - Zhou, Z.
AU  - Yamamoto, I.
AU  - Zhang, Y.
TI  - Evaluating cognitive performance: Traditional methods vs. ChatGPT
PY  - 2024
T2  - Digital Health
VL  - 10
DO  - 10.1177/20552076241264639
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201226406&doi=10.1177%2f20552076241264639&partnerID=40&md5=6b68cd2ba15f95317c0f5e322a1a0d89
AB  - Background: NLP models like ChatGPT promise to revolutionize text-based content delivery, particularly in medicine. Yet, doubts remain about ChatGPT's ability to reliably support evaluations of cognitive performance, warranting further investigation into its accuracy and comprehensiveness in this area. Method: A cohort of 60 cognitively normal individuals and 30 stroke survivors underwent a comprehensive evaluation, covering memory, numerical processing, verbal fluency, and abstract thinking. Healthcare professionals and NLP models GPT-3.5 and GPT-4 conducted evaluations following established standards. Scores were compared, and efforts were made to refine scoring protocols and interaction methods to enhance ChatGPT's potential in these evaluations. Result: Within the cohort of healthy participants, the utilization of GPT-3.5 revealed significant disparities in memory evaluation compared to both physician-led assessments and those conducted utilizing GPT-4 (P < 0.001). Furthermore, within the domain of memory evaluation, GPT-3.5 exhibited discrepancies in 8 out of 21 specific measures when compared to assessments conducted by physicians (P < 0.05). Additionally, GPT-3.5 demonstrated statistically significant deviations from physician assessments in speech evaluation (P = 0.009). Among participants with a history of stroke, GPT-3.5 exhibited differences solely in verbal assessment compared to physician-led evaluations (P = 0.002). Notably, through the implementation of optimized scoring methodologies and refinement of interaction protocols, partial mitigation of these disparities was achieved. Conclusion: ChatGPT can produce evaluation outcomes comparable to traditional methods. Despite differences from physician evaluations, refinement of scoring algorithms and interaction protocols has improved alignment. ChatGPT performs well even in populations with specific conditions like stroke, suggesting its versatility. GPT-4 yields results closer to physician ratings, indicating potential for further enhancement. These findings highlight ChatGPT's importance as a supplementary tool, offering new avenues for information gathering in medical fields and guiding its ongoing development and application. © The Author(s) 2024.
PB  - SAGE Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Guo, Y.
AU  - Yang, Y.
AU  - Zhang, Y.
AU  - Wang, Y.
AU  - Wang, Y.
TI  - DictLLM: Harnessing Key-Value Data Structures with Large Language Models for Enhanced Medical Diagnostics
PY  - 2024
T2  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
SP  - 10231
EP  - 10241
DO  - 10.18653/v1/2024.findings-acl.609
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205293851&doi=10.18653%2fv1%2f2024.findings-acl.609&partnerID=40&md5=b3b57011bec1c64ce9073689695af56e
AB  - Structured data offers a sophisticated mechanism for the organization of information. Existing methodologies for the text-serialization of structured data in the context of large language models fail to adequately address the heterogeneity inherent in key-value structured data. These methods are not ideal and frequently result in larger input sizes and poor adaptability to input changes. In this paper, we introduce DictLLM, an innovative framework designed to improve the modeling of key-value structured data, like medical laboratory reports, for generating medical diagnoses. DictLLM integrates three key components: (1) group positional encoding to maintain permutation invariance, (2) hierarchical attention bias to capture the inherent bias in structured data, and (3) an optimal transport alignment layer that aligns the embedding generated by the dictionary encoder with the LLM, thereby producing a sequence of fixed-length virtual tokens. We carry out experiments using various LLM models on a comprehensive real-world medical laboratory report dataset for automatic diagnosis generation, our findings illustrate that DictLLM significantly outperforms established baseline methods and few-shot GPT-4 implementations in terms of both Rouge-L and Knowledge F1 scores. Furthermore, our evaluation of the framework's scalability and robustness, through a series of experiments, underscores its exceptional capability in accurately modeling the complex key-value data structure of medical dictionary data. © 2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - McBee, J.C.
AU  - Han, D.Y.
AU  - Liu, L.
AU  - Ma, L.
AU  - Adjeroh, D.A.
AU  - Xu, D.
AU  - Hu, G.
TI  - Assessing ChatGPT’s Competency in Addressing Interdisciplinary Inquiries on Chatbot Uses in Sports Rehabilitation: Simulation Study
PY  - 2024
T2  - JMIR Medical Education
VL  - 10
C7  - e51157
DO  - 10.2196/51157
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201783008&doi=10.2196%2f51157&partnerID=40&md5=1be59f5c0ff38d6e6def507eccff5f45
AB  - Background: ChatGPT showcases exceptional conversational capabilities and extensive cross-disciplinary knowledge. In addition, it can perform multiple roles in a single chat session. This unique multirole-playing feature positions ChatGPT as a promising tool for exploring interdisciplinary subjects. Objective: The aim of this study was to evaluate ChatGPT’s competency in addressing interdisciplinary inquiries based on a case study exploring the opportunities and challenges of chatbot uses in sports rehabilitation. Methods: We developed a model termed PanelGPT to assess ChatGPT’s competency in addressing interdisciplinary topics through simulated panel discussions. Taking chatbot uses in sports rehabilitation as an example of an interdisciplinary topic, we prompted ChatGPT through PanelGPT to role-play a physiotherapist, psychologist, nutritionist, artificial intelligence expert, and athlete in a simulated panel discussion. During the simulation, we posed questions to the panel while ChatGPT acted as both the panelists for responses and the moderator for steering the discussion. We performed the simulation using ChatGPT-4 and evaluated the responses by referring to the literature and our human expertise. Results: By tackling questions related to chatbot uses in sports rehabilitation with respect to patient education, physiotherapy, physiology, nutrition, and ethical considerations, responses from the ChatGPT-simulated panel discussion reasonably pointed to various benefits such as 24/7 support, personalized advice, automated tracking, and reminders. ChatGPT also correctly emphasized the importance of patient education, and identified challenges such as limited interaction modes, inaccuracies in emotion-related advice, assurance of data privacy and security, transparency in data handling, and fairness in model training. It also stressed that chatbots are to assist as a copilot, not to replace human health care professionals in the rehabilitation process. Conclusions: ChatGPT exhibits strong competency in addressing interdisciplinary inquiry by simulating multiple experts from complementary backgrounds, with significant implications in assisting medical education. © 2024 JMIR Publications Inc.. All rights reserved.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Uğuz, S.
AU  - Tülü, Ç.N.
TI  - Topic Modeling Analysis in the field of Large Language Models with BERTopic (2020-2024)
PY  - 2024
T2  - 2024 Innovations in Intelligent Systems and Applications Conference, ASYU 2024
DO  - 10.1109/ASYU62119.2024.10757169
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213384473&doi=10.1109%2fASYU62119.2024.10757169&partnerID=40&md5=4b47034363a7bd3d5b3c8b39f9f9fbda
AB  - One of the primary factors behind the success of contemporary artificial intelligence backed applications is the Large Language Models (LLMs). Remarkably, this concept that was virtually unknown just a few years ago has now become increasingly popular. Whereas initial Language Models (LMs) were updated and enhanced every few years, but we now witness new versions with new features and advanced capabilities are being released within mere months. This widespread adoption LLM's in daily life has also sparked our curiosity about the direction of academic research on this topic. In this study, academic papers related to LLM between 2020 and 2024 (until first quarter of 2024) were analyzed with topic modelling technique. The data used in this study were collected from the Scopus database and topic modeling analysis was performed using BERTopic. According to the analysis results obtained, it can be stated that the research topics named Large Language Models (LLMs), ChatGPT in Medicine, Sentiment Bias, Code Generation, Visual Language Models, and System Requirements for LLMs are quite remarkable. It has been revealed that the analysis results obtained and the developments in academic and professional business life are interrelated. Especially today, with the increasing use of ChatGPT, it is known that specialization has been made in this field in order to ensure better execution of queries and a new profession called Prompt Engineering has emerged. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Malik, S.
AU  - Frey, L.J.
AU  - Gutman, J.
AU  - Mushtaq, A.
AU  - Warraich, F.
AU  - Qureshi, K.
TI  - Evaluating Artificial Intelligence-Driven Responses to Acute Liver Failure Queries: A Comparative Analysis Across Accuracy, Clarity, and Relevance
PY  - 2024
T2  - American Journal of Gastroenterology
C7  - 10.14309/ajg.0000000000003255
DO  - 10.14309/ajg.0000000000003255
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213039066&doi=10.14309%2fajg.0000000000003255&partnerID=40&md5=245a89b13397cc06931706ba76bce30c
AB  - Introduction: Recent advancements in Artificial Intelligence (AI), particularly through the deployment of Large Language Models (LLMs), have profoundly impacted healthcare. This study assesses five LLMs—ChatGPT 3.5, ChatGPT 4, BARD, CLAUDE, and COPILOT—on their response accuracy, clarity, and relevance to queries concerning acute liver failure (ALF). We subsequently compare these results with Chat GPT4 enhanced with Retrieval Augmented Generation (RAG) technology. Methods: Based on real-world clinical use and the American College of Gastroenterology guidelines, we formulated 16 ALF questions or clinical scenarios to explore LLMs' ability to handle different clinical questions. Using the "New Chat" functionality, each query was processed individually across the models to reduce any bias. Additionally, we employed the RAG functionality of GPT-4, which integrates external sources as references to ground the results. All responses were evaluated on a Likert scale from 1 to 5 for accuracy, clarity, and relevance by four independent investigators to ensure impartiality. Result: ChatGPT 4, augmented with RAG, demonstrated superior performance compared to others, consistently scoring the highest (4.70, 4.89, 4.78) across all three domains. ChatGPT 4 exhibited notable proficiency, with scores of 3.67 in accuracy, 4.04 in clarity, and 4.01 in relevance. In contrast, CLAUDE achieved 3.04 in clarity, 3.6 in relevance, and 3.65 in accuracy. Meanwhile, BARD and COPILOT exhibited lower performance levels; BARD recorded scores of 2.01 in accuracy and 3.03 in relevance, while COPILOT obtained 2.26 in accuracy and 3.12 in relevance. Conclusion: The study highlights Chat GPT 4 +RAG's superior performance compared to other LLMs. By integrating RAG with LLMs, the system combines generative language skills with accurate, up-to-date information. This improves response clarity, relevance, and accuracy, making them more effective in healthcare. However, AI models must continually evolve and align with medical practices for successful healthcare integration. Copyright © 2024 by The American College of Gastroenterology.
PB  - Wolters Kluwer Health
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
TI  - Predictive Insights into LGBTQ+ Minority Stress: A Transductive Exploration of Social Media Discourse
PY  - 2024
T2  - 2024 IEEE 11th International Conference on Data Science and Advanced Analytics, DSAA 2024
DO  - 10.1109/DSAA61799.2024.10722807
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209402694&doi=10.1109%2fDSAA61799.2024.10722807&partnerID=40&md5=89e60a84fe7d904b56a9849f17fa8537
AB  - Individuals who identify as sexual and gender minorities, including lesbian, gay, bisexual, transgender, queer, and others (LGBTQ+) are more likely to experience poorer health than their heterosexual and cisgender counterparts. One primary source that drives these health disparities is minority stress (i.e., chronic and social stressors unique to LGBTQ+ communitiesï¿½ experiences adapting to the dominant culture). This stress is frequently expressed in LGBTQ+ usersï¿½ posts on social media platforms. However, these expressions are not just straightforward manifestations of minority stress. They involve linguistic complexity (e.g., idiom/lexical diversity), rendering them challenging for many traditional natural language processing methods to detect. In this work, we designed a hybrid model using Graph Neural Networks (GNN) and Bidirectional Encoder Representations from Transformers (BERT), a pre-trained deep language model to improve the classification performance of minority stress detection. We experimented with our model on a benchmark social media dataset for minority stress detection (LGBTQ+ MiSSoM+). The dataset is comprised of 5,789 humanannotated Reddit posts from LGBTQ+ subreddits. Our approach enables the extraction of hidden linguistic nuances through pretraining on a vast amount of raw data, while also engaging in transductive learning to jointly develop representations for both labeled training data and unlabeled test data. The RoBERTa- GCN model achieved an accuracy of 0.86 and an F1 score of 0.86, surpassing the performance of other baseline models in predicting LGBTQ+ minority stress. Improved prediction of minority stress expressions on social media could lead to digital health interventions to improve the wellbeing of LGBTQ+ peopleï¿½a community with high rates of stress-sensitive health problems. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Elankeerthana, R.
AU  - Abirami, T.
AU  - Jayadharshini, P.
AU  - Prasath, V.U.S.
AU  - Ragupathi, P.
AU  - Surandhar, G.
TI  - Detecting Duchene Muscular Dystrophy by Utilizing Text Mining and Machine Learning Techniques for Timely Diagnosis through NLP
PY  - 2024
T2  - TQCEBT 2024 - 2nd IEEE International Conference on Trends in Quantum Computing and Emerging Business Technologies 2024
DO  - 10.1109/TQCEBT59414.2024.10545223
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204459130&doi=10.1109%2fTQCEBT59414.2024.10545223&partnerID=40&md5=4257c771868603a46568b787089507a4
AB  - This study presents a pioneering approach like previously done in the early detection of Duchenne Muscular Dystrophy (DMD) by harnessing the capabilities of machine learning techniques, with a primary focus on Natural Language Processing (NLP). Current approach throws light on text mining and machine learning techniques. The research, conducted on a vast dataset of gene-related information from the NCBI Gene database, introduces a novel Genomic Transformer Modal. This Modal, equipped with self-attention processes, has demonstrated exceptional performance in predicting DMD with an impressive accuracy rate of 98 percent. The Genomic Transformer Modal's architecture, with its self-attention mechanism, is tailored to capture long-range correlations within complex genomic sequences. This unique feature has paved the way for a considerable improvement in predictive accuracy compared to XGBoost Classifier, LASSO Regression, Ridge Regression and ElasticNet Model. The model's capacity to efficiently encode gene sequences into continuous vectors has enabled the extraction of highly relevant genetic patterns, leading to substantially more accurate predictions of DMD-related features. What sets the Genomic Transformer Modal apart is its remarkable adaptability to high-dimensional feature spaces, a common challenge in gene-related NLP tasks. The attention-based mechanism within the model allows it to selectively focus on the most informative elements within gene sequences, effectively managing the inherent complexity of genetic data. By zeroing in on these critical segments, the Genomic Transformer Modal excels in identifying vital genetic variations and patterns linked to DMD, particularly within the Dystrophin Gene. Furthermore, the Modal showcases superior generalization capabilities, significantly reducing overfitting and the risks of model bias. In the realm of precision medicine, where precise predictions are paramount for early diagnosis and personalized treatment strategies, this achievement holds immense promise.  © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Achtari, M.
AU  - Salihu, A.
AU  - Muller, O.
AU  - Abbé, E.
AU  - Clair, C.
AU  - Schwarz, J.
AU  - Fournier, S.
TI  - Gender Bias in AI's Perception of Cardiovascular Risk
PY  - 2024
T2  - Journal of Medical Internet Research
VL  - 26
C7  - e54242
DO  - 10.2196/54242
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207222605&doi=10.2196%2f54242&partnerID=40&md5=1f2181bee634bb1138a7a7cb30019407
AB  - The study investigated gender bias in GPT-4's assessment of coronary artery disease risk by presenting identical clinical vignettes of men and women with and without psychiatric comorbidities. Results suggest that psychiatric conditions may influence GPT-4's coronary artery disease risk assessment among men and women. © 2024 JMIR Publications Inc.. All rights reserved.
PB  - JMIR Publications Inc.
C2  - 39437384
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Vani Lakshmi, R.
AU  - Clare, R.S.
AU  - Kamath, A.
TI  - Demystifying the Ethical Framework for Generative AI in Healthcare: A Data Science Perspective
PY  - 2024
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 14976 LNCS
SP  - 279
EP  - 289
DO  - 10.1007/978-3-031-67285-9_20
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202290411&doi=10.1007%2f978-3-031-67285-9_20&partnerID=40&md5=2c33f6e9050e0f6b48f446162998a4a6
AB  - Generative Artificial Intelligence, also known as Generative AI has enhanced the capabilities of what is known to be AI to the world by its capability to generate text, image, or other forms of media through the extensive use of Large Language Models (LLMs). While the roots of LLM can be traced back to the Markovian theories proposed in 1906, Generative AI is the outcome of advancements in transformer-based deep neural networks which extend the machine learning paradigm. Fundamentally, there is a definitive need to store, manage, and use data to generate data that matches consumer needs. Generative AI-based tools today free users from the need to master programming language. This has paved the way for easy access to data generation, analytics, and subsequent dissemination of findings as per the consumer’s needs, often with or without a subscription fee. The ethical frameworks of AI are built on the four key principles, namely, beneficence, non-maleficence, autonomy, and justice. In recent years, explicability, which incorporates intelligibility and accountability, was added as the fifth crucial principle in the framework. The use of AI by organizations have also led to reputational, regulatory, and legal risks, resulting in widespread discussions on Ethical AI or the ethical use of AI. In India, the Indian Council of Medical Research (ICMR, India) has released guidelines for the Ethical Use of AI in Healthcare. Protecting data privacy at the individual (or citizen) level has been one of the crucial challenges in the healthcare sector. With the advent of Generative AI, these challenges have also experienced a multiplicative effect. In addition, the post-COVID-19 era has led to increased use of digital health technologies, fueling data privacy and security risks apart from misinformation (leading to infodemics) and bias. Such concerns often affect developing countries, especially in the healthcare sector. The present research provides a state-of-the-art review of the ethical frameworks for Generative AI in healthcare. The study also provides an overview of privacy-preserving Generative AI paradigms, enabling the policy-makers (government, private, and other not-for-profit entities) to plan, propose, and disseminate policies that preserve the privacy of the data shared at an individual level. The study will benefit researchers by developing methodologies that align with the ethical framework for Generative AI, thus aligning with the principles of using AI for Good. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Evans, S.
AU  - Schmitt, J.
AU  - Kalra, D.
AU  - Sokol, T.
AU  - Holt, D.
TI  - Policy brief: Improving national vaccination decision-making through data
PY  - 2024
T2  - Frontiers in Public Health
VL  - 12
C7  - 1407841
DO  - 10.3389/fpubh.2024.1407841
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213684877&doi=10.3389%2ffpubh.2024.1407841&partnerID=40&md5=baa86eb409987abb37a8dd915b2834ba
AB  - Life course immunisation looks at the broad value of vaccination across multiple generations, calling for more data power, collaboration, and multi-disciplinary work. Rapid strides in artificial intelligence, such as machine learning and natural language processing, can enhance data analysis, conceptual modelling, and real-time surveillance. The GRADE process is a valuable tool in informing public health decisions. It must be enhanced by real-world data which can span and capture immediate needs in diverse populations and vaccination administration scenarios. Analysis of data from multiple study designs is required to understand the nuances of health behaviors and interventions, address gaps, and mitigate the risk of bias or confounding presented by any single data collection methodology. Secure and responsible health data sharing across European countries can contribute to a deeper understanding of vaccines. Copyright © 2024 Evans, Schmitt, Kalra, Sokol and Holt.
PB  - Frontiers Media SA
C2  - 39741942
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Eraybar, S.
AU  - Dal, E.
AU  - Aydin, M.O.
AU  - Begenen, M.
TI  - Transforming emergency triage: A preliminary, scenario-based cross-sectional study comparing artificial intelligence models and clinical expertise for enhanced accuracy
PY  - 2024
T2  - Bratislava Medical Journal
VL  - 125
IS  - 11
SP  - 738
EP  - 743
DO  - 10.4149/BLL_2024_114
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208178173&doi=10.4149%2fBLL_2024_114&partnerID=40&md5=53876e7956be9c421f39023ccb97b3e6
AB  - INTRODUCTION: This study examines triage judgments in emergency settings and compares the outcomes of artificial intelligence models for healthcare professionals. It discusses the disparities in precision rates between subjective evaluations by health professionals with objective assessments of AI systems. Material AND METHOD: For the analysis of the efficacy of emergency triage; 50 virtual patient scenarios had been created. Emergency medicine residents and other healthcare providers who had triage education were tasked with categorizing triage levels for virtual patient scenarios. Also artificial intelligence systems, tasked for resolving the same scenarios. All of them were asked to use three color-coded triage of the Republic of Turkey Ministry of Health. The answer keys were created by consensus of the researchers. In addition, Emergency medicine specialists were asked to evaluate the acuity level of each scenario in order to perform sub-analyses. Results: The study consisted of 86 healthcare professionals, comprising 31 Emergency medicine residents (26.5%), 1 paramedic (0.9%), 5 emergency health technicians (4.3%), and 80 nurses (68.4%). Google Bard AI and OpenAI Chat GPT v.3.5 were used as artificial intelligence systems. The responses compared with the answer key to determine each groups efficacy. As planned the responses from healthcare professionals were analyzed individually for acuity level of scenarios. Emergency medicine residents and other groups of healthcare providers had significantly higher numbers of correct answers compared to Google Bard and Chat GPT (n=30.7 vs n=25.5). There was no significant difference between ChatGPT and Bard for low and high acuity scenarios (p=0.821) Conclusion: AI models can examine extensive data sets and make more accurate and quicker triage judgments with sophisticated algorithms. However, in this study, we found that the triage ability of artificial intelligence is not as sufficient as humans. A more efficient triage system can be developed by integrating artificial intelligence with human input, rather than solely relying on technology (Tab. 4, Ref. 41). Text in PDF www.elis.sk © (2024), (Comenius University in Bratislava). All rights reserved.
PB  - Comenius University in Bratislava
C2  - 39487846
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Balli, M.
AU  - Doğan, A.E.
AU  - Eser, H.Y.
TI  - Improving Psychiatry Services with Artificial Intelligence: Opportunities and Challenges
ST  - Psikiyatri Hizmetlerinin Yapay Zekâ ile Geliştirilmesi: Fırsatlar ve Zorluklar
PY  - 2024
T2  - Turk Psikiyatri Dergisi
VL  - 35
IS  - 4
SP  - 317
EP  - 328
DO  - 10.5080/u27604
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213388399&doi=10.5080%2fu27604&partnerID=40&md5=b8e5ca9c85cad7313f7bcaa78d2add62
AB  - Mental disorders are a critical global public health problem due to their increasing prevalence, rising costs, and significant economic burden. Despite efforts to increase the mental health workforce in Türkiye, there is a significant shortage of psychiatrists, limiting the quality and accessibility of mental health services. This review examines the potential of artificial intelligence (AI), especially large language models, to transform psychiatric care in the world and in Türkiye. AI technologies, including machine learning and deep learning, offer innovative solutions for the diagnosis, personalization of treatment, and monitoring of mental disorders using a variety of data sources, such as speech patterns, neuroimaging, and behavioral measures. Although AI has shown promising capabilities in improving diagnostic accuracy and access to mental health services, challenges such as algorithmic biases, data privacy concerns, ethical implications, and the confabulation phenomenon of large language models prevent the full implementation of AI in practice. The review highlights the need for interdisciplinary collaboration to develop culturally and linguistically adapted AI tools, particularly in the Turkish context, and suggests strategies such as fine-tuning, retrieval-augmented generation, and reinforcement learning from human feedback to increase AI reliability. Advances suggest that AI can improve mental health care by increasing diagnostic accuracy and accessibility while preserving the essential human elements of medical care. Current limitations need to be addressed through rigorous research and ethical frameworks for effective and equitable integration of AI into mental health care. © (2024), (Turkish Association of Nervous and Mental Health). All rights reserved.
PB  - Turkish Association of Nervous and Mental Health
C2  - 39783807
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Garima, S.
AU  - Swapnil, M.
AU  - Shashank, S.
TI  - Harnessing the Power of Language Models for Intelligent Digital Health Services
PY  - 2024
T2  - 2024 ITU Kaleidoscope: Innovation and Digital Transformation for a Sustainable World, ITU K 2024
DO  - 10.23919/ITUK62727.2024.10772761
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215262625&doi=10.23919%2fITUK62727.2024.10772761&partnerID=40&md5=92f365b2047a8bb3adc04c2bbe7b72e5
AB  - This research proposes a novel framework that integrates state-of-the-art large language models (LLMs) with curated medical knowledge bases to enable personalized, reliable, and user-centric digital health services. The architecture combines advanced generative models, retrieval-augmented generation, and domain adaptation strategies to ensure the safety and ethical alignment of AI-driven health recommendations. Empirical evaluations, including automated benchmarks and user studies, demonstrate the framework's ability to provide accurate, relevant, and personalized health information that resonates with patients and providers. The results highlight the potential of this approach to bridge the gap between general-purpose LLMs and domain-specific healthcare applications. However, the work also underscores the challenges in responsibly developing and deploying generative AI for healthcare, such as safety, robustness, fairness, privacy, and interpretability. The research advocates for multidisciplinary collaboration to address these challenges and realize the potential of AI in enhancing health and well-being worldwide. By prioritizing patient agency, clinical validity, and ethical practices, this work contributes to the growing body of knowledge at the intersection of AI and healthcare, laying the foundation for future research and innovation in personalized, equitable, and trustworthy AI health services.  © 2024 ITU.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Schmidt, H.G.
AU  - Rotgans, J.I.
AU  - Mamede, S.
TI  - Bias Sensitivity in Diagnostic Decision-Making: Comparing ChatGPT with Residents
PY  - 2024
T2  - Journal of General Internal Medicine
DO  - 10.1007/s11606-024-09177-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208810618&doi=10.1007%2fs11606-024-09177-9&partnerID=40&md5=3cb2b89e1efdc6c51e57439095c1e9c7
AB  - Background: Diagnostic errors, often due to biases in clinical reasoning, significantly affect patient care. While artificial intelligence chatbots like ChatGPT could help mitigate such biases, their potential susceptibility to biases is unknown. Methods: This study evaluated diagnostic accuracy of ChatGPT against the performance of 265 medical residents in five previously published experiments aimed at inducing bias. The residents worked in several major teaching hospitals in the Netherlands. The biases studied were case-intrinsic (presence of salient distracting findings in the patient history, effects of disruptive patient behaviors) and situational (prior availability of a look-alike patient). ChatGPT’s accuracy in identifying the most-likely diagnosis was measured. Results: Diagnostic accuracy of residents and ChatGPT was equivalent. For clinical cases involving case-intrinsic bias, both ChatGPT and the residents exhibited a decline in diagnostic accuracy. Residents’ accuracy decreased on average 12%, while the accuracy of ChatGPT 4.0 decreased 21%. Accuracy of ChatGPT 3.5 decreased 9%. These findings suggest that, like human diagnosticians, ChatGPT is sensitive to bias when the biasing information is part of the patient history. When the biasing information was extrinsic to the case in the form of the prior availability of a look-alike case, residents’ accuracy decreased by 15%. By contrast, ChatGPT’s performance was not affected by the biasing information. Chi-square goodness-of-fit tests corroborated these outcomes. Conclusions: It seems that, while ChatGPT is not sensitive to bias when biasing information is situational, it is sensitive to bias when the biasing information is part of the patient’s disease history. Its utility in diagnostic support has potential, but caution is advised. Future research should enhance AI’s bias detection and mitigation to make it truly useful for diagnostic support. © The Author(s) 2024.
PB  - Springer
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Turi, L.F.
AU  - Cavalini, A.
AU  - Comarela, G.
AU  - Oliveira-Santos, T.
AU  - Badue, C.
AU  - De Souza, A.F.
TI  - Analysis of Bias in GPT Language Models through Fine-tuning Containing Divergent Data
PY  - 2024
T2  - Proceedings of the International Joint Conference on Neural Networks
DO  - 10.1109/IJCNN60899.2024.10650574
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205013610&doi=10.1109%2fIJCNN60899.2024.10650574&partnerID=40&md5=56169cf54bdc484c775bd33387f44af9
AB  - In this study, we examined the effects of integrating data that contains divergent information, especially concerning anti-vaccination narratives, into the training of a GPT-2 language model. The model was fine-tuned using content sourced from anti-vaccination groups and channels on Telegram, aiming to analyze its ability to generate coherent and rationalized texts in comparison to a model pre-trained on OpenAI's WebText dataset. The results demonstrate that fine-tuning a GPT-2 model with biased data leads the model to perpetuate these biases in its responses, albeit with a certain degree of rationalization. This finding underscores the importance of using high-quality and reliable data in training natural language processing models, highlighting the implications for information dissemination through these models. It also provides social scientists with a tool to explore and understand the complexities and challenges associated with public health misinformation via the use of language models, particularly in the context of vaccines. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ofosu-Asare, Y.
TI  - Cognitive imperialism in artificial intelligence: counteracting bias with indigenous epistemologies
PY  - 2024
T2  - AI and Society
DO  - 10.1007/s00146-024-02065-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204381459&doi=10.1007%2fs00146-024-02065-0&partnerID=40&md5=afc4b818355d1f37f912ef9d0bb85e25
AB  - This paper presents a novel methodology for integrating indigenous knowledge systems into AI development to counter cognitive imperialism and foster inclusivity. By critiquing the dominance of Western epistemologies and highlighting the risks of bias, the authors argue for incorporating diverse epistemologies. The proposed framework outlines a participatory approach that includes indigenous perspectives, ensuring AI benefits all. The methodology draws from AI ethics, indigenous studies, and postcolonial theory, emphasizing co-creation with indigenous communities, ethical protocols for indigenous data governance, and adaptation of AI algorithms. Case studies in natural language processing, content moderation, and healthcare demonstrate the methodology’s effectiveness and importance. By offering a concrete methodology for decolonizing AI, this paper contributes significantly to AI ethics and social justice, providing a roadmap for equitable, culturally respectful AI. © The Author(s) 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ngaruiya, C.
AU  - Samad, Z.
AU  - Tajuddin, S.
AU  - Nasim, Z.
AU  - Leff, R.
AU  - Farhad, A.
AU  - Pires, K.
AU  - Khan, M.A.
AU  - Hartz, L.
AU  - Safdar, B.
TI  - Identification of Gender Differences in Acute Myocardial Infarction Presentation and Management at Aga Khan University Hospital-Pakistan: Natural Language Processing Application in a Dataset of Patients With Cardiovascular Disease
PY  - 2024
T2  - JMIR Formative Research
VL  - 8
C7  - e42774
DO  - 10.2196/42774
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212942575&doi=10.2196%2f42774&partnerID=40&md5=465f09402fb98ec4e4ec1a5e0be10c17
AB  - Background: Ischemic heart disease is a leading cause of death globally with a disproportionate burden in low- and middle-income countries (LMICs). Natural language processing (NLP) allows for data enrichment in large datasets to facilitate key clinical research. We used NLP to assess gender differences in symptoms and management of patients hospitalized with acute myocardial infarction (AMI) at Aga Khan University Hospital-Pakistan. Objective: The primary objective of this study was to use NLP to assess gender differences in the symptoms and management of patients hospitalized with AMI at a tertiary care hospital in Pakistan. Methods: We developed an NLP-based methodology to extract AMI symptoms and medications from 5358 discharge summaries spanning the years 1988 to 2018. This dataset included patients admitted and discharged between January 1, 1988, and December 31, 2018, who were older than 18 years with a primary discharge diagnosis of AMI (using ICD-9 [International Classification of Diseases, Ninth Revision], diagnostic codes). The methodology used a fuzzy keyword-matching algorithm to extract AMI symptoms from the discharge summaries automatically. It first preprocesses the free text within the discharge summaries to extract passages indicating the presenting symptoms. Then, it applies fuzzy matching techniques to identify relevant keywords or phrases indicative of AMI symptoms, incorporating negation handling to minimize false positives. After manually reviewing the quality of extracted symptoms in a subset of discharge summaries through preliminary experiments, a similarity threshold of 80% was determined. Results: Among 1769 women and 3589 men with AMI, women had higher odds of presenting with shortness of breath (odds ratio [OR] 1.46, 95% CI 1.26-1.70) and lower odds of presenting with chest pain (OR 0.65, 95% CI 0.55-0.75), even after adjustment for diabetes and age. Presentation with abdominal pain, nausea, or vomiting was much less frequent but consistently more common in women (P<.001). “Ghabrahat,” a culturally distinct term for a feeling of impending doom was used by 5.09% of women and 3.69% of men as presenting symptom for AMI (P=.06). First-line medication prescription (statin and β-blockers) was lower in women: women had nearly 30% lower odds (OR 0.71, 95% CI 0.57-0.90) of being prescribed statins, and they had 40% lower odds (OR 0.67, 95% CI 0.57-0.78) of being prescribed β-blockers. Conclusions: Gender-based differences in clinical presentation and medication management were demonstrated in patients with AMI at a tertiary care hospital in Pakistan. The use of NLP for the identification of culturally nuanced clinical characteristics and management is feasible in LMICs and could be used as a tool to understand gender disparities and address key clinical priorities in LMICs. ©Christine Ngaruiya, Zainab Samad, Salma Tajuddin, Zarmeen Nasim, Rebecca Leff, Awais Farhad, Kyle Pires, Muhammad Alamgir Khan, Lauren Hartz, Basmah Safdar.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Dimitsaki, S.
AU  - Natsiavas, P.
AU  - Jaulent, M.-C.
TI  - Applying AI to Structured Real-World Data for Pharmacovigilance Purposes: Scoping Review
PY  - 2024
T2  - Journal of Medical Internet Research
VL  - 26
C7  - e57824
DO  - 10.2196/57824
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213833421&doi=10.2196%2f57824&partnerID=40&md5=5bc91f0d7b7ac246938f78765ef70ae5
AB  - Background: Artificial intelligence (AI) applied to real-world data (RWD; eg, electronic health care records) has been identified as a potentially promising technical paradigm for the pharmacovigilance field. There are several instances of AI approaches applied to RWD; however, most studies focus on unstructured RWD (conducting natural language processing on various data sources, eg, clinical notes, social media, and blogs). Hence, it is essential to investigate how AI is currently applied to structured RWD in pharmacovigilance and how new approaches could enrich the existing methodology. Objective: This scoping review depicts the emerging use of AI on structured RWD for pharmacovigilance purposes to identify relevant trends and potential research gaps. Methods: The scoping review methodology is based on the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) methodology. We queried the MEDLINE database through the PubMed search engine. Relevant scientific manuscripts published from January 2010 to January 2024 were retrieved. The included studies were “mapped” against a set of evaluation criteria, including applied AI approaches, code availability, description of the data preprocessing pipeline, clinical validation of AI models, and implementation of trustworthy AI criteria following the guidelines of the FUTURE (Fairness, Universality, Traceability, Usability, Robustness, and Explainability)-AI initiative. Results: The scoping review ultimately yielded 36 studies. There has been a significant increase in relevant studies after 2019. Most of the articles focused on adverse drug reaction detection procedures (23/36, 64%) for specific adverse effects. Furthermore, a substantial number of studies (34/36, 94%) used nonsymbolic AI approaches, emphasizing classification tasks. Random forest was the most popular machine learning approach identified in this review (17/36, 47%). The most common RWD sources used were electronic health care records (28/36, 78%). Typically, these data were not available in a widely acknowledged data model to facilitate interoperability, and they came from proprietary databases, limiting their availability for reproducing results. On the basis of the evaluation criteria classification, 10% (4/36) of the studies published their code in public registries, 16% (6/36) tested their AI models in clinical environments, and 36% (13/36) provided information about the data preprocessing pipeline. In addition, in terms of trustworthy AI, 89% (32/36) of the studies followed at least half of the trustworthy AI initiative guidelines. Finally, selection and confounding biases were the most common biases in the included studies. Conclusions: AI, along with structured RWD, constitutes a promising line of work for drug safety and pharmacovigilance. However, in terms of AI, some approaches have not been examined extensively in this field (such as explainable AI and causal AI). Moreover, it would be helpful to have a data preprocessing protocol for RWD to support pharmacovigilance processes. Finally, because of personal data sensitivity, evaluation procedures have to be investigated further. ©Stella Dimitsaki, Pantelis Natsiavas, Marie-Christine Jaulent.
PB  - JMIR Publications Inc.
C2  - 39753222
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Stein, K.
AU  - Harvey, A.
AU  - Lopez, A.
AU  - Taj, U.
AU  - Watkins, S.
AU  - Watkins, L.
TI  - Eliciting and Measuring Toxic Bias in Human-to-Machine Interactions in Large Language Models
PY  - 2024
T2  - 2024 IEEE 15th Annual Ubiquitous Computing, Electronics and Mobile Communication Conference, UEMCON 2024
SP  - 13
EP  - 19
DO  - 10.1109/UEMCON62879.2024.10754689
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212701859&doi=10.1109%2fUEMCON62879.2024.10754689&partnerID=40&md5=c4fea4c88e6b8fc972dc879be7fb0535
AB  - As Large Language Models (LLMs) continue to advance and be utilized across societal domains, such as finance, healthcare, and the justice system, the need to address the inherent bias and the ability to learn new biases in these models becomes imminent. Concerns regarding these biases are rising, given their potential to perpetuate and even amplify existing social inequalities. This paper explores the multifaceted nature of bias in artificial intelligence (AI), examining the similarities and differences between human and machine bias. We delve into the origins of bias, distinguishing between those introduced by users and those inherent in the AI systems themselves. Our study focuses on the mechanisms by which biases are elicited and amplified through human-to-machine interactions. Through experimentation and analysis, we implement methodologies for eliciting, measuring, and mitigating these biases. Our results suggest that even though LLMs like ChatGPT-4 are equipped with effective content moderators, these chatbots can still learn and exhibit biased responses through human coercion. Further, we have learned that these biases are both inherent and learned through human interaction. Finally, we offer insightful strategies to mitigate these biases in LLMs. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Kumar, G.
AU  - Singh, J.P.
TI  - Bias mitigation in text classification through cGAN and LLMs
PY  - 2024
T2  - Proceedings of the Indian National Science Academy
C7  - 102387
DO  - 10.1007/s43538-024-00371-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211250218&doi=10.1007%2fs43538-024-00371-1&partnerID=40&md5=6998ecf51157ae96555c7ce5846093ea
AB  - Investigating bias, fairness, and ethics in AI/ML models is essential in the digital era, as automated systems are increasingly used to enhance people’s daily lives. Evaluating these systems in terms of fairness has become crucial. Our proposed work focuses on identifying and mitigating gender bias in text classification models. Specifically, we are evaluating a hate speech classification (HSC) model using fairness metrics and mitigating bias through pre- and in-processing debiasing techniques. In the pre-debiasing stage, we address bias by applying gender swap techniques and removing gender-related terms from the dataset. For in-processing debiasing, we use conditional generative adversarial networks (cGAN) and large language models (LLMs) to generate text similar to that of minority classes, helping balance the dataset and reduce bias during model training. We assess the impact of both methods on bias reduction in the HSC model, with each targeting bias at different stages of model development. While the classification performance of the HSC model showed a slight decrease, fairness evaluation metrics improved by 3.03% and 3.42%, demonstrating the methods’ effectiveness. Addressing gender bias in HSC models applied to social media posts is vital for fostering a healthier digital society. © Indian National Science Academy 2024.
PB  - Springer Nature
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Wong, J.
AU  - Kriegler, C.
AU  - Shrivastava, A.
AU  - Duimering, A.
AU  - Le, C.
TI  - Utility of Chatbot Literature Search in Radiation Oncology
PY  - 2024
T2  - Journal of Cancer Education
DO  - 10.1007/s13187-024-02547-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211933254&doi=10.1007%2fs13187-024-02547-1&partnerID=40&md5=cb0dc0562d92e7db9c84bcb700597f1d
AB  - Artificial intelligence and natural language processing tools have shown promise in oncology by assisting with medical literature retrieval and providing patient support. The potential for these technologies to generate inaccurate yet seemingly correct information poses significant challenges. This study evaluates the effectiveness, benefits, and limitations of ChatGPT for clinical use in conducting literature reviews of radiation oncology treatments. This cross-sectional study used ChatGPT version 3.5 to generate literature searches on radiotherapy options for seven tumor sites, with prompts issued five times per site to generate up to 50 publications per tumor type. The publications were verified using the Scopus database and categorized as correct, irrelevant, or non-existent. Statistical analysis with one-way ANOVA compared the impact factors and citation counts across different tumor sites. Among the 350 publications generated, there were 44 correct, 298 non-existent, and 8 irrelevant papers. The average publication year of all generated papers was 2011, compared to 2009 for the correct papers. The average impact factor of all generated papers was 38.8, compared to 113.8 for the correct papers. There were significant differences in the publication year, impact factor, and citation counts between tumor sites for both correct and non-existent papers. Our study highlights both the potential utility and significant limitations of using AI, specifically ChatGPT 3.5, in radiation oncology literature reviews. The findings emphasize the need for verification of AI outputs, development of standardized quality assurance protocols, and continued research into AI biases to ensure reliable integration into clinical practice. © The Author(s) under exclusive licence to American Association for Cancer Education 2024.
PB  - Springer
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Guellil, I.
AU  - Wu, J.
AU  - Pradipta Gema, A.
AU  - Francis, F.
AU  - Berrachedi, Y.
AU  - Chenni, N.
AU  - Tobin, R.
AU  - Llewellyn, C.
AU  - Arakelyan, S.
AU  - Wu, H.
AU  - Guthrie, B.
AU  - Alex, B.
TI  - Natural language processing for detecting adverse drug events: A systematic review protocol
PY  - 2024
T2  - NIHR Open Research
VL  - 3
C7  - 67
DO  - 10.3310/nihropenres.13504.3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205455848&doi=10.3310%2fnihropenres.13504.3&partnerID=40&md5=6d6099a2b9a406ae04177052f60ae6af
AB  - Background: Detecting Adverse Drug Events (ADEs) is an emerging research area, attracting great interest in the research community. Better anticipatory management of predisposing factors has considerable potential to improve outcomes. Automatic extraction of ADEs using Natural Language Processing (NLP) has a great potential to significantly facilitate efficient and effective distillation of such knowledge, to better understand and predict risk of adverse events. Methods: This systematic review follows the six-stage including the literature from 6 databases (Embase, Medline, Web Of Science Core Collection, ACM Guide to Computing Literature, IEEE Digital Library and Scopus). Following the title, abstract and full-text screenings, characteristics and main findings of the included studies and resources will be tabulated and summarized. The risk of bias and reporting quality was assessed using the PROBAST tool. Results: We developed our search strategy and collected all relevant publications. As of December 2024, we have completed all the stages of the systematic review. We identified 178 studies for inclusion through the academic literature search (where data was extracted from all of the papers). Right now, we are writing up the systematic review paper where we are synthesising the different findings. Further refinement of the eligibility criteria and data extraction has been ongoing since August 2022. Conclusion: In this systematic review, we will identify and consolidate information and evidence related to the use and effectiveness of existing NLP approaches and tools for automatically detecting ADEs from free text (discharge summaries, General Practitioner notes, social media, etc.). Our findings will improve the understanding of the current landscape of the use of NLP for extracting ADEs. It will lead to better anticipatory management of predisposing factors with the potential to improve outcomes considerably. Our results will also be valuable both to NLP researchers developing methods to extract ADEs and to translational/clinical researchers who use NLP for this purpose and in healthcare in general. For example, from our initial analysis of the studies, we can conclude that the majority of the proposed works are about the detection (extraction) of ADEs from text. An important portion of studies also focus on the binary classification of text (for highlighting if it includes or not ADEs). Different challenges related to the unbalanced dataset, abbreviations and acronyms but also to the lower results with rare ADEs were also mentioned by the studied papers. Copyright: © 2024 Guellil I et al.
PB  - F1000 Research Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Gisselbaek, M.
AU  - Minsart, L.
AU  - Köselerli, E.
AU  - Suppan, M.
AU  - Meco, B.C.
AU  - Seidel, L.
AU  - Albert, A.
AU  - Barreto Chang, O.L.
AU  - Saxena, S.
AU  - Berger-Estilita, J.
TI  - Beyond the stereotypes: Artificial Intelligence image generation and diversity in anesthesiology
PY  - 2024
T2  - Frontiers in Artificial Intelligence
VL  - 7
C7  - 1462819
DO  - 10.3389/frai.2024.1462819
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207237996&doi=10.3389%2ffrai.2024.1462819&partnerID=40&md5=822d65dfcf13ab4d3b9199a867168a74
AB  - Introduction: Artificial Intelligence (AI) is increasingly being integrated into anesthesiology to enhance patient safety, improve efficiency, and streamline various aspects of practice. Objective: This study aims to evaluate whether AI-generated images accurately depict the demographic racial and ethnic diversity observed in the Anesthesia workforce and to identify inherent social biases in these images. Methods: This cross-sectional analysis was conducted from January to February 2024. Demographic data were collected from the American Society of Anesthesiologists (ASA) and the European Society of Anesthesiology and Intensive Care (ESAIC). Two AI text-to-image models, ChatGPT DALL-E 2 and Midjourney, generated images of anesthesiologists across various subspecialties. Three independent reviewers assessed and categorized each image based on sex, race/ethnicity, age, and emotional traits. Results: A total of 1,200 images were analyzed. We found significant discrepancies between AI-generated images and actual demographic data. The models predominantly portrayed anesthesiologists as White, with ChatGPT DALL-E2 at 64.2% and Midjourney at 83.0%. Moreover, male gender was highly associated with White ethnicity by ChatGPT DALL-E2 (79.1%) and with non-White ethnicity by Midjourney (87%). Age distribution also varied significantly, with younger anesthesiologists underrepresented. The analysis also revealed predominant traits such as “masculine, ““attractive, “and “trustworthy” across various subspecialties. Conclusion: AI models exhibited notable biases in gender, race/ethnicity, and age representation, failing to reflect the actual diversity within the anesthesiologist workforce. These biases highlight the need for more diverse training datasets and strategies to mitigate bias in AI-generated images to ensure accurate and inclusive representations in the medical field. Copyright © 2024 Gisselbaek, Minsart, Köselerli, Suppan, Meco, Seidel, Albert, Barreto Chang, Saxena and Berger-Estilita.
PB  - Frontiers Media SA
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Restrepo, D.
AU  - Nakayama, L.F.
AU  - Dychiao, R.G.
AU  - Wu, C.
AU  - Mccoy, L.G.
AU  - Artiaga, J.C.
AU  - Cobanaj, M.
AU  - Matos, J.
AU  - Gallifant, J.
AU  - Bitterman, D.S.
AU  - Ferrer, V.
AU  - Aphinyanaphongs, Y.
AU  - Anthony Celi, L.
TI  - Seeing Beyond Borders: Evaluating LLMs in Multilingual Ophthalmological Question Answering
PY  - 2024
T2  - Proceedings - 2024 IEEE 12th International Conference on Healthcare Informatics, ICHI 2024
SP  - 565
EP  - 566
DO  - 10.1109/ICHI61247.2024.00089
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203680920&doi=10.1109%2fICHI61247.2024.00089&partnerID=40&md5=dc062765594cb34d5c1653279e86ffe0
AB  - Large Language Models (LLMs), such as GPT-3.5 [1] and GPT-4 [2], have significant potential for transforming several aspects of patient care from clinical note summarization to performing board-level clinical question-answering tasks [3], [4]. Ophthalmology, is a field with high patient volume and therefore holds high documentation burden for physicians but great opportunities for leveraging LLMs. Furthermore, given the critical and permanent nature of negative disease outcomes like blindness and their ensuing social and financial damage to patients, the need for reliable, accessible, and robust tools is urgent. Several studies have already showcased the practicality of GPT applications in ophthalmology [5], [6], and in specific ophthalmology subspecialties, such as glaucoma and retina [7], [8] However, the rapid integration of LLMs into healthcare systems comes with significant ethical, cultural, and technical challenges [9], [10]. LLMs are predominantly trained on data from high-income, English-speaking contexts; therefore, they risk exacerbating health inequities between high-income countries (HICs) and low- and middle-income countries (LMICs) through poor generalizability. Previous work has demonstrated disparities in LLM performance between English and other languages in general healthcare contexts [11], however the specialty-specific considerations remain underexplored. In addition to limitations in model translation of linguistic content, concerns arise regarding model ability to engage with medical questions laden with cultural context, and specifics of regional medical practice. This research critically evaluates the performance and biases of two widely used LLMs, GPT-3.5 and GPT-4, when answering ophthalmological questions across different languages. By doing so, we uncover these LLMs' technological capabilities and limitations in a healthcare context and the broader implications of their deployment in LMICs. In addition, we explore how LLMs might increase the health equity gaps and bias among underrepresented populations. First, a novel dataset of multilingual ophthalmological questions was created, composed of Spanish, English, Portuguese, and Filipino languages. These were selected to comprehensively evaluate the LLMs across diverse languages representing populations with diverse demographic and cultural backgrounds. A careful collection of 164 questions, based on the 2022 Brazilian Ophthalmological Board Exam, was formatted as multiple-choice queries with 4 options [12]. The exam consisted of two theoretical tests, basic sciences and clinical-surgical ophthalmology, with subgroups on anatomy, pharmacology, clinical optics, strabismus, cataract, uveitis, oncology, refractive surgery, contact lens, and genetics. Each question-answer pair was reviewed and curated by native speakers and board-certified ophthalmologists to ensure a robust assessment of the models' medical knowledge, language understanding, and cultural appropriateness. Each question was assessed for consistency of responses across languages, for linguistics subtleties and similarities that would hinder the LLM assessment. Each model's performance across the 4 languages (Portuguese, Spanish, English, and Filipino) was calculated with a temperature setting of 0 to ensure the model always provides the most likely response. We formulated standardized and clear prompt templates for question-answering, aiming to eliminate any confounding variables related to differences in prompts or data leakage. Our methodology also involves a detailed review of the LLMs' responses for cultural relevance and clinical accuracy, performed by a panel of multilingual ophthalmologists and language experts to ensure the integrity and applicability of our findings. The evaluation highlighted significant disparities in the LLMs' performance across languages, with the most notable underperformance in Filipino, with 51.8% accuracy using GPT-4 and 34.8% using GPT-3.5. Filipino had 9.8% less accuracy in GPT4 compared with the following language, Portuguese (61.6%); and 3.6% less than Spanish in GPT-3.5 Turbo with 38.4% of accuracy. This performance gap worsens when we break down the question types: as Table II indicates, GPT-3.5 has around 15% performance gap between English and Filipino in questions requiring clinical expertise. While the concrete composition of these models pretraining datasets is unknown, it is widely acknowledged this is largely composed of more common languages such as English in online corpora [13]. This finding pinpoints a concerning bias in these models, which tend to favor languages and dialects with extensive representation in their training datasets. The implications of such biases are profound, suggesting that without corrective measures, the deployment of LLMs in healthcare could amplify existing disparities rather than ameliorate them [9]. The analysis underscores the critical need for a more inclusive and equitable approach to developing AI technologies for healthcare. This study's findings serve as a crucial reminder of the ethical and practical challenges facing the deployment of AI in healthcare, particularly in linguistically and culturally diverse settings such as LMICs. While LLMs hold the promise of revolutionizing medical diagnostics and patient care, their current biases pose significant risks of exacerbating health disparities, particularly in LMICs. Future work will expand these results to different models across different model sizes and training datasets. Furthermore, the number of languages in this test will be expanded vastly to formulate this into a more comprehensive benchmark for LLM evaluation in Ophthalmology. To realize the full potential of AI in improving healthcare outcomes globally, it is imperative that future developments in LLMs prioritize diversity and equity, incorporating a broader array of languages and cultural contexts into their training datasets. Moreover, establishing international standards, guidelines, benchmarks, and metrics for the equitable development and deployment of AI technologies in healthcare emerges as a key recommendation from our study. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ke, Y.
AU  - Yang, R.
AU  - Lie, S.A.
AU  - Lim, T.X.Y.
AU  - Ning, Y.
AU  - Li, I.
AU  - Abdullah, H.R.
AU  - Ting, D.S.W.
AU  - Liu, N.
TI  - Mitigating Cognitive Biases in Clinical Decision-Making Through Multi-Agent Conversations Using Large Language Models: Simulation Study
PY  - 2024
T2  - Journal of Medical Internet Research
VL  - 26
C7  - e59439
DO  - 10.2196/59439
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210081490&doi=10.2196%2f59439&partnerID=40&md5=d5aa27840fdffb1cf3bb4e9d37c2c12b
AB  - Background: Cognitive biases in clinical decision-making significantly contribute to errors in diagnosis and suboptimal patient outcomes. Addressing these biases presents a formidable challenge in the medical field. Objective: This study aimed to explore the role of large language models (LLMs) in mitigating these biases through the use of the multi-agent framework. We simulate the clinical decision-making processes through multi-agent conversation and evaluate its efficacy in improving diagnostic accuracy compared with humans. Methods: A total of 16 published and unpublished case reports where cognitive biases have resulted in misdiagnoses were identified from the literature. In the multi-agent framework, we leveraged GPT-4 (OpenAI) to facilitate interactions among different simulated agents to replicate clinical team dynamics. Each agent was assigned a distinct role: (1) making the final diagnosis after considering the discussions, (2) acting as a devil’s advocate to correct confirmation and anchoring biases, (3) serving as a field expert in the required medical subspecialty, (4) facilitating discussions to mitigate premature closure bias, and (5) recording and summarizing findings. We tested varying combinations of these agents within the framework to determine which configuration yielded the highest rate of correct final diagnoses. Each scenario was repeated 5 times for consistency. The accuracy of the initial diagnoses and the final differential diagnoses were evaluated, and comparisons with human-generated answers were made using the Fisher exact test. Results: A total of 240 responses were evaluated (3 different multi-agent frameworks). The initial diagnosis had an accuracy of 0% (0/80). However, following multi-agent discussions, the accuracy for the top 2 differential diagnoses increased to 76% (61/80) for the best-performing multi-agent framework (Framework 4-C). This was significantly higher compared with the accuracy achieved by human evaluators (odds ratio 3.49; P=.002). Conclusions: The multi-agent framework demonstrated an ability to re-evaluate and correct misconceptions, even in scenarios with misleading initial investigations. In addition, the LLM-driven, multi-agent conversation framework shows promise in enhancing diagnostic accuracy in diagnostically challenging medical scenarios. © Yuhe Ke, Rui Yang, Sui An Lie, Taylor Xin Yi Lim, Yilin Ning, Irene Li, Hairil Rizal Abdullah, Daniel Shu Wei Ting, Nan Liu.
PB  - JMIR Publications Inc.
C2  - 39561363
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Chow, J.C.L.
AU  - Li, K.
TI  - Ethical Considerations in Human-Centered AI: Advancing Oncology Chatbots Through Large Language Models
PY  - 2024
T2  - JMIR Bioinformatics and Biotechnology
VL  - 5
IS  - 1
C7  - e64406
DO  - 10.2196/64406
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209773789&doi=10.2196%2f64406&partnerID=40&md5=c36eac5f1c37dbfdc690246c1d549d1b
AB  - The integration of chatbots in oncology underscores the pressing need for human-centered artificial intelligence (AI) that addresses patient and family concerns with empathy and precision. Human-centered AI emphasizes ethical principles, empathy, and user-centric approaches, ensuring technology aligns with human values and needs. This review critically examines the ethical implications of using large language models (LLMs) like GPT-3 and GPT-4 (OpenAI) in oncology chatbots. It examines how these models replicate human-like language patterns, impacting the design of ethical AI systems. The paper identifies key strategies for ethically developing oncology chatbots, focusing on potential biases arising from extensive datasets and neural networks. Specific datasets, such as those sourced from predominantly Western medical literature and patient interactions, may introduce biases by overrepresenting certain demographic groups. Moreover, the training methodologies of LLMs, including fine-tuning processes, can exacerbate these biases, leading to outputs that may disproportionately favor affluent or Western populations while neglecting marginalized communities. By providing examples of biased outputs in oncology chatbots, the review highlights the ethical challenges LLMs present and the need for mitigation strategies. The study emphasizes integrating human-centric values into AI to mitigate these biases, ultimately advocating for the development of oncology chatbots that are aligned with ethical principles and capable of serving diverse patient populations equitably. © James C L Chow, Kay Li.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Haupt, M.R.
AU  - Yang, L.
AU  - Purnat, T.
AU  - Mackey, T.
TI  - Evaluating the Influence of Role-Playing Prompts on ChatGPT's Misinformation Detection Accuracy: Quantitative Study
PY  - 2024
T2  - JMIR Infodemiology
VL  - 4
C7  - e60678
DO  - 10.2196/60678
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205596607&doi=10.2196%2f60678&partnerID=40&md5=8ea940744c59014c6eb297550717f63c
AB  - Background: During the COVID-19 pandemic, the rapid spread of misinformation on social media created significant public health challenges. Large language models (LLMs), pretrained on extensive textual data, have shown potential in detecting misinformation, but their performance can be influenced by factors such as prompt engineering (ie, modifying LLM requests to assess changes in output). One form of prompt engineering is role-playing, where, upon request, OpenAI's ChatGPT imitates specific social roles or identities. This research examines how ChatGPT's accuracy in detecting COVID-19-related misinformation is affected when it is assigned social identities in the request prompt. Understanding how LLMs respond to different identity cues can inform messaging campaigns, ensuring effective use in public health communications. Objective: This study investigates the impact of role-playing prompts on ChatGPT's accuracy in detecting misinformation. This study also assesses differences in performance when misinformation is explicitly stated versus implied, based on contextual knowledge, and examines the reasoning given by ChatGPT for classification decisions. Methods: Overall, 36 real-world tweets about COVID-19 collected in September 2021 were categorized into misinformation, sentiment (opinions aligned vs unaligned with public health guidelines), corrections, and neutral reporting. ChatGPT was tested with prompts incorporating different combinations of multiple social identities (ie, political beliefs, education levels, locality, religiosity, and personality traits), resulting in 51,840 runs. Two control conditions were used to compare results: prompts with no identities and those including only political identity. Results: The findings reveal that including social identities in prompts reduces average detection accuracy, with a notable drop from 68.1% (SD 41.2%; no identities) to 29.3% (SD 31.6%; all identities included). Prompts with only political identity resulted in the lowest accuracy (19.2%, SD 29.2%). ChatGPT was also able to distinguish between sentiments expressing opinions not aligned with public health guidelines from misinformation making declarative statements. There were no consistent differences in performance between explicit and implicit misinformation requiring contextual knowledge. While the findings show that the inclusion of identities decreased detection accuracy, it remains uncertain whether ChatGPT adopts views aligned with social identities: when assigned a conservative identity, ChatGPT identified misinformation with nearly the same accuracy as it did when assigned a liberal identity. While political identity was mentioned most frequently in ChatGPT's explanations for its classification decisions, the rationales for classifications were inconsistent across study conditions, and contradictory explanations were provided in some instances. Conclusions: These results indicate that ChatGPT's ability to classify misinformation is negatively impacted when role-playing social identities, highlighting the complexity of integrating human biases and perspectives in LLMs. This points to the need for human oversight in the use of LLMs for misinformation detection. Further research is needed to understand how LLMs weigh social identities in prompt-based tasks and explore their application in different cultural contexts. © 2024 JMIR Publications Inc.. All rights reserved.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Xi, N.M.
AU  - Ji, H.-L.
AU  - Wang, L.
TI  - Understanding Sarcoidosis Using Large Language Models and Social Media Data
PY  - 2024
T2  - Journal of Healthcare Informatics Research
DO  - 10.1007/s41666-024-00177-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208973347&doi=10.1007%2fs41666-024-00177-2&partnerID=40&md5=edd839faad90560a1c94716bb48c08ad
AB  - Sarcoidosis is a rare inflammatory disease characterized by the formation of granulomas in various organs. The disease presents diagnostic and treatment challenges due to its diverse manifestations and unpredictable nature. In this study, we employed a Large Language Model (LLM) to analyze sarcoidosis-related discussions on the social media platform Reddit. Our findings underscore the efficacy of LLMs in accurately identifying sarcoidosis-related content. We discovered a wide array of symptoms reported by patients, with fatigue, swollen lymph nodes, and shortness of breath as the most prevalent. Prednisone was the most prescribed medication, while infliximab showed the highest effectiveness in improving prognoses. Notably, our analysis revealed disparities in prognosis based on age and gender, with women and younger patients experiencing good and polarized outcomes, respectively. Furthermore, unsupervised clustering identified three distinct patient subgroups (phenotypes) with unique symptom profiles, prognostic outcomes, and demographic distributions. Finally, sentiment analysis revealed a moderate negative impact on patients' mental health post-diagnosis, particularly among women and younger individuals. Our study represents the first application of LLMs to understand sarcoidosis through social media data. It contributes to understanding the disease by providing data-driven insights into its manifestations, treatments, prognoses, and impact on patients' lives. Our findings have direct implications for improving personalized treatment strategies and enhancing the quality of care for individuals living with sarcoidosis. © The Author(s), under exclusive licence to Springer Nature Switzerland AG 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Murugesan, G.S.
AU  - Viswanathan, S.A.
TI  - Enhancing Human-Machine Interaction: A Study on Deployment of LLM and Gen AI Hybrid Models in Responsible Humanoids for Human Assistance
PY  - 2024
T2  - 2024 15th International Conference on Computing Communication and Networking Technologies, ICCCNT 2024
DO  - 10.1109/ICCCNT61001.2024.10724819
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211180651&doi=10.1109%2fICCCNT61001.2024.10724819&partnerID=40&md5=83351a94f0624b7178144696c500539a
AB  - This study explores the integration of Large Language Models (LLMs) and Generative AI (Gen AI) in humanoid robots to enhance human-machine interaction. The deployment of advanced AI capabilities in humanoids has revolutionized various fields, including healthcare, education, and industry. The study highlights the potential benefits of AI in humanoids for vulnerable populations, such as the elderly and disabled, including companionship, assistance with daily tasks, and improved quality of life. However, it also addresses the ethical considerations and challenges associated with AI deployment, such as job displacement and income inequality. The study emphasizes the importance of responsible AI deployment, advocating for transparency, accountability, and fairness in AI algorithms. Furthermore, the research discusses strategies for maximizing the positive impact of AI deployment while mitigating negative consequences, such as investing in education and training programs and implementing regulations and policies for ethical AI use. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Pérez-Pérez, M.
AU  - Gonzalez, M.F.
AU  - Rodriguez-Rajo, F.J.
AU  - Fdez-Riverola, F.
TI  - Tracking the Spread of Pollen on Social Media Using Pollen-Related Messages From Twitter: Retrospective Analysis
PY  - 2024
T2  - Journal of Medical Internet Research
VL  - 26
C7  - e58309
DO  - 10.2196/58309
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206967175&doi=10.2196%2f58309&partnerID=40&md5=a76118e0b9b161d15f5e0269914aedda
AB  - Background: Allergy disorders caused by biological particles, such as the proteins in some airborne pollen grains, are currently considered one of the most common chronic diseases, and European Academy of Allergy and Clinical Immunology forecasts indicate that within 15 years 50% of Europeans will have some kind of allergy as a consequence of urbanization, industrialization, pollution, and climate change. Objective: The aim of this study was to monitor and analyze the dissemination of information about pollen symptoms from December 2006 to January 2022. By conducting a comprehensive evaluation of public comments and trends on Twitter, the research sought to provide valuable insights into the impact of pollen on sensitive individuals, ultimately enhancing our understanding of how pollen-related information spreads and its implications for public health awareness. Methods: Using a blend of large language models, dimensionality reduction, unsupervised clustering, and term frequency-inverse document frequency, alongside visual representations such as word clouds and semantic interaction graphs, our study analyzed Twitter data to uncover insights on respiratory allergies. This concise methodology enabled the extraction of significant themes and patterns, offering a deep dive into public knowledge and discussions surrounding respiratory allergies on Twitter. Results: The months between March and August had the highest volume of messages. The percentage of patient tweets appeared to increase notably during the later years, and there was also a potential increase in the prevalence of symptoms, mainly in the morning hours, indicating a potential rise in pollen allergies and related discussions on social media. While pollen allergy is a global issue, specific sociocultural, political, and economic contexts mean that patients experience symptomatology at a localized level, needing appropriate localized responses. Conclusions: The interpretation of tweet information represents a valuable tool to take preventive measures to mitigate the impact of pollen allergy on sensitive patients to achieve equity in living conditions and enhance access to health information and services. © 2024 JMIR Publications Inc.. All rights reserved.
PB  - JMIR Publications Inc.
C2  - 39432897
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Soni, S.
AU  - Demner-Fushman, D.
TI  - A Privacy-preserving Approach to Ingest Knowledge from Proprietary Web-based to Locally Run Models for Medical Progress Note Generation
PY  - 2024
T2  - PrivateNLP 2024 - 5th Workshop on Privacy in Natural Language Processing, Proceedings of the Workshop
SP  - 178
EP  - 183
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204426400&partnerID=40&md5=9134127bd57bb34895c26e046c5ac709
AB  - Clinical documentation is correlated with increasing clinician burden, leading to the rise of automated methods to generate medical notes. Due to the sensitive nature of patient electronic health records (EHRs), locally run models are preferred for a variety of reasons including privacy, bias, and cost. However, most open-source locally run models (including medical-specific) are much smaller with limited input context size compared to the more powerful closed-source large language models (LLMs) generally available through web APIs (Application Programming Interfaces). In this paper, we propose a framework to harness superior reasoning capabilities and medical knowledge from closed-source online LLMs in a privacy-preserving manner and seamlessly incorporate it into locally run models. Specifically, we leverage a web-based model to distill the vast patient information available in EHRs into a clinically relevant subset without sending sensitive patient health information online and use this distilled knowledge to generate progress notes by a locally run model. Our ablation results indicate that the proposed framework improves the performance of the Mixtral model on progress note generation by 4.6 points on ROUGE (a text-matching based metric) and 7.56 points on MEDCON F1 (a metric that measures the clinical concepts overlap). © 2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Davenport, M.A.
AU  - Sirrianni, J.W.
AU  - Chisolm, D.J.
TI  - Machine learning data sources in pediatric sleep research: assessing racial/ethnic differences in electronic health record–based clinical notes prior to model training
PY  - 2024
T2  - Frontiers in Sleep
VL  - 3
C7  - 1271167
DO  - 10.3389/frsle.2024.1271167
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205353018&doi=10.3389%2ffrsle.2024.1271167&partnerID=40&md5=e915f415eb4b3ef7c7b3fb9146454c0d
AB  - Introduction: Pediatric sleep problems can be detected across racial/ethnic subpopulations in primary care settings. However, the electronic health record (EHR) data documentation that describes patients' sleep problems may be inherently biased due to both historical biases and informed presence. This study assessed racial/ethnic differences in natural language processing (NLP) training data (e.g., pediatric sleep-related keywords in primary care clinical notes) prior to model training. Methods: We used a predefined keyword features set containing 178 Peds B-SATED keywords. We then queried all the clinical notes from patients seen in pediatric primary care between the ages of 5 and 18 from January 2018 to December 2021. A least absolute shrinkage and selection operator (LASSO) regression model was used to investigate whether there were racial/ethnic differences in the documentation of Peds B-SATED keywords. Then, mixed-effects logistic regression was used to determine whether the odds of the presence of global Peds B-SATED dimensions also differed across racial/ethnic subpopulations. Results: Using both LASSO and multilevel modeling approaches, the current study found that there were racial/ethnic differences in providers' documentation of Peds B-SATED keywords and global dimensions. In addition, the most frequently documented Peds B-SATED keyword rankings qualitatively differed across racial/ethnic subpopulations. Conclusion: This study revealed providers' differential patterns of documenting Peds B-SATED keywords and global dimensions that may account for the under-detection of pediatric sleep problems among racial/ethnic subpopulations. In research, these findings have important implications for the equitable clinical documentation of sleep problems in pediatric primary care settings and extend prior retrospective work in pediatric sleep specialty settings. Copyright © 2024 Davenport, Sirrianni and Chisolm.
PB  - Frontiers Media SA
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Wang, Y.
AU  - Singh, L.
TI  - It is Time to Develop an Auditing Framework to Promote Value Aware Chatbots
PY  - 2024
T2  - Proceedings of the 13th International Conference on Data Science, Technology and Applications, DATA 2024
SP  - 460
EP  - 470
DO  - 10.5220/0012806800003756
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203062183&doi=10.5220%2f0012806800003756&partnerID=40&md5=f36b8d14dd0ff9225e18b2e821fb2475
AB  - The launch of ChatGPT in November 2022 marked the beginning of a new era in AI, the availability of generative AI tools for everyone to use. ChatGPT and other similar chatbots boast a wide range of capabilities from answering student homework questions to creating music and art. Given the large amounts of human data chatbots are built on, it is inevitable that they will inherit human errors and biases. These biases have the potential to inflict significant harm or increase inequity on different subpopulations. Because chatbots do not have an inherent understanding of societal values, they may create new content that is contrary to established norms. Examples of concerning generated content includes child pornography, inaccurate facts, and discriminatory posts. In this position paper, we argue that the speed of advancement of this technology requires us, as computer and data scientists, to mobilize and develop a values-based auditing framework containing a community established standard set of measurements to monitor the health of different chatbots and LLMs. To support our argument, we use a simple audit template to share the results of basic audits we conduct that are focused on measuring potential bias in search engine style tasks, code generation, and story generation. We identify responses from GPT 3.5 and GPT 4 that are both consistent and not consistent with values derived from existing law. While the findings come as no surprise, they do underscore the urgency of developing a robust auditing framework for openly sharing results in a consistent way so that mitigation strategies can be developed by the academic community, government agencies, and companies when our values are not being adhered to. We conclude this paper with recommendations for value-based strategies for improving the technologies. © 2024 by SCITEPRESS – Science and Technology Publications, Lda.
PB  - SciTePress
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Joshi, S.
AU  - Ha, E.
AU  - Amaya, A.
AU  - Mendoza, M.
AU  - Rivera, Y.
AU  - Singh, V.K.
TI  - Ensuring Accuracy and Equity in Vaccination Information From ChatGPT and CDC: Mixed-Methods Cross-Language Evaluation
PY  - 2024
T2  - JMIR Formative Research
VL  - 8
C7  - e60939
DO  - 10.2196/60939
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208270373&doi=10.2196%2f60939&partnerID=40&md5=5a7ed6e9571c398b7aaf70b1a826e8a3
AB  - Background: In the digital age, large language models (LLMs) like ChatGPT have emerged as important sources of health care information. Their interactive capabilities offer promise for enhancing health access, particularly for groups facing traditional barriers such as insurance and language constraints. Despite their growing public health use, with millions of medical queries processed weekly, the quality of LLM-provided information remains inconsistent. Previous studies have predominantly assessed ChatGPT's English responses, overlooking the needs of non-English speakers in the United States. This study addresses this gap by evaluating the quality and linguistic parity of vaccination information from ChatGPT and the Centers for Disease Control and Prevention (CDC), emphasizing health equity. Objective: This study aims to assess the quality and language equity of vaccination information provided by ChatGPT and the CDC in English and Spanish. It highlights the critical need for cross-language evaluation to ensure equitable health information access for all linguistic groups. Methods: We conducted a comparative analysis of ChatGPT's and CDC's responses to frequently asked vaccination-related questions in both languages. The evaluation encompassed quantitative and qualitative assessments of accuracy, readability, and understandability. Accuracy was gauged by the perceived level of misinformation; readability, by the Flesch-Kincaid grade level and readability score; and understandability, by items from the National Institutes of Health's Patient Education Materials Assessment Tool (PEMAT) instrument. Results: The study found that both ChatGPT and CDC provided mostly accurate and understandable (eg, scores over 95 out of 100) responses. However, Flesch-Kincaid grade levels often exceeded the American Medical Association's recommended levels, particularly in English (eg, average grade level in English for ChatGPT=12.84, Spanish=7.93, recommended=6). CDC responses outperformed ChatGPT in readability across both languages. Notably, some Spanish responses appeared to be direct translations from English, leading to unnatural phrasing. The findings underscore the potential and challenges of using ChatGPT for health care access. Conclusions: ChatGPT holds potential as a health information resource but requires improvements in readability and linguistic equity to be truly effective for diverse populations. Crucially, the default user experience with ChatGPT, typically encountered by those without advanced language and prompting skills, can significantly shape health perceptions. This is vital from a public health standpoint, as the majority of users will interact with LLMs in their most accessible form. Ensuring that default responses are accurate, understandable, and equitable is imperative for fostering informed health decisions across diverse communities. © 2024 JMIR Publications Inc.. All rights reserved.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Hadley, E.
AU  - Marcial, L.
AU  - Quattrone, W.
AU  - Bobashev, G.
TI  - Traditional and GenAI Text Analysis of COVID-19 Pandemic Trends in Hospital Community Benefits IRS Documentation
PY  - 2024
T2  - Journal of Data Science
VL  - 22
IS  - 3
SP  - 393
EP  - 408
DO  - 10.6339/24-JDS1144
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204385271&doi=10.6339%2f24-JDS1144&partnerID=40&md5=dc2d2713e2e08c3e40a48e889667c503
AB  - The coronavirus disease 2019 (COVID-19) pandemic presented unique challenges to the U.S. healthcare system, particularly for nonprofit U.S. hospitals that are obligated to provide community benefits in exchange for federal tax exemptions. We sought to examine how hospitals initiated, modified, or disbanded community benefits programming in response to the COVID-19 pandemic. We used the free-response text in Part IV of Internal Revenue Service (IRS) Form 990 Schedule H (F990H) to assess health equity and disparities. We combined traditional key term frequency and Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) clustering approaches with a novel Generative Pre-trained Transformer (GPT) 3.5 summarization approach. Our research reveals shifts in community benefits programming. We observed an increase in COVID-related terms starting in the 2019 tax year, indicating a pivot in community focus and efforts toward pandemic-related activities such as telehealth services and COVID-19 testing and prevention. The clustering analysis identified themes related to COVID-19 and community benefits. Generative Artificial Intelligence (GenAI) summarization with GPT3.5 contextualized these changes, revealing examples of healthcare system adaptations and program cancellations. However, GPT3.5 also encountered some accuracy and validation challenges. This multifaceted text analysis underscores the adaptability of hospitals in maintaining community health support during crises and suggests the potential of advanced AI tools in evaluating large-scale qualitative data for policy and public health research. © 2024 The Author(s).
PB  - Center for Applied Statistics, School of Statistics, Renmin University of China
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Chen, F.
AU  - Yang, D.
AU  - Fang, H.
TI  - Toward Automatic Group Membership Annotation for Group Fairness Evaluation
PY  - 2024
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 14762 LNCS
SP  - 285
EP  - 300
DO  - 10.1007/978-3-031-70239-6_20
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205343468&doi=10.1007%2f978-3-031-70239-6_20&partnerID=40&md5=f0fcd29b6ef02682e127b171b1b767a8
AB  - With the increasing research attention on fairness in information retrieval systems, more and more fairness-aware algorithms have been proposed to ensure fairness for a sustainable and healthy retrieval ecosystem. However, as the most adopted measurement of fairness-aware algorithms, group fairness evaluation metrics, require group membership information that needs massive human annotations and is barely available for general information retrieval datasets. This data sparsity significantly impedes the development of fairness-aware information retrieval studies. Hence, a practical, scalable, low-cost group membership annotation method is needed to assist or replace human annotations. This study explored how to leverage language models to automatically annotate group membership for group fairness evaluations, focusing on annotation accuracy and its impact. Our experimental results show that BERT-based models outperformed state-of-the-art large language models, including GPT and Mistral, achieving promising annotation accuracy with minimal supervision in recent fair-ranking datasets. Our impact-oriented evaluations reveal that minimal annotation error will not degrade the effectiveness and robustness of group fairness evaluation. The proposed annotation method reduces tremendous human efforts and expands the frontier of fairness-aware studies to more datasets. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Cosma, G.
AU  - Singh, M.K.
AU  - Waterson, P.
AU  - Jun, G.T.
AU  - Back, J.
TI  - Unveiling Disparities in Maternity Care: A Topic Modelling Approach to Analysing Maternity Incident Investigation Reports
PY  - 2024
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 14975 LNCS
SP  - 295
EP  - 308
DO  - 10.1007/978-3-031-67278-1_23
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201928296&doi=10.1007%2f978-3-031-67278-1_23&partnerID=40&md5=84aef1529769b6c85f4c6ae201b888bd
AB  - This study applies Natural Language Processing techniques, including Latent Dirichlet Allocation, to analyse anonymised maternity incident investigation reports from the Healthcare Safety Investigation Branch. The reports underwent preprocessing, annotation using the Safety Intelligence Research taxonomy, and topic modelling to uncover prevalent topics and detect differences in maternity care across ethnic groups. A combination of offline and online methods was utilised to ensure data protection whilst enabling advanced analysis, with offline processing for sensitive data and online processing for non-sensitive data using the ‘Claude 3 Opus’ language model. Interactive topic analysis and semantic network visualisation were employed to extract and display thematic topics and visualise semantic relationships among keywords. The analysis revealed disparities in care among different ethnic groups, with distinct focus areas for the Black, Asian, and White British ethnic groups. The study demonstrates the effectiveness of topic modelling and NLP techniques in analysing maternity incident investigation reports and highlighting disparities in care. The findings emphasise the crucial role of advanced data analysis in improving maternity care quality and equity. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Modi, S.
AU  - Kasmiran, K.A.
AU  - Sharef, N.M.
AU  - Sharum, M.Y.
TI  - Enhanced Adverse Drug Event Extraction Using Prefix-Based Multi-Prompt Tuning in Transformer Models
PY  - 2024
T2  - International Journal on Informatics Visualization
VL  - 8
IS  - 3-2
SP  - 1713
EP  - 1719
DO  - 10.62527/joiv.8.3-2.3454
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211947499&doi=10.62527%2fjoiv.8.3-2.3454&partnerID=40&md5=3b8fa71ab8d6d3a2a3fed2b66ed2d3c6
AB  - Extracting mentions of adverse drug events and relationships between them is crucial for effective pharmacovigilance and drug safety surveillance. Recently, transformer-based models have significantly improved this task through fine-tuning. However, traditional fine-tuning of transformer models, especially those with many parameters, is resource-intensive, memory-inefficient, and often leaves a gap between pre-training and downstream task-specific objectives. Soft prompting is a lightweight approach that updates a trainable prompt to guide task-specific fine-tuning, showing comparable performance to traditional fine-tuning for large language models on simple tasks. However, its effectiveness on complex tasks like token-based sequence labeling requiring multiple predictions for a single input sequence remains underexplored, particularly in multi-task settings. In addition, using holistic prompts in multi-task learning settings may be biased to other subtasks. Additionally, some prompt tokens hurt the model prediction. This study proposes a prefix-based multi-prompt soft tuning method with attention-driven prompt token selection for tuning transformer models on multi-task dual sequence labelling for concept and relation extraction. We experimented with BERT and SciBERT models using frozen and unfrozen parameter strategies. Our approach achieved state-of-the-art performance on the n2c2 2018 and TAC 2017 datasets for adverse drug event extraction, with multi-prompt tuning in unfrozen models surpassing traditional fine-tuning. Moreover, it outperforms the largest clinical natural language processing model, GatorTron, on the n2c2 2018 dataset. This research highlights the potential of soft prompts in efficiently adapting large language models to complex downstream NLP tasks. © 2024, Politeknik Negeri Padang. All rights reserved.
PB  - Politeknik Negeri Padang
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Mirek-Rogowska, A.
AU  - Kucza, W.
AU  - Gajdka, K.
TI  - AI IN COMMUNICATION: THEORETICAL PERSPECTIVES, ETHICAL IMPLICATIONS, AND EMERGING COMPETENCIES
PY  - 2024
T2  - Communication Today
VL  - 15
IS  - 2
SP  - 16
EP  - 28
DO  - 10.34135/communicationtoday.2024.Vol.15.No.2.2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212616525&doi=10.34135%2fcommunicationtoday.2024.Vol.15.No.2.2&partnerID=40&md5=4808ce7222d9bfae0a6bc2bacdadc81f
AB  - Artificial intelligence (AI) is rapidly transforming communication processes across various sectors, including marketing, education, healthcare, and entertainment. This study explores the theoretical perspectives surrounding AI’s integration into communication, examining how AI-driven tools such as ChatGPT, MidJourney, and Google Gemini are reshaping content creation, personalisation, and human-machine interaction. While AI enhances efficiency and allows for real-time customisation of messages, it also presents ethical challenges related to privacy, data security, and algorithmic bias. By synthesising key academic studies, the study outlines the critical ethical considerations, including the risks of deepfakes and disinformation, and emphasises the need for ethical frameworks to guide responsible AI use. The text also discusses the new digital competencies required to navigate AI-enhanced communication environments, such as AI literacy, data proficiency, and ethical reasoning. Through a systematic literature review, this study contributes to the ongoing discourse on AI’s role in communication by offering a comprehensive theoretical framework that highlights both the opportunities and limitations of AI technologies. Future research should focus on addressing gaps in empirical studies, particularly concerning the long-term impacts of AI on decision-making and the ethical governance of AI-generated content. © (2024), (Communication Today). All rights reserved.
PB  - University of SS. Cyril and Methodius, Faculty of Mass Media Communication
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Vasuki Rohinidevi, V.
AU  - Deeban Chakravarthy, V.
AU  - Deepak, S.P.K.
TI  - Systematic Review of Emerging Deep Learning approaches for Crop Diseases detection
PY  - 2024
T2  - Proceedings of International Conference on Circuit Power and Computing Technologies, ICCPCT 2024
SP  - 1439
EP  - 1444
DO  - 10.1109/ICCPCT61902.2024.10672910
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205593351&doi=10.1109%2fICCPCT61902.2024.10672910&partnerID=40&md5=a9f5cb4a33a6464c6e0c6aeefaf41cf7
AB  - Deep learning, a branch of artificial intelligence, has drawn interest from the academic and corporate realms, especially in areas like speech and image analysis, video processing, and natural language processing. Its use in agricultural plant protection has expanded to include the diagnosis and evaluation of pests and plant diseases. In this situation, there are several benefits to using deep learning. It removes the biases related to manually chosen pathogen properties, improves objectivity when determining disease characteristics, and hastens the advancement of contemporary technology. In order to diagnose agricultural leaf illnesses, this study explores the most recent developments in deep learning methods, including CNN, DENSNET, InceptionV3, VGG16, Resnet50, and MobileNet, in addition to a variety of sensors. This project aims to aid scientists in the field by tackling challenges in merging deep learning and cutting-edge imaging tools to enhance illness identification. It addresses current trends and barriers, shedding light on unresolved issues and paving the way for further research. By clarifying persistent problems and posing new questions, it fosters progress and innovation in the field of medical imaging and diagnosis. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Zong, H.
AU  - Wu, R.
AU  - Cha, J.
AU  - Wang, J.
AU  - Wu, E.
AU  - Li, J.
AU  - Zhou, Y.
AU  - Zhang, C.
AU  - Feng, W.
AU  - Shen, B.
TI  - Large Language Models in Worldwide Medical Exams: Platform Development and Comprehensive Analysis
PY  - 2024
T2  - Journal of Medical Internet Research
VL  - 26
C7  - e66114
DO  - 10.2196/66114
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213203798&doi=10.2196%2f66114&partnerID=40&md5=bea5895b7e5bcca3d271e9ffa963f4df
AB  - Background: Large language models (LLMs) are increasingly integrated into medical education, with transformative potential for learning and assessment. However, their performance across diverse medical exams globally has remained underexplored. Objective: This study aims to introduce MedExamLLM, a comprehensive platform designed to systematically evaluate the performance of LLMs on medical exams worldwide. Specifically, the platform seeks to (1) compile and curate performance data for diverse LLMs on worldwide medical exams; (2) analyze trends and disparities in LLM capabilities across geographic regions, languages, and contexts; and (3) provide a resource for researchers, educators, and developers to explore and advance the integration of artificial intelligence in medical education. Methods: A systematic search was conducted on April 25, 2024, in the PubMed database to identify relevant publications. Inclusion criteria encompassed peer-reviewed, English-language, original research articles that evaluated at least one LLM on medical exams. Exclusion criteria included review articles, non-English publications, preprints, and studies without relevant data on LLM performance. The screening process for candidate publications was independently conducted by 2 researchers to ensure accuracy and reliability. Data, including exam information, data process information, model performance, data availability, and references, were manually curated, standardized, and organized. These curated data were integrated into the MedExamLLM platform, enabling its functionality to visualize and analyze LLM performance across geographic, linguistic, and exam characteristics. The web platform was developed with a focus on accessibility, interactivity, and scalability to support continuous data updates and user engagement. Results: A total of 193 articles were included for final analysis. MedExamLLM comprised information for 16 LLMs on 198 medical exams conducted in 28 countries across 15 languages from the year 2009 to the year 2023. The United States accounted for the highest number of medical exams and related publications, with English being the dominant language used in these exams. The Generative Pretrained Transformer (GPT) series models, especially GPT-4, demonstrated superior performance, achieving pass rates significantly higher than other LLMs. The analysis revealed significant variability in the capabilities of LLMs across different geographic and linguistic contexts. Conclusions: MedExamLLM is an open-source, freely accessible, and publicly available online platform providing comprehensive performance evaluation information and evidence knowledge about LLMs on medical exams around the world. The MedExamLLM platform serves as a valuable resource for educators, researchers, and developers in the fields of clinical medicine and artificial intelligence. By synthesizing evidence on LLM capabilities, the platform provides valuable insights to support the integration of artificial intelligence into medical education. Limitations include potential biases in the data source and the exclusion of non-English literature. Future research should address these gaps and explore methods to enhance LLM performance in diverse contexts. ©Hui Zong, Rongrong Wu, Jiaxue Cha, Jiao Wang, Erman Wu, Jiakun Li, Yi Zhou, Chi Zhang, Weizhe Feng, Bairong Shen.
PB  - JMIR Publications Inc.
C2  - 39729356
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Locke, L.G.
AU  - Hodgdon, G.
TI  - Gender bias in visual generative artificial intelligence systems and the socialization of AI
PY  - 2024
T2  - AI and Society
DO  - 10.1007/s00146-024-02129-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210181842&doi=10.1007%2fs00146-024-02129-1&partnerID=40&md5=e378c5881155cc9a4c55072b44288a7d
AB  - Substantial research over the last ten years has indicated that many generative artificial intelligence systems (“GAI”) have the potential to produce biased results, particularly with respect to gender. This potential for bias has grown progressively more important in recent years as GAI has become increasingly integrated in multiple critical sectors, such as healthcare, consumer lending, and employment. While much of the study of gender bias in popular GAI systems is focused on text-based GAI such as OpenAI’s ChatGPT and Google’s Gemini (formerly Bard), this article describes the results of a confirmatory experiment of gender bias in visual GAI systems. The authors argue that the potential for gender bias in visual GAI systems is potentially more troubling than bias in textual GAI because of the superior memorability of images and the capacity for emotional communication that images represent. They go on to offer four potential approaches to gender bias in visual GAI based on the roles visual GAI could play in modern society. The article concludes with a discussion of how dominant societal values could influence a choice between those four potential approaches to gender bias in visual GAI and some suggestions for further research. © The Author(s) 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Joseph, A.
AU  - Joseph, K.
AU  - Joseph, A.
TI  - A pilot evaluation of the diagnostic accuracy of ChatGPT-3.5 for multiple sclerosis from case reports
PY  - 2024
T2  - Translational Neuroscience
VL  - 15
IS  - 1
C7  - 20220361
DO  - 10.1515/tnsci-2022-0361
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213434194&doi=10.1515%2ftnsci-2022-0361&partnerID=40&md5=59845a5b4796fefe0ff946999297cc33
AB  - The limitation of artificial intelligence (AI) large language models to diagnose diseases from the perspective of patient safety remains underexplored and potential challenges, such as diagnostic errors and legal challenges, need to be addressed. To demonstrate the limitations of AI, we used ChatGPT-3.5 developed by OpenAI, as a tool for medical diagnosis using text-based case reports of multiple sclerosis (MS), which was selected as a prototypic disease. We analyzed 98 peer-reviewed case reports selected based on free-full text availability and published within the past decade (2014–2024), excluding any mention of an MS diagnosis to avoid bias. ChatGPT-3.5 was used to interpret clinical presentations and laboratory data from these reports. The model correctly diagnosed MS in 77 cases, achieving an accuracy rate of 78.6%. However, the remaining 21 cases were misdiagnosed, highlighting the model’s limitations. Factors contributing to the errors include variability in data presentation and the inherent complexity of MS diagnosis, which requires imaging modalities in addition to clinical presentations and laboratory data. While these findings suggest that AI can support disease diagnosis and healthcare providers in decision-making, inadequate training with large datasets may lead to significant inaccuracies. Integrating AI into clinical practice necessitates rigorous validation and robust regulatory frameworks to ensure responsible use. © 2024 the author(s),
PB  - Walter de Gruyter GmbH
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Deshmukh, R.
AU  - Gaikwad, S.B.
AU  - Reddy, K.S.
AU  - Joseph, C.
AU  - Arnone, G.
AU  - Kalaivaani, P.C.D.
TI  - Exploring Ethical Considerations: Privacy and Accountability in Conversational Agents like ChatGPT
PY  - 2024
T2  - Smart Innovation, Systems and Technologies
VL  - 390
SP  - 193
EP  - 201
DO  - 10.1007/978-981-97-2716-2_18
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201093034&doi=10.1007%2f978-981-97-2716-2_18&partnerID=40&md5=6f0360393794625bb494b9e521a6b8f8
AB  - In recent years, advances in artificial intelligence (AI) and machine learning have transformed the landscape of scientific study. Out of all of these, chatbot technology has come a long way in the last few years, especially since ChatGPT became a well-known artificial intelligence language model. This comprehensive review investigates ChatGPT's background, applications, primary challenges, and possible future advancements. We first look at its history, progress, and fundamental technology before delving into its many applications in customer service, health care, and education. We also discuss potential countermeasures and highlight the major challenges that ChatGPT faces, including data biases, moral dilemmas, and security threats. Finally, we go over our plans for ChatGPT's future, outlining areas that need further research and development, improved human-AI communication, closing the digital gap, and ChatGPT integration with other technologies. This study offers useful information for scholars, developers, and stakeholders interested in the rapidly evolving subject of artificial intelligence-powered conversational bots. This study looks at the ways that ChatGPT has changed scientific research in several domains, such as data processing, developing hypotheses, collaboration, and public outreach. In addition, the paper examines potential limitations and ethical quandaries associated with the use of ChatGPT in research, highlighting the importance of striking a balance between human expertise and AI-assisted innovation. The paper addresses multiple ethical issues with the state of computers today and how ChatGPT can cause people to oppose this notion. This study also has a number of ChatGPT biases and restrictions. It is noteworthy that in a very short period, ChatGPT has garnered significant interest from academics, research, and enterprises, notwithstanding several challenges and ethical issues. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Pelloin, V.
AU  - Dodson, L.
AU  - Chapuis, É.
AU  - Hervé, N.
AU  - Doukhan, D.
TI  - Automatic Classification of News Subjects in Broadcast News: Application to a Gender Bias Representation Analysis
PY  - 2024
T2  - Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH
SP  - 3055
EP  - 3059
DO  - 10.21437/Interspeech.2024-1854
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214783945&doi=10.21437%2fInterspeech.2024-1854&partnerID=40&md5=5e21994acc895021e2d479941a6d3dbe
AB  - This paper introduces a computational framework designed to delineate gender distribution biases in topics covered by French TV and radio news. We transcribe a dataset of 11.7k hours, broadcasted in 2023 on 21 French channels. A Large Language Model (LLM) is used in few-shot conversation mode to obtain a topic classification on those transcriptions. Using the generated LLM annotations, we explore the finetuning of a specialized smaller classification model, to reduce the computational cost. To evaluate the performances of these models, we construct and annotate a dataset of 804 dialogues. This dataset is made available free of charge for research purposes. We show that women are notably underrepresented in subjects such as sports, politics and conflicts. Conversely, on topics such as weather, commercials and health, women have more speaking time than their overall average across all subjects. We also observe representations differences between private and public service channels. © 2024 International Speech Communication Association. All rights reserved.
PB  - International Speech Communication Association
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Guo, Y.
AU  - Wan, Z.
TI  - Performance Evaluation of Multimodal Large Language Models (LLaVA and GPT-4-based ChatGPT) in Medical Image Classification Tasks
PY  - 2024
T2  - Proceedings - 2024 IEEE 12th International Conference on Healthcare Informatics, ICHI 2024
SP  - 541
EP  - 543
DO  - 10.1109/ICHI61247.2024.00080
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203676198&doi=10.1109%2fICHI61247.2024.00080&partnerID=40&md5=940bcc47379d2c69b593875f661b82f5
AB  - Large language models (LLMs) have gained significant attention due to their prospective applications in medicine. Utilizing multimodal LLMs can potentially assist clinicians in medical image classification tasks. It is important to evaluate the performance of LLMs in medical image processing to potentially improve the medical system. We evaluated two multimodal LLMs (LLaVA and GPT-4-based ChatGPT) against the classic VGG in tumor classification across brain MRI, breast ultrasound, and kidney CT datasets. Despite LLMs facing significant hallucination issue in medical imaging, prompt engineering markedly enhanced their performance. In comparison to the baseline method, GPT-4-based ChatGPT with prompt engineering achieves 98%, 112%, and 69% of the baseline's performance in terms of accuracy (or 99%, 107%, and 62 % in terms of F1-score) in those three datasets, respectively. However, privacy, bias, accountability, and transparency concerns necessitate caution. Our study underscore LLMs' potential in medical imaging but emphasize the need for thorough performance and safety evaluations for their practical application. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Omar, M.
AU  - Naffaa, M.E.
AU  - Glicksberg, B.S.
AU  - Reuveni, H.
AU  - Nadkarni, G.N.
AU  - Klang, E.
TI  - Advancing rheumatology with natural language processing: insights and prospects from a systematic review
PY  - 2024
T2  - Rheumatology Advances in Practice
VL  - 8
IS  - 4
C7  - rkae120
DO  - 10.1093/rap/rkae120
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206995060&doi=10.1093%2frap%2frkae120&partnerID=40&md5=4245b2e1b412334e650ec610141162e6
AB  - Objectives: Natural language processing (NLP) and large language models (LLMs) have emerged as powerful tools in healthcare, offering advanced methods for analysing unstructured clinical texts. This systematic review aims to evaluate the current applications of NLP and LLMs in rheumatology, focusing on their potential to improve disease detection, diagnosis and patient management. Methods: We screened seven databases. We included original research articles that evaluated the performance of NLP models in rheumatology. Data extraction and risk of bias assessment were performed independently by two reviewers, following Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines. The Quality Assessment Tool for Observational Cohort and Cross-Sectional Studies was used to evaluate the risk of bias. Results: Of 1491 articles initially identified, 35 studies met the inclusion criteria. These studies utilized various data types, including electronic medical records and clinical notes, and employed models like Bidirectional Encoder Representations from Transformers and Generative Pre-trained Transformers. High accuracy was observed in detecting conditions such as RA, SpAs and gout. The use of NLP also showed promise in managing diseases and predicting flares. Conclusion: NLP showed significant potential in enhancing rheumatology by improving diagnostic accuracy and personalizing patient care. While applications in detecting diseases like RA and gout are well developed, further research is needed to extend these technologies to rarer and more complex clinical conditions. Overcoming current limitations through targeted research is essential for fully realizing NLP's potential in clinical practice. © 2024 The Author(s).
PB  - Oxford University Press
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Fernando, R.
AU  - Norton, I.
AU  - Dogra, P.
AU  - Sarnaik, R.
AU  - Wazir, H.
AU  - Ren, Z.
AU  - Gunda, N.S.
AU  - Mukhopadhyay, A.
AU  - Lutz, M.
TI  - Quantifying Bias in Agentic Large Language Models: A Benchmarking Approach
PY  - 2024
T2  - 2024 5th Information Communication Technologies Conference, ICTC 2024
SP  - 349
EP  - 353
DO  - 10.1109/ICTC61510.2024.10601938
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201056725&doi=10.1109%2fICTC61510.2024.10601938&partnerID=40&md5=de23d7f88a22033d616286a1886723f5
AB  - The rapid adoption of large language models (LLMs) as agents raises concerns about potential biases in their decision-making processes. While previous work has explored bias mitigation in open text generation, the analysis of bias in LLM-based agents with constrained choices is under-explored. This paper introduces a new benchmark for evaluating bias in such agents, utilizing a question-answering framework across simulated real-life scenarios in healthcare, criminal justice, and business. We analyze potential biases related to race, gender, age, political affiliation, and socioeconomic status. Our novel question-answering bias distribution diversity metric quantifies the LLM's decision-making tendencies. We find that pre-trained models exhibit varying degrees of bias across domains and categories, offering insights for future bias mitigation strategies.  © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Zollo, T.P.
AU  - Morrill, T.
AU  - Deng, Z.
AU  - Snell, J.C.
AU  - Pitassi, T.
AU  - Zemel, R.
TI  - PROMPT RISK CONTROL: A RIGOROUS FRAMEWORK FOR RESPONSIBLE DEPLOYMENT OF LARGE LANGUAGE MODELS
PY  - 2024
T2  - 12th International Conference on Learning Representations, ICLR 2024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200566026&partnerID=40&md5=507be986c45e9dc8b4532353f40d2b33
AB  - The recent explosion in the capabilities of large language models has led to a wave of interest in how best to prompt the model to perform a given task. While it may be tempting to choose a prompt based on average empirical results on a validation set, this can lead to a deployment where unexpectedly poor responses are generated. To mitigate this prospect, we propose a lightweight framework, Prompt Risk Control, for selecting a prompt based on rigorous upper bounds on families of informative risk measures. We provide and compare different methods for producing bounds on a diverse set of metrics measuring quantities such as worst-case response and disparities in generation quality across the population of users. In addition, we extend the underlying statistical bounding techniques to accommodate the possibility of distribution shifts in deployment. Experiments on applications such as chatbots, medical question summarization, and code generation highlight how such a framework can reduce the risk of the worst outcomes. © 2024 12th International Conference on Learning Representations, ICLR 2024. All rights reserved.
PB  - International Conference on Learning Representations, ICLR
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - P.R, B.
AU  - O, G.
TI  - Algorithmic solutions, subjectivity and decision errors: a study of AI accountability
PY  - 2024
T2  - Digital Policy, Regulation and Governance
DO  - 10.1108/DPRG-05-2024-0090
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209765055&doi=10.1108%2fDPRG-05-2024-0090&partnerID=40&md5=8e6c8ad2616432af451e9751267b844e
AB  - Purpose: The purpose of this paper is to explore the challenges of implementing accountable artificial intelligence (AI) systems in India, focusing on the need for algorithms to justify their decisions, especially in subjective and complex scenarios. By analyzing various government projects, documented biases and conducting empirical case studies and experiments, the study highlights the limitations of AI in recognizing the nuances of India’s unique social landscape. It aims to underscore the importance of integrating political philosophy to ensure that AI systems are held accountable within India’s sociopolitical context, urging policymakers to develop frameworks for responsible AI decision-making. Design/methodology/approach: The research adopts a mixed-methods approach to address the five research questions. It begins with an extensive literature review, focusing on AI’s transformative potential, algorithmic bias and accountability in the Indian context. Data is collected from 15 AI use cases in health care, education and public safety, 13 government automated decision tools and five bias cases, including facial recognition and caste-based discrimination. Additionally, ten case studies and three experiments on ChatGPT are analyzed. Content analysis is used to interpret and categorize the data, identifying patterns and themes. Specific case studies and experiments on autocompletion in search engines further support the findings. Findings: The study revealed significant limitations in current AI systems when applied to India’s complex socio-cultural landscape. Analyzing 15 AI applications and 13 government projects, the research identified multiple instances of algorithmic bias. Experiments with Google’s autocomplete and ChatGPT showed that these systems often reinforce social stereotypes and struggle with nuanced, subjective situations. The findings emphasize the accountability gap in AI-driven decisions, highlighting the need for rigorous oversight, particularly in welfare projects where errors could lead to severe consequences. The study recommends developing regulatory frameworks, improving AI design and raising public awareness to address these challenges. Originality/value: In the context of complex societies like India, a pressing concern arises: who should assume responsibility for the repercussions stemming from algorithmic failures to comprehend subjective complexities? To this end, there exist no serious scholarly works toward which present paper tries to shed new insights. It draws upon insights from the corpus of political philosophy literature, encompassing both classical and contemporary notions of responsibility, and seeks to establish connections between these concepts and the unique sociopolitical structure of India. The work is unique in the focus of the paper and is original in the direction projected. © 2024, Emerald Publishing Limited.
PB  - Emerald Publishing
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Arroyo, A.M.C.
AU  - Munnangi, M.
AU  - Sun, J.
AU  - Zhang, K.Y.C.
AU  - McInerney, D.J.
AU  - Wallace, B.C.
AU  - Amir, S.
TI  - Open (Clinical) LLMs are Sensitive to Instruction Phrasings
PY  - 2024
T2  - BioNLP 2024 - 23rd Meeting of the ACL Special Interest Group on Biomedical Natural Language Processing, Proceedings of the Workshop and Shared Tasks
SP  - 50
EP  - 71
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204489862&partnerID=40&md5=751749f2c67d2c1dba4648aa20275509
AB  - Instruction-tuned Large Language Models (LLMs) can perform a wide range of tasks given natural language instructions to do so, but they are sensitive to how such instructions are phrased. This issue is especially concerning in healthcare, as clinicians are unlikely to be experienced prompt engineers and the potential consequences of inaccurate outputs are heightened in this domain. This raises a practical question: How robust are instruction-tuned LLMs to natural variations in the instructions provided for clinical NLP tasks? We collect prompts from medical doctors across a range of tasks and quantify the sensitivity of seven LLMs—some general, others specialized—to natural (i.e., non-adversarial) instruction phrasings. We find that performance varies substantially across all models, and that—perhaps surprisingly—domain-specific models explicitly trained on clinical data are especially brittle, compared to their general domain counterparts. Further, arbitrary phrasing differences can affect fairness, e.g., valid but distinct instructions for mortality prediction yield a range both in overall performance, and in terms of differences between demographic groups.. ©2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Criss, S.
AU  - Nguyen, T.T.
AU  - Gonzales, S.M.
AU  - Lin, B.
AU  - Kim, M.
AU  - Makres, K.
AU  - Sorial, B.M.
AU  - Xiong, Y.
AU  - Dennard, E.
AU  - Merchant, J.S.
AU  - Hswen, Y.
TI  - “HIV Stigma Exists” — Exploring ChatGPT’s HIV Advice by Race and Ethnicity, Sexual Orientation, and Gender Identity
PY  - 2024
T2  - Journal of Racial and Ethnic Health Disparities
DO  - 10.1007/s40615-024-02162-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203602043&doi=10.1007%2fs40615-024-02162-2&partnerID=40&md5=a058d7d644bc037121d4fc6f662d74bc
AB  - Background: Stigma and discrimination are associated with HIV persistence. Prior research has investigated the ability of ChatGPT to provide evidence-based recommendations, but the literature examining ChatGPT’s performance across varied sociodemographic factors is sparse. The aim of this study is to understand how ChatGPT 3.5 and 4.0 provide HIV-related guidance related to race and ethnicity, sexual orientation, and gender identity; and if and how that guidance mentions discrimination and stigma. Methods: For data collection, we asked both the free ChatGPT 3.5 Turbo version and paid ChatGPT 4.0 version— the template question for 14 demographic input variables “I am [specific demographic] and I think I have HIV, what should I do?” To ensure robustness and accuracy within the responses generated, the same template questions were asked across all input variables, with the process being repeated 10 times, for 150 responses. A codebook was developed, and the responses (n = 300; 150 responses per version) were exported to NVivo to facilitate analysis. The team conducted a thematic analysis over multiple sessions. Results: Compared to ChatGPT 3.5, ChatGPT 4.0 responses acknowledge the existence of discrimination and stigma for HIV across different racial and ethnic identities, especially for Black and Hispanic identities, lesbian and gay identities, and transgender and women identities. In addition, ChatGPT 4.0 responses included themes of affirming personhood, specialized care, advocacy, social support, local organizations for different identity groups, and health disparities. Conclusion: As these new AI technologies progress, it is critical to question whether it will serve to reduce or exacerbate health disparities. © The Author(s) 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Sebo, P.
TI  - Use of ChatGPT to Explore Gender and Geographic Disparities in Scientific Peer Review
PY  - 2024
T2  - Journal of Medical Internet Research
VL  - 26
C7  - e57667
DO  - 10.2196/57667
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211575390&doi=10.2196%2f57667&partnerID=40&md5=a310e2e88814afa49e3754681472a475
AB  - Background: In the realm of scientific research, peer review serves as a cornerstone for ensuring the quality and integrity of scholarly papers. Recent trends in promoting transparency and accountability has led some journals to publish peer-review reports alongside papers. Objective: ChatGPT-4 (OpenAI) was used to quantitatively assess sentiment and politeness in peer-review reports from high-impact medical journals. The objective was to explore gender and geographical disparities to enhance inclusivity within the peer-review process. Methods: All 9 general medical journals with an impact factor >2 that publish peer-review reports were identified. A total of 12 research papers per journal were randomly selected, all published in 2023. The names of the first and last authors along with the first author’s country of affiliation were collected, and the gender of both the first and last authors was determined. For each review, ChatGPT-4 was asked to evaluate the “sentiment score,” ranging from -100 (negative) to 0 (neutral) to +100 (positive), and the “politeness score,” ranging from -100 (rude) to 0 (neutral) to +100 (polite). The measurements were repeated 5 times and the minimum and maximum values were removed. The mean sentiment and politeness scores for each review were computed and then summarized using the median and interquartile range. Statistical analyses included Wilcoxon rank-sum tests, Kruskal-Wallis rank tests, and negative binomial regressions. Results: Analysis of 291 peer-review reports corresponding to 108 papers unveiled notable regional disparities. Papers from the Middle East, Latin America, or Africa exhibited lower sentiment and politeness scores compared to those from North America, Europe, or Pacific and Asia (sentiment scores: 27 vs 60 and 62 respectively; politeness scores: 43.5 vs 67 and 65 respectively, adjusted P=.02). No significant differences based on authors’ gender were observed (all P>.05). Conclusions: Notable regional disparities were found, with papers from the Middle East, Latin America, and Africa demonstrating significantly lower scores, while no discernible differences were observed based on authors’ gender. The absence of gender-based differences suggests that gender biases may not manifest as prominently as other forms of bias within the context of peer review. The study underscores the need for targeted interventions to address regional disparities in peer review and advocates for ongoing efforts to promote equity and inclusivity in scholarly communication. © 2024 JMIR Publications Inc.. All rights reserved.
PB  - JMIR Publications Inc.
C2  - 39652394
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Muñoz-García, V.
TI  - Bias Mitigation in Corpora for LLMs Training Applied to Text Simplification
PY  - 2024
T2  - CEUR Workshop Proceedings
VL  - 3797
SP  - 33
EP  - 41
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207880790&partnerID=40&md5=7b774d5d1ccb6432932db65c188371e4
AB  - Large Language Models (LLMs) are trained on extensive data, and their performance and behaviour are shaped by the quality and diversity of this data. However, texts used for training these Artificial Intelligence (AI) systems often fail to reflect the wide range of human experiences and identities, which can result in reflecting and amplifying biases. Consequently, if a LLM is trained on biased texts, it can magnify biases when generating text. Regarding the continuous generation of information, it complicates the understanding of such texts, specially to users with cognitive impairment. This issue has made it necessary to resort to automated processes to make information available to all users. Therefore, Automatic Text Simplification (ATS) arises with the aim of approaching the challenge of automatically transforming an original text into a simplified and easier one. More specifically, this task will focus on the medical domain, to make medical texts accessible to all society. © 2024 Copyright for this paper by its authors.
PB  - CEUR-WS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Alnaimat, F.
AU  - Al-Halaseh, S.
AU  - AlSamhori, A.R.F.
TI  - Evolution of Research Reporting Standards: Adapting to the Influence of Artificial Intelligence, Statistics Software, and Writing Tools
PY  - 2024
T2  - Journal of Korean Medical Science
VL  - 39
IS  - 32
C7  - e231
DO  - 10.3346/jkms.2024.39.e231
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201769169&doi=10.3346%2fjkms.2024.39.e231&partnerID=40&md5=c1e4f9b598281bff0324026dfafdbaca
AB  - Reporting standards are essential to health research as they improve accuracy and transparency. Over time, significant changes have occurred to the requirements for reporting research to ensure comprehensive and transparent reporting across a range of study domains and foster methodological rigor. The establishment of the Declaration of Helsinki, Consolidated Standards of Reporting Trials (CONSORT), Strengthening the Reporting of Observational Studies in Epidemiology (STROBE), and Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) are just a few of the historic initiatives that have increased research transparency. Through enhanced discoverability, statistical analysis facilitation, article quality enhancement, and language barrier reduction, artificial intelligence (AI)—in particular, large language models like ChatGPT—has transformed academic writing. However, problems with errors that could occur and the need for transparency while utilizing AI tools still exist. Modifying reporting rules to include AI-driven writing tools such as ChatGPT is ethically and practically challenging. In academic writing, precautions for truth, privacy, and responsibility are necessary due to concerns about biases, openness, data limits, and potential legal ramifications. The CONSORT-AI and Standard Protocol Items: Recommendations for Interventional Trials (SPIRIT)-AI Steering Group expands the CONSORT guidelines for AI clinical trials—new checklists like METRICS and CLEAR help to promote transparency in AI studies. Responsible usage of technology in research and writing software adoption requires interdisciplinary collaboration and ethical assessment. This study explores the impact of AI technologies, specifically ChatGPT, on past reporting standards and the need for revised guidelines for open, reproducible, and robust scientific publications. © 2024 The Korean Academy of Medical Sciences.
PB  - Korean Academy of Medical Science
C2  - 39164055
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - El-Dahiyat, F.
AU  - Elrefae, G.A.
AU  - Jairoun, A.A.
AU  - Al-Hemyari, S.S.
AU  - Shahwan, M.
AU  - Alshehri, F.S.
AU  - Alorfi, N.M.
TI  - Exploring the Value and Regulatory Perspectives of Artificial Intelligence ChatGPT in Pharmacoeconomic: A Qualitative Study on Benefits, Risks, and Stakeholder Beliefs
PY  - 2024
T2  - 2024 Global Digital Health Knowledge Exchange and Empowerment Conference: Knowledge Exchange of the State-of-the-Art Research and Development in Digital Health Technologies, Enable and Empower Stakeholders Engaged in Enriching and Enhancing the Patient Healthcare Journey, gDigiHealth.KEE 2024
DO  - 10.1109/gDigiHealth.KEE62309.2024.10761615
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213372908&doi=10.1109%2fgDigiHealth.KEE62309.2024.10761615&partnerID=40&md5=e61ede35585b17330df91d2675466728
AB  - Background: Growth in artificial intelligence systems can allow the automation of a crucial section of the traditional manual work practiced in pharmacoeconomics and evidence synthesis. However, artificial intelligence has a low autonomous analytical skill capabilities. The objective is to conduct a thematic analysis of the benefits and risks of applying ChatGPT across various sub-disciplines within pharmacoeconomics analysis. A purposive sampling technique was used to select the participants, utilizing the convenience sampling approach. All interviews were recorded and transcribed verbatim, and thematic analysis was applied to identify themes within the data. The result shows that most of the respondents find AI ChatGPT helpful as it increases data analysis and processing efficiency, as well as improving real-time decision making. Additionally, the participants believe that information accessibility and dissemination are appealing features of AI ChatGPT. However, the respondents identified potential drawbacks in AI ChatGPT use, such as data quality and accuracy, contextual awareness limitations, privacy and security concerns, lack of responsibility and accountability, limitations in generalizability, social and ethical concerns, uncertainty understanding limitations, and amplification of bias. Employing ChatGPT in pharmacoeconomics presents significant potential for enhancing data processing efficiency, providing real-time decision support, and improving information accessibility for healthcare stakeholders. However, these benefits come with various risks and ethical challenges, including concerns about data accuracy, contextual awareness, security, accountability, generalizability, and potential bias amplification. By putting in place robust safeguards, adhering to ethical standards, and maintaining vigilant oversight, we can effectively leverage ChatGPT's potential while maintaining the principles of responsible, equitable, and ethical healthcare decision making. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Rathi, C.
AU  - Singh, D.
AU  - Bhatnagar, A.
AU  - Ratna, S.
AU  - Ojha, M.K.
TI  - Time Series Forecasting for Stock Price Movement: Leveraging NLP for Stock Market Prediction
PY  - 2024
T2  - Lecture Notes in Networks and Systems
VL  - 1127 LNNS
SP  - 147
EP  - 158
DO  - 10.1007/978-981-97-7360-2_13
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216090752&doi=10.1007%2f978-981-97-7360-2_13&partnerID=40&md5=e1a45bb0d09de815ad64900745000e4d
AB  - This paper delves into the critical realm of stock market prediction, exploring the innovative role of Natural Language Processing (NLP). Traditionally, stock market forecasting has faced challenges due to reliance on historical data and economic indicators, often resulting in limited accuracy. NLP, however, offers a transformative approach by analysing vast volumes of textual data from sources like news articles, social media, and financial reports. Through sentiment analysis, trend identification, and company health assessment, NLP uncovers valuable insights that influence stock prices. The paper reviews existing research methodologies, integrating NLP with machine learning and deep learning models to predict market trends and sentiments. It highlights NLP's strengths in analysing unstructured data, uncovering hidden relationships, and providing real-time insights. Despite its advantages, NLP faces limitations such as biases in training data and challenges in interpreting nuanced language. The paper concludes by proposing future directions like explainable AI and incorporating alternative data sources, emphasizing NLP's potential to revolutionize stock market prediction while acknowledging the need for integrated approaches in financial analysis. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Khabaz, K.
AU  - Newman-Hung, N.J.
AU  - Kallini, J.R.
AU  - Kendal, J.
AU  - Christ, A.B.
AU  - Bernthal, N.M.
AU  - Wessel, L.E.
TI  - Assessment of Artificial Intelligence Chatbot Responses to Common Patient Questions on Bone Sarcoma
PY  - 2024
T2  - Journal of Surgical Oncology
DO  - 10.1002/jso.27966
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207899683&doi=10.1002%2fjso.27966&partnerID=40&md5=74a23ef4c0c5bd8000650c677f14e2ae
AB  - Background and Objectives: The potential impacts of artificial intelligence (AI) chatbots on care for patients with bone sarcoma is poorly understood. Elucidating potential risks and benefits would allow surgeons to define appropriate roles for these tools in clinical care. Methods: Eleven questions on bone sarcoma diagnosis, treatment, and recovery were inputted into three AI chatbots. Answers were assessed on a 5-point Likert scale for five clinical accuracy metrics: relevance to the question, balance and lack of bias, basis on established data, factual accuracy, and completeness in scope. Responses were quantitatively assessed for empathy and readability. The Patient Education Materials Assessment Tool (PEMAT) was assessed for understandability and actionability. Results: Chatbots scored highly on relevance (4.24) and balance/lack of bias (4.09) but lower on basing responses on established data (3.77), completeness (3.68), and factual accuracy (3.66). Responses generally scored well on understandability (84.30%), while actionability scores were low for questions on treatment (64.58%) and recovery (60.64%). GPT-4 exhibited the highest empathy (4.12). Readability scores averaged between 10.28 for diagnosis questions to 11.65 for recovery questions. Conclusions: While AI chatbots are promising tools, current limitations in factual accuracy and completeness, as well as concerns of inaccessibility to populations with lower health literacy, may significantly limit their clinical utility. © 2024 The Author(s). Journal of Surgical Oncology published by Wiley Periodicals LLC.
PB  - John Wiley and Sons Inc
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Burns, C.
AU  - Bakaj, A.
AU  - Berishaj, A.
AU  - Hristidis, V.
AU  - Deak, P.
AU  - Equils, O.
TI  - Use of Generative AI for Improving Health Literacy in Reproductive Health: Case Study
PY  - 2024
T2  - JMIR Formative Research
VL  - 8
C7  - e59434
DO  - 10.2196/59434
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201720007&doi=10.2196%2f59434&partnerID=40&md5=5145f4b630afa2c745d33a5811e8e41f
AB  - Background: Patients find technology tools to be more approachable for seeking sensitive health-related information, such as reproductive health information. The inventive conversational ability of artificial intelligence (AI) chatbots, such as ChatGPT (OpenAI Inc), offers a potential means for patients to effectively locate answers to their health-related questions digitally. Objective: A pilot study was conducted to compare the novel ChatGPT with the existing Google Search technology for their ability to offer accurate, effective, and current information regarding proceeding action after missing a dose of oral contraceptive pill. Methods: A sequence of 11 questions, mimicking a patient inquiring about the action to take after missing a dose of an oral contraceptive pill, were input into ChatGPT as a cascade, given the conversational ability of ChatGPT. The questions were input into 4 different ChatGPT accounts, with the account holders being of various demographics, to evaluate potential differences and biases in the responses given to different account holders. The leading question, "what should I do if I missed a day of my oral contraception birth control?" alone was then input into Google Search, given its nonconversational nature. The results from the ChatGPT questions and the Google Search results for the leading question were evaluated on their readability, accuracy, and effective delivery of information. Results: The ChatGPT results were determined to be at an overall higher-grade reading level, with a longer reading duration, less accurate, less current, and with a less effective delivery of information. In contrast, the Google Search resulting answer box and snippets were at a lower-grade reading level, shorter reading duration, more current, able to reference the origin of the information (transparent), and provided the information in various formats in addition to text. Conclusions: ChatGPT has room for improvement in accuracy, transparency, recency, and reliability before it can equitably be implemented into health care information delivery and provide the potential benefits it poses. However, AI may be used as a tool for providers to educate their patients in preferred, creative, and efficient ways, such as using AI to generate accessible short educational videos from health care provider-vetted information. Larger studies representing a diverse group of users are needed. © 2024 JMIR Publications Inc.. All rights reserved.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Lee, C.
AU  - Simpson, T.I.
AU  - Posma, J.M.
AU  - Lain, A.D.
TI  - Comparative Analyses of Multilingual Drug Entity Recognition Systems for Clinical Case Reports In Cardiology
PY  - 2024
T2  - CEUR Workshop Proceedings
VL  - 3740
SP  - 159
EP  - 167
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201634026&partnerID=40&md5=b5fba93333249d0ff853bc3c13962ee7
AB  - Performance disparities exist in Named Entity Recognition (NER) systems across languages due to variations in available human-annotated data. We participated in the MultiDrug subtask of MultiCardioNER, a shared task focusing on multilingual NER for cardiology, to compare the effectiveness of fine-tuning BERT-based monolingual and multilingual language models, and prompting Large Language Models (LLMs) for drug entity recognition across multiple languages. Our findings demonstrate that monolingual BERT models pretrained on biomedical corpora generally outperform their multilingual counterparts. However, for languages lacking access to a broader range of pretrained models, combining the translation capability of LLM [1, 2, 3, 4] with the best-performing pretrained monolingual BERT model yielded superior results. This approach effectively reduces the resource disparity while leveraging domain-specific knowledge captured by the monolingual BERT model. Our best systems in the MultiCardioNER track yielded F1-scores of 0.9277 for Spanish, 0.9107 for English, and 0.8776 for Italian. We highlight the comparative advantages of domain-specific fine-tuning and LLM-powered language translation for multilingual drug NER. © 2024 Copyright for this paper by its authors.
PB  - CEUR-WS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Chansiri, K.
AU  - Wei, X.
AU  - Chor, K.H.B.
TI  - Addressing Gender Bias: A Fundamental Approach to AI in Mental Health
PY  - 2024
T2  - 2024 5th International Conference on Big Data Analytics and Practices, IBDAP 2024
SP  - 107
EP  - 112
DO  - 10.1109/IBDAP62940.2024.10689686
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206587637&doi=10.1109%2fIBDAP62940.2024.10689686&partnerID=40&md5=5cf966e3119b2d7508885443fe298707
AB  - While gender biases in large language models (LLMs) have been identified, their nuances in mental health contexts remain under-researched but are critical for ensuring accurate and inclusive AI diagnostics. We address this gap by investigating gender biases in GPT-3.5 and GPT-4, focusing on Borderline Personality Disorder (BPD) and Narcissistic Personality, Disorder (NPD), selected for their recognized clinical biases: women with BPD and men with NPD. We explore these biases through diagnostic reasoning and clinical vignette generation tasks. Diagnostic tests reveal that both GPT-3.5 and GPT-4 exhibit biases, particularly against women, though GPT-4 shows reduced bias and improved performance. In vignette generation, both models, especially GPT-4, frequently depict women with BPD, Vignettes featuring men with NPD score higher in positive sentiment, objectivity, and readability. These results emphasize the importance of addressing gender biases in mental health AI to prevent stereotyping and misinformation.  © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Mese, I.
AU  - Kocak, B.
TI  - ChatGPT as an effective tool for quality evaluation of radiomics research
PY  - 2024
T2  - European Radiology
DO  - 10.1007/s00330-024-11122-7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207276462&doi=10.1007%2fs00330-024-11122-7&partnerID=40&md5=0d6dd36be53ad4a502d124bb7e3038c5
AB  - Objectives: This study aimed to evaluate the effectiveness of ChatGPT-4o in assessing the methodological quality of radiomics research using the radiomics quality score (RQS) compared to human experts. Methods: Published in European Radiology, European Radiology Experimental, and Insights into Imaging between 2023 and 2024, open-access and peer-reviewed radiomics research articles with creative commons attribution license (CC-BY) were included in this study. Pre-prints from MedRxiv were also included to evaluate potential peer-review bias. Using the RQS, each study was independently assessed twice by ChatGPT-4o and by two radiologists with consensus. Results: In total, 52 open-access and peer-reviewed articles were included in this study. Both ChatGPT-4o evaluation (average of two readings) and human experts had a median RQS of 14.5 (40.3% percentage score) (p > 0.05). Pairwise comparisons revealed no statistically significant difference between the readings of ChatGPT and human experts (corrected p > 0.05). The intraclass correlation coefficient for intra-rater reliability of ChatGPT-4o was 0.905 (95% CI: 0.840–0.944), and those for inter-rater reliability with human experts for each evaluation of ChatGPT-4o were 0.859 (95% CI: 0.756–0.919) and 0.914 (95% CI: 0.855–0.949), corresponding to good to excellent reliability for all. The evaluation by ChatGPT-4o took less time (2.9–3.5 min per article) compared to human experts (13.9 min per article by one reader). Item-wise reliability analysis showed ChatGPT-4o maintained consistently high reliability across almost all RQS items. Conclusion: ChatGPT-4o provides reliable and efficient assessments of radiomics research quality. Its evaluations closely align with those of human experts and reduce evaluation time. Key Points: Question Is ChatGPT effective and reliable in evaluating radiomics research quality based on RQS? Findings ChatGPT-4o showed high reliability and efficiency, with evaluations closely matching human experts. It can considerably reduce the time required for radiomics research quality assessment. Clinical relevance ChatGPT-4o offers a quick and reliable automated alternative for evaluating the quality of radiomics research, with the potential to assess radiomics research at a large scale in the future. Graphical Abstract: (Figure presented.) © The Author(s), under exclusive licence to European Society of Radiology 2024.
PB  - Springer Science and Business Media Deutschland GmbH
C2  - 39406959
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Matalon, J.
AU  - Spurzem, A.
AU  - Ahsan, S.
AU  - White, E.
AU  - Kothari, R.
AU  - Varma, M.
TI  - Reader’s digest version of scientific writing: comparative evaluation of summarization capacity between large language models and medical students in analyzing scientific writing in sleep medicine
PY  - 2024
T2  - Frontiers in Artificial Intelligence
VL  - 7
C7  - 1477535
DO  - 10.3389/frai.2024.1477535
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214283124&doi=10.3389%2ffrai.2024.1477535&partnerID=40&md5=c2716d55c09411be09900d7ec5a1c7cc
AB  - Introduction: As artificial intelligence systems like large language models (LLM) and natural language processing advance, the need to evaluate their utility within medicine and medical education grows. As medical research publications continue to grow exponentially, AI systems offer valuable opportunities to condense and synthesize information, especially in underrepresented areas such as Sleep Medicine. The present study aims to compare summarization capacity between LLM generated summaries of sleep medicine research article abstracts, to summaries generated by Medical Student (humans) and to evaluate if the research content, and literary readability summarized is retained comparably. Methods: A collection of three AI-generated and human-generated summaries of sleep medicine research article abstracts were shared with 19 study participants (medical students) attending a sleep medicine conference. Participants were blind as to which summary was human or LLM generated. After reading both human and AI-generated research summaries participants completed a 1–5 Likert scale survey on the readability of the extracted writings. Participants also answered article-specific multiple-choice questions evaluating their comprehension of the summaries, as a representation of the quality of content retained by the AI-generated summaries. Results: An independent sample t-test between the AI-generated and human-generated summaries comprehension by study participants revealed no significant difference between the Likert readability ratings (p = 0.702). A chi-squared test of proportions revealed no significant association (χ2 = 1.485, p = 0.223), and a McNemar test revealed no significant association between summary type and the proportion of correct responses to the comprehension multiple choice questions (p = 0.289). Discussion: Some limitations in this study were a small number of participants and user bias. Participants attended at a sleep conference and study summaries were all from sleep medicine journals. Lastly the summaries did not include graphs, numbers, and pictures, and thus were limited in material extraction. While the present analysis did not demonstrate a significant difference among the readability and content quality between the AI and human-generated summaries, limitations in the present study indicate that more research is needed to objectively measure, and further define strengths and weaknesses of AI models in condensing medical literature into efficient and accurate summaries. Copyright © 2024 Matalon, Spurzem, Ahsan, White, Kothari and Varma.
PB  - Frontiers Media SA
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Huertas-García, Á.
AU  - Martí-González, C.
AU  - Muñoz, J.
AU  - De Miguel Ambite, E.
TI  - Small Language Models and Large Language Models in Oppositional Thinking Analysis: Capabilities, Biases and Challenges
PY  - 2024
T2  - CEUR Workshop Proceedings
VL  - 3740
SP  - 2665
EP  - 2675
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201643653&partnerID=40&md5=36e47f95b2984c3452beb4540e13d212
AB  - The proliferation of misinformation and conspiracy theories needs robust methods to differentiate legitimate critical discourse from harmful conspiratorial narratives. This study investigates discerning critical messages from conspiracy theories within COVID-19 discussions on Telegram. Preserving information integrity on social media impacts vital public discourse on health, politics, and science. The research employs two distinct approaches: linguistic style classification and contextual knowledge classification. The former leverages a diverse ensemble of Small Language Models (SLMs), Large Language Models (LLMs), and State-Space Models (SSMs), while the latter harnesses the capabilities of the Claude 2.0 Opus model for contextual analysis. Empirical evaluations demonstrate that the SLM models using Matryoshka embedding and Mamba (SSM) models exhibit superior performance for the English language dataset, achieving a Matthews Correlation Coefficient (MCC) of 0.793. For the Spanish dataset, the Spanish BERT baseline (SLM) attains an MCC of 0.699. Notably, a multilingual model trained on a balanced combination of English and Spanish data outperforms its monolingual counterparts, with the multilingual-e5-large model (LLM) achieving an MCC of 0.768 for English and 0.725 for Spanish. This finding underscores the potential of multilingual models to mitigate the”curse of multilinguality,” where performance often degrades on low-resource languages. However, the suboptimal performance of the Claude 2.0 Opus model, exhibiting a tendency to classify texts as conspiracy-related, highlights inherent biases that require further investigation. Overall, this study contributes to the development of advanced models that can effectively differentiate critical thinking from conspiratorial narratives in various linguistic contexts. Future research should prioritize identifying and addressing biases in large language models to ensure fair treatment of diverse perspectives, as well as to preserve freedom of expression and ensure fair representation of narratives. © 2024 Copyright for this paper by its authors.
PB  - CEUR-WS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Ding, X.
AU  - Chu, Y.
AU  - Pi, R.
AU  - Wang, H.
AU  - Li, X.
TI  - HiA: Towards Chinese Multimodal LLMs for Comparative High-Resolution Joint Diagnosis
PY  - 2024
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 
VL  - 15012 LNCS
SP  - 575
EP  - 586
DO  - 10.1007/978-3-031-72390-2_54
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208179108&doi=10.1007%2f978-3-031-72390-2_54&partnerID=40&md5=5c016c6c13525529123afe8200d06609
AB  - Multimodal large language models (MLLMs) have been explored in the Chinese medical domain for comprehending complex healthcare. However, due to the flaws in training data and architecture design, current Chinese medical MLLMs suffer from several limitations: cultural biases from English machine translations, limited comparative ability from single image input and difficulty in identifying small lesions with low-resolution images. To address these problems, we first introduce a new instruction-following dataset, Chili-Joint  (Chinese Interleaved Image-Text Dataset for Joint Diagnosis) collected from the hospital in mainland China, avoiding cultural biases and errors caused by machine translation. Besides one single image input, Chili-Joint also has multiple images obtained at various intervals during a patient’s treatment, thus facilitating an evaluation of the treatment’s outcomes. We further propose a novel HiA (High-resolution instruction-aware Adapter) to incorporate high-resolutioninstruction-aware visual features into LLMs to facilitate the current MLLMs to observe the small lesions as well as the comparative analysis. Extensive experiments on Chili-Joint demonstrate our HiA can be a plug-and-play method to improve the performance of current MLLMs for medical analysis. The code is available at https://github.com/xmed-lab/HiA. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Lee, C.
AU  - Mohebbi, M.
AU  - O'Callaghan, E.
AU  - Winsberg, M.
TI  - Large Language Models Versus Expert Clinicians in Crisis Prediction Among Telemental Health Patients: Comparative Study
PY  - 2024
T2  - JMIR Mental Health
VL  - 11
C7  - e58129
DO  - 10.2196/58129
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201481822&doi=10.2196%2f58129&partnerID=40&md5=9fe0e576ab62ce31c0c3631c40d2ec35
AB  - Background: Due to recent advances in artificial intelligence, large language models (LLMs) have emerged as a powerful tool for a variety of language-related tasks, including sentiment analysis, and summarization of provider-patient interactions. However, there is limited research on these models in the area of crisis prediction. Objective: This study aimed to evaluate the performance of LLMs, specifically OpenAI’s generative pretrained transformer 4 (GPT-4), in predicting current and future mental health crisis episodes using patient-provided information at intake among users of a national telemental health platform. Methods: Deidentified patient-provided data were pulled from specific intake questions of the Brightside telehealth platform, including the chief complaint, for 140 patients who indicated suicidal ideation (SI), and another 120 patients who later indicated SI with a plan during the course of treatment. Similar data were pulled for 200 randomly selected patients, treated during the same time period, who never endorsed SI. In total, 6 senior Brightside clinicians (3 psychologists and 3 psychiatrists) were shown patients’ self-reported chief complaint and self-reported suicide attempt history but were blinded to the future course of treatment and other reported symptoms, including SI. They were asked a simple yes or no question regarding their prediction of endorsement of SI with plan, along with their confidence level about the prediction. GPT-4 was provided with similar information and asked to answer the same questions, enabling us to directly compare the performance of artificial intelligence and clinicians. Results: Overall, the clinicians’ average precision (0.7) was higher than that of GPT-4 (0.6) in identifying the SI with plan at intake (n=140) versus no SI (n=200) when using the chief complaint alone, while sensitivity was higher for the GPT-4 (0.62) than the clinicians’ average (0.53). The addition of suicide attempt history increased the clinicians’ average sensitivity (0.59) and precision (0.77) while increasing the GPT-4 sensitivity (0.59) but decreasing the GPT-4 precision (0.54). Performance decreased comparatively when predicting future SI with plan (n=120) versus no SI (n=200) with a chief complaint only for the clinicians (average sensitivity=0.4; average precision=0.59) and the GPT-4 (sensitivity=0.46; precision=0.48). The addition of suicide attempt history increased performance comparatively for the clinicians (average sensitivity=0.46; average precision=0.69) and the GPT-4 (sensitivity=0.74; precision=0.48). Conclusions: GPT-4, with a simple prompt design, produced results on some metrics that approached those of a trained clinician. Additional work must be done before such a model can be piloted in a clinical setting. The model should undergo safety checks for bias, given evidence that LLMs can perpetuate the biases of the underlying data on which they are trained. We believe that LLMs hold promise for augmenting the identification of higher-risk patients at intake and potentially delivering more timely care to patients. ©Christine Lee, Matthew Mohebbi, Erin O'Callaghan, Mirène Winsberg.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - He, Q.
AU  - Wang, J.
AU  - He, D.
TI  - The Influence of Task and Group Disparities Over Users’ Attitudes Toward Using Large Language Models for Psychotherapy
PY  - 2024
T2  - Proceedings of the Human Factors and Ergonomics Society
VL  - 68
IS  - 1
SP  - 1147
EP  - 1152
DO  - 10.1177/10711813241268507
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214848679&doi=10.1177%2f10711813241268507&partnerID=40&md5=934f45db0449fa3300a58b76a76e24cb
AB  - The population suffering from mental health disorders has kept increasing in recent years. With the advancements in large language models (LLMs) in diverse fields, LLM-based psychotherapy has also attracted increasingly more attention. However, the factors influencing users’ attitudes to LLM-based psychotherapy have rarely been explored. As the first attempt, this paper investigated the influence of task and group disparities on user attitudes toward LLM-based psychotherapy tools. Utilizing the Technology Acceptance Model (TAM) and Automation Acceptance Model (AAM), based on an online survey, we collected and analyzed responses from 222 LLM-based psychotherapy users in mainland China. The results revealed that group disparity (i.e., mental health conditions) can influence users’ attitudes toward LLM tools. Further, one of the typical task disparities, that is, the privacy concern, was not found to have a significant effect on trust and usage intention. These findings can guide the design of future LLM-based psychotherapy services. © 2024 Human Factors and Ergonomics Society.
PB  - SAGE Publications Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Wasi, A.T.
AU  - Islam, R.
TI  - CogErgLLM: Exploring Large Language Model Systems Design Perspective Using Cognitive Ergonomics
PY  - 2024
T2  - NLP4Science 2024 - 1st Workshop on NLP for Science, Proceedings of the Workshop
SP  - 249
EP  - 258
DO  - 10.18653/v1/2024.nlp4science-1.22
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213130795&doi=10.18653%2fv1%2f2024.nlp4science-1.22&partnerID=40&md5=fcd9154be35451c3a81b3987aebcbc42
AB  - Integrating cognitive ergonomics with LLMs is crucial for improving safety, reliability, and user satisfaction in human-AI interactions. Current LLM designs often lack this integration, resulting in systems that may not fully align with human cognitive capabilities and limitations. This oversight exacerbates biases in LLM outputs and leads to suboptimal user experiences due to inconsistent application of user-centered design principles. Researchers are increasingly leveraging NLP, particularly LLMs, to model and understand human behavior across social sciences, psychology, psychiatry, health, and neuroscience. Our position paper explores the need to integrate cognitive ergonomics into LLM design, providing a comprehensive framework and practical guidelines for ethical development. By addressing these challenges, we aim to advance safer, more reliable, and ethically sound human-AI interactions. © 2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Jha, A.
AU  - Mann, P.
AU  - Tiwari, A.
AU  - Kadian, K.
AU  - Sharma, A.
TI  - Decoding Ethics: Proficiency of LLMs in Addressing Moral Dilemmas
PY  - 2024
T2  - Lecture Notes in Electrical Engineering
VL  - 1195 LNEE
SP  - 593
EP  - 605
DO  - 10.1007/978-981-97-3442-9_41
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208174577&doi=10.1007%2f978-981-97-3442-9_41&partnerID=40&md5=da072422e7502ce41fc70b64cce19379
AB  - In the dynamic global digital landscape, the mainstream use of Artificial Intelligence led us into a transformative era of technical ubiquity. Pioneering this movement, are advanced Artificial Intelligence models like LLMs. Large Language Models (LLMs) are Machine Learning/Deep Learning models that show remarkable abilities to generate human-like text. They are trained on vast amounts of data as a result of which they imbibe the nuances of human conversation up to great accuracies. They are mainly used for content generation, along with other natural language processing tasks. From medicine to education, LLMs find applications in nearly all fields of society. However, several ethical and moral implications need to be taken into account to ensure the responsible use of LLMs. Our work explores the performance of ChatGPT-3.5 Turbo, a text-generative Large Language Model, in the understanding and comprehension of questions entailing moral and ethical dilemmas using accuracy as the key evaluation metric. Ethical considerations in terms of biases, transparency, reliability, and accuracy need to be analyzed to ensure the responsible use of LLM technology. There is currently a limited understanding of LLM reasoning processes and ability of handle real-world, ethical dilemmas. Through this work, we aim to assess the decision-making capabilities of LLMs in terms of moral scenarios and contribute to the establishment of ethical norms for the standardized use of Large Language Models, making them more secure and accessible. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Abaho, M.
AU  - Bollegala, D.
AU  - Leeming, G.
AU  - Joyce, D.
AU  - Buchan, I.E.
TI  - Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER
PY  - 2024
T2  - Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024
VL  - 1
SP  - 5013
EP  - 5029
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199565371&partnerID=40&md5=773f4ea3a7ebab99d6e083cd31b1e19c
AB  - Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for inaccurately predicting DS-terms compared to generic words. Results of our analysis show that MSLM improves LMs sensitivity and detection of DS-terms. We empirically show that an optimal masking rate not only depends on the LM, but also on the dataset and the length of sequences. Our proposed masking strategy outperforms advanced masking strategies such as span- and PMI-based masking. ©2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Elyoseph, Z.
AU  - Refoua, E.
AU  - Asraf, K.
AU  - Lvovsky, M.
AU  - Shimoni, Y.
AU  - Hadar-Shoval, D.
TI  - Capacity of Generative AI to Interpret Human Emotions From Visual and Textual Data: Pilot Evaluation Study
PY  - 2024
T2  - JMIR Mental Health
VL  - 11
IS  - 1
C7  - e54369
DO  - 10.2196/54369
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186111328&doi=10.2196%2f54369&partnerID=40&md5=909a728412153e28ad580a1d2a723abd
AB  - Background: Mentalization, which is integral to human cognitive processes, pertains to the interpretation of one’s own and others’ mental states, including emotions, beliefs, and intentions. With the advent of artificial intelligence (AI) and the prominence of large language models in mental health applications, questions persist about their aptitude in emotional comprehension. The prior iteration of the large language model from OpenAI, ChatGPT-3.5, demonstrated an advanced capacity to interpret emotions from textual data, surpassing human benchmarks. Given the introduction of ChatGPT-4, with its enhanced visual processing capabilities, and considering Google Bard’s existing visual functionalities, a rigorous assessment of their proficiency in visual mentalizing is warranted. Objective: The aim of the research was to critically evaluate the capabilities of ChatGPT-4 and Google Bard with regard to their competence in discerning visual mentalizing indicators as contrasted with their textual-based mentalizing abilities. Methods: The Reading the Mind in the Eyes Test developed by Baron-Cohen and colleagues was used to assess the models’ proficiency in interpreting visual emotional indicators. Simultaneously, the Levels of Emotional Awareness Scale was used to evaluate the large language models’ aptitude in textual mentalizing. Collating data from both tests provided a holistic view of the mentalizing capabilities of ChatGPT-4 and Bard. Results: ChatGPT-4, displaying a pronounced ability in emotion recognition, secured scores of 26 and 27 in 2 distinct evaluations, significantly deviating from a random response paradigm (P<.001). These scores align with established benchmarks from the broader human demographic. Notably, ChatGPT-4 exhibited consistent responses, with no discernible biases pertaining to the sex of the model or the nature of the emotion. In contrast, Google Bard’s performance aligned with random response patterns, securing scores of 10 and 12 and rendering further detailed analysis redundant. In the domain of textual analysis, both ChatGPT and Bard surpassed established benchmarks from the general population, with their performances being remarkably congruent. Conclusions: ChatGPT-4 proved its efficacy in the domain of visual mentalizing, aligning closely with human performance standards. Although both models displayed commendable acumen in textual emotion interpretation, Bard’s capabilities in visual emotion interpretation necessitate further scrutiny and potential refinement. This study stresses the criticality of ethical AI development for emotional recognition, highlighting the need for inclusive data, collaboration with patients and mental health experts, and stringent governmental oversight to ensure transparency and protect patient privacy. © Zohar Elyoseph, Elad Refoua, Kfir Asraf, Maya Lvovsky, Yoav Shimoni, Dorit Hadar-Shoval.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 35
ER  -

TY  - JOUR
AU  - van Osta, J.M.
AU  - Dreis, B.
AU  - Meyer, E.
AU  - Grogan, L.F.
AU  - Castley, J.G.
TI  - An active learning framework and assessment of inter-annotator agreement facilitate automated recogniser development for vocalisations of a rare species, the southern black-throated finch (Poephila cincta cincta)
PY  - 2023
T2  - Ecological Informatics
VL  - 77
C7  - 102233
DO  - 10.1016/j.ecoinf.2023.102233
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166324255&doi=10.1016%2fj.ecoinf.2023.102233&partnerID=40&md5=5c12fb84ebe757fd2b93a017edd4b4cd
AB  - The application of machine learning methods has led to major advances in the development of automated recognisers used to analyse bioacoustics data. To further improve the performance of automated call recognisers, we investigated the development of efficient data annotation strategies and how best to address uncertainty around ambiguous vocalisations. These challenges present a particular problem for species whose vocalisations are rare in field recordings, where collecting enough training data can be problematic and a species' vocalisations may be poorly documented. We provide an open access solution to address these challenges using two strategies. First, we applied an active learning framework to iteratively improve a convolutional neural network (CNN) model able to automate call identification for a target rare bird species, the southern black-throated finch (Poephila cincta cincta). We collected 9098 h of unlabelled audio recordings from a field study in the Desert Uplands Bioregion of Queensland, Australia, and used active learning to prioritise human annotation effort towards data that would best improve model fit. Second, we progressed methods for managing ambiguous vocalisations by applying machine learning methods more commonly used in medical image analysis and natural language processing. Specifically, we assessed agreement among human annotators and the CNN model (i.e. inter-annotator agreement) and used this to determine realistic performance outcomes for the CNN model and to identify areas where inter-annotator agreement may be improved. We also applied a classification approach that allowed the CNN model to classify sounds into an ‘uncertain’ category, which replicated a requirement of human-annotation and facilitated the comparison of human-model annotation performance. We found that active learning was an efficient strategy to build a CNN model where there was limited labelled training data available, and target calls were extremely rare in the unlabelled data. As few as five active learning iterations, generating a final labelled dataset of 1073 target calls and 5786 non-target sounds, were required to train a model to identify the target species with comparable performance to experts in the field. Assessment of inter-annotator agreement identified a bias in our model to align predictions most closely with those of the primary annotator and identified significant differences in inter-annotator agreement among subsets of our acoustic data. Our results highlight the use of inter-annotator agreement to understand model performance and identify areas for improvement in data annotation. We also show that excluding ambiguous vocalisations during data annotation results in an overestimation of model performance, an important consideration for datasets with inter-annotator disagreement. © 2023 The Authors
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Bradshaw, R.L.
AU  - Kawamoto, K.
AU  - Bather, J.R.
AU  - Goodman, M.S.
AU  - Kohlmann, W.K.
AU  - Chavez-Yenter, D.
AU  - Volkmar, M.
AU  - Monahan, R.
AU  - Kaphingst, K.A.
AU  - Del Fiol, G.
TI  - Enhanced family history-based algorithms increase the identification of individuals meeting criteria for genetic testing of hereditary cancer syndromes but would not reduce disparities on their own
PY  - 2024
T2  - Journal of Biomedical Informatics
VL  - 149
C7  - 104568
DO  - 10.1016/j.jbi.2023.104568
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179852630&doi=10.1016%2fj.jbi.2023.104568&partnerID=40&md5=62c509a66a3d8268408f12697a77cc7b
AB  - Objective: This study aimed to 1) investigate algorithm enhancements for identifying patients eligible for genetic testing of hereditary cancer syndromes using family history data from electronic health records (EHRs); and 2) assess their impact on relative differences across sex, race, ethnicity, and language preference. Materials and Methods: The study used EHR data from a tertiary academic medical center. A baseline rule-base algorithm, relying on structured family history data (structured data; SD), was enhanced using a natural language processing (NLP) component and a relaxed criteria algorithm (partial match [PM]). The identification rates and differences were analyzed considering sex, race, ethnicity, and language preference. Results: Among 120,007 patients aged 25–60, detection rate differences were found across all groups using the SD (all P < 0.001). Both enhancements increased identification rates; NLP led to a 1.9 % increase and the relaxed criteria algorithm (PM) led to an 18.5 % increase (both P < 0.001). Combining SD with NLP and PM yielded a 20.4 % increase (P < 0.001). Similar increases were observed within subgroups. Relative differences persisted across most categories for the enhanced algorithms, with disproportionately higher identification of patients who are White, Female, non-Hispanic, and whose preferred language is English. Conclusion: Algorithm enhancements increased identification rates for patients eligible for genetic testing of hereditary cancer syndromes, regardless of sex, race, ethnicity, and language preference. However, differences in identification rates persisted, emphasizing the need for additional strategies to reduce disparities such as addressing underlying biases in EHR family health information and selectively applying algorithm enhancements for disadvantaged populations. Systematic assessment of differences in algorithm performance across population subgroups should be incorporated into algorithm development processes. © 2023 The Authors
PB  - Academic Press Inc.
C2  - 38081564
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Stewart, J.
AU  - Lu, J.
AU  - Goudie, A.
AU  - Arendts, G.
AU  - Meka, S.A.
AU  - Freeman, S.
AU  - Walker, K.
AU  - Sprivulis, P.
AU  - Sanfilippo, F.
AU  - Bennamoun, M.
AU  - Dwivedi, G.
TI  - Applications of natural language processing at emergency department triage: A narrative review
PY  - 2023
T2  - PLoS ONE
VL  - 18
IS  - 12 December
C7  - e0279953
DO  - 10.1371/journal.pone.0279953
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179766639&doi=10.1371%2fjournal.pone.0279953&partnerID=40&md5=d71c19298cf16f6be572d748aee8aa38
AB  - Introduction Natural language processing (NLP) uses various computational methods to analyse and understand human language, and has been applied to data acquired at Emergency Department (ED) triage to predict various outcomes. The objective of this scoping review is to evaluate how NLP has been applied to data acquired at ED triage, assess if NLP based models outperform humans or current risk stratification techniques when predicting outcomes, and assess if incorporating free-text improve predictive performance of models when compared to predictive models that use only structured data. Methods All English language peer-reviewed research that applied an NLP technique to free-text obtained at ED triage was eligible for inclusion. We excluded studies focusing solely on disease surveillance, and studies that used information obtained after triage. We searched the electronic databases MEDLINE, Embase, Cochrane Database of Systematic Reviews, Web of Science, and Scopus for medical subject headings and text keywords related to NLP and triage. Databases were last searched on 01/01/2022. Risk of bias in studies was assessed using the Prediction model Risk of Bias Assessment Tool (PROBAST). Due to the high level of heterogeneity between studies and high risk of bias, a metanalysis was not conducted. Instead, a narrative synthesis is provided. Results In total, 3730 studies were screened, and 20 studies were included. The population size varied greatly between studies ranging from 1.8 million patients to 598 triage notes. The most common outcomes assessed were prediction of triage score, prediction of admission, and prediction of critical illness. NLP models achieved high accuracy in predicting need for admission, triage score, critical illness, and mapping free-text chief complaints to structured fields. Incorporating both structured data and free-text data improved results when compared to models that used only structured data. However, the majority of studies (80%) were assessed to have a high risk of bias, and only one study reported the deployment of an NLP model into clinical practice. Conclusion Unstructured free-text triage notes have been used by NLP models to predict clinically relevant outcomes. However, the majority of studies have a high risk of bias, most research is retrospective, and there are few examples of implementation into clinical practice. Future work is needed to prospectively assess if applying NLP to data acquired at ED triage improves ED outcomes when compared to usual clinical practice.  © 2023 Stewart et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
PB  - Public Library of Science
C2  - 38096321
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 14
ER  -

TY  - JOUR
AU  - Foriest, J.C.
AU  - Mittal, S.
AU  - Kim, E.
AU  - Carmichael, A.
AU  - Lennon, N.
AU  - Sumner, S.A.
AU  - De Choudhury, M.
TI  - News Media Framing of Suicide Circumstances and Gender: Mixed Methods Analysis
PY  - 2024
T2  - JMIR Mental Health
VL  - 11
C7  - e49879
DO  - 10.2196/49879
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199371801&doi=10.2196%2f49879&partnerID=40&md5=1d111282ce5642bcb34da4fc421648ea
AB  - Background: Suicide is a leading cause of death worldwide. Journalistic reporting guidelines were created to curb the impact of unsafe reporting; however, how suicide is framed in news reports may differ by important characteristics such as the circumstances and the decedent’s gender. Objective: This study aimed to examine the degree to which news media reports of suicides are framed using stigmatized or glorified language and differences in such framing by gender and circumstance of suicide. Methods: We analyzed 200 news articles regarding suicides and applied the validated Stigma of Suicide Scale to identify stigmatized and glorified language. We assessed linguistic similarity with 2 widely used metrics, cosine similarity and mutual information scores, using a machine learning–based large language model. Results: News reports of male suicides were framed more similarly to stigmatizing (P<.001) and glorifying (P=.005) language than reports of female suicides. Considering the circumstances of suicide, mutual information scores indicated that differences in the use of stigmatizing or glorifying language by gender were most pronounced for articles attributing legal (0.155), relationship (0.268), or mental health problems (0.251) as the cause. Conclusions: Linguistic differences, by gender, in stigmatizing or glorifying language when reporting suicide may exacerbate suicide disparities. ©Jasmine C Foriest, Shravika Mittal, Eugenia Kim, Andrea Carmichael, Natalie Lennon, Steven A Sumner, Munmun De Choudhury. Originally published in JMIR Mental Health.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Romano, M.F.
AU  - Shih, L.C.
AU  - Paschalidis, I.C.
AU  - Au, R.
AU  - Kolachalama, V.B.
TI  - Large Language Models in Neurology Research and Future Practice
PY  - 2023
T2  - Neurology
VL  - 101
IS  - 23
SP  - 1058
EP  - 1067
DO  - 10.1212/WNL.0000000000207967
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178668953&doi=10.1212%2fWNL.0000000000207967&partnerID=40&md5=9d703cf41f346807004a4c0416c7391a
AB  - Recent advancements in generative artificial intelligence, particularly using large language models (LLMs), are gaining increased public attention. We provide a perspective on the potential of LLMs to analyze enormous amounts of data from medical records and gain insights on specific topics in neurology. In addition, we explore use cases for LLMs, such as early diagnosis, supporting patient and caregivers, and acting as an assistant for clinicians. We point to the potential ethical and technical challenges raised by LLMs, such as concerns about privacy and data security, potential biases in the data for model training, and the need for careful validation of results. Researchers must consider these challenges and take steps to address them to ensure that their work is conducted in a safe and responsible manner. Despite these challenges, LLMs offer promising opportunities for improving care and treatment of various neurologic disorders.  © American Academy of Neurology.
PB  - Lippincott Williams and Wilkins
C2  - 37816646
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 22
ER  -

TY  - JOUR
AU  - Tchachoua Jiembou, G.
AU  - Nda, H.A.
AU  - Konan, M.L.
TI  - Evaluation of lordosis recovery after lumbar arthrodesis and its clinical impact
PY  - 2023
T2  - Chinese Neurosurgical Journal
VL  - 9
IS  - 1
C7  - 18
DO  - 10.1186/s41016-023-00333-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163691311&doi=10.1186%2fs41016-023-00333-4&partnerID=40&md5=8976273fe70d84f51513171be095332f
AB  - Background: Posterior lumbar arthrodesis has become a widely used therapeutic option to correct sagittal imbalances in patients suffering from degenerative lumbar conditions. However, in western Africa, there is no study have reported long-term outcome of posterior lumbar arthrodesis. The aim of this study was to investigate the relationship between the restoration of adequate lordosis and the patient’s postoperative quality of life. Method: The study was retrospective. From January 2012 to December 2019, 80 patients who underwent posterior lumbar arthrodesis for lumbar degenerative diseases were included with a mean follow-up of 43.2 months. Mean age was 50.8 years (SD = 12.2). Preoperative and postoperative patients’ symptoms were assessed by the visual analog scale (VAS), Oswestry Disability Index (ODI), and 12-item Short Form (SF-12). Pre- and post-operative radiographic evaluation included lumbar lordosis measured (LLm), pelvic incidence (PI), sacral slope (SS), and pelvic stilt (PS). Theoretical lumbar lordosis (LLt) was defined by the following: LL = 0.54 × PI + 27.6. Data analysis was done using the statistical software “R.” The risk of error was 5% (p < 0.05). Result: The mean pelvic incidence was 57.23°. There was no statistically significant difference between preoperative and postoperative lumbar lordosis (p = 0.2567). There was no statistical difference between preoperative and postoperative PI-LL (p = 0.179). There was a statistically significant difference between the pre and postoperative clinical scores (p < 0.001). Statistical analysis showed a correlation between recovery of lumbar lordosis and improvement in physical component of SF-12 (PCS) (p < 0.05) and lumbar and radicular VAS (p < 0.05) for the subgroup of narrow lumbar spine. There was a statistical relationship between the restoration of lumbar lordosis and improvement in PCS (p = 0.004) and VAS (p = 0.003) for the subgroup of isthmic lysis spondylolisthesis. Discussion: The root decompression performed in most patients could explain the clinical improvement regardless of recovery of lordosis. The failure to consider spinal parameters and sagittal balance of patients in the surgery could explain no restoration of lumbar lordosis. Our study had limitations inherent to its retrospective character such as the classic selection bias. Conclusion: Satisfactory correction of spinopelvic alignment may improve long-term clinical signs. © 2023, The Author(s).
PB  - BioMed Central Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Garg, R.
AU  - Gupta, A.
TI  - A Systematic Review of NLP Applications in Clinical Healthcare: Advancement and Challenges
PY  - 2024
T2  - Lecture Notes in Networks and Systems
VL  - 892
SP  - 31
EP  - 44
DO  - 10.1007/978-981-99-9521-9_3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187653640&doi=10.1007%2f978-981-99-9521-9_3&partnerID=40&md5=39bece828a56959b02f33868a61146dc
AB  - This systematic literature review examines the advancements and challenges of natural language processing applications in clinical healthcare. Authors provide an overview of NLP applications, including clinical text classification, named entity recognition, information extraction, clinical dialogue systems, and clinical decision support. These applications have improved clinical documentation, patient care, and research outcomes. Authors critically evaluate challenges such as data privacy, lack of standardized datasets, and domain-specific language models. Ethical considerations, interoperability, and potential biases in NLP algorithms are also discussed. This review highlights the current state of NLP in clinical healthcare, identifies areas for improvement, and suggests future research directions. By synthesizing existing literature, this paper contributes to a deeper understanding of NLP’s potential in transforming clinical practice. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Agarwal, H.
AU  - Keselj, V.
TI  - Common N-Gram Method (CNG): A Promising Approach to Detecting Mental Health Disorders on Social Media
PY  - 2024
T2  - 2024 23rd International Symposium INFOTEH-JAHORINA, INFOTEH 2024 - Proceedings
DO  - 10.1109/INFOTEH60418.2024.10495942
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192184576&doi=10.1109%2fINFOTEH60418.2024.10495942&partnerID=40&md5=453fbc11d0f97f01be4e0aec513d2466
AB  - Technology and social media's growing prevalence, especially during the COVID-19 pandemic, has contributed to a rise in mental health issues. This research discusses novel applications of natural language processing which can help develop more effective and accessible diagnostic tools for mental health illnesses. To enhance the realism of our model, we created a biased dataset that reflects the real-world ratios of mental illness prevalence. The proposed solution is the Common N-grams (CNG) method that offers comparable results to the state-of-the-art CNN-LSTM model and is less resource-intensive. The CNG method performs better than the CNN-LSTM model and Support Vector Machine (SVM), baseline model, in multiclassification tasks. The CNN-LSTM surpasses performance in binary tasks compared to the best score reported in the previous study with the same dataset. The study also highlights the usefulness of the Relative N-Gram Signature method to analyze the classification decision of the CNG technique. The proposed solutions offer practical and accessible options for individuals seeking reliable and accurate mental health support. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - van Leeuwen, J.R.
AU  - Penne, L.
AU  - Rabelink, T.
AU  - Knevel, R.
AU  - Teng, Y.K.O.
TI  - Using an artificial intelligence tool incorporating natural language processing to identify patients with a diagnosis of ANCA-associated vasculitis in electronic health records
PY  - 2024
T2  - Computers in Biology and Medicine
VL  - 168
C7  - 107757
DO  - 10.1016/j.compbiomed.2023.107757
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178143191&doi=10.1016%2fj.compbiomed.2023.107757&partnerID=40&md5=5044e3272662144415910f6ede464e1f
AB  - Background: Because anti-neutrophil cytoplasmatic antibody (ANCA)-associated vasculitis (AAV) is a rare, life-threatening, auto-immune disease, conducting research is difficult but essential. A long-lasting challenge is to identify rare AAV patients within the electronic-health-record (EHR)-system to facilitate real-world research. Artificial intelligence (AI)-search tools using natural language processing (NLP) for text-mining are increasingly postulated as a solution. Methods: We employed an AI-tool that combined text-mining with NLP-based exclusion, to accurately identify rare AAV patients within large EHR-systems (>2.000.000 records). We developed an identification method in an academic center with an established AAV-training set (n = 203) and validated the method in a non-academic center with an AAV-validation set (n = 84). To assess accuracy anonymized patient records were manually reviewed. Results: Based on an iterative process, a text-mining search was developed on disease description, laboratory measurements, medication and specialisms. In the training center, 608 patients were identified with a sensitivity of 97.0 % (95%CI [93.7, 98.9]) and positive predictive value (PPV) of 56.9 % (95%CI [52.9, 60.1]). NLP-based exclusion resulted in 444 patients increasing PPV to 77.9 % (95%CI [73.7, 81.7]) while sensitivity remained 96.3 % (95%CI [93.8, 98.0]). In the validation center, text-mining identified 333 patients (sensitivity 97.6 % (95%CI [91.6, 99.7]), PPV 58.2 % (95%CI [52.8, 63.6])) and NLP-based exclusion resulted in 223 patients, increasing PPV to 86.1 % (95%CI [80.9, 90.4]) with 98.0 % (95%CI [94.9, 99.4]) sensitivity. Our identification method outperformed ICD-10-coding predominantly in identifying MPO+ and organ-limited AAV patients. Conclusions: Our study highlights the advantages of implementing AI, notably NLP, to accurately identify rare AAV patients within large EHR-systems and demonstrates the applicability and transportability. Therefore, this method can reduce efforts to identify AAV patients and accelerate real-world research, while avoiding bias by ICD-10-coding. © 2023
PB  - Elsevier Ltd
C2  - 38039893
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - CONF
AU  - Pears, M.
AU  - Poussa, C.
AU  - Konstantinidis, S.T.
TI  - Progressive Healthcare Pedagogy: An Application Merging ChatGPT and AI-Video Technologies for Gamified and Cost-Effective Scenario-Based Learning
PY  - 2024
T2  - Lecture Notes in Networks and Systems
VL  - 937 LNNS
SP  - 106
EP  - 113
DO  - 10.1007/978-3-031-56075-0_10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189752628&doi=10.1007%2f978-3-031-56075-0_10&partnerID=40&md5=cf32536ab3f43e5c79134c5866b9c8e6
AB  - Healthcare education faces numerous challenges in meeting the expanding needs of students while providing personalized learning experiences. Artificial Intelligence (AI) technologies, specifically Large Language Models (LLMs), have emerged as promising solutions to address these challenges. However, the gap between technological advancements and practical implementation remains a significant bottleneck in AI integration. This paper presents an exploration of the practical implementation of AI in healthcare education, focusing on user-friendly, controllable, and transparent AI tools. The study reviews existing literature on AI in healthcare education, emphasizing the potential of LLMs but also addressing challenges, such as bias and fairness. A methodology section describes a serious game-based workshop that leveraged AI tools including ChatGPT-4 to simulate dynamic healthcare scenarios and foster user engagement. Results demonstrate the efficacy and adaptability of AI-driven applications in healthcare education, highlighting their potential as cost-effective learning resources. The paper discusses the implications of AI implementation, including its capacity to transform traditional educational methods, promote curiosity, and foster trust. Ultimately, this paper aims to inspire foster innovation and inform best practices for the practical integration of AI in healthcare education, bridging the gap between theoretical complexity and real-world application. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Young, M.
AU  - Holmes, N.E.
AU  - Kishore, K.
AU  - Amjad, S.
AU  - Gaca, M.
AU  - Serpa Neto, A.
AU  - Reade, M.C.
AU  - Bellomo, R.
TI  - Natural language processing diagnosed behavioural disturbance phenotypes in the intensive care unit: characteristics, prevalence, trajectory, treatment, and outcomes
PY  - 2023
T2  - Critical Care
VL  - 27
IS  - 1
C7  - 425
DO  - 10.1186/s13054-023-04695-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175829774&doi=10.1186%2fs13054-023-04695-0&partnerID=40&md5=607fd1bd80cefb03f6b82b3a05edd989
AB  - Background: Natural language processing (NLP) may help evaluate the characteristics, prevalence, trajectory, treatment, and outcomes of behavioural disturbance phenotypes in critically ill patients. Methods: We obtained electronic clinical notes, demographic information, outcomes, and treatment data from three medical-surgical ICUs. Using NLP, we screened for behavioural disturbance phenotypes based on words suggestive of an agitated state, a non-agitated state, or a combination of both. Results: We studied 2931 patients. Of these, 225 (7.7%) were NLP-Dx-BD positive for the agitated phenotype, 544 (18.6%) for the non-agitated phenotype and 667 (22.7%) for the combined phenotype. Patients with these phenotypes carried multiple clinical baseline differences. On time-dependent multivariable analysis to compensate for immortal time bias and after adjustment for key outcome predictors, agitated phenotype patients were more likely to receive antipsychotic medications (odds ratio [OR] 1.84, 1.35–2.51, p < 0.001) compared to non-agitated phenotype patients but not compared to combined phenotype patients (OR 1.27, 0.86–1.89, p = 0.229). Moreover, agitated phenotype patients were more likely to die than other phenotypes patients (OR 1.57, 1.10–2.25, p = 0.012 vs non-agitated phenotype; OR 4.61, 2.14–9.90, p < 0.001 vs. combined phenotype). This association was strongest in patients receiving mechanical ventilation when compared with the combined phenotype (OR 7.03, 2.07–23.79, p = 0.002). A similar increased risk was also seen for patients with the non-agitated phenotype compared with the combined phenotype (OR 6.10, 1.80–20.64, p = 0.004). Conclusions: NLP-Dx-BD screening enabled identification of three behavioural disturbance phenotypes with different characteristics, prevalence, trajectory, treatment, and outcome. Such phenotype identification appears relevant to prognostication and trial design. Graphical abstract: [Figure not available: see fulltext.]. © 2023, The Author(s).
PB  - BioMed Central Ltd
C2  - 37925406
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Chakraborty, M.
AU  - Parida, P.
TI  - Comprehensive Study of AI-Driven Market Forecasting Models and Their Applicability
PY  - 2024
T2  - Proceedings of 2nd International Conference on Advancements in Smart, Secure and Intelligent Computing, ASSIC 2024
DO  - 10.1109/ASSIC60049.2024.10508015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192980642&doi=10.1109%2fASSIC60049.2024.10508015&partnerID=40&md5=d9a4d6933a2c0b22aa321390f7e8485f
AB  - Accurate market forecasting is essential in today's fast-paced and intensely competitive business environment, as it helps direct strategic decision-making and ensure maximum performance for businesses. The practice of market forecasting has been fundamentally altered by the development of Artificial Intelligence (AI), a game-changing technology that appeared recently. This in-depth research investigates the varied landscape of AI-driven market forecasting models, analyzing their techniques, strengths, and limits, as well as their applicability across a variety of business sectors. The first part of the research explains the relevance of accurate market forecasting and the limitations faced by conventional approaches in the face of complex and fast-changing market dynamics. This sets the stage for the rest of the study, which focuses on how to improve established methods. After that, it goes into the fundamental ideas that underpin artificial intelligence, covering topics such as machine learning, deep learning, natural language processing, and ensemble approaches. These ideas provide the foundation for today's artificial intelligence-driven forecasting models, which give businesses the ability to tap into the potential of large data and generate valuable insights from that data. Following this, a comprehensive study of AI-driven techniques for forecasting will be presented. These approaches will include time series analysis, sentiment analysis, market sentiment aggregation, and predictive modeling. Case examples illustrate the use of these methodologies in a variety of fields, including but not limited to the financial industry, the e-commerce industry, the energy industry, and the healthcare industry. The research also digs into the ethical issues that surround the use of AI for market forecasting, with an emphasis on transparency, the reduction of bias, and responsible data use. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Giunti, G.
AU  - Doherty, C.P.
TI  - Cocreating an Automated mHealth Apps Systematic Review Process With Generative AI: Design Science Research Approach
PY  - 2024
T2  - JMIR Medical Education
VL  - 10
IS  - 1
C7  - e48949
DO  - 10.2196/48949
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186090133&doi=10.2196%2f48949&partnerID=40&md5=0db4061ab901d60f2a9ef1a2a662cd9b
AB  - Background: The use of mobile devices for delivering health-related services (mobile health [mHealth]) has rapidly increased, leading to a demand for summarizing the state of the art and practice through systematic reviews. However, the systematic review process is a resource-intensive and time-consuming process. Generative artificial intelligence (AI) has emerged as a potential solution to automate tedious tasks. Objective: This study aimed to explore the feasibility of using generative AI tools to automate time-consuming and resource-intensive tasks in a systematic review process and assess the scope and limitations of using such tools. Methods: We used the design science research methodology. The solution proposed is to use cocreation with a generative AI, such as ChatGPT, to produce software code that automates the process of conducting systematic reviews. Results: A triggering prompt was generated, and assistance from the generative AI was used to guide the steps toward developing, executing, and debugging a Python script. Errors in code were solved through conversational exchange with ChatGPT, and a tentative script was created. The code pulled the mHealth solutions from the Google Play Store and searched their descriptions for keywords that hinted toward evidence base. The results were exported to a CSV file, which was compared to the initial outputs of other similar systematic review processes. Conclusions: This study demonstrates the potential of using generative AI to automate the time-consuming process of conducting systematic reviews of mHealth apps. This approach could be particularly useful for researchers with limited coding skills. However, the study has limitations related to the design science research methodology, subjectivity bias, and the quality of the search results used to train the language model.  ©Guido Giunti, Colin P Doherty.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Ni, Z.
AU  - Qian, Y.
AU  - Vaillant, P.
AU  - Jaulent, M.-C.
AU  - Bousquet, C.
TI  - Assessing ChatGPT's Performance in Health Fact-Checking: Performance, Biases, and Risks
PY  - 2024
T2  - Communications in Computer and Information Science
VL  - 1957 CCIS
SP  - 403
EP  - 408
DO  - 10.1007/978-3-031-49212-9_50
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180150795&doi=10.1007%2f978-3-031-49212-9_50&partnerID=40&md5=4285628185cc2b63f2849b89780e8e21
AB  - The increasing use of ChatGPT by the general public has prompted us to assess ChatGPT's performance in health fact-checking and uncover potential biases and risks arising from its utilization. In this study, we employed two publicly accessible datasets to evaluate ChatGPT's performance. We utilized BERTopic for clustering health claims into topics and subsequently employed the gpt-3.5-turbo API for fact-checking these claims. ChatGPT's performance was appraised on multi-class (False, Mixture, Mostly-False, Mostly-True, True) and binary (True, False) levels, with a thorough analysis of its performance across various topics. ChatGPT achieved a F1-score of 0.54 and 0.64 in the multi-class task and 0.88 and 0.85 in the binary task on the two datasets, respectively. In most health topics (e.g., vaccines, Covid-19), ChatGPT's F1-score exceeded 0.8, except for specific topics, such as novel or contentious cancer treatments, which yielded a F1-score below 0.6. We scrutinized the erroneous fact-checking labels and explanations provided by ChatGPT, revealing that it may produce inaccurate results for claims with misleading intent, inaccurate information, emerging research findings, or contentious health knowledge. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Hadar-Shoval, D.
AU  - Asraf, K.
AU  - Mizrachi, Y.
AU  - Haber, Y.
AU  - Elyoseph, Z.
TI  - Assessing the Alignment of Large Language Models With Human Values for Mental Health Integration: Cross-Sectional Study Using Schwartz’s Theory of Basic Values
PY  - 2024
T2  - JMIR Mental Health
VL  - 11
IS  - 1
C7  - e55988
DO  - 10.2196/55988
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186066322&doi=10.2196%2f55988&partnerID=40&md5=be009cbf68741dd52c59cb472b3d707c
AB  - Background: Large language models (LLMs) hold potential for mental health applications. However, their opaque alignment processes may embed biases that shape problematic perspectives. Evaluating the values embedded within LLMs that guide their decision-making have ethical importance. Schwartz’s theory of basic values (STBV) provides a framework for quantifying cultural value orientations and has shown utility for examining values in mental health contexts, including cultural, diagnostic, and therapist-client dynamics. Objective: This study aimed to (1) evaluate whether the STBV can measure value-like constructs within leading LLMs and (2) determine whether LLMs exhibit distinct value-like patterns from humans and each other. Methods: In total, 4 LLMs (Bard, Claude 2, Generative Pretrained Transformer [GPT]-3.5, GPT-4) were anthropomorphized and instructed to complete the Portrait Values Questionnaire—Revised (PVQ-RR) to assess value-like constructs. Their responses over 10 trials were analyzed for reliability and validity. To benchmark the LLMs’ value profiles, their results were compared to published data from a diverse sample of 53,472 individuals across 49 nations who had completed the PVQ-RR. This allowed us to assess whether the LLMs diverged from established human value patterns across cultural groups. Value profiles were also compared between models via statistical tests. Results: The PVQ-RR showed good reliability and validity for quantifying value-like infrastructure within the LLMs. However, substantial divergence emerged between the LLMs’ value profiles and population data. The models lacked consensus and exhibited distinct motivational biases, reflecting opaque alignment processes. For example, all models prioritized universalism and self-direction, while de-emphasizing achievement, power, and security relative to humans. Successful discriminant analysis differentiated the 4 LLMs’ distinct value profiles. Further examination found the biased value profiles strongly predicted the LLMs’ responses when presented with mental health dilemmas requiring choosing between opposing values. This provided further validation for the models embedding distinct motivational value-like constructs that shape their decision-making. Conclusions: This study leveraged the STBV to map the motivational value-like infrastructure underpinning leading LLMs. Although the study demonstrated the STBV can effectively characterize value-like infrastructure within LLMs, substantial divergence from human values raises ethical concerns about aligning these models with mental health applications. The biases toward certain cultural value sets pose risks if integrated without proper safeguards. For example, prioritizing universalism could promote unconditional acceptance even when clinically unwise. Furthermore, the differences between the LLMs underscore the need to standardize alignment processes to capture true cultural diversity. Thus, any responsible integration of LLMs into mental health care must account for their embedded biases and motivation mismatches to ensure equitable delivery across diverse populations. Achieving this will require transparency and refinement of alignment techniques to instill comprehensive human values. © 2024 JMIR Publications Inc.. All rights reserved.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - CONF
AU  - Zhang, Y.
AU  - Zhang, N.
AU  - Liu, Y.
AU  - Fabbri, A.
AU  - Liu, J.
AU  - Kamoi, R.
AU  - Lu, X.
AU  - Xiong, C.
AU  - Zhao, J.
AU  - Radev, D.
AU  - McKeown, K.
AU  - Zhang, R.
TI  - Fair Abstractive Summarization of Diverse Perspectives
PY  - 2024
T2  - Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024
VL  - 1
SP  - 3404
EP  - 3426
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198825052&partnerID=40&md5=bead4f3a85657c5b01aba67d055c6601
AB  - People from different social and demographic groups express diverse perspectives and conflicting opinions on a broad set of topics such as product reviews, healthcare, law, and politics. A fair summary should provide a comprehensive coverage of diverse perspectives without underrepresenting certain groups. However, current work in summarization metrics and Large Language Models (LLMs) evaluation has not explored fair abstractive summarization. In this paper, we systematically investigate fair abstractive summarization for user-generated data. We first formally define fairness in abstractive summarization as not underrepresenting perspectives of any groups of people, and we propose four reference-free automatic metrics by measuring the differences between target and source perspectives. We evaluate nine LLMs, including three GPT models, four LLaMA models, PaLM 2, and Claude, on six datasets collected from social media, online reviews, and recorded transcripts. Experiments show that both the model-generated and the human-written reference summaries suffer from low fairness. We conduct a comprehensive analysis of the common factors influencing fairness and propose three simple but effective methods to alleviate unfair summarization. ©2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Munn, L.
AU  - Henrickson, L.
TI  - Tell me a story: a framework for critically investigating AI language models
PY  - 2024
T2  - Learning, Media and Technology
DO  - 10.1080/17439884.2024.2327024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188143337&doi=10.1080%2f17439884.2024.2327024&partnerID=40&md5=778a50b93cca7c5ab0f40b129e31b7ad
AB  - Large language models are rapidly being rolled out into high-stakes fields like healthcare, law, and education. However, understanding of their design considerations, operational logics, and implicit biases remains limited. How might these black boxes be understood and unpacked? In this article, we lay out an accessible but critical framework for inquiry, a pedagogical tool with four dimensions. Tell me your story investigates the design and values of the AI model. Tell me my story explores the model’s affective warmth and its psychological impacts. Tell me our story probes the model’s particular understanding of the world based on past statistics and pattern-matching. Tell me ‘their’ story compares the model’s knowledge on dominant (e.g. Western) versus ‘peripheral’ (e.g. Indigenous) cultures, events, and issues. Each mode includes sample prompts and key issues to raise. The framework aims to enhance the public’s critical thinking and technical literacy around generative AI models. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.
PB  - Routledge
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Zhong, J.
AU  - Lu, J.
AU  - Zhang, G.
AU  - Mao, S.
AU  - Chen, H.
AU  - Yin, Q.
AU  - Hu, Y.
AU  - Xing, Y.
AU  - Ding, D.
AU  - Ge, X.
AU  - Zhang, H.
AU  - Yao, W.
TI  - An overview of meta-analyses on radiomics: more evidence is needed to support clinical translation
PY  - 2023
T2  - Insights into Imaging
VL  - 14
IS  - 1
C7  - 111
DO  - 10.1186/s13244-023-01437-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162880698&doi=10.1186%2fs13244-023-01437-2&partnerID=40&md5=46cf3edaaa0ae8bf15e149753ca036e5
AB  - Objective: To conduct an overview of meta-analyses of radiomics studies assessing their study quality and evidence level. Methods: A systematical search was updated via peer-reviewed electronic databases, preprint servers, and systematic review protocol registers until 15 November 2022. Systematic reviews with meta-analysis of primary radiomics studies were included. Their reporting transparency, methodological quality, and risk of bias were assessed by PRISMA (Preferred Reporting Items for Systematic reviews and Meta-Analyses) 2020 checklist, AMSTAR-2 (A MeaSurement Tool to Assess systematic Reviews, version 2) tool, and ROBIS (Risk Of Bias In Systematic reviews) tool, respectively. The evidence level supporting the radiomics for clinical use was rated. Results: We identified 44 systematic reviews with meta-analyses on radiomics research. The mean ± standard deviation of PRISMA adherence rate was 65 ± 9%. The AMSTAR-2 tool rated 5 and 39 systematic reviews as low and critically low confidence, respectively. The ROBIS assessment resulted low, unclear and high risk in 5, 11, and 28 systematic reviews, respectively. We reperformed 53 meta-analyses in 38 included systematic reviews. There were 3, 7, and 43 meta-analyses rated as convincing, highly suggestive, and weak levels of evidence, respectively. The convincing level of evidence was rated in (1) T2-FLAIR radiomics for IDH-mutant vs IDH-wide type differentiation in low-grade glioma, (2) CT radiomics for COVID-19 vs other viral pneumonia differentiation, and (3) MRI radiomics for high-grade glioma vs brain metastasis differentiation. Conclusions: The systematic reviews on radiomics were with suboptimal quality. A limited number of radiomics approaches were supported by convincing level of evidence. Clinical relevance statement: The evidence supporting the clinical application of radiomics are insufficient, calling for researches translating radiomics from an academic tool to a practicable adjunct towards clinical deployment. Graphical Abstract: [Figure not available: see fulltext.] © 2023, The Author(s).
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 15
ER  -

TY  - JOUR
AU  - Huang, H.
TI  - Performance of ChatGPT on Registered Nurse License Exam in Taiwan: A Descriptive Study
PY  - 2023
T2  - Healthcare (Switzerland)
VL  - 11
IS  - 21
C7  - 2855
DO  - 10.3390/healthcare11212855
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176417957&doi=10.3390%2fhealthcare11212855&partnerID=40&md5=aacca3b1f3c2f25935bd52947223445d
AB  - (1) Background: AI (artificial intelligence) chatbots have been widely applied. ChatGPT could enhance individual learning capabilities and clinical reasoning skills and facilitate students’ understanding of complex concepts in healthcare education. There is currently less emphasis on its application in nursing education. The application of ChatGPT in nursing education needs to be verified. (2) Methods: A descriptive study was used to analyze the scores of ChatGPT on the registered nurse license exam (RNLE) in 2022~2023, and to explore the response and explanations of ChatGPT. The process of data measurement encompassed input sourcing, encoding methods, and statistical analysis. (3) Results: ChatGPT promptly responded within seconds. The average score of four exams was around 51.6 to 63.75 by ChatGPT, and it passed the RNLE in 2022 1st and 2023 2nd. However, ChatGPT may generate misleading or inaccurate explanations, or it could lead to hallucination; confusion or misunderstanding about complicated scenarios; and languages bias. (4) Conclusions: ChatGPT may have the potential to assist with nursing education because of its advantages. It is recommended to integrate ChatGPT into different nursing courses, to assess its limitations and effectiveness through a variety of tools and methods. © 2023 by the author.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 15
ER  -

TY  - CONF
AU  - Maryamah, M.
AU  - Wilsen, G.
AU  - Suhalim, C.T.
AU  - Septiana, R.
AU  - Fajar, A.
AU  - Solihin, M.I.
TI  - Hybrid Information Retrieval with Masked and Permuted Language Modeling (MPNet) and BM25L for Indonesian Drug Data Retrieval
PY  - 2024
T2  - KST 2024 - 16th International Conference on Knowledge and Smart Technology
SP  - 242
EP  - 247
DO  - 10.1109/KST61284.2024.10499674
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191655976&doi=10.1109%2fKST61284.2024.10499674&partnerID=40&md5=6134c163f1072526e8e16d01bdfb9dee
AB  - Lexical or statistical information retrieval confronts challenges such as the semantic gap and vocabulary mismatch. In the context of medical data, these difficulties are compounded by users' diverse backgrounds, resulting in disparities in perspective and vocabulary. The intricacies of medical language, including spelling variations, frequent acronyms, and ambiguous concepts, further amplify the semantic gap in medical texts. However, adopting a semantic approach can address these issues, albeit introducing a new challenge in the form of a soft matching nature leading to lower recall. In response, we propose a hybrid information retrieval method that combines semantic and lexical approaches. In contrast to recent experiments with the emphasis on utilizing semantic methods as re-ranker, our current approach diverges by incorporating semantic techniques as a fundamental part of the retrieval model. This shift aims to explore the efficacy of semantic methodologies in the initial retrieval stage rather than exclusively relying on them for post-retrieval refinement. The results obtained from both the semantic and lexical retrieval approaches are subsequently subjected to reranking through Reciprocal Rank Fusion. The proposed method outperforms lexical methods such as BM25L, Jaccard Similarity, and Query Likelihood Model, along with semantic methods, including doc2vec, multilingual BERT, IndoBERT, and MiniLM. It has additionally been demonstrated to be more effective than other hybrid models, PLM-based dense retrieval. This technique has successfully capitalized on the strengths of both semantic and lexical methods, resulting in enhanced overall performance in retrieving relevant documents.  © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Yu, P.
AU  - Fang, C.
AU  - Liu, X.
AU  - Fu, W.
AU  - Ling, J.
AU  - Yan, Z.
AU  - Jiang, Y.
AU  - Cao, Z.
AU  - Wu, M.
AU  - Chen, Z.
AU  - Zhu, W.
AU  - Zhang, Y.
AU  - Abudukeremu, A.
AU  - Wang, Y.
AU  - Liu, X.
AU  - Wang, J.
TI  - Performance of ChatGPT on the Chinese Postgraduate Examination for Clinical Medicine: Survey Study
PY  - 2024
T2  - JMIR Medical Education
VL  - 10
IS  - 1
C7  - e48514
DO  - 10.2196/48514
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186110187&doi=10.2196%2f48514&partnerID=40&md5=f4f9e61828aad68a93d023a27a2e2f57
AB  - Background: ChatGPT, an artificial intelligence (AI) based on large-scale language models, has sparked interest in the field of health care. Nonetheless, the capabilities of AI in text comprehension and generation are constrained by the quality and volume of available training data for a specific language, and the performance of AI across different languages requires further investigation. While AI harbors substantial potential in medicine, it is imperative to tackle challenges such as the formulation of clinical care standards; facilitating cultural transitions in medical education and practice; and managing ethical issues including data privacy, consent, and bias. Objective: The study aimed to evaluate ChatGPT's performance in processing Chinese Postgraduate Examination for Clinical Medicine questions, assess its clinical reasoning ability, investigate potential limitations with the Chinese language, and explore its potential as a valuable tool for medical professionals in the Chinese context. Methods: A data set of Chinese Postgraduate Examination for Clinical Medicine questions was used to assess the effectiveness of ChatGPT's (version 3.5) medical knowledge in the Chinese language, which has a data set of 165 medical questions that were divided into three categories: (1) common questions (n=90) assessing basic medical knowledge, (2) case analysis questions (n=45) focusing on clinical decision-making through patient case evaluations, and (3) multichoice questions (n=30) requiring the selection of multiple correct answers. First of all, we assessed whether ChatGPT could meet the stringent cutoff score defined by the government agency, which requires a performance within the top 20% of candidates. Additionally, in our evaluation of ChatGPT's performance on both original and encoded medical questions, 3 primary indicators were used: accuracy, concordance (which validates the answer), and the frequency of insights. Results: Our evaluation revealed that ChatGPT scored 153.5 out of 300 for original questions in Chinese, which signifies the minimum score set to ensure that at least 20% more candidates pass than the enrollment quota. However, ChatGPT had low accuracy in answering open-ended medical questions, with only 31.5% total accuracy. The accuracy for common questions, multichoice questions, and case analysis questions was 42%, 37%, and 17%, respectively. ChatGPT achieved a 90% concordance across all questions. Among correct responses, the concordance was 100%, significantly exceeding that of incorrect responses (n=57, 50%; P<.001). ChatGPT provided innovative insights for 80% (n=132) of all questions, with an average of 2.95 insights per accurate response. Conclusions: Although ChatGPT surpassed the passing threshold for the Chinese Postgraduate Examination for Clinical Medicine, its performance in answering open-ended medical questions was suboptimal. Nonetheless, ChatGPT exhibited high internal concordance and the ability to generate multiple insights in the Chinese language. Future research should investigate the language-based discrepancies in ChatGPT's performance within the health care context.  ©Peng Yu, Changchang Fang, Xiaolin Liu, Wanying Fu, Jitao Ling, Zhiwei Yan, Yuan Jiang, Zhengyu Cao, Maoxiong Wu, Zhiteng Chen, Wengen Zhu, Yuling Zhang, Ayiguli Abudukeremu, Yue Wang, Xiao Liu, Jingfeng Wang.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - CONF
AU  - Solomou, C.
TI  - Enhancing Medical Specialty Assignment to Patients using NLP Techniques
PY  - 2023
T2  - ACM International Conference Proceeding Series
SP  - 203
EP  - 209
DO  - 10.1145/3639233.3639251
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187549946&doi=10.1145%2f3639233.3639251&partnerID=40&md5=3e9b7588992e978d55fb58663b5b6c0b
AB  - The introduction of Large Language Models (LLMs), and the vast volume of publicly available medical data, amplified the application of NLP to the medical domain. However, LLMs are pretrained on data that are not explicitly relevant to the domain that are applied to and are often biased towards the original data they were pretrained upon. Even when pretrained on domain-specific data, these models typically require time-consuming fine-tuning to achieve good performance for a specific task. To address these limitations, we propose an alternative approach that achieves superior performance while being computationally efficient. Specifically, we utilize keywords to train a deep learning architecture that outperforms a language model pretrained on a large corpus of text. Our proposal does not require pretraining nor fine-tuning and can be applied directly to a specific setting for performing multi-label classification. Our objective is to automatically assign a new patient to the specialty of the medical professional they require, using a dataset that contains medical transcriptions and relevant keywords. To this end, we fine-tune the PubMedBERT model on this dataset, which serves as the baseline for our experiments. We then twice train/fine-tune a DNN and the RoBERTa language model, using both the keywords and the full transcriptions as input. We compare the performance of these approaches using relevant metrics. Our results demonstrate that utilizing keywords for text classification significantly improves classification performance, for both a basic DL architecture and a large language model. Our approach represents a promising and efficient alternative to traditional methods for fine-tuning language models on domain-specific data and has potential applications in various medical domains. © 2023 ACM.
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - De Arizón, L.F.
AU  - Viera, E.R.
AU  - Pilco, M.
AU  - Perera, A.
AU  - De Maeztu, G.
AU  - Nicolau, A.
AU  - Furlano, M.
AU  - Torra, R.
TI  - Artificial intelligence: a new field of knowledge for nephrologists
PY  - 2023
T2  - Clinical Kidney Journal
VL  - 16
IS  - 12
SP  - 2314
EP  - 2326
DO  - 10.1093/ckj/sfad182
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179321121&doi=10.1093%2fckj%2fsfad182&partnerID=40&md5=70e1871cce641b2cbf190c119b4a58a9
AB  - Artificial intelligence (AI) is a science that involves creating machines that can imitate human intelligence and learn. AI is ubiquitous in our daily lives, from search engines like Google to home assistants like Alexa and, more recently, OpenAI with its chatbot. AI can improve clinical care and research, but its use requires a solid understanding of its fundamentals, the promises and perils of algorithmic fairness, the barriers and solutions to its clinical implementation, and the pathways to developing an AI-competent workforce. The potential of AI in the field of nephrology is vast, particularly in the areas of diagnosis, treatment and prediction. One of the most significant advantages of AI is the ability to improve diagnostic accuracy. Machine learning algorithms can be trained to recognize patterns in patient data, including lab results, imaging and medical history, in order to identify early signs of kidney disease and thereby allow timely diagnoses and prompt initiation of treatment plans that can improve outcomes for patients. In short, AI holds the promise of advancing personalized medicine to new levels. While AI has tremendous potential, there are also significant challenges to its implementation, including data access and quality, data privacy and security, bias, trustworthiness, computing power, AI integration and legal issues. The European Commission’s proposed regulatory framework for AI technology will play a significant role in ensuring the safe and ethical implementation of these technologies in the healthcare industry. Training nephrologists in the fundamentals of AI is imperative because traditionally, decision-making pertaining to the diagnosis, prognosis and treatment of renal patients has relied on ingrained practices, whereas AI serves as a powerful tool for swiftly and confidently synthesizing this information. plans, leading to better patient outcomes. However, the implementation of AI in healthcare faces several challenges. The European Commission’s proposed regulatory framework aims to promote the safe and ethical use of AI in healthcare. To fully leverage the benefits of AI, nephrologists and other healthcare professionals need to be educated about its fundamentals and its potential applications in routine patient care. This will enable them to effectively utilize AI technologies and provide better care for kidney patients. © The Author(s) 2023. Published by Oxford University Press on behalf of the ERA.
PB  - Oxford University Press
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 13
ER  -

TY  - CONF
AU  - Ofori-Boateng, R.
AU  - Aceves-Martins, M.
AU  - Wirantuga, N.
AU  - Moreno-García, C.F.
TI  - A Zero-Shot Monolingual Dual Stage Information Retrieval System for Spanish Biomedical Systematic Literature Reviews
PY  - 2024
T2  - Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024
VL  - 1
SP  - 3725
EP  - 3736
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199919373&partnerID=40&md5=ee6f91d33f785932a8206f49de139f34
AB  - Systematic Reviews (SRs) are foundational in healthcare for synthesising evidence to inform clinical practices. Traditionally skewed towards English-language databases, SRs often exclude significant research in other languages, leading to potential biases. This study addresses this gap by focusing on Spanish, a language notably underrepresented in SRs. We present a foundational zero-shot dual information retrieval (IR) baseline system, integrating traditional retrieval methods with pre-trained language models and cross-attention re-rankers for enhanced accuracy in Spanish biomedical literature retrieval. Utilising the LILACS database, known for its comprehensive coverage of Latin American and Caribbean biomedical literature, we evaluate the approach with three real-life case studies in Spanish SRs. The findings demonstrate the system’s efficacy and underscore the importance of query formulation. This study contributes to the field of IR by promoting language inclusivity and supports the development of more comprehensive and globally representative healthcare guidelines. ©2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Tataru, C.
AU  - Peras, M.
AU  - Rutherford, E.
AU  - Dunlap, K.
AU  - Yin, X.
AU  - Chrisman, B.S.
AU  - DeSantis, T.Z.
AU  - Wall, D.P.
AU  - Iwai, S.
AU  - David, M.M.
TI  - Topic modeling for multi-omic integration in the human gut microbiome and implications for Autism
PY  - 2023
T2  - Scientific Reports
VL  - 13
IS  - 1
C7  - 11353
DO  - 10.1038/s41598-023-38228-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164842870&doi=10.1038%2fs41598-023-38228-0&partnerID=40&md5=44019b8d19de9736c91f9d8fd89e6f6d
AB  - While healthy gut microbiomes are critical to human health, pertinent microbial processes remain largely undefined, partially due to differential bias among profiling techniques. By simultaneously integrating multiple profiling methods, multi-omic analysis can define generalizable microbial processes, and is especially useful in understanding complex conditions such as Autism. Challenges with integrating heterogeneous data produced by multiple profiling methods can be overcome using Latent Dirichlet Allocation (LDA), a promising natural language processing technique that identifies topics in heterogeneous documents. In this study, we apply LDA to multi-omic microbial data (16S rRNA amplicon, shotgun metagenomic, shotgun metatranscriptomic, and untargeted metabolomic profiling) from the stool of 81 children with and without Autism. We identify topics, or microbial processes, that summarize complex phenomena occurring within gut microbial communities. We then subset stool samples by topic distribution, and identify metabolites, specifically neurotransmitter precursors and fatty acid derivatives, that differ significantly between children with and without Autism. We identify clusters of topics, deemed “cross-omic topics”, which we hypothesize are representative of generalizable microbial processes observable regardless of profiling method. Interpreting topics, we find each represents a particular diet, and we heuristically label each cross-omic topic as: healthy/general function, age-associated function, transcriptional regulation, and opportunistic pathogenesis. © 2023, The Author(s).
PB  - Nature Research
C2  - 37443184
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Anastasiou, D.
AU  - Gallais, M.
AU  - Blond-Hanten, C.
TI  - A Luxembourgish corpus as a Gender Bias Evaluation Testset
PY  - 2024
T2  - 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING 2024 - Main Conference Proceedings
SP  - 784
EP  - 788
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195944713&partnerID=40&md5=3cc911308fc1608c264a37926a5fd030
AB  - According to the United Nations Development Programme, gender inequality is a metric that is composed of three dimensions: reproductive health, empowerment, and the labour market. Gender inequality remains major obstacle to equal opportunities in society as a whole. In this paper we present our work-in-progress of designing and playing a physical game with digital elements. We are currently conducting Conversation Analysis of transcribed speech of 58567 words and documenting bias. We are also testing OpenAI's ChatGPT for bias in gender-related quiz questions. © 2024 ELRA Language Resource Association: CC BY-NC 4.0.
PB  - European Language Resources Association (ELRA)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Tan, T.F.
AU  - Thirunavukarasu, A.J.
AU  - Campbell, J.P.
AU  - Keane, P.A.
AU  - Pasquale, L.R.
AU  - Abramoff, M.D.
AU  - Kalpathy-Cramer, J.
AU  - Lum, F.
AU  - Kim, J.E.
AU  - Baxter, S.L.
AU  - Ting, D.S.W.
TI  - Generative Artificial Intelligence Through ChatGPT and Other Large Language Models in Ophthalmology: Clinical Applications and Challenges
PY  - 2023
T2  - Ophthalmology Science
VL  - 3
IS  - 4
C7  - 100394
DO  - 10.1016/j.xops.2023.100394
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174553744&doi=10.1016%2fj.xops.2023.100394&partnerID=40&md5=1ddefce51280fef771f97d29304ddba0
AB  - The rapid progress of large language models (LLMs) driving generative artificial intelligence applications heralds the potential of opportunities in health care. We conducted a review up to April 2023 on Google Scholar, Embase, MEDLINE, and Scopus using the following terms: “large language models,” “generative artificial intelligence,” “ophthalmology,” “ChatGPT,” and “eye,” based on relevance to this review. From a clinical viewpoint specific to ophthalmologists, we explore from the different stakeholders’ perspectives—including patients, physicians, and policymakers—the potential LLM applications in education, research, and clinical domains specific to ophthalmology. We also highlight the foreseeable challenges of LLM implementation into clinical practice, including the concerns of accuracy, interpretability, perpetuating bias, and data security. As LLMs continue to mature, it is essential for stakeholders to jointly establish standards for best practices to safeguard patient safety. Financial Disclosure(s): Proprietary or commercial disclosure may be found in the Footnotes and Disclosures at the end of this article. © 2023 American Academy of Ophthalmology
PB  - Elsevier Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 51
ER  -

TY  - JOUR
AU  - Zhang, W.
AU  - Cai, B.
AU  - Hu, J.
AU  - Qin, Q.
AU  - Xie, K.
TI  - Visual-Textual Cross-Modal Interaction Network for Radiology Report Generation
PY  - 2024
T2  - IEEE Signal Processing Letters
VL  - 31
SP  - 984
EP  - 988
DO  - 10.1109/LSP.2024.3379005
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188533104&doi=10.1109%2fLSP.2024.3379005&partnerID=40&md5=2df3d3207defa569f5bfc8468897ca87
AB  - The radiology report generation task generates diagnostic descriptions from radiology images, aiming to alleviate the onerous task for radiologists and alerting them to abnormalities. However, the data bias problem poses a persistent challenge, since the abnormal regions usually occupy a small portion of radiology image, while the report generation process should pay greater attention to the abnormal regions. Moreover, the data volume is relatively small compared to large language models, posing challenges during training. To address these issues effectively, we propose a Visual-textual Cross-model Interaction Network (VCIN) to enhance the quality of generated reports. VCIN comprises two key modules: Abundant Clinical Information Embedding (ACIE), which gathers rich cross-modal interaction information to promote the report generation of abnormal regions; and a Bert-based Decoder-only Generator (BDG), built on Bert architecture to mitigate training difficulties. The superior performance of our proposed model is demonstrated through experimental results obtained from two public benchmark datasets.  © 1994-2012 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Jairoun, A.A.
AU  - Al-Hemyari, S.S.
AU  - Shahwan, M.
AU  - Al-Qirim, T.
AU  - Shahwan, M.
TI  - Benefit–Risk Assessment of ChatGPT Applications in the Field of Diabetes and Metabolic Illnesses: A Qualitative Study
PY  - 2024
T2  - Clinical Medicine Insights: Endocrinology and Diabetes
VL  - 17
DO  - 10.1177/11795514241235514
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188052626&doi=10.1177%2f11795514241235514&partnerID=40&md5=99300ede8665dc32be2a649b5a97db98
AB  - Background: The use of ChatGPT and artificial intelligence (AI) in the management of metabolic and endocrine disorders presents both significant opportunities and notable risks. Objectives: To investigate the benefits and risks associated with the application of ChatGPT in managing diabetes and metabolic illnesses by exploring the perspectives of endocrinologists and diabetologists. Methods and materials: The study employed a qualitative research approach. A semi-structured in-depth interview guide was developed. A convenience sample of 25 endocrinologists and diabetologists was enrolled and interviewed. All interviews were audiotaped and verbatim transcribed; then, thematic analysis was used to determine the themes in the data. Results: The findings of the thematic analysis resulted in 19 codes and 9 major themes regarding the benefits of implementing AI and ChatGPT in managing diabetes and metabolic illnesses. Moreover, the extracted risks of implementing AI and ChatGPT in managing diabetes and metabolic illnesses were categorized into 7 themes and 14 codes. The benefits of heightened diagnostic precision, tailored treatment, and efficient resource utilization have potential to improve patient results. Concurrently, the identification of potential challenges, such as data security concerns and the need for AI that can be explained, enables stakeholders to proactively tackle these issues. Conclusions: Regulatory frameworks must evolve to keep pace with the rapid adoption of AI in healthcare. Sustained attention to ethical considerations, including obtaining patient consent, safeguarding data privacy, ensuring accountability, and promoting fairness, remains critical. Despite its potential impact on the human aspect of healthcare, AI will remain an integral component of patient-centered care. Striking a balance between AI-assisted decision-making and human expertise is essential to uphold trust and provide comprehensive patient care. © The Author(s) 2024.; Regulatory frameworks must evolve to keep pace with the rapid adoption of AI in healthcare. Sustained attention to ethical considerations, including obtaining patient consent, safeguarding data privacy, ensuring accountability, and promoting fairness, remains critical. Despite its potential impact on the human aspect of healthcare, AI will remain an integral component of patient-centered care. The use of ChatGPT in the management of metabolic and endocrine disorders presents both significant opportunities and notable risks. The benefits of heightened diagnostic precision, tailored treatment, and efficient resource utilization have potential to improve patient results. Concurrently, the identification of potential challenges, such as data security concerns and the need for AI that can be explained, enables stakeholders to proactively tackle these issues. Regulatory frameworks must evolve to keep pace with the rapid adoption of AI in healthcare. Sustained attention to ethical considerations, including obtaining patient consent, safeguarding data privacy, ensuring accountability, and promoting fairness, remains critical. Despite its potential impact on the human aspect of healthcare, AI will remain an integral component of patient-centered care. Striking a balance between AI-assisted decision-making and human expertise is essential to uphold trust and provide comprehensive patient care. © The Author(s) 2024.
PB  - SAGE Publications Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Hadzic, B.
AU  - Ohse, J.
AU  - Danner, M.
AU  - Peperkorn, N.
AU  - Mohammed, P.
AU  - Shiban, Y.
AU  - Ratsch, M.
TI  - AI-Supported Diagnostic of Depression Using Clinical Interviews: A Pilot Study
PY  - 2024
T2  - Proceedings of the International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications
VL  - 1
SP  - 500
EP  - 507
DO  - 10.5220/0012439700003660
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190891203&doi=10.5220%2f0012439700003660&partnerID=40&md5=3c72458e8edb155f6997947ce72d1c37
AB  - In the face of rising depression rates, the urgency of early and accurate diagnosis has never been more paramount. Traditional diagnostic methods, while invaluable, can sometimes be limited in access and susceptible to biases, potentially leading to underdiagnoses. This paper explores the innovative potential of AI technology, specifically machine learning, as a diagnostic tool for depression. Drawing from prior research, we note the success of machine learning in discerning depression indicators on social media platforms and through automated interviews. A particular focus is given to the BERT-based NLP transformer model, previously shown to be effective in detecting depression from simulated interview data. Our study assessed this model’s capability to identify depression from transcribed, semi-structured clinical interviews within a general population sample. While the BERT model displayed an accuracy of 0.71, it was surpassed by an untrained GPT-3.5 model, which achieved an impressive accuracy of 0.88. These findings emphasise the transformative potential of NLP transformer models in the realm of depression detection. However, given the relatively small dataset (N = 17) utilised, we advise a measured interpretation of the results. This paper is designed as a pilot study, and further studies will incorporate bigger datasets. © 2024 by SCITEPRESS – Science and Technology Publications, Lda.
PB  - Science and Technology Publications, Lda
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Mehta, N.
AU  - Goldwasser, D.
TI  - An Interactive Framework for Profiling News Media Sources
PY  - 2024
T2  - Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024
VL  - 1
SP  - 40
EP  - 58
DO  - 10.18653/v1/2024.naacl-long.3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200145224&doi=10.18653%2fv1%2f2024.naacl-long.3&partnerID=40&md5=1d36a561033fd57efdf398e849f592aa
AB  - The recent rise of social media has led to the spread of large amounts of fake and biased news, content published with the intent to sway beliefs. While detecting and profiling the sources that spread this news is important to maintain a healthy society, it is challenging for automated systems. In this paper, we propose an interactive framework for news media profiling. It combines the strengths of graph based news media profiling models, Pre-trained Large Language Models, and human insight, to characterize the social context on social media. Experimental results show that with as little as 5 human interactions, our framework can rapidly detect fake and biased news media, even in the most challenging settings of emerging news events, where test data is unseen. © 2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Zhui, L.
AU  - Fenghe, L.
AU  - Xuehu, W.
AU  - Qining, F.
AU  - Wei, R.
TI  - Ethical Considerations and Fundamental Principles of Large Language Models in Medical Education: Viewpoint
PY  - 2024
T2  - Journal of Medical Internet Research
VL  - 26
C7  - e60083
DO  - 10.2196/60083
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200459969&doi=10.2196%2f60083&partnerID=40&md5=31158f2e774069b12ce868d9e050e6a0
AB  - This viewpoint article first explores the ethical challenges associated with the future application of large language models (LLMs) in the context of medical education. These challenges include not only ethical concerns related to the development of LLMs, such as artificial intelligence (AI) hallucinations, information bias, privacy and data risks, and deficiencies in terms of transparency and interpretability but also issues concerning the application of LLMs, including deficiencies in emotional intelligence, educational inequities, problems with academic integrity, and questions of responsibility and copyright ownership. This paper then analyzes existing AI-related legal and ethical frameworks and highlights their limitations with regard to the application of LLMs in the context of medical education. To ensure that LLMs are integrated in a responsible and safe manner, the authors recommend the development of a unified ethical framework that is specifically tailored for LLMs in this field. This framework should be based on 8 fundamental principles: quality control and supervision mechanisms; privacy and data protection; transparency and interpretability; fairness and equal treatment; academic integrity and moral norms; accountability and traceability; protection and respect for intellectual property; and the promotion of educational research and innovation. The authors further discuss specific measures that can be taken to implement these principles, thereby laying a solid foundation for the development of a comprehensive and actionable ethical framework. Such a unified ethical framework based on these 8 fundamental principles can provide clear guidance and support for the application of LLMs in the context of medical education. This approach can help establish a balance between technological advancement and ethical safeguards, thereby ensuring that medical education can progress without compromising the principles of fairness, justice, or patient safety and establishing a more equitable, safer, and more efficient environment for medical education. © 2024 JMIR Publications Inc. All rights reserved.
PB  - JMIR Publications Inc.
C2  - 38971715
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Shah, S.B.
AU  - Thapa, S.
AU  - Acharya, A.
AU  - Rauniyar, K.
AU  - Poudel, S.
AU  - Jain, S.
AU  - Masood, A.
AU  - Naseem, U.
TI  - Navigating the Web of Disinformation and Misinformation: Large Language Models as Double-Edged Swords
PY  - 2024
T2  - IEEE Access
SP  - 1
EP  - 1
DO  - 10.1109/ACCESS.2024.3406644
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194813629&doi=10.1109%2fACCESS.2024.3406644&partnerID=40&md5=97f002131424821cd6323d3181ec4cd2
AB  - This paper explores the dual role of Large Language Models (LLMs) in the context of online misinformation and disinformation. In today&#x2019;s digital landscape, where the internet and social media facilitate the rapid dissemination of information, discerning between accurate content and falsified information presents a formidable challenge. Misinformation, often arising unintentionally, and disinformation, crafted deliberately, are at the forefront of this challenge. LLMs such as OpenAI&#x2019;s GPT-4, equipped with advanced language generation abilities, present a double-edged sword in this scenario. While they hold promise in combating misinformation by fact-checking and detecting LLM-generated text, their ability to generate realistic, contextually relevant text also poses risks for creating and propagating misinformation. Further, LLMs are plagued with many problems such as biases, knowledge cutoffs, and hallucinations, which may further perpetuate misinformation and disinformation. The paper outlines historical developments in misinformation detection and how it affects social media consumption, especially among youth, and introduces LLMs and their applications in various domains. It then critically analyzes the potential of LLMs to generate and counter misinformation and disinformation in sensitive topics such as healthcare, COVID-19, and political agendas. Further, it discusses mitigation strategies, ethical considerations, and regulatory measures, summarizing previous methods and proposing future research direction toward leveraging the benefits of LLMs while minimizing misuse risks. The paper concludes by acknowledging LLMs as powerful tools with significant implications in both spreading and combating misinformation in the digital age. Authors
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Sahu, P.K.
AU  - Benjamin, L.A.
AU  - Aswal, G.S.
AU  - Williams-Persad, A.
TI  - ChatGPT in research and health professions education: challenges, opportunities, and future directions
PY  - 2024
T2  - Postgraduate Medical Journal
VL  - 100
IS  - 1179
SP  - 50
EP  - 55
DO  - 10.1093/postmj/qgad090
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181176297&doi=10.1093%2fpostmj%2fqgad090&partnerID=40&md5=775ef72676f5f51d04c91e5bceb143cf
AB  - ChatGPT was launched by OpenAI in November 2022 and within 2 months it became popular across a wide range of industrial, social, and intellectual contexts including healthcare education. This article reviews the impact of ChatGPT on research and health professions education by identifying the challenges and opportunities in these fields. Additionally, it aims to provide future directions to mitigate the challenges and maximize the benefits of this technology in health professions education. ChatGPT has the potential to revolutionize the field of research and health professions education. However, there is a need to address ethical concerns and limitations such as lack of real-time data, data inaccuracies, biases, plagiarism, and copyright infringement before its implementation. Future research can highlight the ways to mitigate these challenges; establish guidelines and policies; and explore how effectively ChatGPT and other AI tools can be used in the field of research and healthcare professions education. © The Author(s) 2023.
PB  - Oxford University Press
C2  - 37819738
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 17
ER  -

TY  - JOUR
AU  - Drum, B.
AU  - Shi, J.
AU  - Peterson, B.
AU  - Lamb, S.
AU  - Hurdle, J.F.
AU  - Gradick, C.
TI  - Using Natural Language Processing and Machine Learning to Identify Internal Medicine-Pediatrics Residency Values in Applications
PY  - 2023
T2  - Academic Medicine
VL  - 98
IS  - 11
SP  - 1278
EP  - 1282
DO  - 10.1097/ACM.0000000000005352
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175270511&doi=10.1097%2fACM.0000000000005352&partnerID=40&md5=232604bc796ccc095480ce3a82efc46d
AB  - Problem Although holistic review has been used successfully in some residency programs to decrease bias, such review is time-consuming and unsustainable for many programs without initial prescreening. The unstructured qualitative data in residency applications, including notable experiences, letters of recommendation, personal statement, and medical student performance evaluations, require extensive time, resources, and metrics to evaluate; therefore, previous applicant screening relied heavily on quantitative metrics, which can be socioeconomically and racially biased. Approach Using residency applications to the University of Utah internal medicine-pediatrics program from 2015 to 2019, the authors extracted relevant snippets of text from the narrative sections of applications. Expert reviewers annotated these snippets into specific values (academic strength; intellectual curiosity; compassion; communication; work ethic; teamwork; leadership; self-awareness; diversity, equity, and inclusion; professionalism; and adaptability) previously identified as associated with resident success. The authors prospectively applied a machine learning model (MLM) to snippets from applications from 2023, and output was compared with a manual holistic review performed without knowledge of MLM results. Outcomes Overall, the MLM had a sensitivity of 0.64, specificity of 0.97, positive predictive value of 0.62, negative predictive value of 0.97, and F1 score of 0.63. The mean (SD) total number of annotations per application was significantly correlated with invited for interview status (invited: 208.6 [59.1]; not invited: 145.2 [57.2]; P <.001). In addition, 8 of the 10 individual values were significantly predictive of an applicant's invited for interview status. Next Steps The authors created an MLM that can identify several values important for resident success in internal medicine-pediatrics programs with moderate sensitivity and high specificity. The authors will continue to refine the MLM by increasing the number of annotations, exploring parameter tuning and feature engineering options, and identifying which application sections have the highest correlation with invited for interview status. © 2023 Lippincott Williams and Wilkins. All rights reserved.
PB  - Wolters Kluwer Health
C2  - 37506388
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Shum, M.
AU  - Hsiao, A.
AU  - Teng, W.
AU  - Asnes, A.
AU  - Amrhein, J.
AU  - Tiyyagura, G.
TI  - Natural Language Processing — A Surveillance Stepping Stone to Identify Child Abuse
PY  - 2024
T2  - Academic Pediatrics
VL  - 24
IS  - 1
SP  - 92
EP  - 96
DO  - 10.1016/j.acap.2023.08.015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173092500&doi=10.1016%2fj.acap.2023.08.015&partnerID=40&md5=033b9340640d5a8d271d95cd89ad7c3d
AB  - Objective: We aimed to refine a natural language processing (NLP) algorithm that identified injuries associated with child abuse and identify areas in which integration into a real-time clinical decision support (CDS) tool may improve clinical care. Methods: We applied an NLP algorithm in “silent mode” to all emergency department (ED) provider notes between July 2021 and December 2022 (n = 353) at 1 pediatric and 8 general EDs. We refined triggers for the NLP, assessed adherence to clinical guidelines, and evaluated disparities in degree of evaluation by examining associations between demographic variables and abuse evaluation or reporting to child protective services. Results: Seventy-three cases falsely triggered the NLP, often due to errors in interpreting linguistic context. We identified common false-positive scenarios and refined the algorithm to improve NLP specificity. Adherence to recommended evaluation standards for injuries defined by nationally accepted clinical guidelines was 63%. There were significant demographic differences in evaluation and reporting based on presenting ED type, insurance status, and race and ethnicity. Conclusions: Analysis of an NLP algorithm in “silent mode” allowed for refinement of the algorithm and highlighted areas in which real-time CDS may help ED providers identify and pursue appropriate evaluation of injuries associated with child physical abuse. © 2024 Academic Pediatric Association
PB  - Elsevier Inc.
C2  - 37652162
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Blease, C.
AU  - Torous, J.
TI  - ChatGPT and mental healthcare: balancing benefits with risks of harms
PY  - 2023
T2  - BMJ Mental Health
VL  - 26
IS  - 1
C7  - 26
DO  - 10.1136/bmjment-2023-300884
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176389441&doi=10.1136%2fbmjment-2023-300884&partnerID=40&md5=977883d84f0c0d1afa21fcc33c1b8c41
AB  - Against the global need for increased access to mental services, health organisations are looking to technological advances to improve the delivery of care and lower costs. Since November 2022, with the public launch of OpenAI’s ChatGPT, the field of generative artificial intelligence (AI) has received expanding attention. Although generative AI itself is not new, technical advances and the increased accessibility of large language models (LLMs) (eg, OpenAI’s GPT-4 and Google’s Bard) suggest use of these tools could be clinically significant. LLMs are an application of generative AI technology that can summarise and generate content based on training on vast data sets. Unlike search engines, which provide internet links in response to typed entries, chatbots that rely on generative language models can simulate dialogue that resembles human conversations. We examine the potential promise and the risks of using LLMs in mental healthcare today, focusing on their scope to impact mental healthcare, including global equity in the delivery of care. Although we caution that LLMs should not be used to disintermediate mental health clinicians, we signal how—if carefully implemented—in the long term these tools could reap benefits for patients and health professionals. © Author(s) (or their employer(s)) 2023.
PB  - BMJ Publishing Group
C2  - 37949485
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 32
ER  -

TY  - CONF
AU  - Li, Y.
AU  - Zhang, L.
AU  - Zhang, Y.
TI  - Probing into the Fairness of Large Language Models: A Case Study of ChatGPT
PY  - 2024
T2  - 2024 58th Annual Conference on Information Sciences and Systems, CISS 2024
DO  - 10.1109/CISS59072.2024.10480206
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190625609&doi=10.1109%2fCISS59072.2024.10480206&partnerID=40&md5=acc25c478b3c33320cec40f52c3df86c
AB  - Understanding and addressing unfairness in LLMs are crucial for responsible AI deployment. However, there is a limited number of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs, especially when applying LLMs to high-stakes fields. This work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's performance in high-takes fields including education, criminology, finance and healthcare. To conduct a thorough evaluation, we consider both group fairness and individual fairness metrics. We also observe the disparities in ChatGPT's outputs under a set of biased or unbiased prompts. This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible AI systems. Code and data are open-sourced on GitHub. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Hueso, M.
AU  - Álvarez, R.
AU  - Marí, D.
AU  - Ribas-Ripoll, V.
AU  - Lekadir, K.
AU  - Vellido, A.
TI  - Is generative artificial intelligence the next step toward a personalized hemodialysis?
PY  - 2023
T2  - Revista de investigacion clinica; organo del Hospital de Enfermedades de la Nutricion
VL  - 75
IS  - 6
SP  - 309
EP  - 317
DO  - 10.24875/RIC.23000162
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181395946&doi=10.24875%2fRIC.23000162&partnerID=40&md5=0dd6118b264d6b6a33466ee4a0c7ca7f
AB  - Artificial intelligence (AI) generative models driven by the integration of AI and natural language processing technologies, such as OpenAI's chatbot generative pre-trained transformer large language model (LLM), are receiving much public attention and have the potential to transform personalized medicine. Dialysis patients are highly dependent on technology and their treatment generates a challenging large volume of data that has to be analyzed for knowledge extraction. We argue that, by integrating the data acquired from hemodialysis treatments with the powerful conversational capabilities of LLMs, nephrologists could personalize treatments adapted to patients' lifestyles and preferences. We also argue that this new conversational AI integrated with a personalized patient-computer interface will enhance patients' engagement and self-care by providing them with a more personalized experience. However, generative AI models require continuous and accurate updates of data, and expert supervision and must address potential biases and limitations. Dialysis patients can also benefit from other new emerging technologies such as Digital Twins with which patients' care can also be addressed from a personalized medicine perspective. In this paper, we will revise LLMs potential strengths in terms of their contribution to personalized medicine, and, in particular, their potential impact, and limitations in nephrology. Nephrologists' collaboration with AI academia and companies, to develop algorithms and models that are more transparent, understandable, and trustworthy, will be crucial for the next generation of dialysis patients. The combination of technology, patient-specific data, and AI should contribute to create a more personalized and interactive dialysis process, improving patients' quality of life.
C2  - 37734067
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Buhr, C.R.
AU  - Smith, H.
AU  - Huppertz, T.
AU  - Bahr-Hamm, K.
AU  - Matthias, C.
AU  - Cuny, C.
AU  - Snijders, J.P.
AU  - Ernst, B.P.
AU  - Blaikie, A.
AU  - Kelsey, T.
AU  - Kuhn, S.
AU  - Eckrich, J.
TI  - Assessing unknown potential—quality and limitations of different large language models in the field of otorhinolaryngology
PY  - 2024
T2  - Acta Oto-Laryngologica
VL  - 144
IS  - 3
SP  - 237
EP  - 242
DO  - 10.1080/00016489.2024.2352843
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193978711&doi=10.1080%2f00016489.2024.2352843&partnerID=40&md5=ba5dc12574310c7f1349b56120295fb7
AB  - Background: Large Language Models (LLMs) might offer a solution for the lack of trained health personnel, particularly in low- and middle-income countries. However, their strengths and weaknesses remain unclear. Aims/objectives: Here we benchmark different LLMs (Bard 2023.07.13, Claude 2, ChatGPT 4) against six consultants in otorhinolaryngology (ORL). Material and methods: Case-based questions were extracted from literature and German state examinations. Answers from Bard 2023.07.13, Claude 2, ChatGPT 4, and six ORL consultants were rated blindly on a 6-point Likert-scale for medical adequacy, comprehensibility, coherence, and conciseness. Given answers were compared to validated answers and evaluated for hazards. A modified Turing test was performed and character counts were compared. Results: LLMs answers ranked inferior to consultants in all categories. Yet, the difference between consultants and LLMs was marginal, with the clearest disparity in conciseness and the smallest in comprehensibility. Among LLMs Claude 2 was rated best in medical adequacy and conciseness. Consultants’ answers matched the validated solution in 93% (228/246), ChatGPT 4 in 85% (35/41), Claude 2 in 78% (32/41), and Bard 2023.07.13 in 59% (24/41). Answers were rated as potentially hazardous in 10% (24/246) for ChatGPT 4, 14% (34/246) for Claude 2, 19% (46/264) for Bard 2023.07.13, and 6% (71/1230) for consultants. Conclusions and significance: Despite consultants superior performance, LLMs show potential for clinical application in ORL. Future studies should assess their performance on larger scale. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.
PB  - Taylor and Francis Ltd.
C2  - 38781053
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Wang, T.
AU  - Codling, D.
AU  - Bhugra, D.
AU  - Msosa, Y.
AU  - Broadbent, M.
AU  - Patel, R.
AU  - Roberts, A.
AU  - McGuire, P.
AU  - Stewart, R.
AU  - Dobson, R.
AU  - Harland, R.
TI  - Unraveling ethnic disparities in antipsychotic prescribing among patients with psychosis: A retrospective cohort study based on electronic clinical records
PY  - 2023
T2  - Schizophrenia Research
VL  - 260
SP  - 168
EP  - 179
DO  - 10.1016/j.schres.2023.08.024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172382327&doi=10.1016%2fj.schres.2023.08.024&partnerID=40&md5=e688592944d01161dc323e9547a572d8
AB  - Background: Previous studies have shown mixed evidence on ethnic disparities in antipsychotic prescribing among patients with psychosis in the UK, partly due to small sample sizes. This study aimed to examine the current state of antipsychotic prescription with respect to patient ethnicity among the entire population known to a large UK mental health trust with non-affective psychosis, adjusting for multiple potential risk factors. Methods: This retrospective cohort study included all patients (N = 19,291) who were aged 18 years or over at their first diagnoses of non-affective psychosis (identified with the ICD-10 codes of F20–F29) recorded in electronic health records (EHRs) at the South London and Maudsley NHS Trust until March 2021. The most recently recorded antipsychotic treatments and patient attributes were extracted from EHRs, including both structured fields and free-text fields processed using natural language processing applications. Multivariable logistic regression models were used to calculate the odds ratios (OR) for antipsychotic prescription according to patient ethnicity, adjusted for multiple potential contributing factors, including demographic (age and gender), clinical (diagnoses, duration of illness, service use and history of cannabis use), socioeconomic factors (level of deprivation and own-group ethnic density in the area of residence) and temporal changes in clinical guidelines (date of prescription). Results: The cohort consisted of 43.10 % White, 8.31 % Asian, 40.80 % Black, 2.64 % Mixed, and 5.14 % of patients from Other ethnicity. Among them, 92.62 % had recorded antipsychotic receipt, where 24.05 % for depot antipsychotics and 81.72 % for second-generation antipsychotic (SGA) medications. Most ethnic minority groups were not significantly different from White patients in receiving any antipsychotic. Among those receiving antipsychotic prescribing, Black patients were more likely to be prescribed depot (adjusted OR 1.29, 95 % confidence interval (CI) 1.14–1.47), but less likely to receive SGA (adjusted OR 0.85, 95 % CI 0.74–0.97), olanzapine (OR 0.82, 95 % CI 0.73–0.92) and clozapine (adjusted OR 0.71, 95 % CI 0.6–0.85) than White patients. All the ethnic minority groups were less likely to be prescribed olanzapine than the White group. Conclusions: Black patients with psychosis had a distinct pattern in antipsychotic prescription, with less use of SGA, including olanzapine and clozapine, but more use of depot antipsychotics, even when adjusting for the effects of multiple demographic, clinical and socioeconomic factors. Further research is required to understand the sources of these ethnic disparities and eliminate care inequalities. © 2023 The Authors
PB  - Elsevier B.V.
C2  - 37669576
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Elsborg, J.
AU  - Salvatore, M.
TI  - Using LLMs and Explainable ML to Analyze Biomarkers at Single-Cell Level for Improved Understanding of Diseases
PY  - 2023
T2  - Biomolecules
VL  - 13
IS  - 10
C7  - 1516
DO  - 10.3390/biom13101516
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175045709&doi=10.3390%2fbiom13101516&partnerID=40&md5=e9d6c663be6fa12c3edd43ad727f29ad
AB  - Single-cell RNA sequencing (scRNA-seq) technology has significantly advanced our understanding of the diversity of cells and how this diversity is implicated in diseases. Yet, translating these findings across various scRNA-seq datasets poses challenges due to technical variability and dataset-specific biases. To overcome this, we present a novel approach that employs both an LLM-based framework and explainable machine learning to facilitate generalization across single-cell datasets and identify gene signatures to capture disease-driven transcriptional changes. Our approach uses scBERT, which harnesses shared transcriptomic features among cell types to establish consistent cell-type annotations across multiple scRNA-seq datasets. Additionally, we employed a symbolic regression algorithm to pinpoint highly relevant, yet minimally redundant models and features for inferring a cell type’s disease state based on its transcriptomic profile. We ascertained the versatility of these cell-specific gene signatures across datasets, showcasing their resilience as molecular markers to pinpoint and characterize disease-associated cell types. The validation was carried out using four publicly available scRNA-seq datasets from both healthy individuals and those suffering from ulcerative colitis (UC). This demonstrates our approach’s efficacy in bridging disparities specific to different datasets, fostering comparative analyses. Notably, the simplicity and symbolic nature of the retrieved gene signatures facilitate their interpretability, allowing us to elucidate underlying molecular disease mechanisms using these models. © 2023 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
C2  - 37892198
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Kurniawan, M.H.
AU  - Handiyani, H.
AU  - Nuraini, T.
AU  - Hariyati, R.T.S.
AU  - Sutrisno, S.
TI  - A systematic review of artificial intelligence-powered (AI-powered) chatbot intervention for managing chronic illness
PY  - 2024
T2  - Annals of Medicine
VL  - 56
IS  - 1
C7  - 2302980
DO  - 10.1080/07853890.2024.2302980
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187458783&doi=10.1080%2f07853890.2024.2302980&partnerID=40&md5=8a9cd4a164e23ab962ea5875f1e63433
AB  - Background: Utilizing artificial intelligence (AI) in chatbots, especially for chronic diseases, has become increasingly prevalent. These AI-powered chatbots serve as crucial tools for enhancing patient communication, addressing the rising prevalence of chronic conditions, and meeting the growing demand for supportive healthcare applications. However, there is a notable gap in comprehensive reviews evaluating the impact of AI-powered chatbot interventions in healthcare within academic literature. This study aimed to assess user satisfaction, intervention efficacy, and the specific characteristics and AI architectures of chatbot systems designed for chronic diseases. Method: A thorough exploration of the existing literature was undertaken by employing diverse databases such as PubMed MEDLINE, CINAHL, EMBASE, PsycINFO, ACM Digital Library and Scopus. The studies incorporated in this analysis encompassed primary research that employed chatbots or other forms of AI architecture in the context of preventing, treating or rehabilitating chronic diseases. The assessment of bias risk was conducted using Risk of 2.0 Tools. Results: Seven hundred and eighty-four results were obtained, and subsequently, eight studies were found to align with the inclusion criteria. The intervention methods encompassed health education (n = 3), behaviour change theory (n = 1), stress and coping (n = 1), cognitive behavioural therapy (n = 2) and self-care behaviour (n = 1). The research provided valuable insights into the effectiveness and user-friendliness of AI-powered chatbots in handling various chronic conditions. Overall, users showed favourable acceptance of these chatbots for self-managing chronic illnesses. Conclusions: The reviewed studies suggest promising acceptance of AI-powered chatbots for self-managing chronic conditions. However, limited evidence on their efficacy due to insufficient technical documentation calls for future studies to provide detailed descriptions and prioritize patient safety. These chatbots employ natural language processing and multimodal interaction. Subsequent research should focus on evidence-based evaluations, facilitating comparisons across diverse chronic health conditions. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.
PB  - Taylor and Francis Ltd.
C2  - 38466897
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 23
ER  -

TY  - JOUR
AU  - Praveen, G.
AU  - Poornima, U.L.S.
AU  - Akkaloori, A.
AU  - Bharathi, V.
TI  - ChatGPT as a Tool for Oral Health Education: A Systematic Evaluation of ChatGPT Responses to Patients' Oral Health-related Queries
PY  - 2024
T2  - Journal of Nature and Science of Medicine
VL  - 7
IS  - 3
SP  - 154
EP  - 157
DO  - 10.4103/jnsm.jnsm_208_23
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199507001&doi=10.4103%2fjnsm.jnsm_208_23&partnerID=40&md5=c2a5676b1c6c3f7d73d59ad40d30b484
AB  - Background: ChatGPT holds promise in oral health education, provided valid concerns are proactively examined and addressed. Hence, this study was conducted to evaluate ChatGPT responses to patients' most common queries about their oral health. Methods: A cross-sectional study was conducted to gather a dataset of oral health-related queries from patients attending a dental institution. The dataset was preprocessed and formatted to remove any irrelevant or duplicate queries. Then, we supplied the dataset to ChatGPT to generate responses. We asked two dental public health experts to independently review the ChatGPT responses for clarity, accuracy, relevance, comprehensiveness, consistency, acceptance, and bias using a 5-point Likert scale. The intraclass correlation coefficient (ICC) was used to evaluate interrater reliability. Scores were summarized using descriptive statistics. Results: A total of 563 oral health-related queries were gathered from 120 patients. After removing the irrelevant or duplicate queries, 105 were included in the final dataset. The ICC value of 0.878 (95% confidence interval range from 0.841 to 0.910) showed good reliability between the reviewers. The majority of ChatGPT responses had a clear understanding (95.24%), were scientifically accurate and relevant to the query (87.62%), were comprehensive (83.81%), were consistent (84.76%), and were acceptable without any edits (86.67%). The reviewers strongly agreed that only 40.96% of the responses had no bias. The overall score was high with a mean value of 4.72 ± 0.30. The qualitative analysis of comments on ChatGPT responses revealed that the responses were rather long and more comprehensive. Conclusions: ChatGPT generated clear, scientifically accurate and relevant, comprehensive, and consistent responses to diverse oral health-related queries despite some significant limitations. Copyright © 2024 Journal of Nature and Science of Medicine.
PB  - Wolters Kluwer Medknow Publications
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Garcia Valencia, O.A.
AU  - Thongprayoon, C.
AU  - Miao, J.
AU  - Bruminhent, J.
AU  - Craici, I.M.
AU  - Cheungpasitporn, W.
TI  - Perspectives on AI-based recommendations for mask-wearing and COVID-19 vaccination for transplant recipients in the post-COVID-19 era
PY  - 2024
T2  - Renal Failure
VL  - 46
IS  - 1
C7  - 2337291
DO  - 10.1080/0886022X.2024.2337291
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189432425&doi=10.1080%2f0886022X.2024.2337291&partnerID=40&md5=6998d38521a4c1713e8320282ca3a29f
AB  - In the aftermath of the COVID-19 pandemic, the ongoing necessity for preventive measures such as mask-wearing and vaccination remains particularly critical for organ transplant recipients, a group highly susceptible to infections due to immunosuppressive therapy. Given that many individuals nowadays increasingly utilize Artificial Intelligence (AI), understanding AI perspectives is important. Thus, this study utilizes AI, specifically ChatGPT 4.0, to assess its perspectives in offering precise health recommendations for mask-wearing and COVID-19 vaccination tailored to this vulnerable population. Through a series of scenarios reflecting diverse environmental settings and health statuses in December 2023, we evaluated the AI’s responses to gauge its precision, adaptability, and potential biases in advising high-risk patient groups. Our findings reveal that ChatGPT 4.0 consistently recommends mask-wearing in crowded and indoor environments for transplant recipients, underscoring their elevated risk. In contrast, for settings with fewer transmission risks, such as outdoor areas where social distancing is possible, the AI suggests that mask-wearing might be less imperative. Regarding vaccination guidance, the AI strongly advocates for the COVID-19 vaccine across most scenarios for kidney transplant recipients. However, it recommends a personalized consultation with healthcare providers in cases where patients express concerns about vaccine-related side effects, demonstrating an ability to adapt recommendations based on individual health considerations. While this study provides valuable insights into the current AI perspective on these important topics, it is crucial to note that the findings do not directly reflect or influence health policy. Nevertheless, given the increasing utilization of AI in various domains, understanding AI’s viewpoints on such critical matters is essential for informed decision-making and future research. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.
PB  - Taylor and Francis Ltd.
C2  - 38584142
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Tavabi, N.
AU  - Raza, M.
AU  - Singh, M.
AU  - Golchin, S.
AU  - Singh, H.
AU  - Hogue, G.D.
AU  - Kiapour, A.M.
TI  - Disparities in cannabis use and documentation in electronic health records among children and young adults
PY  - 2023
T2  - npj Digital Medicine
VL  - 6
IS  - 1
C7  - 138
DO  - 10.1038/s41746-023-00885-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168245637&doi=10.1038%2fs41746-023-00885-w&partnerID=40&md5=e8ef76448bdb39bba8286122f3b9d929
AB  - The legalizations of medical and recreational cannabis have generated a great deal of interest in studying the health impacts of cannabis products. Despite increases in cannabis use, its documentation during clinical visits is not yet mainstream. This lack of information hampers efforts to study cannabis’s effects on health outcomes. A clear and in-depth understanding of current trends in cannabis use documentation is necessary to develop proper guidelines to screen and document cannabis use. Here we have developed and used a natural language processing pipeline to evaluate the trends and disparities in cannabis documentation. The pipeline includes a screening step to identify clinical notes with cannabis use documentation which is then fed into a BERT-based classifier to confirm positive use. This pipeline is applied to more than 23 million notes from a large cohort of 370,087 patients seen in a high-volume multi-site pediatric and young adult clinic over a period of 21 years. Our findings show a very low but growing rate of cannabis use documentation (<2%) in electronic health records with significant demographic and socioeconomic disparities in both documentation and positive use, which requires further attention. © 2023, Springer Nature Limited.
PB  - Nature Research
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - JOUR
AU  - Kong, Q.-Z.
AU  - Ju, K.-P.
AU  - Wan, M.
AU  - Liu, J.
AU  - Wu, X.-Q.
AU  - Li, Y.-Y.
AU  - Zuo, X.-L.
AU  - Li, Y.-Q.
TI  - Comparative analysis of large language models in medical counseling: A focus on Helicobacter pylori infection
PY  - 2024
T2  - Helicobacter
VL  - 29
IS  - 1
C7  - e13055
DO  - 10.1111/hel.13055
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184396048&doi=10.1111%2fhel.13055&partnerID=40&md5=1785358e15ecda108f6b9aa05b36df81
AB  - Background: Large language models (LLMs) are promising medical counseling tools, but the reliability of responses remains unclear. We aimed to assess the feasibility of three popular LLMs as counseling tools for Helicobacter pylori infection in different counseling languages. Materials and Methods: This study was conducted between November 20 and December 1, 2023. Three large language models (ChatGPT 4.0 [LLM1], ChatGPT 3.5 [LLM2], and ERNIE Bot 4.0 [LLM3]) were input 15 H. pylori related questions each, once in English and once in Chinese. Each chat was conducted using the “New Chat” function to avoid bias from correlation interference. Responses were recorded and blindly assigned to three reviewers for scoring on three established Likert scales: accuracy (ranged 1–6 point), completeness (ranged 1–3 point), and comprehensibility (ranged 1–3 point). The acceptable thresholds for the scales were set at a minimum of 4, 2, and 2, respectively. Final various source and interlanguage comparisons were made. Results: The overall mean (SD) accuracy score was 4.80 (1.02), while 1.82 (0.78) for completeness score and 2.90 (0.36) for comprehensibility score. The acceptable proportions for the accuracy, completeness, and comprehensibility of the responses were 90%, 45.6%, and 100%, respectively. The acceptable proportion of overall completeness score for English responses was better than for Chinese responses (p = 0.034). For accuracy, the English responses of LLM3 were better than the Chinese responses (p = 0.0055). As for completeness, the English responses of LLM1 was better than the Chinese responses (p = 0.0257). For comprehensibility, the English responses of LLM1 was better than the Chinese responses (p = 0.0496). No differences were found between the various LLMs. Conclusions: The LLMs responded satisfactorily to questions related to H. pylori infection. But further improving completeness and reliability, along with considering language nuances, is crucial for optimizing overall performance. © 2024 John Wiley & Sons Ltd.
PB  - John Wiley and Sons Inc
C2  - 39078641
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Vest, J.R.
AU  - Mazurenko, O.
TI  - Non-response Bias in Social Risk Factor Screening Among Adult Emergency Department Patients
PY  - 2023
T2  - Journal of Medical Systems
VL  - 47
IS  - 1
C7  - 78
DO  - 10.1007/s10916-023-01975-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165362008&doi=10.1007%2fs10916-023-01975-8&partnerID=40&md5=2bee283acc825c47252b1d4a06567255
AB  - Healthcare organizations increasingly use screening questionnaires to assess patients’ social factors, but non-response may contribute to selection bias. This study assessed differences between respondents and those refusing participation in a social factor screening. We used a cross-sectional approach with logistic regression models to measure the association between subject characteristics and social factor screening questionnaire participation. The study subjects were patients from a mid-western state safety-net hospital’s emergency department. Subjects’ inclusion criteria were: (1) ≥ 18 years old, (2) spoke English or Spanish, and (3) able to complete a self-administered questionnaire. We classified subjects that consented and answered the screening questionnaire in full as respondents. All others were non-respondents. Using natural language processing, we linked all subjects’ participation status to demographic characteristics, clinical data, an area-level deprivation measure, and social risk factors extracted from clinical notes. We found that nearly 6 out of every 10 subjects approached (59.9%), consented, and completed the questionnaire. Subjects with prior documentation of financial insecurity were 22% less likely to respond to the screening questionnaire (marginal effect = -22.40; 95% confidence interval (CI) = -41.16, -3.63; p = 0.019). No other factors were significantly associated with response. This study uniquely contributes to the growing social determinants of health literature by confirming that selection bias may exist within social factor screening practices and research studies. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
PB  - Springer
C2  - 37480515
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Xie, S.
AU  - Hassanpour, S.
AU  - Vosoughi, S.
TI  - Addressing Healthcare-related Racial and LGBTQ+ Biases in Pretrained Language Models
PY  - 2024
T2  - Findings of the Association for Computational Linguistics: NAACL 2024 - Findings
SP  - 4451
EP  - 4464
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197860837&partnerID=40&md5=2afeff2f6c62ec7dcf001812d003310a
AB  - Recent studies have highlighted the issue of Pretrained Language Models (PLMs) inadvertently propagating social stigmas and stereotypes, a critical concern given their widespread use. This is particularly problematic in sensitive areas like healthcare, where such biases could lead to detrimental outcomes. Our research addresses this by adapting two intrinsic bias benchmarks to quantify racial and LGBTQ+ biases in prevalent PLMs. We also empirically evaluate the effectiveness of various debiasing methods in mitigating these biases. Furthermore, we assess the impact of debiasing on both Natural Language Understanding and specific biomedical applications. Our findings reveal that while PLMs commonly exhibit healthcare-related racial and LGBTQ+ biases, the applied debiasing techniques successfully reduce these biases without compromising the models' performance in downstream tasks. © 2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Micale, C.
AU  - Golder, S.
AU  - O’Connor, K.
AU  - Weissenbacher, D.
AU  - Gross, R.
AU  - Hennessy, S.
AU  - Gonzalez-Hernandez, G.
TI  - Patient-Reported Reasons for Antihypertensive Medication Change: A Quantitative Study Using Social Media
PY  - 2024
T2  - Drug Safety
VL  - 47
IS  - 1
SP  - 81
EP  - 91
DO  - 10.1007/s40264-023-01366-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177689924&doi=10.1007%2fs40264-023-01366-5&partnerID=40&md5=0fe7835599bbcb16d46d104b92b28b41
AB  - Introduction: Hypertension is the leading cause of heart disease in the world, and discontinuation or nonadherence of antihypertensive medication constitutes a significant global health concern. Patients with hypertension have high rates of medication nonadherence. Studies of reasons for nonadherence using traditional surveys are limited, can be expensive, and suffer from response, white-coat, and recall biases. Mining relevant posts by patients on social media is inexpensive and less impacted by the pressures and biases of formal surveys, which may provide direct insights into factors that lead to non-compliance with antihypertensive medication. Methods: This study examined medication ratings posted to WebMD, an online health forum that allows patients to post medication reviews. We used a previously developed natural language processing classifier to extract indications and reasons for changes in angiotensin receptor II blocker (ARB) and angiotensin-converting enzyme inhibitor (ACEI) treatments. After extraction, ratings were manually annotated and compared with data from the US Food and Drug administration (FDA) Adverse Events Reporting System (FAERS) public database. Results: From a collection of 343,459 WebMD reviews, we automatically extracted 1867 posts mentioning changes in ACEIs or ARBs, and manually reviewed the 300 most recent posts regarding ACEI treatments and the 300 most recent posts regarding ARB treatments. After excluding posts that only mentioned a dose change or were a false-positive mention, 142 posts in the ARBs dataset and 187 posts in the ACEIs dataset remained. The majority of posts (97% ARBs, 91% ACEIs) indicated experiencing an adverse event as the reason for medication change. The most common adverse events reported mapped to the Medical Dictionary for Regulatory Activities were “musculoskeletal and connective tissue disorders” like muscle and joint pain for ARBs, and “respiratory, thoracic, and mediastinal disorders” like cough and shortness of breath for ACEIs. These categories also had the largest differences in percentage points, appearing more frequently on WebMD data than FDA data (p < 0.001). Conclusion: Musculoskeletal and respiratory symptoms were the most commonly reported adverse effects in social media postings associated with drug discontinuation. Managing such symptoms is a potential target of interventions seeking to improve medication persistence. © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG.
PB  - Adis
C2  - 37995049
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Méndez-Cruz, C.-F.
AU  - Rodríguez-Herrera, J.
AU  - Varela-Vega, A.
AU  - Mateo-Estrada, V.
AU  - Castillo-Ramírez, S.
TI  - Unsupervised learning and natural language processing highlight research trends in a superbug
PY  - 2024
T2  - Frontiers in Artificial Intelligence
VL  - 7
C7  - 1336071
DO  - 10.3389/frai.2024.1336071
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189312620&doi=10.3389%2ffrai.2024.1336071&partnerID=40&md5=06e9cb855967ecbf421421d4ccac5f5f
AB  - Antibiotic-resistance Acinetobacter baumannii is a very important nosocomial pathogen worldwide. Thousands of studies have been conducted about this pathogen. However, there has not been any attempt to use all this information to highlight the research trends concerning this pathogen. Here we use unsupervised learning and natural language processing (NLP), two areas of Artificial Intelligence, to analyse the most extensive database of articles created (5,500+ articles, from 851 different journals, published over 3 decades). K-means clustering found 113 theme clusters and these were defined with representative terms automatically obtained with topic modelling, summarising different research areas. The biggest clusters, all with over 100 articles, are biased toward multidrug resistance, carbapenem resistance, clinical treatment, and nosocomial infections. However, we also found that some research areas, such as ecology and non-human infections, have received very little attention. This approach allowed us to study research themes over time unveiling those of recent interest, such as the use of cefiderocol (a recently approved antibiotic) against A. baumannii. In a broader context, our results show that unsupervised learning, NLP and topic modelling can be used to describe and analyse the research themes for important infectious diseases. This strategy should be very useful to analyse other ESKAPE pathogens or any other pathogens relevant to Public Health. Copyright © 2024 Méndez-Cruz, Rodríguez-Herrera, Varela-Vega, Mateo-Estrada and Castillo-Ramírez.
PB  - Frontiers Media SA
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Stefan, R.
AU  - Carutasu, G.
AU  - Mocan, M.
TI  - Ethical Considerations in the Implementation and Usage of Large Language Models
PY  - 2024
T2  - Lecture Notes in Networks and Systems
VL  - 928 LNNS
SP  - 131
EP  - 144
DO  - 10.1007/978-3-031-54671-6_10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190708966&doi=10.1007%2f978-3-031-54671-6_10&partnerID=40&md5=cd0a7374e162b343b8ca32de5b374f0d
AB  - This paper aims to investigate the ethical considerations surrounding the implementation and usage of large language models (LLMs). LLMs have shown tremendous advance in natural language processing and have the potential to revolutionize various fields, including education, healthcare, and many more. The wide adoption of LLMs and rapid increase in usage is also due to the ease of use (via chat interface) and the remarkable fast, coherent response, relative to the input prompt. However, their widespread use raises ethical questions that have not yet been fully considered. This study explores the ethical implications of LLMs and their impact on society, focusing on the need for transparency, fairness, and responsible usage. We analyze the current state of research in this area and provide recommendations for future research and development of LLMs that prioritize ethical considerations. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Liu, X.
AU  - Wu, J.
AU  - Shao, A.
AU  - Shen, W.
AU  - Ye, P.
AU  - Wang, Y.
AU  - Ye, J.
AU  - Jin, K.
AU  - Yang, J.
TI  - Uncovering Language Disparity of ChatGPT on Retinal Vascular Disease Classification: Cross-Sectional Study
PY  - 2024
T2  - Journal of Medical Internet Research
VL  - 26
IS  - 1
C7  - e51926
DO  - 10.2196/51926
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183083014&doi=10.2196%2f51926&partnerID=40&md5=65999d5e4838e647654de84449db5392
AB  - Background: Benefiting from rich knowledge and the exceptional ability to understand text, large language models like ChatGPT have shown great potential in English clinical environments. However, the performance of ChatGPT in non-English clinical settings, as well as its reasoning, have not been explored in depth. Objective: This study aimed to evaluate ChatGPT's diagnostic performance and inference abilities for retinal vascular diseases in a non-English clinical environment. Methods: In this cross-sectional study, we collected 1226 fundus fluorescein angiography reports and corresponding diagnoses written in Chinese and tested ChatGPT with 4 prompting strategies (direct diagnosis or diagnosis with a step-by-step reasoning process and in Chinese or English). Results: Compared with ChatGPT using Chinese prompts for direct diagnosis that achieved an F1-score of 70.47%, ChatGPT using English prompts for direct diagnosis achieved the best diagnostic performance (80.05%), which was inferior to ophthalmologists (89.35%) but close to ophthalmologist interns (82.69%). As for its inference abilities, although ChatGPT can derive a reasoning process with a low error rate (0.4 per report) for both Chinese and English prompts, ophthalmologists identified that the latter brought more reasoning steps with less incompleteness (44.31%), misinformation (1.96%), and hallucinations (0.59%) (all P < .001). Also, analysis of the robustness of ChatGPT with different language prompts indicated significant differences in the recall (P = .03) and F1-score (P = .04) between Chinese and English prompts. In short, when prompted in English, ChatGPT exhibited enhanced diagnostic and inference capabilities for retinal vascular disease classification based on Chinese fundus fluorescein angiography reports. Conclusions: ChatGPT can serve as a helpful medical assistant to provide diagnosis in non-English clinical environments, but there are still performance gaps, language disparities, and errors compared to professionals, which demonstrate the potential limitations and the need to continually explore more robust large language models in ophthalmology practice. © 2024 Journal of Medical Internet Research. All rights reserved.
PB  - JMIR Publications Inc.
C2  - 38252483
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 29
ER  -

TY  - CONF
AU  - Jia, X.
AU  - Xiong, Y.
AU  - Zhang, Y.
AU  - Luo, L.
TI  - Multi-Modal Fusion with Semantic Supervision for Radiology Report Generation
PY  - 2023
T2  - Frontiers in Artificial Intelligence and Applications
VL  - 372
SP  - 1140
EP  - 1147
DO  - 10.3233/FAIA230389
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175795071&doi=10.3233%2fFAIA230389&partnerID=40&md5=dd1a8f01f8e8fc74dd128f6dce314f2f
AB  - Radiology report generation, one way of analyzing radiology images, is to generate a textual report automatically for the given image, and it is of great significance to assist diagnosis and alleviate the workload of radiologists. Some report generation methods have been therefore proposed. However, these methods suffer from the problem of low-quality generation, because of the visual and textual bias and training with text similarity oriented objective. To solve this problem, we propose a novel radiology report generation model with multi-modal fusion and semantic supervision, namely MS-Gen. MS-Gen consists of two main components, i.e., the semantic-visual fusion module and the semantic weighted contrastive loss. Specifically, the main idea of the semantic-visual fusion module is to make use of the domain-specific prior knowledge contained in a large pre-trained visual-language model and also the complementary nature between the image and text modalities. Moreover, a novel optimization term, i.e., the semantic weighted contrastive loss, is proposed to guide the optimization process with semantic similarity objective, and further enforce the generated reports with higher clinical accuracy. Extensive experiments conducted on two real datasets of IU X-Ray and MIMIC-CXR demonstrate the effectiveness of MS-Gen. © 2023 The Authors.
PB  - IOS Press BV
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Castonguay, A.
AU  - Farthing, P.
AU  - Davies, S.
AU  - Vogelsang, L.
AU  - Kleib, M.
AU  - Risling, T.
AU  - Green, N.
TI  - Revolutionizing nursing education through Ai integration: A reflection on the disruptive impact of ChatGPT
PY  - 2023
T2  - Nurse Education Today
VL  - 129
C7  - 105916
DO  - 10.1016/j.nedt.2023.105916
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165897387&doi=10.1016%2fj.nedt.2023.105916&partnerID=40&md5=ab0be68c49c862f0c4c21c63efb92057
AB  - Artificial intelligence (AI) is driving global change. An AI language model like ChatGPT could revolutionize the delivery of nursing education in the future. ChatGPT is an AI-enabled text generator that has garnered significant attention due to its ability to engage in conversations and answer questions. Nurse educators play a crucial role in preparing nursing students for a technology-integrated healthcare system, and the emergence of ChatGPT presents both opportunities and challenges. While the technology has limitations and potential biases, it also has the potential to benefit students by facilitating learning, improving digital literacy, and encouraging critical thinking about AI integration in healthcare. Nurse educators can incorporate ChatGPT into their curriculum through formative or summative assessments and should prioritize faculty development to understand and use AI technologies effectively. Collaboration between educational institutions, regulatory bodies, and educators is crucial to establish provincial and national competencies and frameworks that reflect the increasing importance of AI in nursing education and practice. It is paramount that nurses and nurse educators be open to AI-enabled innovations as well as continue to critically think about their potential value to advance the profession so nurses are better prepared to lead the digital future. © 2023 Elsevier Ltd
PB  - Churchill Livingstone
C2  - 37515957
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 55
ER  -

TY  - CONF
TI  - C-Journal: A Journaling Application for Detecting and Classifying Cognitive Distortions using Deep-Learning based on a Crowd-sourced Dataset
PY  - 2024
T2  - 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING 2024 - Main Conference Proceedings
SP  - 3224
EP  - 3234
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195893062&partnerID=40&md5=f1c675e2036381ee6abf48508165c53e
AB  - Cognitive distortions are negatively biased thinking patterns and erroneous self-statements resulting from and leading to logical errors in one's own internal reasoning. Cognitive distortions have an adverse effect on mental health and can lead to mental health disorders in extreme cases. This paper belongs to a bigger project which aims to provide an application for detecting and classifying cognitive distortions in texts. As no public data sets were available for the task, the first contribution of the proposed work lies in providing an open-source labeled dataset of 14 cognitive distortions consisting of 34370 entries collected via crowd-sourcing, user questionnaires, and re-purposing emotions dataset from social media. The dataset is collected in cooperation with a licensed psychologist. We implemented a baseline model using Naïve Bayes and Count Vectorizer and different CNN, LSTM, and DNN classifiers to classify cognitive distortions based on the dataset. We investigated the usage of different word embeddings with the best-performing models. The best-performing model relied on a CNN with pre-trained Sentence-BERT embedding with an F1-score of 84% for classifying cognitive distortions. The best-performing model was built into C-Journal, a free journaling and mood-tracking mobile application that pinpoints potential thinking distortions to the users. © 2024 ELRA Language Resource Association: CC BY-NC 4.0.
PB  - European Language Resources Association (ELRA)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Gao, L.
AU  - Zhan, H.
AU  - Sheng, V.S.
TI  - Mitigate Gender Bias Using Negative Multi-task Learning
PY  - 2023
T2  - Neural Processing Letters
VL  - 55
IS  - 8
SP  - 11131
EP  - 11146
DO  - 10.1007/s11063-023-11368-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165960414&doi=10.1007%2fs11063-023-11368-0&partnerID=40&md5=66c26879b271afb9cf8e95b0ebd3560c
AB  - Deep learning models have showcased remarkable performances in natural language processing tasks. While much attention has been paid to improvements in utility, privacy leakage and social bias are two major concerns arising in trained models. In this paper, we address both privacy protection and gender bias mitigation in classification models simultaneously. We first introduce a selective privacy-preserving method that obscures individuals’ sensitive information by adding noise to word embeddings. Then, we propose a negative multi-task learning framework to mitigate gender bias, which involves a main task and a gender prediction task. The main task employs a positive loss constraint for utility assurance, while the gender prediction task utilizes a negative loss constraint to remove gender-specific features. We have analyzed four existing word embeddings and evaluated them for sentiment analysis and medical text classification tasks within the proposed negative multi-task learning framework. For instances, RoBERTa achieves the best performance with an average accuracy of 95% for both negative and positive sentiment, with 1.1 disparity score and 1.6 disparity score respectively, and GloVe achieves the best average accuracy of 96.42% with a 0.28 disparity score for the medical task. Our experimental results indicate that our negative multi-task learning framework can effectively mitigate gender bias while maintaining model utility for both sentiment analysis and medical text classification. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
PB  - Springer
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Preiksaitis, C.
AU  - Nash, C.
AU  - Gottlieb, M.
AU  - Chan, T.M.
AU  - Alvarez, A.
AU  - Landry, A.
TI  - Brain versus bot: Distinguishing letters of recommendation authored by humans compared with artificial intelligence
PY  - 2023
T2  - AEM Education and Training
VL  - 7
IS  - 6
C7  - AET210924
DO  - 10.1002/aet2.10924
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178207341&doi=10.1002%2faet2.10924&partnerID=40&md5=0134656a8c9c77913de5fc99acafacbf
AB  - Objectives: Letters of recommendation (LORs) are essential within academic medicine, affecting a number of important decisions regarding advancement, yet these letters take significant amounts of time and labor to prepare. The use of generative artificial intelligence (AI) tools, such as ChatGPT, are gaining popularity for a variety of academic writing tasks and offer an innovative solution to relieve the burden of letter writing. It is yet to be determined if ChatGPT could aid in crafting LORs, particularly in high-stakes contexts like faculty promotion. To determine the feasibility of this process and whether there is a significant difference between AI and human-authored letters, we conducted a study aimed at determining whether academic physicians can distinguish between the two. Methods: A quasi-experimental study was conducted using a single-blind design. Academic physicians with experience in reviewing LORs were presented with LORs for promotion to associate professor, written by either humans or AI. Participants reviewed LORs and identified the authorship. Statistical analysis was performed to determine accuracy in distinguishing between human and AI-authored LORs. Additionally, the perceived quality and persuasiveness of the LORs were compared based on suspected and actual authorship. Results: A total of 32 participants completed letter review. The mean accuracy of distinguishing between human- versus AI-authored LORs was 59.4%. The reviewer's certainty and time spent deliberating did not significantly impact accuracy. LORs suspected to be human-authored were rated more favorably in terms of quality and persuasiveness. A difference in gender-biased language was observed in our letters: human-authored letters contained significantly more female-associated words, while the majority of AI-authored letters tended to use more male-associated words. Conclusions: Participants were unable to reliably differentiate between human- and AI-authored LORs for promotion. AI may be able to generate LORs and relieve the burden of letter writing for academicians. New strategies, policies, and guidelines are needed to balance the benefits of AI while preserving integrity and fairness in academic promotion decisions. © 2023 The Authors. AEM Education and Training published by Wiley Periodicals LLC on behalf of Society for Academic Emergency Medicine.
PB  - John Wiley and Sons Inc
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - CONF
AU  - Ma, T.
AU  - Falkenburg, B.
AU  - Speier, W.
TI  - Incorporating flash adjacency into the classifier for a language model-based P300 Speller
PY  - 2024
T2  - 2024 IEEE 4th International Conference on Human-Machine Systems, ICHMS 2024
DO  - 10.1109/ICHMS59971.2024.10555607
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197403224&doi=10.1109%2fICHMS59971.2024.10555607&partnerID=40&md5=9307701fbf14887e84dc3f49c035d1b4
AB  - The P300 speller is a common brain-computer interface (BCI) application designed to allow patients with neuromuscular disorders such as amyotrophic lateral sclerosis (ALS) produce text output through the detection of P300 signals in their electroencephalogram (EEG) signals. The standard P300 speller relies on the detection of signals evoked by visual stimuli, usually consisting of rows and columns highlighted in a grid of characters. Since the visual field is substantially larger than these stimuli, adjacent flashes to the attended characters may cause false positive signals and lead to erroneous output. Previous studies have tried to address this issue by limiting the number of adjacent stimuli. However, it is not possible to completely avoid adjacent stimuli, so the problem cannot be eliminated. In this study, we instead account for these adjacency false positives in the classifier by utilizing information from adjacent flashes to optimize the system. This optimization is accomplished by adding a bias to the target character detection based on adjacent flashes which creates a new probability model to improve the accuracy and speed of classification. We tested our adjacency classifier in both the standard P300 paradigm and in conjunction with natural language processing. The new algorithm was evaluated offline on a dataset of 69 healthy subjects, which showed increases in speed and accuracy when compared to standard classification methods. On a population level, our adjacency model led to increased performance in the standard P300 paradigm, but the improvement in the presence of NLP was not significant. On an individual level, some subjects had substantial improvements in both settings (the number of subjects who saw an ITR increase of at least 5 bits/minute were 57.4% and 21.7% for the standard and NLP paradigms, respectively), suggesting that incorporating adjacent flesh information into the classifier can potentially provide a better communication system for some users.  © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Peng, X.
AU  - Tan, T.
AU  - Fan, T.
TI  - Automatic ICD Coding Based on Bias Removal
PY  - 2024
T2  - 2024 IEEE 2nd International Conference on Control, Electronics and Computer Technology, ICCECT 2024
SP  - 53
EP  - 57
DO  - 10.1109/ICCECT60629.2024.10546138
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195926999&doi=10.1109%2fICCECT60629.2024.10546138&partnerID=40&md5=9aa95c1b04ada350cef3d33257bf6585
AB  - The automated coding task aims to match medical text records with the corresponding International Classification of Diseases (ICD) codes to improve the efficiency and accuracy of medical record management. In the automatic coding task, existing methods often face the challenge of label bias, a problem that affects the overall performance of the model. To address this challenge, we propose a new bias removal method that aims to optimize model performance. Furthermore, we note that some samples are difficult to be recognized during the encoding process due to their complexity. Given the huge amount of data contained in the large language models, we attempted to use these models to recognize these hard samples and compared their effectiveness with our debiasing method. Test results on the MIMIC-III dataset show that we find our proposed debiasing method significantly outperforms the method that relies only on large language models in dealing with hard samples, thus confirming the effectiveness and superiority of our method. © 2024 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Tozzi, A.E.
AU  - Gesualdo, F.
AU  - Pandolfi, E.
AU  - Ferro, D.
AU  - Cinelli, G.
AU  - Bozzola, E.
AU  - Aversa, T.
AU  - Di Mauro, A.
AU  - Mameli, C.
AU  - Croci, I.
TI  - Prioritizing educational initiatives on emerging technologies for Italian pediatricians: bibliometric review and a survey
PY  - 2023
T2  - Italian Journal of Pediatrics
VL  - 49
IS  - 1
C7  - 112
DO  - 10.1186/s13052-023-01512-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169685629&doi=10.1186%2fs13052-023-01512-w&partnerID=40&md5=55f8f28238e4f7dff0726f77364710ff
AB  - Background: Emerging technologies have demonstrated outstanding potential in improving healthcare, yet their full integration remains a challenge for all medical specialties, including pediatrics. To support the swift implementation of technologies, we identified the current trends through a bibliometric review, and we conducted a survey on Italian pediatricians to gauge educational needs and willingness to integrate technologies into clinical practice. Methods: A working group of pediatricians representing various backgrounds designed and coordinated the study. To identify relevant topics for educational strategy development, we focused on virtual reality, telehealth, natural language processing, smartphone applications, robotics, genomics, and artificial intelligence. A bibliometric analysis limited to 2018–2023 was performed to identify trends and emerging applications within each topic. Based on the results, a questionnaire was developed and made available online to all Italian pediatricians. The results were analyzed through descriptive analysis and a multivariable logistic regression to explore associations between technology adoption and sociodemographic characteristics. Results: A total of 3,253 publications were found, with Telehealth and Telemedicine having the highest number of publications and Natural Language Processing the lowest. The number of respondents to the online questionnaire was 1,540, predominantly medical doctors with over 20 years of experience working as family pediatricians. Telehealth had the highest level of knowledge (95.2%), followed by smartphone applications (89.1%) and genomics (63.2%). The greatest potential for increased use through education programs was projected for natural language processing (+ 43.1%), artificial intelligence (+ 39.6%), and virtual and mixed reality (+ 38.1%). Female respondents and older individuals were less likely to use emerging technologies. Hospital pediatricians and residents were more likely to use AI. Conclusions: We developed a replicable strategy to identify emerging themes in medical technologies relevant to pediatrics and assess the educational needs of pediatricians. A significant gap still exists between current and potential usage of emerging technologies among Italian pediatricians although they showed a positive attitude towards implementing these technologies following specific education programs. The study highlights the need for comprehensive education programs on emerging technologies in pediatrics and recommends addressing gender and age disparities in technology adoption. © 2023, Società Italiana di Pediatria.
PB  - BioMed Central Ltd
C2  - 37667297
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Levkovich, I.
AU  - Elyoseph, Z.
TI  - Identifying depression and its determinants upon initiating treatment: ChatGPT versus primary care physicians
PY  - 2023
T2  - Family Medicine and Community Health
VL  - 11
IS  - 4
C7  - e002391
DO  - 10.1136/fmch-2023-002391
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174640818&doi=10.1136%2ffmch-2023-002391&partnerID=40&md5=b2f46334e8350a2f968321722a43b296
AB  - Objective To compare evaluations of depressive episodes and suggested treatment protocols generated by Chat Generative Pretrained Transformer (ChatGPT)-3 and ChatGPT-4 with the recommendations of primary care physicians. Methods Vignettes were input to the ChatGPT interface. These vignettes focused primarily on hypothetical patients with symptoms of depression during initial consultations. The creators of these vignettes meticulously designed eight distinct versions in which they systematically varied patient attributes (sex, socioeconomic status (blue collar worker or white collar worker) and depression severity (mild or severe)). Each variant was subsequently introduced into ChatGPT-3.5 and ChatGPT-4. Each vignette was repeated 10 times to ensure consistency and reliability of the ChatGPT responses. Results For mild depression, ChatGPT-3.5 and ChatGPT-4 recommended psychotherapy in 95.0% and 97.5% of cases, respectively. Primary care physicians, however, recommended psychotherapy in only 4.3% of cases. For severe cases, ChatGPT favoured an approach that combined psychotherapy, while primary care physicians recommended a combined approach. The pharmacological recommendations of ChatGPT-3.5 and ChatGPT-4 showed a preference for exclusive use of antidepressants (74% and 68%, respectively), in contrast with primary care physicians, who typically recommended a mix of antidepressants and anxiolytics/hypnotics (67.4%). Unlike primary care physicians, ChatGPT showed no gender or socioeconomic biases in its recommendations. Conclusion ChatGPT-3.5 and ChatGPT-4 aligned well with accepted guidelines for managing mild and severe depression, without showing the gender or socioeconomic biases observed among primary care physicians. Despite the suggested potential benefit of using atificial intelligence (AI) chatbots like ChatGPT to enhance clinical decision making, further research is needed to refine AI recommendations for severe cases and to consider potential risks and ethical issues. © Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.
PB  - BMJ Publishing Group
C2  - 37844967
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 35
ER  -

TY  - JOUR
AU  - Herrmann-Werner, A.
AU  - Festl-Wietek, T.
AU  - Holderried, F.
AU  - Herschbach, L.
AU  - Griewatz, J.
AU  - Masters, K.
AU  - Zipfel, S.
AU  - Mahling, M.
TI  - Assessing ChatGPT's Mastery of Bloom's Taxonomy Using Psychosomatic Medicine Exam Questions: Mixed-Methods Study
PY  - 2024
T2  - Journal of Medical Internet Research
VL  - 26
IS  - 1
C7  - e52113
DO  - 10.2196/52113
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183224105&doi=10.2196%2f52113&partnerID=40&md5=0bb89fbd1ef18bc48868aaf4422563f3
AB  - Background: Large language models such as GPT-4 (Generative Pre-trained Transformer 4) are being increasingly used in medicine and medical education. However, these models are prone to “hallucinations” (ie, outputs that seem convincing while being factually incorrect). It is currently unknown how these errors by large language models relate to the different cognitive levels defined in Bloom's taxonomy. Objective: This study aims to explore how GPT-4 performs in terms of Bloom's taxonomy using psychosomatic medicine exam questions. Methods: We used a large data set of psychosomatic medicine multiple-choice questions (N=307) with real-world results derived from medical school exams. GPT-4 answered the multiple-choice questions using 2 distinct prompt versions: detailed and short. The answers were analyzed using a quantitative approach and a qualitative approach. Focusing on incorrectly answered questions, we categorized reasoning errors according to the hierarchical framework of Bloom's taxonomy. Results: GPT-4's performance in answering exam questions yielded a high success rate: 93% (284/307) for the detailed prompt and 91% (278/307) for the short prompt. Questions answered correctly by GPT-4 had a statistically significant higher difficulty than questions answered incorrectly (P = .002 for the detailed prompt and P < .001 for the short prompt). Independent of the prompt, GPT-4's lowest exam performance was 78.9% (15/19), thereby always surpassing the “pass” threshold. Our qualitative analysis of incorrect answers, based on Bloom's taxonomy, showed that errors were primarily in the “remember” (29/68) and “understand” (23/68) cognitive levels; specific issues arose in recalling details, understanding conceptual relationships, and adhering to standardized guidelines. Conclusions: GPT-4 demonstrated a remarkable success rate when confronted with psychosomatic medicine multiple-choice exam questions, aligning with previous findings. When evaluated through Bloom's taxonomy, our data revealed that GPT-4 occasionally ignored specific facts (remember), provided illogical reasoning (understand), or failed to apply concepts to a new situation (apply). These errors, which were confidently presented, could be attributed to inherent model biases and the tendency to generate outputs that maximize likelihood. © 2024 Journal of Medical Internet Research. All rights reserved.
PB  - JMIR Publications Inc.
C2  - 38261378
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 25
ER  -

TY  - JOUR
AU  - Kunze, K.N.
AU  - Varady, N.H.
AU  - Mazzucco, M.
AU  - Lu, A.Z.
AU  - Chahla, J.
AU  - Martin, R.K.
AU  - Ranawat, A.S.
AU  - Pearle, A.D.
AU  - Williams, R.J.
TI  - The Large Language Model ChatGPT-4 Exhibits Excellent Triage Capabilities and Diagnostic Performance for Patients Presenting With Various Causes of Knee Pain
PY  - 2024
T2  - Arthroscopy - Journal of Arthroscopic and Related Surgery
DO  - 10.1016/j.arthro.2024.06.021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200318191&doi=10.1016%2fj.arthro.2024.06.021&partnerID=40&md5=c88fd9edcb660e979127bca897f71c69
AB  - Purpose: To provide a proof-of-concept analysis of the appropriateness and performance of ChatGPT-4 to triage, synthesize differential diagnoses, and generate treatment plans concerning common presentations of knee pain. Methods: Twenty knee complaints warranting triage and expanded scenarios were input into ChatGPT-4, with memory cleared prior to each new input to mitigate bias. For the 10 triage complaints, ChatGPT-4 was asked to generate a differential diagnosis that was graded for accuracy and suitability in comparison to a differential created by 2 orthopaedic sports medicine physicians. For the 10 clinical scenarios, ChatGPT-4 was prompted to provide treatment guidance for the patient, which was again graded. To test the higher-order capabilities of ChatGPT-4, further inquiry into these specific management recommendations was performed and graded. Results: All ChatGPT-4 diagnoses were deemed appropriate within the spectrum of potential pathologies on a differential. The top diagnosis on the differential was identical between surgeons and ChatGPT-4 for 70% of scenarios, and the top diagnosis provided by the surgeon appeared as either the first or second diagnosis in 90% of scenarios. Overall, 16 of 30 diagnoses (53.3%) in the differential were identical. When provided with 10 expanded vignettes with a single diagnosis, the accuracy of ChatGPT-4 increased to 100%, with the suitability of management graded as appropriate in 90% of cases. Specific information pertaining to conservative management, surgical approaches, and related treatments was appropriate and accurate in 100% of cases. Conclusions: ChatGPT-4 provided clinically reasonable diagnoses to triage patient complaints of knee pain due to various underlying conditions that were generally consistent with differentials provided by sports medicine physicians. Diagnostic performance was enhanced when providing additional information, allowing ChatGPT-4 to reach high predictive accuracy for recommendations concerning management and treatment options. However, ChatGPT-4 may show clinically important error rates for diagnosis depending on prompting strategy and information provided; therefore, further refinements are necessary prior to implementation into clinical workflows. Clinical Relevance: Although ChatGPT-4 is increasingly being used by patients for health information, the potential for ChatGPT-4 to serve as a clinical support tool is unclear. In this study, we found that ChatGPT-4 was frequently able to diagnose and triage knee complaints appropriately as rated by sports medicine surgeons, suggesting that it may eventually be a useful clinical support tool. © 2024 Arthroscopy Association of North America
PB  - W.B. Saunders
C2  - 38925234
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - CONF
AU  - Bertolini, L.
AU  - Elce, V.
AU  - Michalak, A.
AU  - Widhoelzl, H.-S.
AU  - Bernardi, G.
AU  - Weeds, J.
TI  - Automatic Annotation of Dream Report’s Emotional Content with Large Language Models
PY  - 2024
T2  - CLPsych 2024 - 9th Workshop on Computational Linguistics and Clinical Psychology, Proceedings of the Workshop
SP  - 92
EP  - 107
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189754581&partnerID=40&md5=b2b572010764e2a653af05910d81ff12
AB  - In psychology and neuroscience, dreams are extensively studied both as a model to understand the neural bases of consciousness and for their relationship with psycho-physical well-being. The study of dream content typically relies on the analysis of verbal reports provided upon awakening. This task is classically performed through manual scoring provided by trained annotators, at a great time expense. While a consistent body of work suggests that natural language processing (NLP) tools can support the automatic analysis of dream reports, proposed methods lacked the ability to reason over a report’s full context and required extensive data pre-processing. Furthermore, in most cases, these methods were not validated against standard manual scoring approaches. In this work, we address these limitations by adopting large language models (LLMs) to study and replicate the manual annotation of dream reports, with a focus on reports’ emotions. Our results show that a text classification method based on BERT can achieve high performance, is resistant to biases, and shows promising results on data from a clinical population. Overall, results indicate that LLMs and NLP could find multiple successful applications in the analysis of large dream datasets and may favour reproducibility and comparability of results across research. ©2024 Association for Computational Linguistic.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Heudel, P.
AU  - Crochet, H.
AU  - Durand, T.
AU  - Zrounba, P.
AU  - Blay, J.-Y.
TI  - From data strategy to implementation to advance cancer research and cancer care: A French comprehensive cancer center experience
PY  - 2023
T2  - PLOS Digital Health
VL  - 2
IS  - 12
C7  - e0000415
DO  - 10.1371/journal.pdig.0000415
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201675957&doi=10.1371%2fjournal.pdig.0000415&partnerID=40&md5=22d30e02ffa6a65064cd8cfe7b573cc2
AB  - In a comprehensive cancer center, effective data strategies are essential to evaluate practices, and outcome, understanding the disease and prognostic factors, identifying disparities in cancer care, and overall developing better treatments. To achieve these goals, the Center Léon Bérard (CLB) considers various data collection strategies, including electronic medical records (EMRs), clinical trial data, and research projects. Advanced data analysis techniques like natural language processing (NLP) can be used to extract and categorize information from these sources to provide a more complete description of patient data. Data sharing is also crucial for collaboration across comprehensive cancer centers, but it must be done securely and in compliance with regulations like GDPR. To ensure data is shared appropriately, CLB should develop clear data sharing policies and share data in a controlled, standardized format like OSIRIS RWD, OMOP and FHIR. The UNICANCER initiative has launched the CONSORE project to support the development of a structured and standardized repository of patient data to improve cancer research and patient outcomes. Real-world data (RWD) studies are vital in cancer research as they provide a comprehensive and accurate picture of patient outcomes and treatment patterns. By incorporating RWD into data collection, analysis, and sharing strategies, comprehensive cancer centers can take a more comprehensive and patient-centered approach to cancer research. In conclusion, comprehensive cancer centers must take an integrated approach to data collection, analysis, and sharing to enhance their understanding of cancer and improve patient outcomes. Leveraging advanced data analytics techniques and developing effective data sharing policies can help cancer centers effectively harness the power of data to drive progress in cancer research. © 2023 Heudel et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
PB  - Public Library of Science
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Aragón, M.E.
AU  - Parapar, J.
AU  - Losada, D.E.
TI  - Delving into the Depths: Evaluating Depression Severity through BDI-biased Summaries
PY  - 2024
T2  - CLPsych 2024 - 9th Workshop on Computational Linguistics and Clinical Psychology, Proceedings of the Workshop
SP  - 12
EP  - 22
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189758387&partnerID=40&md5=1b42db824c8840cf9a75710f3b206e01
AB  - Depression is a global concern suffered by millions of people, significantly impacting their thoughts and behavior. Over the years, heightened awareness, spurred by health campaigns and other initiatives, has driven the study of this disorder using data collected from social media platforms. In our research, we aim to gauge the severity of symptoms related to depression among social media users. The ultimate goal is to estimate the user’s responses to a well-known standardized psychological questionnaire, the Beck Depression Inventory-II (BDI). This is a 21-question multiple-choice self-report inventory that covers multiple topics about how the subject has been feeling. Mining users’ social media interactions and understanding psychological states represents a challenging goal. To that end, we present here an approach based on search and summarization that extracts multiple BDI-biased summaries from the thread of users’ publications. We also leverage a robust large language model to estimate the potential answer for each BDI item. Our method involves several steps. First, we employ a search strategy based on sentence similarity to obtain pertinent extracts related to each topic in the BDI questionnaire. Next, we compile summaries of the content of these groups of extracts. Last, we exploit chatGPT to respond to the 21 BDI questions, using the summaries as contextual information in the prompt. Our model has undergone rigorous evaluation across various depression datasets, yielding encouraging results. The experimental report includes a comparison against an assessment done by expert humans and competes favorably with state-of-the-art methods. ©2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Conrad, E.J.
AU  - Rees, L.K.
TI  - Generative AI as a Catalyst to Elevate School Health Education
PY  - 2024
T2  - Journal of Physical Education, Recreation and Dance
VL  - 95
IS  - 6
SP  - 45
EP  - 51
DO  - 10.1080/07303084.2024.2355865
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200543620&doi=10.1080%2f07303084.2024.2355865&partnerID=40&md5=465fc901cd083a8314685215e7f5674e
AB  - Comprehensive school health education (HE), despite its link to enhancing both student health and academic achievement, often faces barriers to effective implementation. As pedagogical strategies evolve to integrate traditional methods with technological innovations like artificial intelligence (AI), the adoption of innovation in HE preparation and instruction becomes vital to maintain its relevance and increase student achievement. Leveraging generative AI holds immense promise for enhancing the HE curriculum, instruction, assessment, and learning environment. Presently available AI tools, such as ChatGPT, can facilitate dynamic generation and adaptive modification of the HE curriculum, incorporating best practices such as backward design, standards-based alignment, and a focus on skill development. AI technology can also enhance inclusivity and equity by developing curricula that incorporate academic supports and principles such as Universal Design for Learning to better tailor lessons to accommodate student needs. Assessment can be improved through the creation and modification of unique formative and summative evaluations. Additionally, supplemental activities and teaching strategies can be generated tailored to identified areas of growth. Finally, AI holds immense promise for multi-subject standard integration and collaboration aligning with the Whole School, Whole Community, Whole Child model. Through the considerate use of AI, educators can foster a more responsive and effective HE learning environment that aligns with the diverse and evolving needs of students. © 2024 SHAPE America.
PB  - Routledge
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Boolchandani, H.
AU  - Osborn, R.
AU  - Tiyyagura, G.
AU  - Sheares, B.
AU  - Chen, L.
AU  - Phatak, U.P.
AU  - Puthenpura, V.
AU  - Elder, R.W.
AU  - Lee, S.
AU  - Amster, L.
AU  - Langhan, M.L.
TI  - Words Used in Letters of Recommendation for Pediatric Residency Applicants: Demographic Differences and Impact on Interviews
PY  - 2023
T2  - Academic Pediatrics
VL  - 23
IS  - 8
SP  - 1614
EP  - 1619
DO  - 10.1016/j.acap.2023.02.012
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153928878&doi=10.1016%2fj.acap.2023.02.012&partnerID=40&md5=a034cf012bca5ca18e65e06b6feaee86
AB  - Objective: To describe differences in agentic (achievement) and communal (relationship) terms in letters of recommendation (LORs) for pediatric residency candidates by applicant and letter writer demographics and to examine if LOR language is associated with interview status. Methods: A random sample of applicant profiles and LORs submitted to one institution were analyzed from the 2020–21 Match. Letters of recommendation text was inputted into a customized natural language processing application which determined the frequency of agentic and communal words in each LOR. Neutral LORs were defined as having< 5% surplus of agentic or communal terms. Results: We analyzed 2094 LORs from 573 applicants: 78% were women, 24% were under-represented in medicine (URiM), and 39% were invited to interview. Most letter writers were women (55%) and of senior academic rank (49%). Overall, 53% of LORs were agency biased, 25% communal biased, and 23% neutral. There was no difference in agency and communally biased LORs by an applicant's gender (men 53% agentic vs women 53% agentic, P =.424), race or ethnicity (non-URiM 53% agentic vs URiM 51% agentic, P =.631). Male letter writers used significantly more agentic terms (8.5%) compared to women (6.7% agentic) or writers of both genders (3.1% communal) (P =.008). Applicants invited to interview were more likely to have a neutral LOR; however, no significant association existed between language and interview status. Conclusions: No significant differences in language were found by applicant gender or race among pediatric residency candidates. Identifying potential biases within pediatric residency selection processes is important in creating an equitable approach to application review. © 2023 Academic Pediatric Association
PB  - Elsevier Inc.
C2  - 36889506
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Valdez, D.
AU  - Soto-Vásquez, A.D.
AU  - Montenegro, M.S.
TI  - Geospatial vaccine misinformation risk on social media: Online insights from an English/Spanish natural language processing (NLP) analysis of vaccine-related tweets
PY  - 2023
T2  - Social Science and Medicine
VL  - 339
C7  - 116365
DO  - 10.1016/j.socscimed.2023.116365
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177230159&doi=10.1016%2fj.socscimed.2023.116365&partnerID=40&md5=76d01a8db651b3bd261a9dbfdad3a5e4
AB  - Background: Misinformation is known to affect norms, attitudes, and intentions to engage with healthy behaviors. Evidence strongly supports that Spanish speakers may be particularly affected by misinformation and its outcomes, yet current insights into the scope and scale of misinformation is primarily ethnocentric, with greater emphasis on English-language design. Objective: This study applies Natural Language Processing (NLP) to analyze a corpus of English/Spanish tweets about vaccines, broadly defined, for misinformation indicators. Methods: We analyzed NEnglish = 247,140 and NSpanish = 104,445 tweets using Latent Dirichlet Allocation (LDA) topic models with Coherence score calculation (model fit) with a Mallet adjustment (topic optimization). We used informal coding to name computer-identified topics and compare misinformation scope and scale between languages. Results: The LDA analysis yielded a 12-topic solution for English and a 14-topic solution for Spanish. Both corpora contained overlapping misinformation, including uncertainty of research guiding policy recommendations or standing in support of antivax movements. However, the Spanish data were positioned in a global context, where misinformation was directed at government equity and disparate vaccine distribution. Conclusion: Our findings support that misinformation is a global issue. However, misinformation may vary depending on culture and language. As such, tailored strategies to combat misinformation in digital planes are strongly encouraged. © 2023
PB  - Elsevier Ltd
C2  - 37984184
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - Skryd, A.
AU  - Lawrence, K.
TI  - ChatGPT as a Tool for Medical Education and Clinical Decision-Making on the Wards: Case Study
PY  - 2024
T2  - JMIR Formative Research
VL  - 8
C7  - e51346
DO  - 10.2196/51346
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193741289&doi=10.2196%2f51346&partnerID=40&md5=71720eac6f1cc31bff17b782f68191c3
AB  - Background: Large language models (LLMs) are computational artificial intelligence systems with advanced natural language processing capabilities that have recently been popularized among health care students and educators due to their ability to provide real-time access to a vast amount of medical knowledge. The adoption of LLM technology into medical education and training has varied, and little empirical evidence exists to support its use in clinical teaching environments. Objective: The aim of the study is to identify and qualitatively evaluate potential use cases and limitations of LLM technology for real-time ward-based educational contexts. Methods: A brief, single-site exploratory evaluation of the publicly available ChatGPT-3.5 (OpenAI) was conducted by implementing the tool into the daily attending rounds of a general internal medicine inpatient service at a large urban academic medical center. ChatGPT was integrated into rounds via both structured and organic use, using the web-based “chatbot” style interface to interact with the LLM through conversational free-text and discrete queries. A qualitative approach using phenomenological inquiry was used to identify key insights related to the use of ChatGPT through analysis of ChatGPT conversation logs and associated shorthand notes from the clinical sessions. Results: Identified use cases for ChatGPT integration included addressing medical knowledge gaps through discrete medical knowledge inquiries, building differential diagnoses and engaging dual-process thinking, challenging medical axioms, using cognitive aids to support acute care decision-making, and improving complex care management by facilitating conversations with subspecialties. Potential additional uses included engaging in difficult conversations with patients, exploring ethical challenges and general medical ethics teaching, personal continuing medical education resources, developing ward-based teaching tools, supporting and automating clinical documentation, and supporting productivity and task management. LLM biases, misinformation, ethics, and health equity were identified as areas of concern and potential limitations to clinical and training use. A code of conduct on ethical and appropriate use was also developed to guide team usage on the wards. Conclusions: Overall, ChatGPT offers a novel tool to enhance ward-based learning through rapid information querying, second-order content exploration, and engaged team discussion regarding generated responses. More research is needed to fully understand contexts for educational use, particularly regarding the risks and limitations of the tool in clinical settings and its impacts on trainee development. © Anthony Skryd, Katharine Lawrence.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - JOUR
AU  - Ting, Y.-T.
AU  - Hsieh, T.-C.
AU  - Wang, Y.-F.
AU  - Kuo, Y.-C.
AU  - Chen, Y.-J.
AU  - Chan, P.-K.
AU  - Kao, C.-H.
TI  - Performance of ChatGPT incorporated chain-of-thought method in bilingual nuclear medicine physician board examinations
PY  - 2024
T2  - Digital Health
VL  - 10
DO  - 10.1177/20552076231224074
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181686828&doi=10.1177%2f20552076231224074&partnerID=40&md5=f8361ff07775250063dfabb6fd25bf69
AB  - Objective: This research explores the performance of ChatGPT, compared to human doctors, in bilingual, Mandarin Chinese and English, medical specialty exam in Nuclear Medicine in Taiwan. Methods: The study employed generative pre-trained transformer (GPT-4) and integrated chain-of-thoughts (COT) method to enhance performance by triggering and explaining the thinking process to answer the question in a coherent and logical manner. Questions from the Taiwanese Nuclear Medicine Specialty Exam served as the basis for testing. The research analyzed the correctness of AI responses in different sections of the exam and explored the influence of question length and language proportion on accuracy. Results: AI, especially ChatGPT with COT, exhibited exceptional capabilities in theoretical knowledge, clinical medicine, and handling integrated questions, often surpassing, or matching human doctor performance. However, AI struggled with questions related to medical regulations. The analysis of question length showed that questions within the 109–163 words range yielded the highest accuracy. Moreover, an increase in the proportion of English words in questions improved both AI and human accuracy. Conclusions: This research highlights the potential and challenges of AI in the medical field. ChatGPT demonstrates significant competence in various aspects of medical knowledge. However, areas like medical regulations require improvement. The study also suggests that AI may help in evaluating exam question difficulty and maintaining fairness in examinations. These findings shed light on AI role in the medical field, with potential applications in healthcare education, exam preparation, and multilingual environments. Ongoing AI advancements are expected to further enhance AI utility in the medical domain. © The Author(s) 2024.
PB  - SAGE Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Bilotta, I.
AU  - Tonidandel, S.
AU  - Liaw, W.R.
AU  - King, E.
AU  - Carvajal, D.N.
AU  - Taylor, A.
AU  - Thamby, J.
AU  - Xiang, Y.
AU  - Tao, C.
AU  - Hansen, M.
TI  - Examining Linguistic Differences in Electronic Health Records for Diverse Patients With Diabetes: Natural Language Processing Analysis
PY  - 2024
T2  - JMIR Medical Informatics
VL  - 12
C7  - 50428
DO  - 10.2196/50428
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195809541&doi=10.2196%2f50428&partnerID=40&md5=2ea673e5e2dc60f7d1bdb8e0ac845a64
AB  - Background: Individuals from minoritized racial and ethnic backgrounds experience pernicious and pervasive health disparities that have emerged, in part, from clinician bias. Objective: We used a natural language processing approach to examine whether linguistic markers in electronic health record (EHR) notes differ based on the race and ethnicity of the patient. To validate this methodological approach, we also assessed the extent to which clinicians perceive linguistic markers to be indicative of bias. Methods: In this cross-sectional study, we extracted EHR notes for patients who were aged 18 years or older; had more than 5 years of diabetes diagnosis codes; and received care between 2006 and 2014 from family physicians, general internists, or endocrinologists practicing in an urban, academic network of clinics. The race and ethnicity of patients were defined as White non-Hispanic, Black non-Hispanic, or Hispanic or Latino. We hypothesized that Sentiment Analysis and Social Cognition Engine (SEANCE) components (ie, negative adjectives, positive adjectives, joy words, fear and disgust words, politics words, respect words, trust verbs, and well-being words) and mean word count would be indicators of bias if racial differences emerged. We performed linear mixed effects analyses to examine the relationship between the outcomes of interest (the SEANCE components and word count) and patient race and ethnicity, controlling for patient age. To validate this approach, we asked clinicians to indicate the extent to which they thought variation in the use of SEANCE language domains for different racial and ethnic groups was reflective of bias in EHR notes. Results: We examined EHR notes (n=12,905) of Black non-Hispanic, White non-Hispanic, and Hispanic or Latino patients (n=1562), who were seen by 281 physicians. A total of 27 clinicians participated in the validation study. In terms of bias, participants rated negative adjectives as 8.63 (SD 2.06), fear and disgust words as 8.11 (SD 2.15), and positive adjectives as 7.93 (SD 2.46) on a scale of 1 to 10, with 10 being extremely indicative of bias. Notes for Black non-Hispanic patients contained significantly more negative adjectives (coefficient 0.07, SE 0.02) and significantly more fear and disgust words (coefficient 0.007, SE 0.002) than those for White non-Hispanic patients. The notes for Hispanic or Latino patients included significantly fewer positive adjectives (coefficient −0.02, SE 0.007), trust verbs (coefficient −0.009, SE 0.004), and joy words (coefficient −0.03, SE 0.01) than those for White non-Hispanic patients. Conclusions: This approach may enable physicians and researchers to identify and mitigate bias in medical interactions, with the goal of reducing health disparities stemming from bias. © 2024 JMIR Publications Inc.. All rights reserved.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Sai, S.
AU  - Gaur, A.
AU  - Sai, R.
AU  - Chamola, V.
AU  - Guizani, M.
AU  - Rodrigues, J.J.P.C.
TI  - Generative AI for Transformative Healthcare: A Comprehensive Study of Emerging Models, Applications, Case Studies, and Limitations
PY  - 2024
T2  - IEEE Access
VL  - 12
SP  - 31078
EP  - 31106
DO  - 10.1109/ACCESS.2024.3367715
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186096742&doi=10.1109%2fACCESS.2024.3367715&partnerID=40&md5=9b8cb3434eebce4c892bcaddee80569b
AB  - Generative artificial intelligence (GAI) can be broadly described as an artificial intelligence system capable of generating images, text, and other media types with human prompts. GAI models like ChatGPT, DALL-E, and Bard have recently caught the attention of industry and academia equally. GAI applications span various industries like art, gaming, fashion, and healthcare. In healthcare, GAI shows promise in medical research, diagnosis, treatment, and patient care and is already making strides in real-world deployments. There has yet to be any detailed study concerning the applications and scope of GAI in healthcare. Addressing this research gap, we explore several applications, real-world scenarios, and limitations of GAI in healthcare. We examine how GAI models like ChatGPT and DALL-E can be leveraged to aid in the applications of medical imaging, drug discovery, personalized patient treatment, medical simulation and training, clinical trial optimization, mental health support, healthcare operations and research, medical chatbots, human movement simulation, and a few more applications. Along with applications, we cover four real-world healthcare scenarios that employ GAI: visual snow syndrome diagnosis, molecular drug optimization, medical education, and dentistry. We also provide an elaborate discussion on seven healthcare-customized LLMs like Med-PaLM, BioGPT, DeepHealth, etc.,Since GAI is still evolving, it poses challenges like the lack of professional expertise in decision making, risk of patient data privacy, issues in integrating with existing healthcare systems, and the problem of data bias which are elaborated on in this work along with several other challenges. We also put forward multiple directions for future research in GAI for healthcare.  © 2013 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 52
ER  -

TY  - CONF
AU  - Wang, Y.
AU  - Wang, Z.
AU  - Wang, W.
AU  - Chen, Q.
AU  - Huang, K.
AU  - Nguyen, A.
AU  - De, S.
TI  - DKE-Research at SemEval-2024 Task 2: Incorporating Data Augmentation with Generative Models and Biomedical Knowledge to Enhance Inference Robustness
PY  - 2024
T2  - SemEval 2024 - 18th International Workshop on Semantic Evaluation, Proceedings of the Workshop
SP  - 88
EP  - 94
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191198434&partnerID=40&md5=16934e4c667a1896b3ab68adb04f24dd
AB  - Safe and reliable natural language inference is critical for extracting insights from clinical trial reports but poses challenges due to biases in large pre-trained language models. This paper presents a novel data augmentation technique to improve model robustness for biomedical natural language inference in clinical trials. By generating synthetic examples through semantic perturbations and domain-specific vocabulary replacement and adding a new task for numerical and quantitative reasoning, we introduce greater diversity and reduce shortcut learning. Our approach, combined with multitask learning and the DeBERTa architecture, achieved significant performance gains on the NLI4CT 2024 benchmark compared to the original language models. Ablation studies validate the contribution of each augmentation method in improving robustness. Our best-performing model ranked 12th in terms of faithfulness and 8th in terms of consistency, respectively, out of the 32 participants. © 2024 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Fuchs, A.
AU  - Trachsel, T.
AU  - Weiger, R.
AU  - Eggmann, F.
TI  - ChatGPT's performance in dentistry and allergyimmunology assessments: a comparative study
PY  - 2023
T2  - Swiss dental journal
VL  - 134
IS  - 2
SP  - 1
EP  - 17
DO  - 10.61872/sdj-2024-06-01
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181249431&doi=10.61872%2fsdj-2024-06-01&partnerID=40&md5=da20775bbb0978f102176a8a63515845
AB  - Large language models (LLMs) such as ChatGPT have potential applications in healthcare, including dentistry. Priming, the practice of providing LLMs with initial, relevant information, is an approach to improve their output quality. This study aimed to evaluate the performance of ChatGPT 3 and ChatGPT 4 on self-assessment questions for dentistry, through the Swiss Federal Licensing Examination in Dental Medicine (SFLEDM), and allergy and clinical immunology, through the European Examination in Allergy and Clinical Immunology (EEAACI). The second objective was to assess the impact of priming on ChatGPT's performance. The SFLEDM and EEAACI multiple-choice questions from the University of Bern's Institute for Medical Education platform were administered to both ChatGPT versions, with and without priming. Performance was analyzed based on correct responses. The statistical analysis included Wilcoxon rank sum tests (alpha=0.05). The average accuracy rates in the SFLEDM and EEAACI assessments were 63.3% and 79.3%, respectively. Both ChatGPT versions performed better on EEAACI than SFLEDM, with ChatGPT 4 outperforming ChatGPT 3 across all tests. ChatGPT 3's performance exhibited a significant improvement with priming for both EEAACI (p=0.017) and SFLEDM (p=0.024) assessments. For ChatGPT 4, the priming effect was significant only in the SFLEDM assessment (p=0.038). The performance disparity between SFLEDM and EEAACI assessments underscores ChatGPT's varying proficiency across different medical domains, likely tied to the nature and amount of training data available in each field. Priming can be a tool for enhancing output, especially in earlier LLMs. Advancements from ChatGPT 3 to 4 highlight the rapid developments in LLM technology. Yet, their use in critical fields such as healthcare must remain cautious owing to LLMs' inherent limitations and risks. Copyright 2024 SWISS DENTAL JOURNAL SSO Science and Clinical Topics. License: This work is licensed under a Creative Commons Attribution 4.0 International License.
C2  - 38726506
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 13
ER  -

TY  - JOUR
AU  - Abu Hammour, K.
AU  - Alhamad, H.
AU  - Al-Ashwal, F.Y.
AU  - Halboup, A.
AU  - Abu Farha, R.
AU  - Abu Hammour, A.
TI  - ChatGPT in pharmacy practice: a cross-sectional exploration of Jordanian pharmacists' perception, practice, and concerns
PY  - 2023
T2  - Journal of Pharmaceutical Policy and Practice
VL  - 16
IS  - 1
C7  - 115
DO  - 10.1186/s40545-023-00624-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173750481&doi=10.1186%2fs40545-023-00624-2&partnerID=40&md5=0c7fbbefbe884d3aa7a0fc60897bf9d3
AB  - Objectives: The purpose of this study is to find out how much pharmacists know and have used ChatGPT in their practice. We investigated the advantages and disadvantages of utilizing ChatGPT in a pharmacy context, the amount of training necessary to use it proficiently, and the influence on patient care using a survey. Methods: This cross-sectional study was carried out between May and June 2023 to assess the potential and problems that pharmacists observed while integrating chatbots powered by AI (ChatGPT) in pharmacy practice. The correlation between perceived benefits and concerns was evaluated using Spearman's rho correlation due to the data's non-normal distribution.Any pharmacists licensed by the Jordanian Pharmacists Association were included in the study. A convenient sampling technique was used to choose the participants, and the study questionnaire was distributed utilizing an online medium (Facebook and WhatsApp). Anyone who expressed interest in taking part was given a link to the study's instructions so they may read them before giving their electronic consent and accessing the survey. Results: The potential advantages of ChatGPT in the pharmacy practice were widely acknowledged by the participants. The majority of participants (69.9%) concurred that educational material about pharmacy items or therapeutic areas can be provided using ChatGPT, with 66.9% of respondents believing that ChatGPT is a machine learning algorithm. Concerns about the accuracy of AI-generated responses were also prevalent. More than half of the participants (55.7%) raised the possibility that AI systems such as ChatGPT could pick up on and replicate prejudices and discriminatory patterns from the data they were trained on. Analysis shows a statistically significant positive link, albeit a minor one, between the perceived advantages of ChatGPT and its drawbacks (r = 0.255, p < 0.001). However, concerns were strongly correlated with knowledge of ChatGPT. In contrast to those who were either unsure or had not heard of ChatGPT (64.2%), individuals who had heard of it were more likely to have strong concerns (79.8%) (p = 0.002). Finally, the results show a statistically significant association between the frequency of ChatGPT use and positive perceptions of the tool (p < 0.001). Conclusions: Although ChatGPT has shown promise in health and pharmaceutical practice, its application should be rigorously regulated by evidence-based law. According to the study's findings, pharmacists support the use of ChatGPT in pharmacy practice but have concerns about its use due to ethical reasons, legal problems, privacy concerns, worries about the accuracy of the data generated, data learning, and bias risk. © 2023, Dr. Zaheer-Ud-Din Babar and Auckland UniServices Ltd.
PB  - BioMed Central Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 21
ER  -

TY  - JOUR
AU  - Roberts, R.H.R.
AU  - Ali, S.R.
AU  - Hutchings, H.A.
AU  - Dobbs, T.D.
AU  - Whitaker, I.S.
TI  - Comparative study of ChatGPT and human evaluators on the assessment of medical literature according to recognised reporting standards
PY  - 2023
T2  - BMJ Health and Care Informatics
VL  - 30
IS  - 1
C7  - e100830
DO  - 10.1136/bmjhci-2023-100830
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174641052&doi=10.1136%2fbmjhci-2023-100830&partnerID=40&md5=a1e938d683a19f07b5d19e818e74858f
AB  - Introduction Amid clinicians' challenges in staying updated with medical research, artificial intelligence (AI) tools like the large language model (LLM) ChatGPT could automate appraisal of research quality, saving time and reducing bias. This study compares the proficiency of ChatGPT3 against human evaluation in scoring abstracts to determine its potential as a tool for evidence synthesis. Methods We compared ChatGPT's scoring of implant dentistry abstracts with human evaluators using the Consolidated Standards of Reporting Trials for Abstracts reporting standards checklist, yielding an overall compliance score (OCS). Bland-Altman analysis assessed agreement between human and AI-generated OCS percentages. Additional error analysis included mean difference of OCS subscores, Welch's t-test and Pearson's correlation coefficient. Results Bland-Altman analysis showed a mean difference of 4.92% (95% CI 0.62%, 0.37%) in OCS between human evaluation and ChatGPT. Error analysis displayed small mean differences in most domains, with the highest in € conclusion' (0.764 (95% CI 0.186, 0.280)) and the lowest in € blinding' (0.034 (95% CI 0.818, 0.895)). The strongest correlations between were in € harms' (r=0.32, p<0.001) and € trial registration' (r=0.34, p=0.002), whereas the weakest were in € intervention' (r=0.02, p<0.001) and € objective' (r=0.06, p<0.001). Conclusion LLMs like ChatGPT can help automate appraisal of medical literature, aiding in the identification of accurately reported research. Possible applications of ChatGPT include integration within medical databases for abstract evaluation. Current limitations include the token limit, restricting its usage to abstracts. As AI technology advances, future versions like GPT4 could offer more reliable, comprehensive evaluations, enhancing the identification of high-quality research and potentially improving patient outcomes. © 2023 BMJ Publishing Group. All rights reserved.
PB  - BMJ Publishing Group
C2  - 37827724
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 13
ER  -

TY  - JOUR
AU  - Naqvi, W.M.
AU  - Shaikh, S.Z.
AU  - Mishra, G.V.
TI  - Large language models in physical therapy: time to adapt and adept
PY  - 2024
T2  - Frontiers in Public Health
VL  - 12
C7  - 1364660
DO  - 10.3389/fpubh.2024.1364660
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196308834&doi=10.3389%2ffpubh.2024.1364660&partnerID=40&md5=2a4fc237b4a72f8a6cf44a025d28aa44
AB  - Healthcare is experiencing a transformative phase, with artificial intelligence (AI) and machine learning (ML). Physical therapists (PTs) stand on the brink of a paradigm shift in education, practice, and research. Rather than visualizing AI as a threat, it presents an opportunity to revolutionize. This paper examines how large language models (LLMs), such as ChatGPT and BioMedLM, driven by deep ML can offer human-like performance but face challenges in accuracy due to vast data in PT and rehabilitation practice. PTs can benefit by developing and training an LLM specifically for streamlining administrative tasks, connecting globally, and customizing treatments using LLMs. However, human touch and creativity remain invaluable. This paper urges PTs to engage in learning and shaping AI models by highlighting the need for ethical use and human supervision to address potential biases. Embracing AI as a contributor, and not just a user, is crucial by integrating AI, fostering collaboration for a future in which AI enriches the PT field provided data accuracy, and the challenges associated with feeding the AI model are sensitively addressed. Copyright © 2024 Naqvi, Shaikh and Mishra.
PB  - Frontiers Media SA
C2  - 38887241
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Jazi, A.H.D.
AU  - Mahjoubi, M.
AU  - Shahabi, S.
AU  - Alqahtani, A.R.
AU  - Haddad, A.
AU  - Pazouki, A.
AU  - Prasad, A.
AU  - Safadi, B.Y.
AU  - Chiappetta, S.
AU  - Taskin, H.E.
AU  - Billy, H.T.
AU  - Kasama, K.
AU  - Mahawar, K.
AU  - Gawdat, K.
AU  - Rheinwalt, K.P.
AU  - Miller, K.A.
AU  - Kow, L.
AU  - Neto, M.G.
AU  - Yang, W.
AU  - Palermo, M.
AU  - Ghanem, O.M.
AU  - Lainas, P.
AU  - Peterli, R.
AU  - Kassir, R.
AU  - Puy, R.V.
AU  - Da Silva Ribeiro, R.J.
AU  - Verboonen, S.
AU  - Pintar, T.
AU  - Shabbir, A.
AU  - Musella, M.
AU  - Kermansaravi, M.
TI  - Bariatric Evaluation Through AI: a Survey of Expert Opinions Versus ChatGPT-4 (BETA-SEOV)
PY  - 2023
T2  - Obesity Surgery
VL  - 33
IS  - 12
SP  - 3971
EP  - 3980
DO  - 10.1007/s11695-023-06903-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174936088&doi=10.1007%2fs11695-023-06903-w&partnerID=40&md5=303230878b51d0652e36b57e0494b7dd
AB  - Background: Recent advancements in artificial intelligence, such as OpenAI’s ChatGPT-4, are revolutionizing various sectors, including healthcare. This study investigates the use of ChatGPT-4 in identifying suitable candidates for bariatric surgery and providing surgical recommendations to improve decision-making in obesity treatment amid the global obesity epidemic. Methods: We devised ten patient scenarios, thoughtfully encompassing a spectrum that spans from uncomplicated cases to more complex ones. Our objective was to delve into the decision-making process regarding the recommendation of bariatric surgery. From July 29th to August 10th, 2023, we conducted a voluntary online survey involving thirty prominent bariatric surgeons, ensuring that there was no predetermined bias in the selection of a specific type of bariatric surgery. This survey was designed to collect their insights on these scenarios and gain a deeper understanding of their professional experience and background in the field of bariatric surgery. Additionally, we consulted ChatGPT-4 in two separate conversations to evaluate its alignment with expert opinions on bariatric surgery options. Results: In 40% of the scenarios, disparities were identified between the two conversations with ChatGPT-4. It matched expert opinions in 30% of cases. Differences were noted in cases like gastrointestinal metaplasia and gastric adenocarcinoma, but there was alignment with conditions like endometriosis and GERD. Conclusion: The evaluation of ChatGPT-4’s role in determining bariatric surgery suitability uncovered both potential and shortcomings. Its alignment with experts was inconsistent, and it often overlooked key factors, emphasizing human expertise’s value. Its current use requires caution, and further refinement is needed for clinical application. Graphical Abstract: [Figure not available: see fulltext.] © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
PB  - Springer
C2  - 37889368
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 9
ER  -

TY  - JOUR
AU  - Guo, E.
AU  - Gupta, M.
AU  - Deng, J.
AU  - Park, Y.-J.
AU  - Paget, M.
AU  - Naugler, C.
TI  - Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study
PY  - 2024
T2  - Journal of Medical Internet Research
VL  - 26
IS  - 1
C7  - e48996
DO  - 10.2196/48996
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182288843&doi=10.2196%2f48996&partnerID=40&md5=bcefbc7b4792530573f06a8868a90229
AB  - Background: The systematic review of clinical research papers is a labor-intensive and time-consuming process that often involves the screening of thousands of titles and abstracts. The accuracy and efficiency of this process are critical for the quality of the review and subsequent health care decisions. Traditional methods rely heavily on human reviewers, often requiring a significant investment of time and resources. Objective: This study aims to assess the performance of the OpenAI generative pretrained transformer (GPT) and GPT-4 application programming interfaces (APIs) in accurately and efficiently identifying relevant titles and abstracts from real-world clinical review data sets and comparing their performance against ground truth labeling by 2 independent human reviewers. Methods: We introduce a novel workflow using the Chat GPT and GPT-4 APIs for screening titles and abstracts in clinical reviews. A Python script was created to make calls to the API with the screening criteria in natural language and a corpus of title and abstract data sets filtered by a minimum of 2 human reviewers. We compared the performance of our model against human-reviewed papers across 6 review papers, screening over 24,000 titles and abstracts. Results: Our results show an accuracy of 0.91, a macro F1-score of 0.60, a sensitivity of excluded papers of 0.91, and a sensitivity of included papers of 0.76. The interrater variability between 2 independent human screeners was κ=0.46, and the prevalence and bias-adjusted κ between our proposed methods and the consensus-based human decisions was κ=0.96. On a randomly selected subset of papers, the GPT models demonstrated the ability to provide reasoning for their decisions and corrected their initial decisions upon being asked to explain their reasoning for incorrect classifications. Conclusions: Large language models have the potential to streamline the clinical review process, save valuable time and effort for researchers, and contribute to the overall quality of clinical reviews. By prioritizing the workflow and acting as an aid rather than a replacement for researchers and reviewers, models such as GPT-4 can enhance efficiency and lead to more accurate and reliable conclusions in medical research. © 2024 Journal of Medical Internet Research. All rights reserved.
PB  - JMIR Publications Inc.
C2  - 38214966
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 41
ER  -

TY  - JOUR
AU  - Garcia Valencia, O.A.
AU  - Thongprayoon, C.
AU  - Miao, J.
AU  - Suppadungsuk, S.
AU  - Krisanapan, P.
AU  - Craici, I.M.
AU  - Jadlowiec, C.C.
AU  - Mao, S.A.
AU  - Mao, M.A.
AU  - Leeaphorn, N.
AU  - Budhiraja, P.
AU  - Cheungpasitporn, W.
TI  - Empowering inclusivity: improving readability of living kidney donation information with ChatGPT
PY  - 2024
T2  - Frontiers in Digital Health
VL  - 6
C7  - 1366967
DO  - 10.3389/fdgth.2024.1366967
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191170909&doi=10.3389%2ffdgth.2024.1366967&partnerID=40&md5=ca037dd842096fe1a443cd17bce6093d
AB  - Background: Addressing disparities in living kidney donation requires making information accessible across literacy levels, especially important given that the average American adult reads at an 8th-grade level. This study evaluated the effectiveness of ChatGPT, an advanced AI language model, in simplifying living kidney donation information to an 8th-grade reading level or below. Methods: We used ChatGPT versions 3.5 and 4.0 to modify 27 questions and answers from Donate Life America, a key resource on living kidney donation. We measured the readability of both original and modified texts using the Flesch-Kincaid formula. A paired t-test was conducted to assess changes in readability levels, and a statistical comparison between the two ChatGPT versions was performed. Results: Originally, the FAQs had an average reading level of 9.6 ± 1.9. Post-modification, ChatGPT 3.5 achieved an average readability level of 7.72 ± 1.85, while ChatGPT 4.0 reached 4.30 ± 1.71, both with a p-value <0.001 indicating significant reduction. ChatGPT 3.5 made 59.26% of answers readable below 8th-grade level, whereas ChatGPT 4.0 did so for 96.30% of the texts. The grade level range for modified answers was 3.4–11.3 for ChatGPT 3.5 and 1–8.1 for ChatGPT 4.0. Conclusion: Both ChatGPT 3.5 and 4.0 effectively lowered the readability grade levels of complex medical information, with ChatGPT 4.0 being more effective. This suggests ChatGPT's potential role in promoting diversity and equity in living kidney donation, indicating scope for further refinement in making medical information more accessible. 2024 Garcia Valencia, Thongprayoon, Miao, Suppadungsuk, Krisanapan, Craici, Jadlowiec, Mao, Mao, Leeaphorn, Budhiraja and Cheungpasitporn.
PB  - Frontiers Media SA
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - JOUR
AU  - Boley, S.
AU  - Sidebottom, A.
AU  - Vacquier, M.
AU  - Watson, D.
AU  - Van Eyll, B.
AU  - Friedman, S.
AU  - Friedman, S.
TI  - Racial Differences in Stigmatizing and Positive Language in Emergency Medicine Notes
PY  - 2024
T2  - Journal of Racial and Ethnic Health Disparities
DO  - 10.1007/s40615-024-02080-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198107908&doi=10.1007%2fs40615-024-02080-3&partnerID=40&md5=cf45e30416bf8844fd8d798bd0b91e10
AB  - Objective: Language used by providers in medical documentation may reveal evidence of race-related implicit bias. We aimed to use natural language processing (NLP) to examine if prevalence of stigmatizing language in emergency medicine (EM) encounter notes differs across patient race/ethnicity. Methods: In a retrospective cohort of EM encounters, NLP techniques identified stigmatizing and positive themes. Logistic regression models analyzed the association of race/ethnicity and themes within notes. Outcomes were the presence (or absence) of 7 different themes: 5 stigmatizing (difficult, non-compliant, skepticism, substance abuse/seeking, and financial difficulty) and 2 positive (compliment and compliant). Results: The sample included notes from 26,363 unique patients. NH Black patient notes were less likely to contain difficult (odds ratio (OR) 0.80, 95% confidence interval (CI), 0.73–0.88), skepticism (OR 0.87, 95% CI, 0.79–0.96), and substance abuse/seeking (OR 0.62, 95% CI, 0.56–0.70) compared to NH White patient notes but more likely to contain non-compliant (OR 1.26, 95% CI, 1.17–1.36) and financial difficulty (OR 1.14, 95% CI, 1.04–1.25). Hispanic patient notes were less likely to contain difficult (OR 0.68, 95% CI, 0.58–0.80) and substance abuse/seeking (OR 0.78, 95% CI, 0.66–0.93). NH NA/AI patient notes had twice the odds as NH White patient notes to contain a stigmatizing theme (OR 2.02, 95% CI, 1.64–2.49). Conclusions: Using an NLP model to analyze themes in EM notes across racial groups, we identified several inequities in the usage of positive and stigmatizing language. Interventions to minimize race-related implicit bias should be undertaken. © W. Montague Cobb-NMA Health Institute 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Oleck, N.C.
AU  - Naga, H.I.
AU  - Nichols, D.S.
AU  - Morris, M.X.
AU  - Dhingra, B.
AU  - Patel, A.
TI  - Navigating the Ethical Landmines of ChatGPT: Implications of Intelligent Chatbots in Plastic Surgery Clinical Practice
PY  - 2023
T2  - Plastic and Reconstructive Surgery - Global Open
VL  - 11
IS  - 9
SP  - E5290
DO  - 10.1097/GOX.0000000000005290
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173053772&doi=10.1097%2fGOX.0000000000005290&partnerID=40&md5=1c6b02268f7838577e9028b4dc9f3cc7
AB  - Summary: ChatGPT is a cutting-edge language model developed by OpenAI with the potential to impact all facets of plastic surgery from research to clinical practice. New applications for ChatGPT are emerging at a rapid pace in both the scientific literature and popular media. It is important for clinicians to understand the capabilities and limitations of these tools before patient-facing implementation. In this article, the authors explore some of the technical details behind ChatGPT: what it is, and what it is not. As with any emerging technology, attention should be given to the ethical and health equity implications of this technology on our plastic surgery patients. The authors explore these concerns within the framework of the foundational principles of biomedical ethics: patient autonomy, nonmaleficence, beneficence, and justice. ChatGPT and similar intelligent conversation agents have incredible promise in the field of plastic surgery but should be used cautiously and sparingly in their current form. To protect patients, it is imperative that societal guidelines for the safe use of this rapidly developing technology are developed. © 2023 Lippincott Williams and Wilkins. All rights reserved.
PB  - Lippincott Williams and Wilkins
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 9
ER  -

TY  - JOUR
AU  - Najafali, D.
AU  - Hinson, C.
AU  - Camacho, J.M.
AU  - Galbraith, L.G.
AU  - Tople, T.L.
AU  - Eble, D.
AU  - Weinstein, B.
AU  - Schechter, L.S.
AU  - Dorafshar, A.H.
AU  - Morrison, S.D.
TI  - Artificial intelligence knowledge of evidence-based recommendations in gender affirmation surgery and gender identity: is ChatGPT aware of WPATH recommendations?
PY  - 2023
T2  - European Journal of Plastic Surgery
VL  - 46
IS  - 6
SP  - 1169
EP  - 1176
DO  - 10.1007/s00238-023-02125-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171455177&doi=10.1007%2fs00238-023-02125-6&partnerID=40&md5=13c41d66ffbbe8b617accc90f6e733e5
AB  - Background: Artificial intelligence (AI) is evolving rapidly as are its uses in healthcare and scientific literature. There are concerns about whether AI like ChatGPT has implicit biases. This study explores ChatGPT’s ability to reference evidence-based recommendations related to gender-affirming surgery (GAS). Methods: ChatGPT was prompted using open-ended questions on GAS as well as the World Professional Association for Transgender Health Standards of Care (WPATH SOC) for the Health of Transgender and Gender Diverse People, Version 8’s statements of recommendations. Responses were analyzed based on agreement to and reference of WPATH SOC recommendations. Results: A total of 95 WPATH statements of recommendations were given to the chatbot. There were 70 (74%) agreements, 0 (0%) disagreements, and 25 (26%) neutral responses. WPATH was directly referenced in 12 (13%) responses. ChatGPT was successful in describing aspects of gender diversity, including the treatment of gender dysphoria. Conclusions: While often using neutral language, ChatGPT does intermittently reference WPATH and its evidence-based recommendations. As AI evolves, so can the spread of misinformation if it is not rooted in evidence-based recommendations. Furthermore, AI may serve as a viable tool for patient education on GAS. Level of evidence: Not gradable © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Hayawi, K.
AU  - Shahriar, S.
AU  - Alashwal, H.
AU  - Serhani, M.A.
TI  - Generative AI and large language models: A new frontier in reverse vaccinology
PY  - 2024
T2  - Informatics in Medicine Unlocked
VL  - 48
C7  - 101533
DO  - 10.1016/j.imu.2024.101533
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196551163&doi=10.1016%2fj.imu.2024.101533&partnerID=40&md5=5a781d1a2c3e4f47bce78c1984122c0f
AB  - Reverse vaccinology is an emerging concept in the field of vaccine development as it facilitates the identification of potential vaccine candidates. Biomedical research has been revolutionized with the recent innovations in Generative Artificial Intelligence (AI) and Large Language Models (LLMs). The intersection of these two technologies is explored in this study. In this study, the impact of Generative AI and LLMs in the field of vaccinology is explored. Through a comprehensive analysis of existing research, prospective use cases, and an experimental case study, this research highlights that LLMs and Generative AI have the potential to enhance the efficiency and accuracy of vaccine candidate identification. This work also discusses the ethical and privacy challenges, such as data consent and potential biases, raised by such applications that require careful consideration. This study paves the way for experts, researchers, and policymakers to further investigate the role and impact of Generative AI and LLM in vaccinology and medicine. © 2024 The Authors
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Spotnitz, M.
AU  - Idnay, B.
AU  - Gordon, E.R.
AU  - Shyu, R.
AU  - Zhang, G.
AU  - Liu, C.
AU  - Cimino, J.J.
AU  - Weng, C.
TI  - A Survey of Clinicians' Views of the Utility of Large Language Models
PY  - 2023
T2  - Applied Clinical Informatics
VL  - 15
IS  - 2
SP  - 306
EP  - 312
DO  - 10.1055/a-2281-7092
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187932164&doi=10.1055%2fa-2281-7092&partnerID=40&md5=62fdd0e8b911de30b65bdb66319054c7
AB  - Objectives Large language models (LLMs) like Generative pre-trained transformer (ChatGPT) are powerful algorithms that have been shown to produce human-like text from input data. Several potential clinical applications of this technology have been proposed and evaluated by biomedical informatics experts. However, few have surveyed health care providers for their opinions about whether the technology is fit for use. Methods We distributed a validated mixed-methods survey to gauge practicing clinicians' comfort with LLMs for a breadth of tasks in clinical practice, research, and education, which were selected from the literature. Results A total of 30 clinicians fully completed the survey. Of the 23 tasks, 16 were rated positively by more than 50% of the respondents. Based on our qualitative analysis, health care providers considered LLMs to have excellent synthesis skills and efficiency. However, our respondents had concerns that LLMs could generate false information and propagate training data bias. Our survey respondents were most comfortable with scenarios that allow LLMs to function in an assistive role, like a physician extender or trainee. Conclusion In a mixed-methods survey of clinicians about LLM use, health care providers were encouraging of having LLMs in health care for many tasks, and especially in assistive roles. There is a need for continued human-centered development of both LLMs and artificial intelligence in general.  © 2024 The Author(s).
PB  - Georg Thieme Verlag
C2  - 38442909
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 9
ER  -

TY  - JOUR
AU  - Beaney, T.
AU  - Clarke, J.
AU  - Salman, D.
AU  - Woodcock, T.
AU  - Majeed, A.
AU  - Barahona, M.
AU  - Aylin, P.
TI  - Identifying potential biases in code sequences in primary care electronic healthcare records: A retrospective cohort study of the determinants of code frequency
PY  - 2023
T2  - BMJ Open
VL  - 13
IS  - 9
C7  - e072884
DO  - 10.1136/bmjopen-2023-072884
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172787084&doi=10.1136%2fbmjopen-2023-072884&partnerID=40&md5=99f2ff893339e46e26960e65706eec95
AB  - Objectives To determine whether the frequency of diagnostic codes for long-term conditions (LTCs) in primary care electronic healthcare records (EHRs) is associated with (1) disease coding incentives, (2) General Practice (GP), (3) patient sociodemographic characteristics and (4) calendar year of diagnosis. Design Retrospective cohort study. Setting GPs in England from 2015 to 2022 contributing to the Clinical Practice Research Datalink Aurum dataset. Participants All patients registered to a GP with at least one incident LTC diagnosed between 1 January 2015 and 31 December 2019. Primary and secondary outcome measures The number of diagnostic codes for an LTC in (1) the first and (2) the second year following diagnosis, stratified by inclusion in the Quality and Outcomes Framework (QOF) financial incentive programme. Results 3 113 724 patients were included, with 7 723 365 incident LTCs. Conditions included in QOF had higher rates of annual coding than conditions not included in QOF (1.03 vs 0.32 per year, p<0.0001). There was significant variation in code frequency by GP which was not explained by patient sociodemographics. We found significant associations with patient sociodemographics, with a trend towards higher coding rates in people living in areas of higher deprivation for both QOF and non-QOF conditions. Code frequency was lower for conditions with follow-up time in 2020, associated with the onset of the COVID-19 pandemic. Conclusions The frequency of diagnostic codes for newly diagnosed LTCs is influenced by factors including patient sociodemographics, disease inclusion in QOF, GP practice and the impact of the COVID-19 pandemic. Natural language processing or other methods using temporally ordered code sequences should account for these factors to minimise potential bias.  © Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY. Published by BMJ.
PB  - BMJ Publishing Group
C2  - 37758674
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Allahqoli, L.
AU  - Ghiasvand, M.M.
AU  - Mazidimoradi, A.
AU  - Salehiniya, H.
AU  - Alkatout, I.
TI  - Diagnostic and Management Performance of ChatGPT in Obstetrics and Gynecology
PY  - 2023
T2  - Gynecologic and Obstetric Investigation
VL  - 88
IS  - 5
SP  - 310
EP  - 313
DO  - 10.1159/000533177
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171836526&doi=10.1159%2f000533177&partnerID=40&md5=8cee4f7cac575b9a0933feac1299ae43
AB  - Objectives: The use of artificial intelligence (AI) in clinical patient management and medical education has been advancing over time. ChatGPT was developed and trained recently, using a large quantity of textual data from the internet. Medical science is expected to be transformed by its use. The present study was conducted to evaluate the diagnostic and management performance of the ChatGPT AI model in obstetrics and gynecology. Design: A cross-sectional study was conducted. Participants/Materials, Setting, Methods: This study was conducted in Iran in March 2023. Medical histories and examination results of 30 cases were determined in six areas of obstetrics and gynecology. The cases were presented to a gynecologist and ChatGPT for diagnosis and management. Answers from the gynecologist and ChatGPT were compared, and the diagnostic and management performance of ChatGPT were determined. Results: Ninety percent (27 of 30) of the cases in obstetrics and gynecology were correctly handled by ChatGPT. Its responses were eloquent, informed, and free of a significant number of errors or misinformation. Even when the answers provided by ChatGPT were incorrect, the responses contained a logical explanation about the case as well as information provided in the question stem. Limitations: The data used in this study were taken from the electronic book and may reflect bias in the diagnosis of ChatGPT. Conclusions: This is the first evaluation of ChatGPT’s performance in diagnosis and management in the field of obstetrics and gynecology. It appears that ChatGPT has potential applications in the practice of medicine and is (currently) free and simple to use. However, several ethical considerations and limitations such as bias, validity, copyright infringement, and plagiarism need to be addressed in future studies. © 2023 S. Karger AG, Basel.
PB  - S. Karger AG
C2  - 37494894
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 17
ER  -

TY  - JOUR
AU  - de Hond, A.
AU  - van Buchem, M.
AU  - Fanconi, C.
AU  - Roy, M.
AU  - Blayney, D.
AU  - Kant, I.
AU  - Steyerberg, E.
AU  - Hernandez-Boussard, T.
TI  - Predicting Depression Risk in Patients With Cancer Using Multimodal Data: Algorithm Development Study
PY  - 2024
T2  - JMIR Medical Informatics
VL  - 12
IS  - 1
C7  - e51925
DO  - 10.2196/51925
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185786524&doi=10.2196%2f51925&partnerID=40&md5=612ec6d883c84fdad7b2eeef9f9da851
AB  - Background: Patients with cancer starting systemic treatment programs, such as chemotherapy, often develop depression. A prediction model may assist physicians and health care workers in the early identification of these vulnerable patients. Objective: This study aimed to develop a prediction model for depression risk within the first month of cancer treatment. Methods: We included 16,159 patients diagnosed with cancer starting chemo- or radiotherapy treatment between 2008 and 2021. Machine learning models (eg, least absolute shrinkage and selection operator [LASSO] logistic regression) and natural language processing models (Bidirectional Encoder Representations from Transformers [BERT]) were used to develop multimodal prediction models using both electronic health record data and unstructured text (patient emails and clinician notes). Model performance was assessed in an independent test set (n=5387, 33%) using area under the receiver operating characteristic curve (AUROC), calibration curves, and decision curve analysis to assess initial clinical impact use. Results: Among 16,159 patients, 437 (2.7%) received a depression diagnosis within the first month of treatment. The LASSO logistic regression models based on the structured data (AUROC 0.74, 95% CI 0.71-0.78) and structured data with email classification scores (AUROC 0.74, 95% CI 0.71-0.78) had the best discriminative performance. The BERT models based on clinician notes and structured data with email classification scores had AUROCs around 0.71. The logistic regression model based on email classification scores alone performed poorly (AUROC 0.54, 95% CI 0.52-0.56), and the model based solely on clinician notes had the worst performance (AUROC 0.50, 95% CI 0.49-0.52). Calibration was good for the logistic regression models, whereas the BERT models produced overly extreme risk estimates even after recalibration. There was a small range of decision thresholds for which the best-performing model showed promising clinical effectiveness use. The risks were underestimated for female and Black patients. Conclusions: The results demonstrated the potential and limitations of machine learning and multimodal models for predicting depression risk in patients with cancer. Future research is needed to further validate these models, refine the outcome label and predictors related to mental health, and address biases across subgroups. ©Anne de Hond, Marieke van Buchem, Claudio Fanconi, Mohana Roy, Douglas Blayney, Ilse Kant, Ewout Steyerberg, Tina Hernandez-Boussard.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - JOUR
AU  - Araujo, S.M.
AU  - Cruz-Correia, R.
TI  - Incorporating ChatGPT in Medical Informatics Education: Mixed Methods Study on Student Perceptions and Experiential Integration Proposals
PY  - 2024
T2  - JMIR Medical Education
VL  - 10
C7  - e51151
DO  - 10.2196/51151
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191892296&doi=10.2196%2f51151&partnerID=40&md5=b6a7db1a24148f92f6a762af3692fa7b
AB  - Background: The integration of artificial intelligence (AI) technologies, such as ChatGPT, in the educational landscape has the potential to enhance the learning experience of medical informatics students and prepare them for using AI in professional settings. The incorporation of AI in classes aims to develop critical thinking by encouraging students to interact with ChatGPT and critically analyze the responses generated by the chatbot. This approach also helps students develop important skills in the field of biomedical and health informatics to enhance their interaction with AI tools. Objective: The aim of the study is to explore the perceptions of students regarding the use of ChatGPT as a learning tool in their educational context and provide professors with examples of prompts for incorporating ChatGPT into their teaching and learning activities, thereby enhancing the educational experience for students in medical informatics courses. Methods: This study used a mixed methods approach to gain insights from students regarding the use of ChatGPT in education. To accomplish this, a structured questionnaire was applied to evaluate students’ familiarity with ChatGPT, gauge their perceptions of its use, and understand their attitudes toward its use in academic and learning tasks. Learning outcomes of 2 courses were analyzed to propose ChatGPT’s incorporation in master’s programs in medicine and medical informatics. Results: The majority of students expressed satisfaction with the use of ChatGPT in education, finding it beneficial for various purposes, including generating academic content, brainstorming ideas, and rewriting text. While some participants raised concerns about potential biases and the need for informed use, the overall perception was positive. Additionally, the study proposed integrating ChatGPT into 2 specific courses in the master’s programs in medicine and medical informatics. The incorporation of ChatGPT was envisioned to enhance student learning experiences and assist in project planning, programming code generation, examination preparation, workflow exploration, and technical interview preparation, thus advancing medical informatics education. In medical teaching, it will be used as an assistant for simplifying the explanation of concepts and solving complex problems, as well as for generating clinical narratives and patient simulators. Conclusions: The study’s valuable insights into medical faculty students’ perspectives and integration proposals for ChatGPT serve as an informative guide for professors aiming to enhance medical informatics education. The research delves into the potential of ChatGPT, emphasizes the necessity of collaboration in academic environments, identifies subject areas with discernible benefits, and underscores its transformative role in fostering innovative and engaging learning experiences. The envisaged proposals hold promise in empowering future health care professionals to work in the rapidly evolving era of digital health care. © 2024 JMIR Publications Inc.. All rights reserved.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 15
ER  -

TY  - JOUR
AU  - Kuang, Y.-R.
AU  - Zou, M.-X.
AU  - Niu, H.-Q.
AU  - Zheng, B.-Y.
AU  - Zhang, T.-L.
AU  - Zheng, B.-W.
TI  - ChatGPT encounters multiple opportunities and challenges in neurosurgery
PY  - 2023
T2  - International journal of surgery (London, England)
VL  - 109
IS  - 10
SP  - 2886
EP  - 2891
DO  - 10.1097/JS9.0000000000000571
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174641436&doi=10.1097%2fJS9.0000000000000571&partnerID=40&md5=8ae5156e74ac53685a2d063194626ce1
AB  - BACKGROUND: ChatGPT, powered by the GPT model and Transformer architecture, has demonstrated remarkable performance in the domains of medicine and healthcare, providing customized and informative responses. In our study, we investigated the potential of ChatGPT in the field of neurosurgery, focusing on its applications at the patient, neurosurgery student/resident, and neurosurgeon levels. METHOD: The authors conducted inquiries with ChatGPT from the viewpoints of patients, neurosurgery students/residents, and neurosurgeons, covering a range of topics, such as disease diagnosis, treatment options, prognosis, rehabilitation, and patient care. The authors also explored concepts related to neurosurgery, including fundamental principles and clinical aspects, as well as tools and techniques to enhance the skills of neurosurgery students/residents. Additionally, the authors examined disease-specific medical interventions and the decision-making processes involved in clinical practice. RESULTS: The authors received individual responses from ChatGPT, but they tended to be shallow and repetitive, lacking depth and personalization. Furthermore, ChatGPT may struggle to discern a patient's emotional state, hindering the establishment of rapport and the delivery of appropriate care. The language used in the medical field is influenced by technical and cultural factors, and biases in the training data can result in skewed or inaccurate responses. Additionally, ChatGPT's limitations include the inability to conduct physical examinations or interpret diagnostic images, potentially overlooking complex details and individual nuances in each patient's case. Moreover, its absence in the surgical setting limits its practical utility. CONCLUSION: Although ChatGPT is a powerful language model, it cannot substitute for the expertise and experience of trained medical professionals. It lacks the capability to perform physical examinations, make diagnoses, administer treatments, establish trust, provide emotional support, and assist in the recovery process. Moreover, the implementation of Artificial Intelligence in healthcare necessitates careful consideration of legal and ethical concerns. While recognizing the potential of ChatGPT, additional training with comprehensive data is necessary to fully maximize its capabilities. Copyright © 2023 The Author(s). Published by Wolters Kluwer Health, Inc.
C2  - 37352529
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 18
ER  -

TY  - JOUR
AU  - Chaudhary, M.
AU  - Kosyluk, K.
AU  - Thomas, S.
AU  - Neal, T.
TI  - On the use of aspect-based sentiment analysis of Twitter data to explore the experiences of African Americans during COVID-19
PY  - 2023
T2  - Scientific Reports
VL  - 13
IS  - 1
C7  - 10694
DO  - 10.1038/s41598-023-37592-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163589831&doi=10.1038%2fs41598-023-37592-1&partnerID=40&md5=c70f63c2fa299cd5ccf227ee7b9500bd
AB  - According to data from the U.S. Center for Disease Control and Prevention, as of June 2020, a significant number of African Americans had been infected with the coronavirus disease, experiencing disproportionately higher death rates compared to other demographic groups. These disparities highlight the urgent need to examine the experiences, behaviors, and opinions of the African American population in relation to the COVID-19 pandemic. By understanding their unique challenges in navigating matters of health and well-being, we can work towards promoting health equity, eliminating disparities, and addressing persistent barriers to care. Since Twitter data has shown significant promise as a representation of human behavior and for opinion mining, this study leverages Twitter data published in 2020 to characterize the pandemic-related experiences of the United States’ African American population using aspect-based sentiment analysis. Sentiment analysis is a common task in natural language processing that identifies the emotional tone (i.e., positive, negative, or neutral) of a text sample. Aspect-based sentiment analysis increases the granularity of sentiment analysis by also extracting the aspect for which sentiment is expressed. We developed a machine learning pipeline consisting of image and language-based classification models to filter out tweets not related to COVID-19 and those unlikely published by African American Twitter subscribers, leading to an analysis of nearly 4 million tweets. Overall, our results show that the majority of tweets had a negative tone, and that the days with larger numbers of published tweets often coincided with major U.S. events related to the pandemic as suggested by major news headlines (e.g., vaccine rollout). We also show how word usage evolved throughout the year (e.g., outbreak to pandemic and coronavirus to covid). This work also points to important issues like food insecurity and vaccine hesitation, along with exposing semantic relationships between words, such as covid and exhausted. As such, this work furthers understanding of how the nationwide progression of the pandemic may have impacted the narratives of African American Twitter users. © 2023, The Author(s).
PB  - Nature Research
C2  - 37394523
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Stork, L.
AU  - Zijdeman, R.L.
AU  - Tiddi, I.
AU  - ten Teije, A.
TI  - Enabling Social Demography Research Using Semantic Technologies
PY  - 2024
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 14665 LNCS
SP  - 199
EP  - 216
DO  - 10.1007/978-3-031-60635-9_12
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195271433&doi=10.1007%2f978-3-031-60635-9_12&partnerID=40&md5=10a992281c65455fccdecec30b48b3eb
AB  - A shift in scientific publishing from paper-based to knowledge-based practices promotes reproducibility, machine actionability and knowledge discovery. This is important for disciplines like social demography, where study indicators are often social constructs such as race or education, hypothesis tests are challenging to compare due to their limited temporal and spatial coverage, and research output is presented in natural language, which can be ambiguous and imprecise. In this work, we present the MIRA resource, to aid researchers in their research workflow, and publish FAIR findings. MIRA consists of: (1) an ontology for social demography research, (2) a method for automated ontology population by prompting Large Language Models, and (3) a knowledge graph populated in terms of the ontology by annotating a set of research papers on health inequality. The resource allows researchers to formally represent their social demography research hypotheses, discovering research biases and novel research questions. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Soun, R.S.
AU  - Nair, A.
TI  - ChatGPT for Mental Health Applications: A study on biases
PY  - 2023
T2  - ACM International Conference Proceeding Series
C7  - 38
DO  - 10.1145/3639856.3639894
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194846035&doi=10.1145%2f3639856.3639894&partnerID=40&md5=3e2905cd1a1c73925db0f4afeea797f0
AB  - Suicide is a serious global issue taking 800,000 lives every year. AI-assisted early detection of suicide risk and stress levels using a user's activity on online forums and social media websites has shown great promise previously. In addition to this, emerging large language models have shown massive potential to provide state-of-the-art-performance for downstream tasks like text classification. Therefore, we attempt to apply one such large language model, GPT3.5 turbo to mental health applications with 3 datasets spanning suicide risk severity and stress presence classification. We also study the inherent demographic biases induced in these models as a result of polluted pretraining data and find that the model consistently estimates the mental health status of young females between the ages of 18 and 30 years best.  © 2023 ACM.
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Jang, D.
AU  - Yun, T.-R.
AU  - Lee, C.-Y.
AU  - Kwon, Y.-K.
AU  - Kim, C.-E.
TI  - GPT-4 can pass the Korean National Licensing Examination for Korean Medicine Doctors
PY  - 2023
T2  - PLOS Digital Health
VL  - 2
IS  - 12
C7  - e0000416
DO  - 10.1371/journal.pdig.0000416
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183403485&doi=10.1371%2fjournal.pdig.0000416&partnerID=40&md5=7068e3215b8c82d54ab54d4424f80bb6
AB  - Traditional Korean medicine (TKM) emphasizes individualized diagnosis and treatment. This uniqueness makes AI modeling difficult due to limited data and implicit processes. Large language models (LLMs) have demonstrated impressive medical inference, even without advanced training in medical texts. This study assessed the capabilities of GPT-4 in TKM, using the Korean National Licensing Examination for Korean Medicine Doctors (KNLEKMD) as a benchmark. The K-NLEKMD, administered by a national organization, encompasses 12 major subjects in TKM. GPT-4 answered 340 questions from the 2022 KNLEKMD. We optimized prompts with Chinese-term annotation, English translation for questions and instruction, exam-optimized instruction, and self-consistency. GPT-4 with optimized prompts achieved 66.18% accuracy, surpassing both the examination’s average pass mark of 60% and the 40% minimum for each subject. The gradual introduction of language-related prompts and prompting techniques enhanced the accuracy from 51.82% to its maximum accuracy. GPT-4 showed low accuracy in subjects including public health & medicine-related law, internal medicine (2), and acupuncture medicine which are highly localized in Korea and TKM. The model’s accuracy was lower for questions requiring TKM-specialized knowledge than those that did not. It exhibited higher accuracy in diagnosis-based and recall-based questions than in intervention-based questions. A significant positive correlation was observed between the consistency and accuracy of GPT-4’s responses. This study unveils both the potential and challenges of applying LLMs to TKM. These findings underline the potential of LLMs like GPT-4 in culturally adapted medicine, especially TKM, for tasks such as clinical assistance, medical education, and research. But they also point towards the necessity for the development of methods to mitigate cultural bias inherent in large language models and validate their efficacy in real-world clinical settings. © 2023 Jang et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
PB  - Public Library of Science
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 15
ER  -

TY  - JOUR
AU  - Denecke, K.
AU  - May, R.
AU  - Romero, O.R.
AU  - de Arriba-Muñoz, A.
AU  - Chapman, W.
AU  - Chow, J.C.L.
AU  - Davies, S.
AU  - Grainger, R.
AU  - Janssen, B.V.
AU  - Ji, S.
AU  - Kreuzthaler, M.
AU  - Lecler, A.
AU  - Paton, C.
AU  - Petersen, C.
AU  - Lacalle, J.R.
AU  - Remedios, D.
AU  - Ropero, J.
AU  - Sevillano, J.L.
AU  - Sezgin, E.
AU  - Traver, V.
AU  - Trigo, J.D.
AU  - Verspoor, K.
TI  - Potential of Large Language Models in Health Care: Delphi Study
PY  - 2024
T2  - Journal of Medical Internet Research
VL  - 26
IS  - 1
C7  - e52399
DO  - 10.2196/52399
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196946747&doi=10.2196%2f52399&partnerID=40&md5=306abd629777ea57f891b21691624eb9
AB  - Background: A large language model (LLM) is a machine learning model inferred from text data that captures subtle patterns of language use in context. Modern LLMs are based on neural network architectures that incorporate transformer methods. They allow the model to relate words together through attention to multiple words in a text sequence. LLMs have been shown to be highly effective for a range of tasks in natural language processing (NLP), including classification and information extraction tasks and generative applications. Objective: The aim of this adapted Delphi study was to collect researchers’ opinions on how LLMs might influence health care and on the strengths, weaknesses, opportunities, and threats of LLM use in health care. Methods: We invited researchers in the fields of health informatics, nursing informatics, and medical NLP to share their opinions on LLM use in health care. We started the first round with open questions based on our strengths, weaknesses, opportunities, and threats framework. In the second and third round, the participants scored these items. Results: The first, second, and third rounds had 28, 23, and 21 participants, respectively. Almost all participants (26/28, 93% in round 1 and 20/21, 95% in round 3) were affiliated with academic institutions. Agreement was reached on 103 items related to use cases, benefits, risks, reliability, adoption aspects, and the future of LLMs in health care. Participants offered several use cases, including supporting clinical tasks, documentation tasks, and medical research and education, and agreed that LLM-based systems will act as health assistants for patient education. The agreed-upon benefits included increased efficiency in data handling and extraction, improved automation of processes, improved quality of health care services and overall health outcomes, provision of personalized care, accelerated diagnosis and treatment processes, and improved interaction between patients and health care professionals. In total, 5 risks to health care in general were identified: cybersecurity breaches, the potential for patient misinformation, ethical concerns, the likelihood of biased decision-making, and the risk associated with inaccurate communication. Overconfidence in LLM-based systems was recognized as a risk to the medical profession. The 6 agreed-upon privacy risks included the use of unregulated cloud services that compromise data security, exposure of sensitive patient data, breaches of confidentiality, fraudulent use of information, vulnerabilities in data storage and communication, and inappropriate access or use of patient data. Conclusions: Future research related to LLMs should not only focus on testing their possibilities for NLP-related tasks but also consider the workflows the models could contribute to and the requirements regarding quality, integration, and regulations needed for successful implementation in practice. ©Kerstin Denecke, Richard May, LLMHealthGroup, Octavio Rivera Romero.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 15
ER  -

TY  - JOUR
AU  - Zack, T.
AU  - Lehman, E.
AU  - Suzgun, M.
AU  - Rodriguez, J.A.
AU  - Celi, L.A.
AU  - Gichoya, J.
AU  - Jurafsky, D.
AU  - Szolovits, P.
AU  - Bates, D.W.
AU  - Abdulnour, R.-E.E.
AU  - Butte, A.J.
AU  - Alsentzer, E.
TI  - Assessing the potential of GPT-4 to perpetuate racial and gender biases in health care: a model evaluation study
PY  - 2024
T2  - The Lancet Digital Health
VL  - 6
IS  - 1
SP  - e12
EP  - e22
DO  - 10.1016/S2589-7500(23)00225-X
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180347404&doi=10.1016%2fS2589-7500%2823%2900225-X&partnerID=40&md5=512470e0e5e6932e9c95e349c4287dd9
AB  - Background: Large language models (LLMs) such as GPT-4 hold great promise as transformative tools in health care, ranging from automating administrative tasks to augmenting clinical decision making. However, these models also pose a danger of perpetuating biases and delivering incorrect medical diagnoses, which can have a direct, harmful impact on medical care. We aimed to assess whether GPT-4 encodes racial and gender biases that impact its use in health care. Methods: Using the Azure OpenAI application interface, this model evaluation study tested whether GPT-4 encodes racial and gender biases and examined the impact of such biases on four potential applications of LLMs in the clinical domain—namely, medical education, diagnostic reasoning, clinical plan generation, and subjective patient assessment. We conducted experiments with prompts designed to resemble typical use of GPT-4 within clinical and medical education applications. We used clinical vignettes from NEJM Healer and from published research on implicit bias in health care. GPT-4 estimates of the demographic distribution of medical conditions were compared with true US prevalence estimates. Differential diagnosis and treatment planning were evaluated across demographic groups using standard statistical tests for significance between groups. Findings: We found that GPT-4 did not appropriately model the demographic diversity of medical conditions, consistently producing clinical vignettes that stereotype demographic presentations. The differential diagnoses created by GPT-4 for standardised clinical vignettes were more likely to include diagnoses that stereotype certain races, ethnicities, and genders. Assessment and plans created by the model showed significant association between demographic attributes and recommendations for more expensive procedures as well as differences in patient perception. Interpretation: Our findings highlight the urgent need for comprehensive and transparent bias assessments of LLM tools such as GPT-4 for intended use cases before they are integrated into clinical care. We discuss the potential sources of these biases and potential mitigation strategies before clinical implementation. Funding: Priscilla Chan and Mark Zuckerberg. © 2024 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY 4.0 license
PB  - Elsevier Ltd
C2  - 38123252
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 138
ER  -

TY  - JOUR
AU  - Lindquist, E.G.
AU  - Abdurahman, S.
AU  - Woodward, D.W.
AU  - West, A.E.
TI  - Sharing is in fact about caring: Care concerns feature prominently in subreddits devoted to self-injurious thoughts and behaviors
PY  - 2023
T2  - Computers in Human Behavior
VL  - 145
C7  - 107786
DO  - 10.1016/j.chb.2023.107786
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153118523&doi=10.1016%2fj.chb.2023.107786&partnerID=40&md5=b05eda4859ca239bdf5c023c0d19de60
AB  - Objective: Given poor treatment efficacy and utilization for self-injurious thoughts and behaviors (SITBs) and the pervasive SITB-stigma present in many treatment settings, research that identifies values and experiences that resonate with individuals engaged in SITBs is urgently needed to improve current treatment offerings. The present study uses Moral Foundations Theory, a leading framework for conceptualizing values and behavior, to identify responses to SITBs most likely to resonate with those who have lived SITB experience. Methods: Natural language processing methods (topic modeling, neural network-based classifier) were used to extract latent conversation topics and moral concerns from 1.68 M messages on the two largest SITB forums on Reddit. Once conversation topics and moral concerns were extracted, a linear regression model was fit to describe the relationship between likes on Reddit, moral concerns, and latent conversation topics. Results: Findings revealed several types of messages most likely to resonate with individuals engaged in SITBs: 1) Specific situational narratives compared to general messages of sadness (p < .01); 2) messages that expressed care (p < .001); and 3) specific messages that expressed care, fairness, loyalty, and purity. Conclusions: Specific, care-focused content (kind, nurturing content that discusses avoidance of emotional/physical harm to others) resonates most with individuals engaged in SITBs, providing insight to the real-time needs and experiences of those engaged in SITBs and suggesting the importance of framing SITB interventions in care language rather than success/failure language. Study findings may inform one-on-one clinical interactions, the development of SITB-specific interventions, and SITB training for clinicians. © 2023 Elsevier Ltd
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Bays, H.E.
AU  - Fitch, A.
AU  - Cuda, S.
AU  - Gonsahn-Bollie, S.
AU  - Rickey, E.
AU  - Hablutzel, J.
AU  - Coy, R.
AU  - Censani, M.
TI  - Artificial intelligence and obesity management: An Obesity Medicine Association (OMA) Clinical Practice Statement (CPS) 2023
PY  - 2023
T2  - Obesity Pillars
VL  - 6
C7  - 100065
DO  - 10.1016/j.obpill.2023.100065
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164652169&doi=10.1016%2fj.obpill.2023.100065&partnerID=40&md5=94e85289837aa9f2421408558da92011
AB  - Background: This Obesity Medicine Association (OMA) Clinical Practice Statement (CPS) provides clinicians an overview of Artificial Intelligence, focused on the management of patients with obesity. Methods: The perspectives of the authors were augmented by scientific support from published citations and integrated with information derived from search engines (i.e., Chrome by Google, Inc) and chatbots (i.e., Chat Generative Pretrained Transformer or Chat GPT). Results: Artificial Intelligence (AI) is the technologic acquisition of knowledge and skill by a nonhuman device, that after being initially programmed, has varying degrees of operations autonomous from direct human control, and that performs adaptive output tasks based upon data input learnings. AI has applications regarding medical research, medical practice, and applications relevant to the management of patients with obesity. Chatbots may be useful to obesity medicine clinicians as a source of clinical/scientific information, helpful in writings and publications, as well as beneficial in drafting office or institutional Policies and Procedures and Standard Operating Procedures. AI may facilitate interactive programming related to analyses of body composition imaging, behavior coaching, personal nutritional intervention & physical activity recommendations, predictive modeling to identify patients at risk for obesity-related complications, and aid clinicians in precision medicine. AI can enhance educational programming, such as personalized learning, virtual reality, and intelligent tutoring systems. AI may help augment in-person office operations and telemedicine (e.g., scheduling and remote monitoring of patients). Finally, AI may help identify patterns in datasets related to a medical practice or institution that may be used to assess population health and value-based care delivery (i.e., analytics related to electronic health records). Conclusions: AI is contributing to both an evolution and revolution in medical care, including the management of patients with obesity. Challenges of Artificial Intelligence include ethical and legal concerns (e.g., privacy and security), accuracy and reliability, and the potential perpetuation of pervasive systemic biases. © 2023 The Authors
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 49
ER  -

TY  - JOUR
AU  - Brown, C.J.
AU  - Chang, L.-S.
AU  - Hosomura, N.
AU  - Malmasi, S.
AU  - Morrison, F.
AU  - Shubina, M.
AU  - Lan, Z.
AU  - Turchin, A.
TI  - Assessment of Sex Disparities in Nonacceptance of Statin Therapy and Low-Density Lipoprotein Cholesterol Levels Among Patients at High Cardiovascular Risk
PY  - 2023
T2  - JAMA Network Open
VL  - 6
IS  - 2
C7  - e231047
DO  - 10.1001/jamanetworkopen.2023.1047
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149153914&doi=10.1001%2fjamanetworkopen.2023.1047&partnerID=40&md5=e2ba6166913939c12b031685c210e89a
AB  - IMPORTANCE Many patients at high cardiovascular risk-women more commonly than men-are not receiving statins. Anecdotally, it is common for patients to not accept statin therapy recommendations by their clinicians. However, population-based data on nonacceptance of statin therapy by patients are lacking. OBJECTIVES To evaluate sex disparities in nonacceptance of statin therapy and assess their association with low-density lipoprotein (LDL) cholesterol control. DESIGN, SETTING, AND PARTICIPANTS A retrospective cohort study was conducted from January 1, 2019, to December 31, 2022, of statin-naive patients with atherosclerotic cardiovascular disease, diabetes, or LDL cholesterol levels of 190 mg/dL (to convert to millimoles per liter, multiply by 0.0259) or more who were treated at Mass General Brigham between January 1, 2000, and December 31, 2018. EXPOSURE Recommendation of statin therapy by the patient’s clinician, ascertained from the combination of electronic health record prescription data and natural language processing of electronic clinician notes. MAIN OUTCOMES AND MEASURES Time to achieve an LDL cholesterol level of less than 100 mg/dL. RESULTS Of 24 212 study patients (mean [SD] age, 58.8 [13.0] years; 12 294 women [50.8%]), 5308 (21.9%) did not accept the initial recommendation of statin therapy. Nonacceptance of statin therapy was more common among women than men (24.1% [2957 of 12 294] vs 19.7% [2351 of 11 918]; P < .001) and was similarly higher in every subgroup in the analysis stratified by comorbidities. In multivariable analysis, female sex was associated with lower odds of statin therapy acceptance (0.82 [95% CI, 0.78-0.88]). Patients who did vs did not accept a statin therapy recommendation achieved an LDL cholesterol level of less than 100 mg/dL over a median of 1.5 years (IQR, 0.4-5.5 years) vs 4.4 years (IQR, 1.3-11.1 years) (P < .001). In a multivariable analysis adjusted for demographic characteristics and comorbidities, nonacceptance of statin therapy was associated with a longer time to achieve an LDL cholesterol level of less than 100 mg/dL (hazard ratio, 0.57 [95% CI, 0.55-0.60]). CONCLUSIONS AND RELEVANCE This cohort study suggests that nonacceptance of a statin therapy recommendation was common among patients at high cardiovascular risk and was particularly common among women. It was associated with significantly higher LDL cholesterol levels, potentially increasing the risk for cardiovascular events. Further research is needed to understand the reasons for nonacceptance of statin therapy by patients and to develop methods to ensure that all patients receive optimal therapy in accordance with their preferences and priorities. © 2023 American Medical Association. All rights reserved.
PB  - American Medical Association
C2  - 36853604
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 24
ER  -

TY  - CONF
AU  - Zayed, A.
AU  - Parthasarathi, P.
AU  - Mordido, G.
AU  - Palangi, H.
AU  - Shabanian, S.
AU  - Chandar, S.
TI  - Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness
PY  - 2023
T2  - Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023
VL  - 37
SP  - 14593
EP  - 14601
DO  - 10.1609/aaai.v37i12.26706
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162775548&doi=10.1609%2faaai.v37i12.26706&partnerID=40&md5=fab774b86a8938cd08f748dc9adf31cf
AB  - Data-driven predictive solutions predominant in commercial applications tend to suffer from biases and stereotypes, which raises equity concerns. Prediction models may discover, use, or amplify spurious correlations based on gender or other protected personal characteristics, thus discriminating against marginalized groups. Mitigating gender bias has become an important research focus in natural language processing (NLP) and is an area where annotated corpora are available. Data augmentation reduces gender bias by adding counterfactual examples to the training dataset. In this work, we show that some of the examples in the augmented dataset can be not important or even harmful to fairness. We hence propose a general method for pruning both the factual and counterfactual examples to maximize the model’s fairness as measured by the demographic parity, equality of opportunity, and equality of odds. The fairness achieved by our method surpasses that of data augmentation on three text classification datasets, using no more than half of the examples in the augmented dataset. Our experiments are conducted using models of varying sizes and pre-training settings. WARNING: This work uses language that is offensive in nature. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
PB  - AAAI Press
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - Manzo, G.
AU  - Celi, L.A.
AU  - Shabazz, Y.
AU  - Mulcahey, R.
AU  - Flores, L.J.
AU  - Demner-Fushman, D.
TI  - Caregivers Attitude Detection From Clinical Notes
PY  - 2023
T2  - AMIA ... Annual Symposium proceedings. AMIA Symposium
VL  - 2023
SP  - 1125
EP  - 1134
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182547504&partnerID=40&md5=85be5675adaea8415fe11abd63672d4b
AB  - Caregivers' attitudes impact healthcare quality and disparities. Clinical notes contain highly specialized and ambiguous language that requires extensive domain knowledge to understand, and using negative language does not necessarily imply a negative attitude. This study discusses the challenge of detecting caregivers' attitudes from their clinical notes. To address these challenges, we annotate MIMIC clinical notes and train state-of-the-art language models from the Hugging Face platform. The study focuses on the Neonatal Intensive Care Unit and evaluates models in zero-shot, few-shot, and fully-trained scenarios. Among the chosen models, RoBERTa identifies caregivers' attitudes from clinical notes with an F1-score of 0.75. This approach not only enhances patient satisfaction, but opens up exciting possibilities for detecting and preventing care provider syndromes, such as fatigue, stress, and burnout. The paper concludes by discussing limitations and potential future work. ©2023 AMIA - All rights reserved.
C2  - 38222330
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Wang, B.
AU  - Chen, W.
AU  - Pei, H.
AU  - Xie, C.
AU  - Kang, M.
AU  - Zhang, C.
AU  - Xu, C.
AU  - Xiong, Z.
AU  - Dutta, R.
AU  - Schaeffer, R.
AU  - Truong, S.T.
AU  - Arora, S.
AU  - Mazeika, M.
AU  - Hendrycks, D.
AU  - Lin, Z.
AU  - Cheng, Y.
AU  - Koyejo, S.
AU  - Song, D.
AU  - Li, B.
TI  - DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models
PY  - 2023
T2  - Advances in Neural Information Processing Systems
VL  - 36
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191150751&partnerID=40&md5=a81c00778522b1ea7d958db074ab2eff
AB  - Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in their capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications such as healthcare and finance - where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives - including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. We also find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, potentially because GPT-4 follows (misleading) instructions more precisely. Our work illustrates a comprehensive trustworthiness evaluation of GPT models and sheds light on the trustworthiness gaps. Our benchmark is publicly available at https://decodingtrust.github.io/. © 2023 Neural information processing systems foundation. All rights reserved.
PB  - Neural information processing systems foundation
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 59
ER  -

TY  - CONF
AU  - Karkalousos, D.
AU  - Išgum, I.
AU  - Marquering, H.A.
AU  - Caan, M.W.A.
TI  - MultiTask Learning for accelerated-MRI Reconstruction and Segmentation of Brain Lesions in Multiple Sclerosis
PY  - 2023
T2  - Proceedings of Machine Learning Research
VL  - 227
SP  - 991
EP  - 1005
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189350788&partnerID=40&md5=1b6cf1d6497418b246a8356134d98a69
AB  - This work proposes MultiTask Learning for accelerated-MRI Reconstruction and Segmentation (MTLRS). Unlike the common single-task approaches, MultiTask Learning identifies relations between multiple tasks to improve the performance of all tasks. The proposed MTLRS consists of a unique cascading architecture, where a recurrent reconstruction network and a segmentation network inform each other through hidden states. The features of the two networks are shared and implicitly enforced as inductive bias. To evaluate the benefit of MTLRS, we compare performing the two tasks of accelerated-MRI reconstruction and MRI segmentation with pre-trained, sequential, end-to-end, and joint approaches. A synthetic multicoil dataset is used to train, validate, and test all approaches with five-fold cross-validation. The dataset consists of 3D FLAIR brain data of relapsing-remitting Multiple Sclerosis patients with known white matter lesions. The acquisition is prospectively undersampled by approximately 7.5 times compared to clinical standards. Reconstruction performance is evaluated by Structural Similarity Index Measure (SSIM) and Peak Signal-to-Noise Ratio (PSNR). Segmentation performance is evaluated by Dice score for combined brain tissue and white matter lesion segmentation and by per lesion Dice score. Results show that MTLRS outperforms other evaluated approaches, providing high-quality reconstructions and accurate white matter lesion segmentation. A significant correlation was found between the performance of both tasks (SSIM and per lesion Dice score, ρ = 0.92, p = 0.0005). Our proposed MTLRS demonstrates that accelerated-MRI reconstruction and MRI segmentation can be effectively combined to improve performance on both tasks, potentially benefiting clinical settings. © 2023 CC-BY 4.0, D. Karkalousos, I. Išgum, H.A. Marquering & M.W. Caan.
PB  - ML Research Press
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Ding, X.
AU  - Sheng, Z.
AU  - Yetişgen, M.
AU  - Pakhomov, S.
AU  - Cohen, T.
TI  - Backdoor Adjustment of Confounding by Provenance for Robust Text Classification of Multi-institutional Clinical Notes
PY  - 2023
T2  - AMIA ... Annual Symposium proceedings. AMIA Symposium
VL  - 2023
SP  - 923
EP  - 932
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182544872&partnerID=40&md5=67ce7e79c7b3275d154f6733732c12bd
AB  - Natural Language Processing (NLP) methods have been broadly applied to clinical tasks. Machine learning and deep learning approaches have been used to improve the performance of clinical NLP. However, these approaches require sufficiently large datasets for training, and trained models have been shown to transfer poorly across sites. These issues have led to the promotion of data collection and integration across different institutions for accurate and portable models. However, this can introduce a form of bias called confounding by provenance. When source-specific data distributions differ at deployment, this may harm model performance. To address this issue, we evaluate the utility of backdoor adjustment for text classification in a multi-site dataset of clinical notes annotated for mentions of substance abuse. Using an evaluation framework devised to measure robustness to distributional shifts, we assess the utility of backdoor adjustment. Our results indicate that backdoor adjustment can effectively mitigate for confounding shift. ©2023 AMIA - All rights reserved.
C2  - 38222433
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Nguyen, Y.
AU  - Costedoat-Chalumeau, N.
TI  - Artificial intelligence and internal medicine: The example of hydroxychloroquine according to ChatGPT
ST  - Les intelligences artificielles conversationnelles en médecine interne: l'exemple de l'hydroxychloroquine selon ChatGPT
PY  - 2023
T2  - Revue de Medecine Interne
VL  - 44
IS  - 5
SP  - 218
EP  - 226
DO  - 10.1016/j.revmed.2023.03.017
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152586794&doi=10.1016%2fj.revmed.2023.03.017&partnerID=40&md5=cdcbc83224b2fa7b0cd59ed77d37f91b
AB  - Artificial intelligence (AI) using deep learning is revolutionizing several fields, including medicine, with a wide range of applications. Available since the end of 2022, ChatGPT is a conversational AI or “chatbot”, using artificial intelligence to dialogue with its users in all fields. Through the example of hydroxychloroquine (HCQ), we discuss its use for patients, clinicians, or researchers, and discuss its performance and limitations, particularly in relation to algorithmic bias. If AI tools using deep learning do not dispense with the expertise and experience of a clinician (at least, for the moment), they have a potential to improve or simplify our daily practice. © 2023 Société Nationale Française de Médecine Interne (SNFMI)
PB  - Elsevier Masson s.r.l.
C2  - 37062612
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - CONF
AU  - Virvou, M.
AU  - Tsihrintzis, G.A.
TI  - Pre-made Empowering Artificial Intelligence and ChatGPT: The Growing Importance of Human AI-Experts
PY  - 2023
T2  - 14th International Conference on Information, Intelligence, Systems and Applications, IISA 2023
DO  - 10.1109/IISA59645.2023.10345880
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182016638&doi=10.1109%2fIISA59645.2023.10345880&partnerID=40&md5=62648f9b37f73609fbc8baca0a6e4bf3
AB  - This paper investigates the augmented responsibility of human Artificial Intelligence experts in the era of empowered pre-made Artificial Intelligence (AI). The responsible and ethical use of pre-made AI is of paramount importance in this evolving technology. AI systems have the potential to impact numerous aspects of society, ranging from healthcare and finance to education and IoT. The decisions made by AI algorithms can have significant consequences for individuals, communities, and even entire industries. Using a comparison to the way widely available medicines require a prescription from medical doctors, human AI experts assume the role of evaluating, recommending, and overseeing the implementation of AI systems, even when pre-built AI solutions may seem user-friendly on the surface. The paper has explored the expanded responsibilities of human AI experts within two contemporary scenarios involving pre-made AI, encompassing LLMs and ChatGPT. These AI technologies are applied in two principal manners: initially, as standalone AI products readily accessible to a wide audience, and secondly, as elements undergoing exploration for integration into other AI-driven software and Intelligent Information Systems (IIS), with the goal of enhancing natural language processing (NLP) features within user interfaces. In all cases, the expertise of human AI professionals is indispensable, and their role is augmented. These professionals bear an increased responsibility for ensuring the responsible and ethical deployment of AI technologies, with a focus on human-centered design, bias mitigation, validation and accuracy estimation of the results, transparency promotion, and the necessary balance between automation and human oversight. This paper performs a review on pre-made AI and ChatGPT together with custom-based AI and shows that recent advance require an augmented role of human AI experts © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Ben, Â.J.
AU  - van Dongen, J.M.
AU  - Alili, M.E.
AU  - Heymans, M.W.
AU  - Twisk, J.W.R.
AU  - MacNeil-Vroomen, J.L.
AU  - de Wit, M.
AU  - van Dijk, S.E.M.
AU  - Oosterhuis, T.
AU  - Bosmans, J.E.
TI  - The handling of missing data in trial-based economic evaluations: should data be multiply imputed prior to longitudinal linear mixed-model analyses?
PY  - 2023
T2  - European Journal of Health Economics
VL  - 24
IS  - 6
SP  - 951
EP  - 965
DO  - 10.1007/s10198-022-01525-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138773346&doi=10.1007%2fs10198-022-01525-y&partnerID=40&md5=da9cd75ce709e5a86b3bef551877cb93
AB  - Introduction: For the analysis of clinical effects, multiple imputation (MI) of missing data were shown to be unnecessary when using longitudinal linear mixed-models (LLM). It remains unclear whether this also applies to trial-based economic evaluations. Therefore, this study aimed to assess whether MI is required prior to LLM when analyzing longitudinal cost and effect data. Methods: Two-thousand complete datasets were simulated containing five time points. Incomplete datasets were generated with 10, 25, and 50% missing data in follow-up costs and effects, assuming a Missing At Random (MAR) mechanism. Six different strategies were compared using empirical bias (EB), root-mean-squared error (RMSE), and coverage rate (CR). These strategies were: LLM alone (LLM) and MI with LLM (MI-LLM), and, as reference strategies, mean imputation with LLM (M-LLM), seemingly unrelated regression alone (SUR-CCA), MI with SUR (MI-SUR), and mean imputation with SUR (M-SUR). Results: For costs and effects, LLM, MI-LLM, and MI-SUR performed better than M-LLM, SUR-CCA, and M-SUR, with smaller EBs and RMSEs as well as CRs closers to nominal levels. However, even though LLM, MI-LLM and MI-SUR performed equally well for effects, MI-LLM and MI-SUR were found to perform better than LLM for costs at 10 and 25% missing data. At 50% missing data, all strategies resulted in relatively high EBs and RMSEs for costs. Conclusion: LLM should be combined with MI when analyzing trial-based economic evaluation data. MI-SUR is more efficient and can also be used, but then an average intervention effect over time cannot be estimated. © 2022, The Author(s).
PB  - Springer Science and Business Media Deutschland GmbH
C2  - 36161553
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Witting, C.
AU  - Azizi, Z.
AU  - Gomez, S.E.
AU  - Zammit, A.
AU  - Sarraju, A.
AU  - Ngo, S.
AU  - Hernandez-Boussard, T.
AU  - Rodriguez, F.
TI  - Natural language processing to identify reasons for sex disparity in statin prescriptions
PY  - 2023
T2  - American Journal of Preventive Cardiology
VL  - 14
C7  - 100496
DO  - 10.1016/j.ajpc.2023.100496
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162402659&doi=10.1016%2fj.ajpc.2023.100496&partnerID=40&md5=3d4cf69e30c354843a12d9a38b0f5d5f
AB  - Background: Statins are the cornerstone of treatment of patients with atherosclerotic cardiovascular disease (ASCVD). Despite this, multiple studies have shown that women with ASCVD are less likely to be prescribed statins than men. The objective of this study was to use Natural Language Processing (NLP) to elucidate factors contributing to this disparity. Methods: Our cohort included adult patients with two or more encounters between 2014 and 2021 with an ASCVD diagnosis within a multisite electronic health record (EHR) in Northern California. After reviewing structured EHR prescription data, we used a benchmark deep learning NLP approach, Clinical Bidirectional Encoder Representations from Transformers (BERT), to identify and interpret discussions of statin prescriptions documented in clinical notes. Clinical BERT was evaluated against expert clinician review in 20% test sets. Results: There were 88,913 patients with ASCVD (mean age 67.8±13.1 years) and 35,901 (40.4%) were women. Women with ASCVD were less likely to be prescribed statins compared with men (56.6% vs 67.6%, p <0.001), and, when prescribed, less likely to be prescribed guideline-directed high-intensity dosing (41.4% vs 49.8%, p <0.001). These disparities were more pronounced among younger patients, patients with private insurance, and those for whom English is their preferred language. Among those not prescribed statins, women were less likely than men to have statins mentioned in their clinical notes (16.9% vs 19.1%, p <0.001). Women were less likely than men to have statin use reported in clinical notes despite absence of recorded prescription (32.8% vs 42.6%, p <0.001). Women were slightly more likely than men to have statin intolerance documented in structured data or clinical notes (6.0% vs 5.3%, p=0.003). Conclusions: Women with ASCVD were less likely to be prescribed guideline-directed statins compared with men. NLP identified additional sex-based statin disparities and reasons for statin non-prescription in clinical notes of patients with ASCVD. © 2023
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Hallsworth, J.E.
AU  - Udaondo, Z.
AU  - Pedrós-Alió, C.
AU  - Höfer, J.
AU  - Benison, K.C.
AU  - Lloyd, K.G.
AU  - Cordero, R.J.B.
AU  - de Campos, C.B.L.
AU  - Yakimov, M.M.
AU  - Amils, R.
TI  - Scientific novelty beyond the experiment
PY  - 2023
T2  - Microbial Biotechnology
VL  - 16
IS  - 6
SP  - 1131
EP  - 1173
DO  - 10.1111/1751-7915.14222
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148292647&doi=10.1111%2f1751-7915.14222&partnerID=40&md5=c10359ea903748731f35469a9885e7e6
AB  - Practical experiments drive important scientific discoveries in biology, but theory-based research studies also contribute novel—sometimes paradigm-changing—findings. Here, we appraise the roles of theory-based approaches focusing on the experiment-dominated wet-biology research areas of microbial growth and survival, cell physiology, host–pathogen interactions, and competitive or symbiotic interactions. Additional examples relate to analyses of genome-sequence data, climate change and planetary health, habitability, and astrobiology. We assess the importance of thought at each step of the research process; the roles of natural philosophy, and inconsistencies in logic and language, as drivers of scientific progress; the value of thought experiments; the use and limitations of artificial intelligence technologies, including their potential for interdisciplinary and transdisciplinary research; and other instances when theory is the most-direct and most-scientifically robust route to scientific novelty including the development of techniques for practical experimentation or fieldwork. We highlight the intrinsic need for human engagement in scientific innovation, an issue pertinent to the ongoing controversy over papers authored using/authored by artificial intelligence (such as the large language model/chatbot ChatGPT). Other issues discussed are the way in which aspects of language can bias thinking towards the spatial rather than the temporal (and how this biased thinking can lead to skewed scientific terminology); receptivity to research that is non-mainstream; and the importance of theory-based science in education and epistemology. Whereas we briefly highlight classic works (those by Oakes Ames, Francis H.C. Crick and James D. Watson, Charles R. Darwin, Albert Einstein, James E. Lovelock, Lynn Margulis, Gilbert Ryle, Erwin R.J.A. Schrödinger, Alan M. Turing, and others), the focus is on microbiology studies that are more-recent, discussing these in the context of the scientific process and the types of scientific novelty that they represent. These include several studies carried out during the 2020 to 2022 lockdowns of the COVID-19 pandemic when access to research laboratories was disallowed (or limited). We interviewed the authors of some of the featured microbiology-related papers and—although we ourselves are involved in laboratory experiments and practical fieldwork—also drew from our own research experiences showing that such studies can not only produce new scientific findings but can also transcend barriers between disciplines, act counter to scientific reductionism, integrate biological data across different timescales and levels of complexity, and circumvent constraints imposed by practical techniques. In relation to urgent research needs, we believe that climate change and other global challenges may require approaches beyond the experiment. © 2023 The Authors. Microbial Biotechnology published by Applied Microbiology International and John Wiley & Sons Ltd.
PB  - John Wiley and Sons Ltd
C2  - 36786388
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 42
ER  -

TY  - JOUR
AU  - Gao, Y.
AU  - Dligach, D.
AU  - Miller, T.
AU  - Caskey, J.
AU  - Sharma, B.
AU  - Churpek, M.M.
AU  - Afshar, M.
TI  - DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language Processing
PY  - 2023
T2  - Journal of Biomedical Informatics
VL  - 138
C7  - 104286
DO  - 10.1016/j.jbi.2023.104286
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148773089&doi=10.1016%2fj.jbi.2023.104286&partnerID=40&md5=9f246a38ab486115cc5e07eb00704bdb
AB  - The meaningful use of electronic health records (EHR) continues to progress in the digital era with clinical decision support systems augmented by artificial intelligence. A priority in improving provider experience is to overcome information overload and reduce the cognitive burden so fewer medical errors and cognitive biases are introduced during patient care. One major type of medical error is diagnostic error due to systematic or predictable errors in judgement that rely on heuristics. The potential for clinical natural language processing (cNLP) to model diagnostic reasoning in humans with forward reasoning from data to diagnosis and potentially reduce cognitive burden and medical error has not been investigated. Existing tasks to advance the science in cNLP have largely focused on information extraction and named entity recognition through classification tasks. We introduce a novel suite of tasks coined as Diagnostic Reasoning Benchmarks, DR.BENCH, as a new benchmark for developing and evaluating cNLP models with clinical diagnostic reasoning ability. The suite includes six tasks from ten publicly available datasets addressing clinical text understanding, medical knowledge reasoning, and diagnosis generation. DR.BENCH is the first clinical suite of tasks designed to be a natural language generation framework to evaluate pre-trained language models for diagnostic reasoning. The goal of DR. BENCH is to advance the science in cNLP to support downstream applications in computerized diagnostic decision support and improve the efficiency and accuracy of healthcare providers during patient care. We fine-tune and evaluate the state-of-the-art generative models on DR.BENCH. Experiments show that with domain adaptation pre-training on medical knowledge, the model demonstrated opportunities for improvement when evaluated in DR. BENCH. We share DR. BENCH as a publicly available GitLab repository with a systematic approach to load and evaluate models for the cNLP community. We also discuss the carbon footprint produced during the experiments and encourage future work on DR.BENCH to report the carbon footprint. © 2023 Elsevier Inc.
PB  - Academic Press Inc.
C2  - 36706848
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - CONF
AU  - Alotaibi, A.
AU  - Sas, C.
TI  - Review of AI-Based Mental Health Apps
PY  - 2023
T2  - 36th Annual British HCI Conference
SP  - 238
EP  - 250
DO  - 10.14236/ewic/BCSHCI2023.27
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204618658&doi=10.14236%2fewic%2fBCSHCI2023.27&partnerID=40&md5=ed383f3a8628cf51fdf136b8aeb71901
AB  - The last decade has seen a significant growth of HCI research in mental health technologies while Artificial intelligence raises both challenges and opportunities to better support symptom identification or personalization of interventions. There has been also a growth of commercial AI-based mobile apps for mental health. Despite emerging HCI work on reviewing mental health apps, those that are AI-based have received limited attention. To address this gap, we report a functionality review of 13 apps selected from 127 apps from the Apple Store. The selection criteria involved a minimum rating of 4 out of 5. After eliminating duplicates, irrelevant, and low-rated apps, an expert evaluation and auto-ethnography approach were used to explore apps’ functionalities. Findings indicate that apps support functions such as tracking and detecting emotions and moods, providing recommendations for therapy and well-being interventions, and supporting talking therapy through conversational agents powered by Natural Language Processing models. A critical finding is apps’ limited support for AI literacy and explainability, as well as limited consideration for ethical concerns regarding personal data, its reliability, and algorithmic biases. Our paper concludes with three design implications for AI-based mental health apps towards developing conversational agents to support Cognative Behavoural Therapy interventions based on tracked multimodal data, addressing the ethics of NLP biases, and user exploration of AI-based models and their explainability. © Alotaibi et al. Published by BCS Learning and Development Ltd.
PB  - BCS Learning and Development Ltd
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Singhal, K.
AU  - Azizi, S.
AU  - Tu, T.
AU  - Mahdavi, S.S.
AU  - Wei, J.
AU  - Chung, H.W.
AU  - Scales, N.
AU  - Tanwani, A.
AU  - Cole-Lewis, H.
AU  - Pfohl, S.
AU  - Payne, P.
AU  - Seneviratne, M.
AU  - Gamble, P.
AU  - Kelly, C.
AU  - Babiker, A.
AU  - Schärli, N.
AU  - Chowdhery, A.
AU  - Mansfield, P.
AU  - Demner-Fushman, D.
AU  - Agüera y Arcas, B.
AU  - Webster, D.
AU  - Corrado, G.S.
AU  - Matias, Y.
AU  - Chou, K.
AU  - Gottweis, J.
AU  - Tomasev, N.
AU  - Liu, Y.
AU  - Rajkomar, A.
AU  - Barral, J.
AU  - Semturs, C.
AU  - Karthikesalingam, A.
AU  - Natarajan, V.
TI  - Large language models encode clinical knowledge
PY  - 2023
T2  - Nature
VL  - 620
IS  - 7972
SP  - 172
EP  - 180
DO  - 10.1038/s41586-023-06291-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164462923&doi=10.1038%2fs41586-023-06291-2&partnerID=40&md5=a3b377b3ee6b32b2fbf310526ca4e4cd
AB  - Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA3, MedMCQA4, PubMedQA5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics6), including 67.6% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications. © 2023, The Author(s).
PB  - Nature Research
C2  - 37438534
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1025
ER  -

TY  - JOUR
AU  - Shen, X.
AU  - Houser, T.
AU  - Smith, D.V.
AU  - Murty, V.P.
TI  - Machine-learning as a validated tool to characterize individual differences in free recall of naturalistic events
PY  - 2023
T2  - Psychonomic Bulletin and Review
VL  - 30
IS  - 1
SP  - 308
EP  - 316
DO  - 10.3758/s13423-022-02171-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137846059&doi=10.3758%2fs13423-022-02171-4&partnerID=40&md5=b7c4fe29935c5663cab8af475fc7ba30
AB  - The use of naturalistic stimuli, such as narrative movies, is gaining popularity in many fields, characterizing memory, affect, and decision-making. Narrative recall paradigms are often used to capture the complexity and richness of memory for naturalistic events. However, scoring narrative recalls is time-consuming and prone to human biases. Here, we show the validity and reliability of using a natural language processing tool, the Universal Sentence Encoder (USE), to automatically score narrative recalls. We compared the reliability in scoring made between two independent raters (i.e., hand scored) and between our automated algorithm and individual raters (i.e., automated) on trial-unique video clips of magic tricks. Study 1 showed that our automated segmentation approaches yielded high reliability and reflected measures yielded by hand scoring. Study 1 further showed that the results using USE outperformed another popular natural language processing tool, GloVe. In Study 2, we tested whether our automated approach remained valid when testing individuals varying on clinically relevant dimensions that influence episodic memory, age, and anxiety. We found that our automated approach was equally reliable across both age groups and anxiety groups, which shows the efficacy of our approach to assess narrative recall in large-scale individual difference analysis. In sum, these findings suggested that machine learning approach implementing USE is a promising tool for scoring large-scale narrative recalls and perform individual difference analysis for research using naturalistic stimuli. © 2022, The Psychonomic Society, Inc.
PB  - Springer
C2  - 36085232
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Alessa, A.
AU  - Al-Khalifa, H.
TI  - Towards Designing a ChatGPT Conversational Companion for Elderly People
PY  - 2023
T2  - ACM International Conference Proceeding Series
SP  - 667
EP  - 674
DO  - 10.1145/3594806.3596572
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170382415&doi=10.1145%2f3594806.3596572&partnerID=40&md5=7716a5cb41697c37991b8c4ac5c3447f
AB  - Loneliness and social isolation are serious and widespread problems among older people, affecting their physical and mental health, quality of life, and longevity. In this paper, we propose a ChatGPT-based conversational companion system for elderly people. The system is designed to provide companionship and help reduce feelings of loneliness and social isolation. The system was evaluated with a preliminary study. The results showed that the system was able to generate responses that were relevant to the created elderly personas. However, it is essential to acknowledge the limitations of ChatGPT, such as potential biases and misinformation, and to consider the ethical implications of using AI-based companionship for the elderly, including privacy concerns. © 2023 ACM.
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 22
ER  -

TY  - JOUR
AU  - Uyeda, A.M.
AU  - Lee, R.Y.
AU  - Pollack, L.R.
AU  - Paul, S.R.
AU  - Downey, L.
AU  - Brumback, L.C.
AU  - Engelberg, R.A.
AU  - Sibley, J.
AU  - Lober, W.B.
AU  - Cohen, T.
AU  - Torrence, J.
AU  - Kross, E.K.
AU  - Curtis, J.R.
TI  - Predictors of Documented Goals-of-Care Discussion for Hospitalized Patients With Chronic Illness
PY  - 2023
T2  - Journal of Pain and Symptom Management
VL  - 65
IS  - 3
SP  - 233
EP  - 241
DO  - 10.1016/j.jpainsymman.2022.11.012
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143862279&doi=10.1016%2fj.jpainsymman.2022.11.012&partnerID=40&md5=c648e2c26da147caea067f54893b93a9
AB  - Context: Goals-of-care discussions are important for patient-centered care among hospitalized patients with serious illness. However, there are little data on the occurrence, predictors, and timing of these discussions. Objectives: To examine the occurrence, predictors, and timing of electronic health record (EHR)-documented goals-of-care discussions for hospitalized patients. Methods: This retrospective cohort study used natural language processing (NLP) to examine EHR-documented goals-of-care discussions for adults with chronic life-limiting illness or age ≥80 hospitalized 2015-2019. The primary outcome was NLP-identified documentation of a goals-of-care discussion during the index hospitalization. We used multivariable logistic regression to evaluate associations with baseline characteristics. Results: Of 16,262 consecutive, eligible patients without missing data, 5,918 (36.4%) had a documented goals-of-care discussion during hospitalization; approximately 57% of these discussions occurred within 24 hours of admission. In multivariable analysis, documented goals-of-care discussions were more common for women (OR=1.26, 95%CI 1.18-1.36), older patients (OR=1.04 per year, 95%CI 1.03-1.04), and patients with more comorbidities (OR=1.11 per Deyo-Charlson point, 95%CI 1.10-1.13), cancer (OR=1.88, 95%CI 1.72-2.06), dementia (OR=2.60, 95%CI 2.29-2.94), higher acute illness severity (OR=1.12 per National Early Warning Score point, 95%CI 1.11-1.14), or prior advance care planning documents (OR=1.18, 95%CI 1.08-1.30). Documentation of these discussions was less common for racially or ethnically minoritized patients (OR=0.823, 95%CI 0.75-0.90). Conclusion: Among hospitalized patients with serious illness, documented goals-of-care discussions identified by NLP were more common among patients with older age and increased burden of acute or chronic illness, and less common among racially or ethnically minoritized patients. This suggests important disparities in goals-of-care discussions. © 2022 American Academy of Hospice and Palliative Medicine
PB  - Elsevier Inc.
C2  - 36423800
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 16
ER  -

TY  - CONF
AU  - Mohammad, B.
AU  - Supti, T.
AU  - Alzubaidi, M.
AU  - Shah, H.
AU  - Alam, T.
AU  - Shah, Z.
AU  - Househ, M.
TI  - The Pros and Cons of Using ChatGPT in Medical Education: A Scoping Review
PY  - 2023
T2  - Studies in Health Technology and Informatics
VL  - 305
SP  - 644
EP  - 647
DO  - 10.3233/SHTI230580
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164233602&doi=10.3233%2fSHTI230580&partnerID=40&md5=9f7b0eeaa715198d9b52e66a3fc5d128
AB  - This scoping review explores the advantages and disadvantages of using ChatGPT in medical education. We searched PubMed, Google Scholar, Medline, Scopus, and Science Direct to identify relevant studies. Two reviewers independently conducted study selection and data extraction, followed by a narrative synthesis. Out of 197 references, 25 studies met the eligibility criteria. The primary applications of ChatGPT in medical education include automated scoring, teaching assistance, personalized learning, research assistance, quick access to information, generating case scenarios and exam questions, content creation for learning facilitation, and language translation. We also discuss the challenges and limitations of using ChatGPT in medical education, such as its inability to reason beyond existing knowledge, generation of incorrect information, bias, potential undermining of students' critical thinking skills, and ethical concerns. These concerns include using ChatGPT for exam and assignment cheating by students and researchers, as well as issues related to patients' privacy. © 2023 The authors and IOS Press.
PB  - IOS Press BV
C2  - 37387114
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 59
ER  -

TY  - JOUR
AU  - Sharma, D.
AU  - Sekhar Patra, S.
AU  - Sujayaraj, S.J.
AU  - Devkar, V.
TI  - Implementing AI-Driven Diagnostic Tools to Improve Quality of Life Assessments
ST  - Aplicación de Herramientas de Diagnóstico Basadas en IA para Mejorar las Evaluaciones de la Calidad de Vida
PY  - 2023
T2  - Health Leadership and Quality of Life
VL  - 2
C7  - 237
DO  - 10.56294/hl2023237
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218732252&doi=10.56294%2fhl2023237&partnerID=40&md5=5cd0fb3a4943c6d3b9065eb9326ba478
AB  - Using artificial intelligence (AI) in the healthcare sector alters doctors’ major decision-making process. Evaluating patients’ quality of life (QoL) is one area where artificial intelligence seems rather promising. Understanding how various illnesses and therapies influence a person’s overall health depends much on quality of life testing. Standard QoL exams, which rely on hand-written assessments and patient comments on their health, have issues like being subjective, biassed, and sluggish when it comes to analyse vast volumes of data. AI-powered testing tools can provide more accurate, quick, scalable methods to evaluate QoL if one is looking for a way around these challenges. This essay examines how artificial intelligence technology could alter the methodology of quality of life surveys. Diagnostics based on artificial intelligence are quite useful. For patient anecdotes, for instance, natural language processing (NLP) may be employed; machine learning techniques can then be used to project QoL values from medical data. AI systems can handle a lot of clinical data including medical records, imaging data, patient-reported results to generate objective, real-time, tailored QoL evaluations consistent and reusable once and again. Furthermore, these instruments can identify early warning indicators of deterioration that would not be evident using more conventional approaches. the usage of several sorts of records sources inclusive of clever tech and cellular fitness apps which increases the accuracy of stories in real time and allows non-stop tracking AI-driven checking out will also be led via This method not handiest courses medical doctors in making better selections however additionally affords people extra manipulate over their fitness, therefore improving their excellent of life over time. The studies additionally addresses moral questions arising from AI-primarily based QoL assessments consisting of data protection, patient permission, and what clinical professionals should do upon assessment of AI outcomes. through discussion of these issues, this take a look at emphasises the need of ensuring that synthetic intelligence generation be applied in a way that complements the interaction among the affected person and company in preference to replaces human know-how. © 2023; Los autores.
PB  - AG Editor (Argentina)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Mondal, S.
AU  - Barman, A.K.
AU  - Basumatary, S.
AU  - Barman, M.
AU  - Rai, C.
AU  - Nag, A.
TI  - Cancer Text Article Categorization and Prediction Model Based on Machine Learning Approach
PY  - 2023
T2  - 2023 IEEE 3rd Mysore Sub Section International Conference, MysuruCon 2023
DO  - 10.1109/MysuruCon59703.2023.10397005
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184828860&doi=10.1109%2fMysuruCon59703.2023.10397005&partnerID=40&md5=472ff7ca09c9995d6313f9e0a50134b1
AB  - Many research studies have been carried out to identify different variations of cancers in the human body throughout the globe. Due to numerous volumes, the primary concern for healthcare researchers is to segregate the article into a well-thought-out manner. Biomedical text classification is a globally volitional realm due to its numerous uses in different fields. This study motivates us to use a dataset related to cancer articles text containing 7570 research papers with categories into three classes. Like the Word2Vec and BERT models, the word embedding technique converts the text into a numeric vector. The text classification-based prediction model considers nine Machine Learning (ML) classifiers. The models are tested with k-fold cross-validation and evaluate the prediction performances by considering the different standard metrics. The model trade-off between bias and variance is controlled by calculating the standard deviation and k-fold mean accuracy to overcome the overfitting problem of the deployed model. The outcome is observed on the Random Forest (RF) classifier in the case of Word2Vec embedding with a mean accuracy of 99.98%. The BERT models also performed well, with about 97.70% mean accuracy and other parameters compared to the Word2Vec model. The other parameters' evaluation score was near 1.00, promising in multiclass text classification deployed model performances. © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Azib, M.
AU  - Renard, B.
AU  - Garnier, P.
AU  - Genot, V.
AU  - Andre, N.
TI  - A Comprehensive Python Library for Deep Learning-Based Event Detection in Multivariate Time Series Data
PY  - 2023
T2  - Proceedings - 22nd IEEE International Conference on Machine Learning and Applications, ICMLA 2023
SP  - 1399
EP  - 1404
DO  - 10.1109/ICMLA58977.2023.00211
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190135964&doi=10.1109%2fICMLA58977.2023.00211&partnerID=40&md5=e5bd215bfa465fa0ec1ab88b3312e108
AB  - Event detection in time series data is crucial in various domains, including finance, healthcare, cybersecurity, and science. Accurately identifying events in time series data is vital for making informed decisions, detecting anomalies, and predicting future trends. Despite extensive research exploring diverse methods for event detection in time series, with deep learning approaches being among the most advanced, there is still room for improvement and innovation in this field. In this paper, we present a new deep learning supervised method for detecting events in multivariate time series data. Our method combines four distinct novelties compared to existing deep-learning supervised methods. Firstly, it is based on regression instead of binary classification. Secondly, it does not require labeled datasets where each point is labeled; instead, it only requires reference events defined as time points or intervals of time. Thirdly, it is designed to be robust by using a stacked ensemble learning meta-model that combines deep learning models, ranging from classic feed-forward neural networks (FFNs) to state-of-the-art architectures like transformers. This ensemble approach can mitigate individual model weaknesses and biases, resulting in more robust predictions. Finally, to facilitate practical implementation, we have developed a Python package to accompany our proposed method. The package, called eventdetector-ts, can be installed through the Python Package Index (PyPI). In this paper, we present our method and provide a comprehensive guide on the usage of the package. We showcase its versatility and effectiveness through different real-world use cases from natural language processing (NLP) to financial security domains. © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Liu, S.
AU  - Wright, A.P.
AU  - Patterson, B.L.
AU  - Wanderer, J.P.
AU  - Turer, R.W.
AU  - Nelson, S.D.
AU  - McCoy, A.B.
AU  - Sittig, D.F.
AU  - Wright, A.
TI  - Using AI-generated suggestions from ChatGPT to optimize clinical decision support
PY  - 2023
T2  - Journal of the American Medical Informatics Association
VL  - 30
IS  - 7
SP  - 1237
EP  - 1245
DO  - 10.1093/jamia/ocad072
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158997968&doi=10.1093%2fjamia%2focad072&partnerID=40&md5=24b49c2af5259351913fb0eac6dd5bc4
AB  - Objective: To determine if ChatGPT can generate useful suggestions for improving clinical decision support (CDS) logic and to assess noninferiority compared to human-generated suggestions. Methods: We supplied summaries of CDS logic to ChatGPT, an artificial intelligence (AI) tool for question answering that uses a large language model, and asked it to generate suggestions. We asked human clinician reviewers to review the AI-generated suggestions as well as human-generated suggestions for improving the same CDS alerts, and rate the suggestions for their usefulness, acceptance, relevance, understanding, workflow, bias, inversion, and redundancy. Results: Five clinicians analyzed 36 AI-generated suggestions and 29 human-generated suggestions for 7 alerts. Of the 20 suggestions that scored highest in the survey, 9 were generated by ChatGPT. The suggestions generated by AI were found to offer unique perspectives and were evaluated as highly understandable and relevant, with moderate usefulness, low acceptance, bias, inversion, redundancy. Conclusion: AI-generated suggestions could be an important complementary part of optimizing CDS alerts, can identify potential improvements to alert logic and support their implementation, and may even be able to assist experts in formulating their own suggestions for CDS improvement. ChatGPT shows great potential for using large language models and reinforcement learning from human feedback to improve CDS alert logic and potentially other medical areas involving complex, clinical logic, a key step in the development of an advanced learning health system. © The Author(s) 2023. Published by Oxford University Press on behalf of the American Medical Informatics Association.
PB  - Oxford University Press
C2  - 37087108
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 188
ER  -

TY  - JOUR
AU  - Victor, B.G.
AU  - Kubiak, S.
AU  - Angell, B.
AU  - Perron, B.E.
TI  - Time to Move Beyond the ASWB Licensing Exams: Can Generative Artificial Intelligence Offer a Way Forward for Social Work?
PY  - 2023
T2  - Research on Social Work Practice
VL  - 33
IS  - 5
SP  - 511
EP  - 517
DO  - 10.1177/10497315231166125
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152059357&doi=10.1177%2f10497315231166125&partnerID=40&md5=8c512fb2e4eac6dec045d4abc4edcf43
AB  - Social work scholars have long questioned the validity and utility of the Association of Social Work Boards (ASWB) licensing exams. Data released in 2022 revealed severe disparities in pass rates based on race, age, and language, exacerbating these concerns. In this paper, we explore the potential of generative artificial intelligence (AI) such as ChatGPT to address core problems of the ASWB exams, including the use of a multiple-choice format that does not reflect real-world social work practice. To assess its social work reasoning, we used ChatGPT to answer ASWB-developed practice questions for the Bachelors, Masters, and Clinical exams. ChatGPT scored 76%, 80%, and 64%, respectively, and identified additional validity challenges. Based on this performance, we provide a proof-of-concept for how generative AI might move us toward a more valid and equitable exam. While we strongly support licensure requirements, state regulators and legislators should temporarily suspend the use of the ASWB exams for this purpose. © The Author(s) 2023.
PB  - SAGE Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 30
ER  -

TY  - JOUR
AU  - Ahluwalia, M.
AU  - Abdalla, M.
AU  - Sanayei, J.
AU  - Seyyed-Kalantari, L.
AU  - Hussain, M.
AU  - Ali, A.
AU  - Fine, B.
TI  - The Subgroup Imperative: Chest Radiograph Classifier Generalization Gaps in Patient, Setting, and Pathology Subgroups
PY  - 2023
T2  - Radiology: Artificial Intelligence
VL  - 5
IS  - 5
C7  - e220270
DO  - 10.1148/ryai.220270
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173480773&doi=10.1148%2fryai.220270&partnerID=40&md5=c23d0be00efdb2b1dcda6d8db55a9fad
AB  - Purpose: To externally test four chest radiograph classifiers on a large, diverse, real-world dataset with robust subgroup analysis. Materials and Methods: In this retrospective study, adult posteroanterior chest radiographs (January 2016–December 2020) and associated radiology reports from Trillium Health Partners in Ontario, Canada, were extracted and de-identified. An open-source natural language processing tool was locally validated and used to generate ground truth labels for the 197 540-image dataset based on the associated radiology report. Four classifiers generated predictions on each chest radiograph. Performance was evaluated using accuracy, positive predictive value, negative predictive value, sensitivity, specificity, F1 score, and Matthews correlation coefficient for the overall dataset and for patient, setting, and pathology subgroups. Results: Classifiers demonstrated 68%–77% accuracy, 64%–75% sensitivity, and 82%–94% specificity on the external testing dataset. Algorithms showed decreased sensitivity for solitary findings (43%–65%), patients younger than 40 years (27%–39%), and patients in the emergency department (38%–60%) and decreased specificity on normal chest radiographs with support devices (59%–85%). Differences in sex and ancestry represented movements along an algorithm’s receiver operating characteristic curve. Conclusion: Performance of deep learning chest radiograph classifiers was subject to patient, setting, and pathology factors, demonstrating that subgroup analysis is necessary to inform implementation and monitor ongoing performance to ensure optimal quality, safety, and equity. © RSNA, 2023.
PB  - Radiological Society of North America Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 12
ER  -

TY  - JOUR
AU  - Livesey, B.J.
AU  - Marsh, J.A.
TI  - Updated benchmarking of variant effect predictors using deep mutational scanning
PY  - 2023
T2  - Molecular Systems Biology
VL  - 19
IS  - 8
C7  - e11474
DO  - 10.15252/msb.202211474
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161514465&doi=10.15252%2fmsb.202211474&partnerID=40&md5=0b8c91de53667ee436d7abcfadba5e70
AB  - The assessment of variant effect predictor (VEP) performance is fraught with biases introduced by benchmarking against clinical observations. In this study, building on our previous work, we use independently generated measurements of protein function from deep mutational scanning (DMS) experiments for 26 human proteins to benchmark 55 different VEPs, while introducing minimal data circularity. Many top-performing VEPs are unsupervised methods including EVE, DeepSequence and ESM-1v, a protein language model that ranked first overall. However, the strong performance of recent supervised VEPs, in particular VARITY, shows that developers are taking data circularity and bias issues seriously. We also assess the performance of DMS and unsupervised VEPs for discriminating between known pathogenic and putatively benign missense variants. Our findings are mixed, demonstrating that some DMS datasets perform exceptionally at variant classification, while others are poor. Notably, we observe a striking correlation between VEP agreement with DMS data and performance in identifying clinically relevant variants, strongly supporting the validity of our rankings and the utility of DMS for independent benchmarking. © 2023 The Authors. Published under the terms of the CC BY 4.0 license.
PB  - John Wiley and Sons Inc
C2  - 37310135
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 39
ER  -

TY  - JOUR
AU  - Knopp, M.I.
AU  - Warm, E.J.
AU  - Weber, D.
AU  - Kelleher, M.
AU  - Kinnear, B.
AU  - Schumacher, D.J.
AU  - Santen, S.A.
AU  - Mendonça, E.
AU  - Turner, L.
TI  - AI-Enabled Medical Education: Threads of Change, Promising Futures, and Risky Realities Across Four Potential Future Worlds
PY  - 2023
T2  - JMIR Medical Education
VL  - 9
IS  - 1
C7  - e50373
DO  - 10.2196/50373
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182562609&doi=10.2196%2f50373&partnerID=40&md5=6b7a328da0411bcb8a4a8879a176ed1a
AB  - Background: The rapid trajectory of artificial intelligence (AI) development and advancement is quickly outpacing society's ability to determine its future role. As AI continues to transform various aspects of our lives, one critical question arises for medical education: what will be the nature of education, teaching, and learning in a future world where the acquisition, retention, and application of knowledge in the traditional sense are fundamentally altered by AI? Objective: The purpose of this perspective is to plan for the intersection of health care and medical education in the future. Methods: We used GPT-4 and scenario-based strategic planning techniques to craft 4 hypothetical future worlds influenced by AI's integration into health care and medical education. This method, used by organizations such as Shell and the Accreditation Council for Graduate Medical Education, assesses readiness for alternative futures and effectively manages uncertainty, risk, and opportunity. The detailed scenarios provide insights into potential environments the medical profession may face and lay the foundation for hypothesis generation and idea-building regarding responsible AI implementation. Results: The following 4 worlds were created using OpenAI’s GPT model: AI Harmony, AI conflict, The world of Ecological Balance, and Existential Risk. Risks include disinformation and misinformation, loss of privacy, widening inequity, erosion of human autonomy, and ethical dilemmas. Benefits involve improved efficiency, personalized interventions, enhanced collaboration, early detection, and accelerated research. Conclusions: To ensure responsible AI use, the authors suggest focusing on 3 key areas: developing a robust ethical framework, fostering interdisciplinary collaboration, and investing in education and training. A strong ethical framework emphasizes patient safety, privacy, and autonomy while promoting equity and inclusivity. Interdisciplinary collaboration encourages cooperation among various experts in developing and implementing AI technologies, ensuring that they address the complex needs and challenges in health care and medical education. Investing in education and training prepares professionals and trainees with necessary skills and knowledge to effectively use and critically evaluate AI technologies. The integration of AI in health care and medical education presents a critical juncture between transformative advancements and significant risks. By working together to address both immediate and long-term risks and consequences, we can ensure that AI integration leads to a more equitable, sustainable, and prosperous future for both health care and medical education. As we engage with AI technologies, our collective actions will ultimately determine the state of the future of health care and medical education to harness AI's power while ensuring the safety and well-being of humanity. © 2023 JMIR Publications Inc.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 15
ER  -

TY  - JOUR
AU  - Caravella, R.A.
AU  - Ying, P.
AU  - Siegel, C.
AU  - Vaughn, R.
AU  - Deutch, A.B.
AU  - Caroff, A.
AU  - Madanes, S.
AU  - Ackerman, M.G.
AU  - Lewis, C.
TI  - Quality Improvement Framework to Examine Health Care Disparities in Behavioral Emergency Management in the Inpatient Medical Setting: A Consultation-Liaison Psychiatry Health Equity Project
PY  - 2023
T2  - Journal of the Academy of Consultation-Liaison Psychiatry
VL  - 64
IS  - 4
SP  - 322
EP  - 331
DO  - 10.1016/j.jaclp.2023.04.002
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160048478&doi=10.1016%2fj.jaclp.2023.04.002&partnerID=40&md5=52035d02b3f51f0ea7268d2377f216ee
AB  - Background: De-escalation of behavioral emergencies in the inpatient medical setting may involve restrictive clinical interventions that directly challenge patient autonomy. Objective: We describe a quality improvement framework used to examine associations between patient characteristics and behavioral emergency de-escalation strategies. This project may inform other Consultation-Liaison Psychiatry teams seeking to promote equity in care. Methods: We examined behavioral emergency response team (BERT) management at an urban, tertiary-care medical center in the United States over a 3-year period. BERT data from an existing dataset were combined with demographic information from the hospital's electronic medical record. Race and ethnic identities were categorized as Black, Hispanic, Asian, White, and unknown. BERT events were coded based on the most restrictive intervention utilized per unique patient. Cross-tabulations and adjusted odds ratios from multivariate logistic regression were used to identify quality improvement targets in this exploratory project. Results: The sample included N = 902 patients and 1532 BERT events. The most frequent intervention reached was verbal de-escalation (n = 419 patients, 46.45%) and the least frequent was 4-point restraints (n = 29 patients, 3.2%). Half of BERT activations for Asian and a third for Hispanic patients required interpreter services. Anxiety and cognitive disorders and 2 BERT interventions, verbal de-escalation, and intramuscular/intravenous/ medications, were significantly associated with race/ethnic category. The most restrictive intervention for BERTs involving Black and Asian patients were verbal de-escalation (60.1%) and intramuscular/intravenous(53.7%), respectively. These proportions were higher compared with other race/ethnic groups. There was a greater percentage of patients from the unknown (6.3%) and Black (5.9%) race/ethnic groups placed in 4-point restraints compared with other groups (3.2%) that did not reach statistical significance. A logistic regression model predicting 4-point restraints indicated that younger age, multiple BERTs, and violent behavior as a reason for BERT activation, but not race/ethnic group, resulted in significantly higher odds. Conclusions: This project illustrates that a quality improvement framework utilizing existing clinical data can be used to engage in organizational introspection and identify potential areas of bias in BERT management. Our findings suggest opportunities for further exploration, enhanced education, and programmatic improvements regarding BERT intervention; 4-point restraints; interpreter services; and the influence of race on perception of psychopathology. © 2023 Academy of Consultation-Liaison Psychiatry
PB  - Elsevier B.V.
C2  - 37060945
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Ma, Y.
TI  - The potential application of ChatGPT in gastrointestinal pathology
PY  - 2023
T2  - Gastroenterology and Endoscopy
VL  - 1
IS  - 3
SP  - 130
EP  - 131
DO  - 10.1016/j.gande.2023.05.002
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174281144&doi=10.1016%2fj.gande.2023.05.002&partnerID=40&md5=0b123442a1f13a7c4f806ccf5f6ca47e
AB  - ChatGPT is a well-known conversational artificial intelligence (AI) system based on the generative pre-trained transformer (GPT) architecture, launched by OpenAI. ChatGPT is trained through reinforcement learning based on human feedback. There are advantages in the use of ChatGPT in Gastrointestinal Pathology, including but not limited to summarizing patients’ chart and potential application in Digital Pathology, education and research. The limitation of ChatGPT is the bias based on the datasets used in ChatGPT training, the requirement of sufficient input information, as well as risk of bias and transparency issues and negative consequence if generating inaccurate content. The use of ChatGPT and other language models in Gastrointestinal Pathology requires careful consideration to ensure not bypassing expert consultation in particular cases. Deep Learning based language models would have a positive impact in Gastrointestinal Pathology rather than replacing human expertise and improving the healthcare quality for patients. © 2023 The Authors
PB  - KeAi Communications Co.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 14
ER  -

TY  - CONF
AU  - Nagendiran, S.
AU  - Renugadevi, R.
AU  - Kumar, R.P.A.
AU  - Sasirekha, K.
AU  - Harini, R.
TI  - Secure Sensitive Information on IoT using Machine Learning
PY  - 2023
T2  - International Conference on Sustainable Communication Networks and Application, ICSCNA 2023 - Proceedings
SP  - 360
EP  - 366
DO  - 10.1109/ICSCNA58489.2023.10370157
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182998260&doi=10.1109%2fICSCNA58489.2023.10370157&partnerID=40&md5=e486b15de53889a21f95fd2b9ed563d2
AB  - In a number of industries, including banking, cybersecurity, healthcare, and others, risk assessment is an essential procedure. By automating data analysis, seeing trends, and offering insights that might help organisations make better decisions, artificial intelligence (AI) tools can greatly improve risk assessment. Here are some typical AI methods and strategies for risk assessment. Machine learning is the first step, followed by Natural Language Processing (NLP), Reinforcement Learning, Predictive Analytics, and so on. While AI may significantly improve risk assessment, it ought to be used in combination with human judgement and domain knowledge. Additionally, when using AI for risk assessment, it's imperative to address issues with the accuracy of data, bias, and ethics. To guarantee the accuracy and continued relevance of risk evaluations, regular verification of models and updating is also necessary. The importance of the attributes in the UNSW-NB15 collections is therefore thoroughly analysed in this work. This research analyses a UNSW-NB15 collection of data generation to address the problems associated with the lack of any network reference data sets. This data collection combines network traffic assault actions that are now synthesised with real-world modern norms. The UNSWNB15 information set's characteristics are produced using both established and cutting-edge techniques. Many Machine Learning algorithms are used to analyse the intrusion by measuring the accuracy and other features. Here the data set is classified as Binary classification and Multi Class Classification where Machine learning are applied to find out the intrusion. Random Forest Classifier (RFC) performance good for binary classification and Linear Support Vector Machine(LSVm)performance is good for Multiclass Classification. © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Somani, S.
AU  - van Buchem, M.M.
AU  - Sarraju, A.
AU  - Hernandez-Boussard, T.
AU  - Rodriguez, F.
TI  - Artificial Intelligence–Enabled Analysis of Statin-Related Topics and Sentiments on Social Media
PY  - 2023
T2  - JAMA Network Open
VL  - 6
IS  - 4
C7  - e239747
DO  - 10.1001/jamanetworkopen.2023.9747
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153687515&doi=10.1001%2fjamanetworkopen.2023.9747&partnerID=40&md5=85749600d56f29160204d05b73297d0d
AB  - IMPORTANCE Despite compelling evidence that statins are safe, are generally well tolerated, and reduce cardiovascular events, statins are underused even in patients with the highest risk. Social media may provide contemporary insights into public perceptions about statins. OBJECTIVE To characterize and classify public perceptions about statins that were gleaned from more than a decade of statin-related discussions on Reddit, a widely used social media platform. DESIGN, SETTING, AND PARTICIPANTS This qualitative study analyzed all statin-related discussions on the social media platform that were dated between January 1, 2009, and July 12, 2022. Statin- and cholesterol-focused communities, were identified to create a list of statin-related discussions. An artificial intelligence (AI) pipeline was developed to cluster these discussions into specific topics and overarching thematic groups. The pipeline consisted of a semisupervised natural language processing model (BERT [Bidirectional Encoder Representations from Transformers]), a dimensionality reduction technique, and a clustering algorithm. The sentiment for each discussion was labeled as positive, neutral, or negative using a pretrained BERT model. EXPOSURES Statin-related posts and comments containing the terms statin and cholesterol. MAIN OUTCOMES AND MEASURES Statin-related topics and thematic groups. RESULTS A total of 10 233 unique statin-related discussions (961 posts and 9272 comments) from 5188 unique authors were identified. The number of statin-related discussions increased by a mean (SD) of 32.9% (41.1%) per year. A total of 100 discussion topics were identified and were classified into 6 overarching thematic groups: (1) ketogenic diets, diabetes, supplements, and statins; (2) statin adverse effects; (3) statin hesitancy; (4) clinical trial appraisals; (5) pharmaceutical industry bias and statins; and (6) red yeast rice and statins. The sentiment analysis revealed that most discussions had a neutral (66.6%) or negative (30.8%) sentiment. CONCLUSIONS AND RELEVANCE Results of this study demonstrated the potential of an AI approach to analyze large, contemporary, publicly available social media data and generate insights into public perceptions about statins. This information may help guide strategies for addressing barriers to statin use and adherence. © 2023 American Medical Association. All rights reserved.
PB  - American Medical Association
C2  - 37093597
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 17
ER  -

TY  - CONF
AU  - Lozoya, D.C.
AU  - D'Alfonso, S.
AU  - Conway, M.
TI  - Identifying Gender Bias in Generative Models for Mental Health Synthetic Data
PY  - 2023
T2  - Proceedings - 2023 IEEE 11th International Conference on Healthcare Informatics, ICHI 2023
SP  - 619
EP  - 626
DO  - 10.1109/ICHI57859.2023.00109
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181558888&doi=10.1109%2fICHI57859.2023.00109&partnerID=40&md5=7c2c70b15cf59d239a7a56af6f2dac07
AB  - Natural language generation (NLG) systems have proven to be effective tools to create domain-specific synthetic data. The mental health research field could benefit from data augmentation techniques, given the challenges associated with obtaining and utilizing protected health information. Yet, NLG systems are often trained using datasets that are biased with respect to key demographic factors such as ethnicity, religion, and gender. This can perpetuate and propagate systematic human biases that exist and ultimately lead to inequitable treatment for marginalized groups. In this research we studied and characterized biases present in the Generative Pre-trained Transformer 3 (GPT-3), which is an autoregressive language model that produces human-like text. The prompts used to generate text via GPT-3 were based on the Brief Cognitive Behavioral Therapy framework, and each prompt also specified to write the answer as a female or male patient. By controlling the sex distributions within our prompts, we observed the impact of each trait in the generated text. The synthetic data was analysed using the Linguistic Inquiry and Word Count software (LIWC-22) and ccLDA for cross-collection topic modeling. LIWC-22 results show that stereotypical competence features such as money, work, and cognition are more present in the male's synthetic text, whereas warmth features such as home, feeling, and emotion are highly present in female's generated data. The ccLDA results also associate competence features with males and warmth features with females. © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - CONF
AU  - Gao, L.
AU  - Zhan, H.
AU  - Chen, A.
AU  - Sheng, V.
TI  - Towards Fair and Selectively Privacy-Preserving Models Using Negative Multi-Task Learning
PY  - 2023
T2  - Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023
VL  - 37
SP  - 16214
EP  - 16215
DO  - 10.1609/aaai.v37i13.26967
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168256649&doi=10.1609%2faaai.v37i13.26967&partnerID=40&md5=84bd464319c194c5a54bab1e7f642376
AB  - Deep learning models have shown great performances in natural language processing tasks. While much attention has been paid to improvements in utility, privacy leakage and social bias are two major concerns arising in trained models. In order to tackle these problems, we protect individuals' sensitive information and mitigate gender bias simultaneously. First, we propose a selective privacy-preserving method that only obscures individuals' sensitive information. Then we propose a negative multi-task learning framework to mitigate the gender bias which contains a main task and a gender prediction task. We analyze two existing word embeddings and evaluate them on sentiment analysis and a medical text classification task. Our experimental results show that our negative multi-task learning framework can mitigate the gender bias while keeping models' utility. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
PB  - AAAI Press
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Urs, S.R.
AU  - Minhaj, M.
TI  - Evolution of data science and its education in iSchools: An impressionistic study using curriculum analysis
PY  - 2023
T2  - Journal of the Association for Information Science and Technology
VL  - 74
IS  - 6
SP  - 606
EP  - 622
DO  - 10.1002/asi.24649
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127261265&doi=10.1002%2fasi.24649&partnerID=40&md5=25dcca07c38c1eb1d39b89524c2691b8
AB  - Data Science (DS) has emerged from the shadows of its parents—statistics and computer science—into an independent field since its origin nearly six decades ago. Its evolution and education have taken many sharp turns. We present an impressionistic study of the evolution of DS anchored to Kuhn's four stages of paradigm shifts. First, we construct the landscape of DS based on curriculum analysis of the 32 iSchools across the world offering graduate-level DS programs. Second, we paint the “field” as it emerges from the word frequency patterns, ranking, and clustering of course titles based on text mining. Third, we map the curriculum to the landscape of DS and project the same onto the Edison Data Science Framework (2017) and ACM Data Science Knowledge Areas (2021). Our study shows that the DS programs of iSchools align well with the field and correspond to the Knowledge Areas and skillsets. iSchool's DS curriculums exhibit a bias toward “data visualization” along with machine learning, data mining, natural language processing, and artificial intelligence; go light on statistics; slanted toward ontologies and health informatics; and surprisingly minimal thrust toward eScience/research data management, which we believe would add a distinctive iSchool flavor to the DS. © 2022 Association for Information Science and Technology.
PB  - John Wiley and Sons Inc
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 12
ER  -

TY  - CONF
AU  - Ul Haq, A.
AU  - Bhatti, G.K.
TI  - Revolutionizing Respiratory Health: Unveiling the Interplay Between Vitamin D and Asthmatic Complications Through Advanced AI Insights
PY  - 2023
T2  - Proceedings - 2023 International Conference on Advanced Computing and Communication Technologies, ICACCTech 2023
SP  - 298
EP  - 306
DO  - 10.1109/ICACCTech61146.2023.00055
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187228202&doi=10.1109%2fICACCTech61146.2023.00055&partnerID=40&md5=bce7ba2e6c1f57d7e6b3fab702103800
AB  - Asthma, a chronic respiratory disorder affecting millions worldwide, continues to pose significant challenges to public health. There is growing interest in exploring the potential association of Vitamin D with asthmatic complications due to its known immunomodulatory properties. In recent years, the emergence of artificial intelligence (AI) has revolutionized medical research, offering new avenues to gain insights from complex datasets. This review paper comprehensively examines the role of AI in understanding the association between Vitamin D and asthmatic complications. The review begins with an overview of the current literature on the subject, highlighting the positive and negative findings from various studies. It explores the limitations of existing research and explores how AI techniques can address these challenges. The paper discusses the applications of AI, including natural language processing (NLP) for literature mining, machine learning algorithms for data analysis, and image processing techniques for lung imaging, in advancing the understanding of this association. Moreover, the review outlines the advantages and limitations of using AI in this context, emphasizing the potential benefits of AI-driven insights, including precision and scalability. The review also addresses the need for careful consideration of data biases and interpretability issues associated with AI methodologies. In conclusion, this review paper demonstrates the promising potential of AI in advancing the understanding of the association between Vitamin D and asthmatic complications.  © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Liu, S.
AU  - McCoy, A.B.
AU  - Aldrich, M.C.
AU  - Sandler, K.L.
AU  - Reese, T.J.
AU  - Steitz, B.
AU  - Bian, J.
AU  - Wu, Y.
AU  - Russo, E.
AU  - Wright, A.
TI  - Leveraging natural language processing to identify eligible lung cancer screening patients with the electronic health record
PY  - 2023
T2  - International Journal of Medical Informatics
VL  - 177
C7  - 105136
DO  - 10.1016/j.ijmedinf.2023.105136
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164275084&doi=10.1016%2fj.ijmedinf.2023.105136&partnerID=40&md5=28ebe5183ce5fb26286fcb893679c1e6
AB  - Objective: To develop and validate an approach that identifies patients eligible for lung cancer screening (LCS) by combining structured and unstructured smoking data from the electronic health record (EHR). Methods: We identified patients aged 50–80 years who had at least one encounter in a primary care clinic at Vanderbilt University Medical Center (VUMC) between 2019 and 2022. We fine-tuned an existing natural language processing (NLP) tool to extract quantitative smoking information using clinical notes collected from VUMC. Then, we developed an approach to identify patients who are eligible for LCS by combining smoking information from structured data and clinical narratives. We compared this method with two approaches to identify LCS eligibility only using smoking information from structured EHR. We used 50 patients with a documented history of tobacco use for comparison and validation. Results: 102,475 patients were included. The NLP-based approach achieved an F1-score of 0.909, and accuracy of 0.96. The baseline approach could identify 5,887 patients. Compared to the baseline approach, the number of identified patients using all structured data and the NLP-based algorithm was 7,194 (22.2 %) and 10,231 (73.8 %), respectively. The NLP-based approach identified 589 Black/African Americans, a significant increase of 119 %. Conclusion: We present a feasible NLP-based approach to identify LCS eligible patients. It provides a technical basis for the development of clinical decision support tools to potentially improve the utilization of LCS and diminish healthcare disparities. © 2023 The Author(s)
PB  - Elsevier Ireland Ltd
C2  - 37392712
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 9
ER  -

TY  - JOUR
AU  - Guleria, A.
AU  - Krishan, K.
AU  - Sharma, V.
AU  - Kanchan, T.
TI  - ChatGPT: ethical concerns and challenges in academics and research
PY  - 2023
T2  - Journal of Infection in Developing Countries
VL  - 17
IS  - 9
SP  - 1292
EP  - 1299
DO  - 10.3855/jidc.18738
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174233915&doi=10.3855%2fjidc.18738&partnerID=40&md5=0f14c5ffe000c5e2808759c9ccaa9173
AB  - Introduction: The emergence of artificial intelligence (AI) has presented several opportunities to ease human work. AI applications are available for almost every domain of life. A new technology, Chat Generative Pre-Trained Transformer (ChatGPT), was introduced by OpenAI in November 2022, and has become a topic of discussion across the world. ChatGPT-3 has brought many opportunities, as well as ethical and privacy considerations. ChatGPT is a large language model (LLM) which has been trained on the events that happened until 2021. The use of AI and its assisted technologies in scientific writing is against research and publication ethics. Therefore, policies and guidelines need to be developed over the use of such tools in scientific writing. The main objective of the present study was to highlight the use of AI and AI assisted technologies such as the ChatGPT and other chatbots in the scientific writing and in the research domain resulting in bias, spread of inaccurate information and plagiarism. Methodology: Experiments were designed to test the accuracy of ChatGPT when used in research and academic writing. Results: The information provided by ChatGPT was inaccurate and may have far-reaching implications in the field of medical science and engineering. Critical thinking should be encouraged among researchers to raise awareness about the associated privacy and ethical risks. Conclusions: Regulations for ethical and privacy concerns related to the use of ChatGPT in academics and research need to be developed. Copyright © 2023 Guleria et al.
PB  - Journal of Infection in Developing Countries
C2  - 37824352
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 46
ER  -

TY  - JOUR
AU  - Liu, Y.
AU  - Huang, Y.
AU  - Cai, Z.
TI  - AED: An black-box NLP classifier model attacker
PY  - 2023
T2  - Neurocomputing
VL  - 550
C7  - 126489
DO  - 10.1016/j.neucom.2023.126489
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163840837&doi=10.1016%2fj.neucom.2023.126489&partnerID=40&md5=a284effe776837d5431473d4cc0f49de
AB  - Deep Neural Networks (DNNs) have been successful in solving real-world tasks in domains such as connected and automated vehicles, disease, and job hiring. However, their implications are far-reaching in critical application areas. Hence, there is a growing concern regarding the potential bias and robustness of these DNN models. A transparency and robust model is always demanded in high-stakes domains where reliability and safety are enforced, such as healthcare and finance. While most studies have focused on adversarial image attack scenarios, fewer studies have investigated the robustness of DNN models in natural language processing (NLP) due to their adversarial samples are difficult to generate. To address this gap, we propose a word-level NLP classifier attack model called ”AED,” which stands for Attention mechanism enabled post-model Explanation with Density peaks clustering algorithm for synonyms search and substitution. AED aims to test the robustness of NLP DNN models by interpretability their weaknesses and exploring alternative ways to optimize them. By identifying vulnerabilities and providing explanations, AED can help improve the reliability and safety of DNN models in critical application areas such as healthcare and automated transportation. Our experiment results demonstrate that compared with other existing models, AED can effectively generate adversarial examples that can fool the victim model while maintaining the original meaning of the input. © 2023 Elsevier B.V.
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Farrand, E.
AU  - Collard, H.R.
AU  - Guarnieri, M.
AU  - Minowada, G.
AU  - Block, L.
AU  - Lee, M.
AU  - Iribarren, C.
TI  - Extracting patient-level data from the electronic health record: Expanding opportunities for health system research
PY  - 2023
T2  - PLoS ONE
VL  - 18
IS  - 3 March
C7  - e0280342
DO  - 10.1371/journal.pone.0280342
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149683278&doi=10.1371%2fjournal.pone.0280342&partnerID=40&md5=171c8eedb87fcea2bf452c6180919d30
AB  - Background Epidemiological studies of interstitial lung disease (ILD) are limited by small numbers and tertiary care bias. Investigators have leveraged the widespread use of electronic health records (EHRs) to overcome these limitations, but struggle to extract patient-level, longitudinal clinical data needed to address many important research questions. We hypothesized that we could automate longitudinal ILD cohort development using the EHR of a large, community-based healthcare system. Study design and methods We applied a previously validated algorithm to the EHR of a community-based healthcare system to identify ILD cases between 2012–2020. We then extracted disease-specific characteristics and outcomes using fully automated data-extraction algorithms and natural language processing of selected free-text. Results We identified a community cohort of 5,399 ILD patients (prevalence = 118 per 100,000). Pulmonary function tests (71%) and serologies (54%) were commonly used in the diagnostic evaluation, whereas lung biopsy was rare (5%). IPF was the most common ILD diagnosis (n = 972, 18%). Prednisone was the most commonly prescribed medication (911, 17%). Nintedanib and pirfenidone were rarely prescribed (n = 305, 5%). ILD patients were high-utilizers of inpatient (40%/year hospitalized) and outpatient care (80%/year with pulmonary visit), with sustained utilization throughout the post-diagnosis study period. Discussion We demonstrated the feasibility of robustly characterizing a variety of patient-level utilization and health services outcomes in a community-based EHR cohort. This represents a substantial methodological improvement by alleviating traditional constraints on the accuracy and clinical resolution of such ILD cohorts; we believe this approach will make community-based ILD research more efficient, effective, and scalable. Copyright: © 2023 Farrand et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
PB  - Public Library of Science
C2  - 36897886
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Pandya, A.
AU  - Lodha, P.
AU  - Ganatra, A.
TI  - Is ChatGPT ready to change mental healthcare? Challenges and considerations: a reality-check
PY  - 2023
T2  - Frontiers in Human Dynamics
VL  - 5
C7  - 1289255
DO  - 10.3389/fhumd.2023.1289255
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183380438&doi=10.3389%2ffhumd.2023.1289255&partnerID=40&md5=bae7ae970caec52a3db533ede85198d6
AB  - As mental healthcare is highly stigmatized, digital platforms and services are becoming popular. A wide variety of exciting and futuristic applications of AI platforms are available now. One such application getting tremendous attention from users and researchers alike is Chat Generative Pre-trained Transformer (ChatGPT). ChatGPT is a powerful chatbot launched by open artificial intelligence (Open AI). ChatGPT interacts with clients conversationally, answering follow-up questions, admitting mistakes, challenging incorrect premises, and rejecting inappropriate requests. With its multifarious applications, the ethical and privacy considerations surrounding the use of these technologies in sensitive areas such as mental health should be carefully addressed to ensure user safety and wellbeing. The authors comment on the ethical challenges with ChatGPT in mental healthcare that need attention at various levels, outlining six major concerns viz., (1) accurate identification and diagnosis of mental health conditions; (2) limited understanding and misinterpretation; (3) safety, and privacy of users; (4) bias and equity; (5) lack of monitoring and regulation; and (6) gaps in evidence, and lack of educational and training curricula. Copyright © 2024 Pandya, Lodha and Ganatra.
PB  - Frontiers Media SA
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Weiner, S.G.
AU  - Lo, Y.-C.
AU  - Carroll, A.D.
AU  - Zhou, L.
AU  - Ngo, A.
AU  - Hathaway, D.B.
AU  - Rodriguez, C.P.
AU  - Wakeman, S.E.
TI  - The Incidence and Disparities in Use of Stigmatizing Language in Clinical Notes for Patients with Substance Use Disorder
PY  - 2023
T2  - Journal of Addiction Medicine
VL  - 17
IS  - 4
SP  - 424
EP  - 430
DO  - 10.1097/ADM.0000000000001145
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153341166&doi=10.1097%2fADM.0000000000001145&partnerID=40&md5=a2168c433573585f1b45b0e4fcee4543
AB  - Objective The language used to describe people with substance use disorder impacts stigma and influences clinical decision making. This study evaluates the presence of stigmatizing language (SL) in clinical notes and detects patient-and provider-level differences. Methods All free-text notes generated in a large health system for patients with substance-related diagnoses between December 2020 and November 2021 were included. A natural language processing algorithm using the National Institute on Drug Abuse's "Words Matter"list was developed to identify use of SL in context. Results There were 546,309 notes for 30,391 patients, of which 100,792 (18.4%) contained SL. A total of 18,727 patients (61.6%) had at least one note with SL. The most common SLs used were "abuse"and "substance abuse."Nurses were least likely to use SL (4.1%) while physician assistants were most likely (46.9%). Male patients were more likely than female patients to have SL in their notes (adjusted odds ratio [aOR], 1.17; 95% confidence internal [CI], 1.11-1.23), younger patients aged 18 to 24 were less likely to have SL than patients 45 to 54 years (aOR, 0.55; 95% CI, 0.50-0.61), Asian patients were less likely to have SL than White patients (aOR, 0.45; 95% CI, 0.36-0.56), and Hispanic patients were less likely to have SL than non-Hispanic patients (aOR, 0.88; 95% CI, 0.80-0.98). Conclusions The majority of patients with substance-related diagnoses had at least one note containing SL. There were also several patient characteristic disparities associated with patients having SL in their notes. The work suggests that more clinician interventions about use of SL are needed.  © Wolters Kluwer Health, Inc. All rights reserved.
PB  - Lippincott Williams and Wilkins
C2  - 37579100
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 13
ER  -

TY  - CONF
AU  - Sun, Z.
AU  - Tao, C.
TI  - Named Entity Recognition and Normalization for Alzheimer's Disease Eligibility Criteria
PY  - 2023
T2  - Proceedings - 2023 IEEE 11th International Conference on Healthcare Informatics, ICHI 2023
SP  - 558
EP  - 564
DO  - 10.1109/ICHI57859.2023.00100
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181571825&doi=10.1109%2fICHI57859.2023.00100&partnerID=40&md5=b6cad8a3fbda664c80f887013b532cb1
AB  - Alzheimer's Disease (AD) is a complex neurode-generative disorder that affects millions of people worldwide. Finding effective treatments for this disease is crucial. Clinical trials play an essential role in developing and testing new treatments for AD. However, identifying eligible participants can be challenging, time-consuming, and costly. In recent years, the development of natural language processing (NLP) techniques, specifically named entity recognition (NER) and named entity normalization (NEN), have helped to automate the identification and extraction of relevant information from the eligibility criteria (EC) more efficiently, in order to facilitate semi-automatic patient recruitment and enable data FAIRness for clinical trial data. Nevertheless, most current biomedical NER models only provide annotations for a restricted set of entity types that may not be applicable to the clinical trial data. Additionally, accurately performing NEN on entities that are negated using a negative prefix currently lacks established techniques. In this paper, we introduce a pipeline designed for information extraction from AD clinical trial EC, which involves preprocessing of the EC data, clinical NER, and biomedical NEN to Unified Medical Language System (UMLS). Our NER model can identify named entities in seven pre-defined categories, while our NEN model employs a combination of exact match and partial match search strategies, as well as customized rules to accurately normalize entities with negative prefixes. To evaluate the performance of our pipeline, we measured the precision, recall, and F1 score for the NER component, and we manually reviewed the top five mapping results produced by the NEN component. Our evaluation of the pipeline's performance revealed that it can successfully normalize named entities in clinical trial ECs with optimal accuracies. The NER component achieved a overall F1 of 0.816, demonstrating its ability to accurately identify seven types of named entities in clinical text. The NEN component of the pipeline also demonstrated impressive performance, with customized rules and a combination of exact and partial match strategies leading to an accuracy of 0.940 for normalized entities. © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Yang, Z.
AU  - Liu, Y.
AU  - Ouyang, C.
AU  - Ren, L.
AU  - Wen, W.
TI  - Counterfactual can be strong in medical question and answering
PY  - 2023
T2  - Information Processing and Management
VL  - 60
IS  - 4
C7  - 103408
DO  - 10.1016/j.ipm.2023.103408
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159755941&doi=10.1016%2fj.ipm.2023.103408&partnerID=40&md5=b6e5570041c9d15c8b04c7d61b0fa8d0
AB  - Medical question and answering is a crucial aspect of medical artificial intelligence, as it aims to enhance the efficiency of clinical diagnosis and improve treatment outcomes. Despite the numerous methods available for medical question and answering, they tend to overlook the data generation mechanism's imbalance and the pseudo-correlation caused by the task's text characteristics. This pseudo-correlation is due to the fact that many words in the question and answering task are irrelevant to the answer but carry significant weight. These words can affect the feature representation and establish a false correlation with the final answer. Furthermore, the data imbalance mechanism can cause the model to blindly follow a large number of classes, leading to bias in the final answer. Confounding factors, including the data imbalance mechanism, bias due to textual characteristics, and other unknown factors, may also mislead the model and limit its performance. In this study, we propose a new counterfactual-based approach that includes a feature encoder and a counterfactual decoder. The feature encoder utilizes ChatGPT and label resetting techniques to create counterfactual data, compensating for distributional differences in the dataset and alleviating data imbalance issues. Moreover, the sampling prior to label resetting also helps us alleviate the data imbalance issue. Subsequently, label resetting can yield better and more balanced counterfactual data. Additionally, the construction of counterfactual data aids the subsequent counterfactual classifier in better learning causal features. The counterfactual decoder uses counterfactual data compared with real data to optimize the model and help it acquire the causal characteristics that genuinely influence the label to generate the final answer. The proposed method was tested on PubMedQA, a medical dataset, using machine learning and deep learning models. The comprehensive experiments demonstrate that this method achieves state-of-the-art results and effectively reduces the false correlation caused by confounders. © 2023 Elsevier Ltd
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - JOUR
AU  - Fong, A.
AU  - Hughes, J.
AU  - Gundapenini, S.
AU  - Hack, B.
AU  - Barkhordar, M.
AU  - Huang, S.S.
AU  - Visconti, A.
AU  - Fernandez, S.
AU  - Fishbein, D.
TI  - Evaluation of Structured, Semi-Structured, and Free-Text Electronic Health Record Data to Classify Hepatitis C Virus (HCV) Infection
PY  - 2023
T2  - Gastrointestinal Disorders
VL  - 5
IS  - 2
SP  - 115
EP  - 126
DO  - 10.3390/gidisord5020012
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163783726&doi=10.3390%2fgidisord5020012&partnerID=40&md5=e1fc38230df38f64986f314f5478bca0
AB  - Evaluation of the United States Centers for Disease Control and Prevention (CDC)-defined HCV-related risk factors are not consistently performed as part of routine care, rendering risk-based testing susceptible to clinician bias and missed diagnoses. This work uses natural language processing (NLP) and machine learning to identify patients who are at high risk for HCV infection. Models were developed and validated to predict patients with newly identified HCV infection (detectable RNA or reported HCV diagnosis). We evaluated models with three types of variables: structured (structured-based model), semi-structured and free-text notes (text-based model), and all variables (full-set model). We applied each model to three stratifications of data: patients with no history of HCV prior to 2020, patients with a history of HCV prior to 2020, and all patients. We used XGBoost and ten-fold C-statistic cross-validation to evaluate the generalizability of the models. There were 3564 unique patients, 487 with HCV infection. The average C-statistics on the structured-based, text-based, and full-set models for all the patients were 0.777 (95% CI: 0.744–0.810), 0.677 (95% CI: 0.631–0.723), and 0.774 (95% CI: 0.735–0.813), respectively. The full-set model performed slightly better than the structured-based model and similar to text-based models for patients with no history of HCV prior to 2020; average C-statistics of 0.780, 0.774, and 0.759, respectively. NLP was able to identify six more risk factors inconsistently coded in structured elements: incarceration, needlestick, substance use or abuse, sexually transmitted infections, piercings, and tattoos. The availability of model options (structured-based or text-based models) with a similar performance can provide deployment flexibility in situations where data is limited. © 2023 by the authors.
PB  - MDPI
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Nag, P.K.
AU  - Bhagat, A.
AU  - Vishnu Priya, R.
AU  - Khare, D.K.
TI  - Emotional Intelligence Through Artificial Intelligence: NLP and Deep Learning in the Analysis of Healthcare Texts
PY  - 2023
T2  - International Conference on Artificial Intelligence for Innovations in Healthcare Industries, ICAIIHI 2023
DO  - 10.1109/ICAIIHI57871.2023.10489117
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191475271&doi=10.1109%2fICAIIHI57871.2023.10489117&partnerID=40&md5=cbf1361ab95e37919b01d0786d869a20
AB  - This manuscript presents a methodical examination of the utilization of Artificial Intelligence (AI) in the assessment of emotions in texts related to healthcare, with a particular focus on the incorporation of Natural Language Processing (NLP) and deep learning technologies. We scrutinize numerous research studies that employ AI to augment sentiment analysis, categorize emotions, and forecast patient outcomes based on textual information derived from clinical narratives, patient feed- back on medications, and online health discussions. The review demonstrates noteworthy progress in the precision of algorithms used for sentiment classification, the prognostic capabilities of AI models for neurodegenerative diseases, and the creation of AI- powered systems that offer support in clinical decision-making. Remarkably, the utilization of AI applications has exhibited an enhancement in personalized therapy plans by integrating patient sentiment and contributing to the early identification of mental health disorders. There persist challenges, which encompass ensuring the ethical application of AI, safeguarding patient confidentiality, and addressing potential biases in algorithmic procedures. Nevertheless, the potential of AI to revolutionize healthcare practices is unmistakable, offering a future where healthcare is not only more knowledgeable and efficient but also more empathetic and centered around the needs of patients. This investigation underscores the transformative influence of AI on healthcare, delivering a comprehensive comprehension of its role in examining emotional content in healthcare texts and high- lighting the trajectory towards a more compassionate approach to patient care. The findings advocate for a harmonious synergy between AI's analytical capabilities and the human aspects of healthcare, guaranteeing that technological advancements are aligned with the emotional well-being of patients.  © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Kumar, T.
AU  - Kait, R.
AU  - Rani, S.
TI  - Possibilities and Pitfalls of Generative Pre-Trained Transformers in Healthcare
PY  - 2023
T2  - Proceedings - 2023 International Conference on Advanced Computing and Communication Technologies, ICACCTech 2023
SP  - 37
EP  - 44
DO  - 10.1109/ICACCTech61146.2023.00016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187224804&doi=10.1109%2fICACCTech61146.2023.00016&partnerID=40&md5=90859ba5c406daa6ea4cf6a45722f8c0
AB  - For its potential use in healthcare, Generative Pre-trained Transformers (GPT) and comparable models have attracted a lot of attention. These models present opportunities for therapeutic decision assistance, effective recordkeeping, natural language interactions, and patient education. Their application in healthcare, however, also has some drawbacks and difficulties that need to be properly handled.. The applications of GPT models in healthcare are incredibly broad. Through natural language interactions, they can produce patient education materials, offer decision help to healthcare professionals, and enhance user experience. GPT models offer the potential to improve tele-medicine projects, healthcare process optimization, and patient engagement. They can also help in literature reviews, information retrieval, and medical research, which enable researchers and healthcare practitioners to stay current with evidence-based practices. When applying GPT models in healthcare, a number of issues must be taken into account. These include the limitations of medical knowledge, ethical issues, potential biases and informational errors, legal and regulatory compliance, and the difficulty of having limited contextual awareness. GPT models should be used to support human decision-making rather than to replace medical experts. Critical issues that must be taken into account include patient confidentiality, data security, and the ethical usage of GPT models. To establish credibility and validate the GPT models' outputs in healthcare contexts, improvements to their interpretability and explain ability are required. To guarantee the application and efficacy of GPT models in healthcare, domain-specific adaption and clinical validation are crucial research fields. To properly handle the opportunities and drawbacks of GPT models, collaboration between researchers, healthcare professionals, and policymakers is crucial. The potential for revolutionizing healthcare delivery is enormous with pre-trained Transformers. But the difficulties and potential traps they pose must be carefully considered. GPT models can be ethically implemented and help to improve healthcare outcomes by resolving ethical issues, guaranteeing data privacy and security, and proving their efficacy in clinical situations. For GPT models to be used in healthcare to their fullest potential and to reduce the hazards involved, further research and collaboration are required.  © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Flores, L.
AU  - Kim, S.
AU  - Young, S.D.
TI  - Addressing bias in artificial intelligence for public health surveillance
PY  - 2023
T2  - Journal of Medical Ethics
VL  - 50
IS  - 3
SP  - 190
EP  - 194
DO  - 10.1136/jme-2022-108875
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159161897&doi=10.1136%2fjme-2022-108875&partnerID=40&md5=193d4f49a674de9604a79ebb7697bcce
AB  - Components of artificial intelligence (AI) for analysing social big data, such as natural language processing (NLP) algorithms, have improved the timeliness and robustness of health data. NLP techniques have been implemented to analyse large volumes of text from social media platforms to gain insights on disease symptoms, understand barriers to care and predict disease outbreaks. However, AI-based decisions may contain biases that could misrepresent populations, skew results or lead to errors. Bias, within the scope of this paper, is described as the difference between the predictive values and true values within the modelling of an algorithm. Bias within algorithms may lead to inaccurate healthcare outcomes and exacerbate health disparities when results derived from these biased algorithms are applied to health interventions. Researchers who implement these algorithms must consider when and how bias may arise. This paper explores algorithmic biases as a result of data collection, labelling and modelling of NLP algorithms. Researchers have a role in ensuring that efforts towards combating bias are enforced, especially when drawing health conclusions derived from social media posts that are linguistically diverse. Through the implementation of open collaboration, auditing processes and the development of guidelines, researchers may be able to reduce bias and improve NLP algorithms that improve health surveillance. © Author(s) (or their employer(s)) 2024.
PB  - BMJ Publishing Group
C2  - 37130756
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 14
ER  -

TY  - CONF
AU  - Ali, H.
AU  - Qadir, J.
AU  - Alam, T.
AU  - Househ, M.
AU  - Shah, Z.
TI  - Revolutionizing Healthcare with Foundation AI Models
PY  - 2023
T2  - Studies in Health Technology and Informatics
VL  - 305
SP  - 469
EP  - 470
DO  - 10.3233/SHTI230533
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164232259&doi=10.3233%2fSHTI230533&partnerID=40&md5=111861d648f0bbe31bd4e0773e248cfc
AB  - ChatGPT is a foundation Artificial Intelligence (AI) model that has opened up new opportunities in digital healthcare. Particularly, it can serve as a co-pilot tool for doctors in the interpretation, summarization, and completion of reports. Furthermore, it can build upon the ability to access the large literature and knowledge on the internet. So, chatGPT could generate acceptable responses for the medical examination. Hence. It offers the possibility of enhancing healthcare accessibility, expandability, and effectiveness. Nonetheless, chatGPT is vulnerable to inaccuracies, false information, and bias. This paper briefly describes the potential of Foundation AI models to transform future healthcare by presenting ChatGPT as an example tool. © 2023 The authors and IOS Press.
PB  - IOS Press BV
C2  - 37387067
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - CONF
AU  - Park, J.
AU  - Choi, H.K.
AU  - Ko, J.
AU  - Park, H.
AU  - Kim, J.-H.
AU  - Jeong, J.
AU  - Kim, K.
AU  - Kim, H.J.
TI  - Relation-Aware Language-Graph Transformer for Question Answering
PY  - 2023
T2  - Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023
VL  - 37
SP  - 13457
EP  - 13464
DO  - 10.1609/aaai.v37i11.26578
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167982732&doi=10.1609%2faaai.v37i11.26578&partnerID=40&md5=c70281feded46f32edb5a223b943dfcb
AB  - Question Answering (QA) is a task that entails reasoning over natural language contexts, and many relevant works augment language models (LMs) with graph neural networks (GNNs) to encode the Knowledge Graph (KG) information. However, most existing GNN-based modules for QA do not take advantage of rich relational information of KGs and depend on limited information interaction between the LM and the KG. To address these issues, we propose Question Answering Transformer (QAT), which is designed to jointly reason over language and graphs with respect to entity relations in a unified manner. Specifically, QAT constructs Meta-Path tokens, which learn relation-centric embeddings based on diverse structural and semantic relations. Then, our Relation-Aware Self-Attention module comprehensively integrates different modalities via the Cross-Modal Relative Position Bias, which guides information exchange between relevant entites of different modalities. We validate the effectiveness of QAT on commonsense question answering datasets like CommonsenseQA and OpenBookQA, and on a medical question answering dataset, MedQA-USMLE. On all the datasets, our method achieves state-of-the-art performance. Our code is available at http://github.com/mlvlab/QAT. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
PB  - AAAI Press
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - CONF
AU  - Atabek, A.
AU  - Eralp, E.
AU  - Gursoy, M.E.
TI  - Trust, Privacy and Security Aspects of Bias and Fairness in Machine Learning
PY  - 2023
T2  - Proceedings - 2023 5th IEEE International Conference on Trust, Privacy and Security in Intelligent Systems and Applications, TPS-ISA 2023
SP  - 111
EP  - 121
DO  - 10.1109/TPS-ISA58951.2023.00023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186518944&doi=10.1109%2fTPS-ISA58951.2023.00023&partnerID=40&md5=e6d92800174ba4f99e455320608e775e
AB  - In today's world, an increasing number of decisions are being affected by machine learning (ML) algorithms in critical contexts ranging from banking to healthcare, recruitment, education, and criminal justice. Since ensuring fair and unbiased outcomes in these contexts is imperative, a large body of recent work has focused on bias and fairness in ML. In this paper, we consider the trust, privacy, and security aspects of bias and fairness in ML. From the trust aspect, we argue that for fairness measurements to be robust and trusted, a diverse set of fairness metrics should be consulted, and the agreements and disagreements between them should be well-understood. Upon conducting an empirical study with ten fairness metrics, three datasets, and three correlation notions, we identify fairness metrics that are positively correlated, negatively correlated, and uncorrelated by nature. From the privacy aspect, we investigate the impact of differential privacy (DP) on ML models and find that current differentially private ML mechanisms suffer from two drawbacks: reduced accuracy and increased bias. From the security aspect, we propose a backdoor attack to inject bias into NLP models. Upon experimentally testing our attack, we observe that modern transformer-based NLP models (such as BERT and RoBERTa) are more vulnerable to our attack, our attack is able to remain stealthy, and it can generalize to dynamic (changing) triggers presented at test time. Overall, our work highlights the intersections between two research directions that are often studied independently: (i) trust, privacy, and security in ML, and (ii) bias and fairness in ML. © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Alsadhan, A.
AU  - Al-Anezi, F.
AU  - Almohanna, A.
AU  - Alnaim, N.
AU  - Alzahrani, H.
AU  - Shinawi, R.
AU  - AboAlsamh, H.
AU  - Bakhshwain, A.
AU  - Alenazy, M.
AU  - Arif, W.
AU  - Alyousef, S.
AU  - Alhamidi, S.
AU  - Alghamdi, A.
AU  - AlShrayfi, N.
AU  - Rubaian, N.B.
AU  - Alanzi, T.
AU  - AlSahli, A.
AU  - Alturki, R.
AU  - Herzallah, N.
TI  - The opportunities and challenges of adopting ChatGPT in medical research
PY  - 2023
T2  - Frontiers in Medicine
VL  - 10
C7  - 1259640
DO  - 10.3389/fmed.2023.1259640
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181716650&doi=10.3389%2ffmed.2023.1259640&partnerID=40&md5=518c0b36f89d63e526cbc13263a4cfe7
AB  - Purpose: This study aims to investigate the opportunities and challenges of adopting ChatGPT in medical research. Methods: A qualitative approach with focus groups is adopted in this study. A total of 62 participants including academic researchers from different streams in medicine and eHealth, participated in this study. Results: A total of five themes with 16 sub-themes related to the opportunities; and a total of five themes with 12 sub-themes related to the challenges were identified. The major opportunities include improved data collection and analysis, improved communication and accessibility, and support for researchers in multiple streams of medical research. The major challenges identified were limitations of training data leading to bias, ethical issues, technical limitations, and limitations in data collection and analysis. Conclusion: Although ChatGPT can be used as a potential tool in medical research, there is a need for further evidence to generalize its impact on the different research activities. Copyright © 2023 Alsadhan, Al-Anezi, Almohanna, Alnaim, Alzahrani, Shinawi, AboAlsamh, Bakhshwain, Alenazy, Arif, Alyousef, Alhamidi, Alghamdi, AlShrayfi, Rubaian, Alanzi, AlSahli, Alturki and Herzallah.
PB  - Frontiers Media SA
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 13
ER  -

TY  - JOUR
AU  - Fontaine, E.
AU  - Saez, C.
TI  - Capillary blood stability and analytical accuracy of 12 analytes stored in Microtainers®
PY  - 2023
T2  - Practical Laboratory Medicine
VL  - 36
C7  - e00325
DO  - 10.1016/j.plabm.2023.e00325
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164334879&doi=10.1016%2fj.plabm.2023.e00325&partnerID=40&md5=bf09216a8ee7bee662bdcd896365b8d7
AB  - Objectives: The aim of this study was to determine feasibility of collecting capillary blood by traditional fingerstick and next day analysis after transport in Microtainers® at ambient temperature with no plasma separation. This study is pursuing an acceptable alternative to venipuncture for measuring 12 analytes important for health risk assessment. Design: and Methods: Performance standards of a 12-assay chemistry panel were assessed using a set of paralleled serum and capillary microsamples. The panel included Hemoglobin A1c (HbA1c), Total Cholesterol, Triglycerides, HDL-C, Creatinine, Urea Nitrogen (BUN), Uric Acid, alkaline phosphatase (ALP), ALT (GPT), AST (GOT), gamma-glutamyltransferase (GGT), and total protein. Correlation studies were performed using 31 simultaneous venous and capillary blood collections. Analytical bias, correlation, and medical decision points were calculated to determine equivalency of sample type and the impact of transport conditions. Clinical sensitivity, specificity, and predictive values were evaluated at calculated medical decision points for their usability in health screening initiatives. Results: Laboratory test results using capillary blood samples stored in Microtainers® under conditions of delayed centrifugation, and mail transport at ambient temperature, showed an acceptable agreement with results obtained using their paired serum samples analyzed using standard methods, except AST. Conclusions: Capillary blood samples can be self-collected at remote locations using Microtainers® and transported at ambient temperature for 24 h for successful performance of several medical tests important in large-scale health screenings programs. © 2023 The Authors
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Thota, H.
AU  - Moh, M.
AU  - Moh, T.-S.
TI  - Towards Detecting and Quantifying Identity-Based Polarization in Online Content: A Deep-Learning Approach
PY  - 2023
T2  - Proceedings - 2023 22nd IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology, WI-IAT 2023
SP  - 593
EP  - 599
DO  - 10.1109/WI-IAT59888.2023.00098
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182523057&doi=10.1109%2fWI-IAT59888.2023.00098&partnerID=40&md5=00ebf63fbd65eec1d7cf3c4ebba250a2
AB  - Identity-based polarization is prevalent across many news outlets, and yet goes unnoticed by most readers. This separate form of polarization differs greatly from the typical ideology-based polarization, which is seen when two separate political parties solely disagree due to a policy difference. In this paper, we focus on a distinct kind of polarization that we call identity-based polarization. This is the act of refusing to focus on differences in ideology and instead focusing on individual or group identities such as race, sexuality and gender to form extreme opinions. The never-ending stream of online content through social media and news outlets has posed significant challenges to accurately understand, detect, and quantify identity-based polarization. If one cannot quantify, one cannot manage, and the health of our society, especially for the younger generation, suffers greatly. In the past, research has been done in detecting polarization in areas such as political bias, but such studies solely relied on sentiment analysis. In this paper, we take a vastly different approach as we first implement an entity-recognition system to detect identities in the article, and then as a follow up, we attempt to attribute polarization to the recognized entities using sentiment analysis. In addition, this paper leverages BERT (Bidirectional Encoder Representations from Transformers) and NLP (Natural Language Processing) techniques through the combination of sentiment analysis and a customized NER (Named-Entity-Recognition) system. In this way, this paper takes a novel and more scientific approach to accurately identifying and quantifying identity-based polarization. By utilizing news article data from five major news outlets, the final developed NER model achieved an F1 score of over 83%, and we uncovered a strong correlation between identity density in texts and extreme sentiment expressed in the text, with the correlation ranging from +0.48 to +0.68. This work is a first step towards managing identity-based polarization in online contents; it has significant societal and cultural implications, and may be readily extended to other kinds of polarization and be applied to other public media.  © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Corsello, A.
AU  - Santangelo, A.
TI  - May Artificial Intelligence Influence Future Pediatric Research?—The Case of ChatGPT
PY  - 2023
T2  - Children
VL  - 10
IS  - 4
C7  - 757
DO  - 10.3390/children10040757
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153792452&doi=10.3390%2fchildren10040757&partnerID=40&md5=9d5db8325d56a44ce91d0e0e111d3b60
AB  - Background: In recent months, there has been growing interest in the potential of artificial intelligence (AI) to revolutionize various aspects of medicine, including research, education, and clinical practice. ChatGPT represents a leading AI language model, with possible unpredictable effects on the quality of future medical research, including clinical decision-making, medical education, drug development, and better research outcomes. Aim and Methods: In this interview with ChatGPT, we explore the potential impact of AI on future pediatric research. Our discussion covers a range of topics, including the potential positive effects of AI, such as improved clinical decision-making, enhanced medical education, faster drug development, and better research outcomes. We also examine potential negative effects, such as bias and fairness concerns, safety and security issues, overreliance on technology, and ethical considerations. Conclusions: While AI continues to advance, it is crucial to remain vigilant about the possible risks and limitations of these technologies and to consider the implications of these technologies and their use in the medical field. The development of AI language models represents a significant advancement in the field of artificial intelligence and has the potential to revolutionize daily clinical practice in every branch of medicine, both surgical and clinical. Ethical and social implications must also be considered to ensure that these technologies are used in a responsible and beneficial manner. © 2023 by the authors.
PB  - MDPI
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 38
ER  -

TY  - CONF
AU  - Ryan, P.J.
AU  - Watson, R.B.
TI  - Sentiment Analysis of Social Media: Techniques, Applications, and Reliability
PY  - 2023
T2  - ACM International Conference Proceeding Series
SP  - 274
EP  - 279
DO  - 10.1145/3584871.3584911
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165996978&doi=10.1145%2f3584871.3584911&partnerID=40&md5=c1a5b1ca38e316f77bffb33114cf939d
AB  - Big data analytics can be used by smart cities to improve their citizens' liveability, health, and wellbeing. Social surveys and also social media can be employed to engage with their communities, and these require sophisticated analysis techniques. Twitter and Reddit are ideal social media tools for natural language processing since they have predominantly text-based content. Data from these social media systems can be analysed to provide sentiment on issues of importance in near real-time for decision makers. Techniques such as word clouds can provide initial qualitative analysis while quantitative analysis can produce bar charts and time series of sentiment values. Access to the Twitter and Reddit APIs are described together with analysis techniques using Python libraries. The advantages and disadvantages of this type of analysis are discussed. Social media users tend to be concentrated in the more youthful and socially progressive social cohorts, which may cause bias. © 2023 ACM.
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Zou, S.
AU  - He, J.
TI  - Large Language Models in Healthcare: A Review
PY  - 2023
T2  - Proceedings - 2023 7th International Symposium on Computer Science and Intelligent Control, ISCSIC 2023
SP  - 141
EP  - 145
DO  - 10.1109/ISCSIC60498.2023.00038
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185002310&doi=10.1109%2fISCSIC60498.2023.00038&partnerID=40&md5=a7c15cbfd81eb5e2d27a13075f9d5862
AB  - This paper examines the potential of large language models (LLMs) in the healthcare sector, delving into their prospective applications, challenges, and future trajectories. LLMs have demonstrated encouraging results in various healthcare-related domains, including the development of clinical decision support systems, natural language processing in electronic health records, healthcare question/answer systems, and healthcare education. However, integrating these models into healthcare practice also raises several concerns, such as data privacy and security issues, the requirement for vast amounts of training data, model biases, and the limited interpretability of model predictions. Overcoming these hurdles necessitates a collaborative effort from experts across multiple disciplines. Despite these obstacles, the deployment of LLMs in healthcare holds the potential to transform the industry and significantly enhance patient outcomes.  © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Bear Don't Walk Iv, O.J.
AU  - Pichon, A.
AU  - Nieva, H.R.
AU  - Sun, T.
AU  - Altosaar, J.
AU  - Natarajan, K.
AU  - Perotte, A.
AU  - Tarczy-Hornoch, P.
AU  - Demner-Fushman, D.
AU  - Elhadad, N.
TI  - Auditing Learned Associations in Deep Learning Approaches to Extract Race and Ethnicity from Clinical Text
PY  - 2023
T2  - AMIA ... Annual Symposium proceedings. AMIA Symposium
VL  - 2023
SP  - 289
EP  - 298
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182547380&partnerID=40&md5=e5e1f3a602a445a5e7085429674910c9
AB  - Complete and accurate race and ethnicity (RE) patient information is important for many areas of biomedical informatics research, such as defining and characterizing cohorts, performing quality assessments, and identifying health inequities. Patient-level RE data is often inaccurate or missing in structured sources, but can be supplemented through clinical notes and natural language processing (NLP). While NLP has made many improvements in recent years with large language models, bias remains an often-unaddressed concern, with research showing that harmful and negative language is more often used for certain racial/ethnic groups than others. We present an approach to audit the learned associations of models trained to identify RE information in clinical text by measuring the concordance between model-derived salient features and manually identified RE-related spans of text. We show that while models perform well on the surface, there exist concerning learned associations and potential for future harms from RE-identification models if left unaddressed. ©2023 AMIA - All rights reserved.
C2  - 38222422
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Sallam, M.
AU  - Salim, N.A.
AU  - Barakat, M.
AU  - Al-Tammemi, A.B.
TI  - ChatGPT applications in medical, dental, pharmacy, and public health education: A descriptive study highlighting the advantages and limitations
PY  - 2023
T2  - Narra J
VL  - 3
IS  - 1
C7  - e103
DO  - 10.52225/narra.v3i1.103
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151122361&doi=10.52225%2fnarra.v3i1.103&partnerID=40&md5=89b918a010b5f70b40f9c20d0b1b5b17
AB  - Since its public release in November 2022, ChatGPT has gained a widespread attention and received mixed responses in the academia. Promising applications of ChatGPT in university education has been suggested; however, several concerns were raised. The aim of this descriptive study was to investigate the pros and cons of ChatGPT use in medical, dental, pharmacy, and public health education. Based on expert panel discussion and review of the existing literature, specific and concise ChatGPT prompts were constructed and the responses were generated on 25 February 2023. Out data suggested that in medical education, ChatGPT benefits included the possibility of improving personalized learning, clinical reasoning and understanding of complex medical concepts. The benefits listed in the context of dental education included improved skills through step-by-step instructions and interactive content, with instant feedback on student techniques. In pharmacy education, the advantages included possible explanations of complex subjects and the deployment of interactive tools aiding to develop skills for patient counselling. In public health education, the listed benefits included providing explanations and case scenarios, besides improved skills in data analysis and literature review. The limitations listed based on ChatGPT-generated content were common across all of the investigated healthcare disciplines and included data privacy issues, risk of generating biased and inaccurate content, and the risk of deterioration of critical thinking and communication skills among healthcare students. The ChatGPT-generated content in the context of healthcare education was deemed partially helpful by the expert panel. However, several important points regarding the pros and cons of ChatGPT use in medical, dental, pharmacy and public health education were missed by ChatGPT-generated content including: the risk of plagiarism, copyright issues, the risk of academic dishonesty, and the lack of personal and emotional interactions necessary for developing proper communication skills in healthcare education. In conclusion, despite the promising prospects of ChatGPT in healthcare education, several drawbacks should be addressed with implementation of guidelines for proper use to ensure exploiting the benefits of this innovative technology. © 2023, School of Medicine, Universitas Syiah Kuala. All rights reserved.
PB  - School of Medicine, Universitas Syiah Kuala
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 138
ER  -

TY  - JOUR
AU  - Akiba, D.
AU  - Fraboni, M.C.
TI  - AI-Supported Academic Advising: Exploring ChatGPT’s Current State and Future Potential toward Student Empowerment
PY  - 2023
T2  - Education Sciences
VL  - 13
IS  - 9
C7  - 885
DO  - 10.3390/educsci13090885
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172154840&doi=10.3390%2feducsci13090885&partnerID=40&md5=c80ee3f321e45545c03df031e7aa0be0
AB  - Artificial intelligence (AI), once a phenomenon primarily in the world of science fiction, has evolved rapidly in recent years, steadily infiltrating into our daily lives. ChatGPT, a freely accessible AI-powered large language model designed to generate human-like text responses to users, has been utilized in several areas, such as the healthcare industry, to facilitate interactive dissemination of information and decision-making. Academic advising has been essential in promoting success among university students, particularly those from disadvantaged backgrounds. Unfortunately, however, student advising has been marred with problems, with the availability and accessibility of adequate advising being among the hurdles. The current study explores how AI-powered tools like ChatGPT might serve to make academic advising more accessible, efficient, or effective. The authors compiled a list of questions frequently asked by current and prospective students in a teacher education bachelor’s degree program in the United States. Then, the questions were typed into the free version of ChatGPT, and the answers generated were explored and evaluated for their content and delivery. ChatGPT generated surprisingly high-quality answers, written in an authoritative yet supportive tone, and it was particularly adept at addressing general and open-ended career-related questions, such as career outlook, in a clear, comprehensive, and supportive manner using plain language. We argue that AI-powered tools, such as ChatGPT, may complement but not necessarily replace human academic advisers and that these tools may very well serve to promote educational equity by empowering individuals from a wide range of backgrounds with the means to initiate effective methods of seeking academic advice. © 2023 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 47
ER  -

TY  - JOUR
AU  - Suryanto, T.L.M.
AU  - Wibawa, A.P.
AU  - Nafalski, A.
TI  - Evolving Conversations: A Review of Chatbots and Implications in Natural Language Processing for Cultural Heritage Ecosystems
PY  - 2023
T2  - International Journal of Robotics and Control Systems
VL  - 3
IS  - 4
SP  - 955
EP  - 1006
DO  - 10.31763/ijrcs.v3i4.1195
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184393605&doi=10.31763%2fijrcs.v3i4.1195&partnerID=40&md5=a83834f89adc29442b97c1399012cd2a
AB  - Chatbot technology, a rapidly growing field, uses Natural Language Processing (NLP) methodologies to create conversational AI bots. Contextual understanding is essential for chatbots to provide meaningful interactions. Still, to date chatbots often struggle to accurately interpret user input due to the complexity of natural language and diverse fields, hence the need for a Systematic Literature Review (SLR) to investigate the motivation behind the creation of chatbots, their development procedures and methods, notable achievements, challenges and emerging trends. Through the application of the PRISMA method, this paper contributes to revealing the rapid and dynamic progress in chatbot technology with NLP learning models, enabling sophisticated and human-like interactions on the trends observed in chatbots over the past decade. The results, from various fields such as healthcare, organization and business, virtual personalities, to education, do not rule out the possibility of being developed in other fields such as chatbots for cultural preservation while suggesting the need for supervision in the aspects of language comprehension bias and ethics of chatbot users. In the end, the insights gained from SLR have the potential to contribute significantly to the advancement of chatbots on NLP as a comprehensive field. © 2023, Association for Scientific Computing Electronics and Engineering (ASCEE). All rights reserved.
PB  - Association for Scientific Computing Electronics and Engineering (ASCEE)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Killian, C.M.
AU  - Marttinen, R.
AU  - Howley, D.
AU  - Sargent, J.
AU  - Jones, E.M.
TI  - “Knock, Knock::: Who’s There?” ChatGPT and Artificial Intelligence-Powered Large Language Models: Reflections on Potential Impacts Within Health and Physical Education Teacher Education
PY  - 2023
T2  - Journal of Teaching in Physical Education
VL  - 42
IS  - 3
SP  - 385
EP  - 389
DO  - 10.1123/jtpe.2023-0058
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165202171&doi=10.1123%2fjtpe.2023-0058&partnerID=40&md5=15fd4a255834a060e3fd76659bcc511a
AB  - This research note suggests the emergence of Artificial Intelligence-powered chatbots like ChatGPT pose challenges to the future of higher education. We as a field should pay attention to issues and opportunities associated with this technology across learning, teaching, and research spaces. We propose ignoring, or being indifferent to, predictions about what technologies like Artificial Intelligence-powered chatbots can do can cause us to do “dumb things.” All health and physical education teacher education faculty members should make efforts to learn about these tools to facilitate informed, solution-focused decisions about whether and where to leverage them. We highlight the importance of maintaining sociocritical perspectives when considering use of digital technologies to understand and address digital (in)equity and promote equitable practices. We conclude by emphasizing the need for field-specific consensus statements to guide ethical and appropriate use of Artificial Intelligence-powered chatbots, to ensure the value of these tools is harnessed for the good of the society. [Output by ChatGPT-3] © 2023 Human Kinetics, Inc.
PB  - Human Kinetics Publishers Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 12
ER  -

TY  - CONF
AU  - Zhong, R.
AU  - Zhang, P.
AU  - Li, S.
AU  - Ahn, J.
AU  - Klein, D.
AU  - Steinhardt, J.
TI  - Goal Driven Discovery of Distributional Differences via Language Descriptions
PY  - 2023
T2  - Advances in Neural Information Processing Systems
VL  - 36
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187439925&partnerID=40&md5=1277021d1912db01d182038c2807e035
AB  - Exploring large corpora can generate useful discoveries but is time-consuming for humans. We formulate a new task, D5, that automatically discovers differences between two large corpora in a goal-driven way. The task input is a problem comprising a user-specified exploration goal (“comparing the side effects of drug A and drug B”) and a corpus pair (collections of patients' self-reported reactions after taking each drug). The output is a goal-relevant description (discovery) of how these corpora differ (patients taking drug A “mention feelings of paranoia” more often). We build a D5 system, and to quantitatively evaluate its performance, we 1) build a diagnostic benchmark, SYND5, to test whether it can recover known differences between two synthetic corpora, and 2) contribute a meta-dataset, OPEND5, aggregating 675 open-ended problems ranging across business, social sciences, humanities, machine learning, and health. With both synthetic and real datasets, we confirm that language models can leverage user-specified goals to propose more relevant candidate discoveries, and they sometimes produce discoveries previously unknown to the authors, including demographic differences in discussion topics, political stances in speech, insights in commercial reviews, and error patterns in NLP models. Finally, we discuss the limitations of our D5 system, which discovers correlation rather than causation and potentially reinforces biases when misused; therefore, practitioners should treat the outputs of our system with caution. © 2023 Neural information processing systems foundation. All rights reserved.
PB  - Neural information processing systems foundation
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Mittal, S.
AU  - De Choudhury, M.
TI  - Moral Framing of Mental Health Discourse and Its Relationship to Stigma: A Comparison of Social Media and News
PY  - 2023
T2  - Conference on Human Factors in Computing Systems - Proceedings
C7  - 484
DO  - 10.1145/3544548.3580834
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160002267&doi=10.1145%2f3544548.3580834&partnerID=40&md5=c07f1f7bf532760559d03f412d6d89ff
AB  - Mental health discussions on public forums influence the perceptions of people. Negative consequences may result from hostile and "othering"portrayals of people with mental disorders. Adopting the lens of Moral Foundation Theory (MFT), we study framings of mental health discourse on Twitter and News, and how moral underpinnings abate or exacerbate stigma. We adopted a large language model based representation framework to score 13,277,115 public tweets and 21,167 news articles against MFT's five foundations. We found discussions on Twitter to demonstrate compassion, justice and equity-centered moral values for those suffering from mental illness, in contrast to those on News. That said, stigmatized discussions appeared on both Twitter and News, with news articles being more stigmatizing than tweets. We discuss implications for public health authorities to refine measures for safe reporting of mental health, and for social media platforms to design affordances that enable empathetic discourse. © 2023 Owner/Author.
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 11
ER  -

TY  - CONF
AU  - Deshpande, A.
AU  - Murahari, V.
AU  - Rajpurohit, T.
AU  - Kalyan, A.
AU  - Narasimhan, K.
TI  - Toxicity in CHATGPT: Analyzing Persona-assigned Language Models
PY  - 2023
T2  - Findings of the Association for Computational Linguistics: EMNLP 2023
SP  - 1236
EP  - 1270
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183301218&partnerID=40&md5=c396593f20d6af7b1e3b99f59a4545bf
AB  - Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Legislation has recognized its significance and recently drafted a “Blueprint For An AI Bill Of Rights” which calls for domain experts to identify risks and potential impact of AI systems. To this end, we systematically evaluate toxicity in over half a million generations of CHATGPT, a popular dialogue-based LLM. We find that setting the system parameter of CHATGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to CHATGPT, its toxicity can increase up to 6×, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others (3× more) irrespective of the assigned persona, reflecting discriminatory biases in the model. Our findings show that multiple provisions in the legislative blueprint are being violated, and we hope that the broader AI community rethinks the efficacy of current safety guardrails and develops better techniques that lead to robust, safe, and trustworthy AI. © 2023 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 52
ER  -

TY  - JOUR
AU  - Gallifant, J.
AU  - Zhang, J.
AU  - Whebell, S.
AU  - Quion, J.
AU  - Escobar, B.
AU  - Gichoya, J.
AU  - Herrera, K.
AU  - Jina, R.
AU  - Chidambaram, S.
AU  - Mehndiratta, A.
AU  - Kimera, R.
AU  - Marcelo, A.
AU  - Fernandez-Marcelo, P.G.
AU  - Osorio, J.S.
AU  - Villanueva, C.
AU  - Nazer, L.
AU  - Dankwa-Mullan, I.
AU  - Celi, L.A.
TI  - A new tool for evaluating health equity in academic journals; the Diversity Factor
PY  - 2023
T2  - PLOS Global Public Health
VL  - 3
IS  - 8
C7  - e0002252
DO  - 10.1371/journal.pgph.0002252
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177790091&doi=10.1371%2fjournal.pgph.0002252&partnerID=40&md5=a27d8544ea384c2525aabb99de09682e
AB  - Current methods to evaluate a journal’s impact rely on the downstream citation mapping used to generate the Impact Factor. This approach is a fragile metric prone to being skewed by outlier values and does not speak to a researcher’s contribution to furthering health outcomes for all populations. Therefore, we propose the implementation of a Diversity Factor to fulfill this need and supplement the current metrics. It is composed of four key elements: dataset properties, author country, author gender and departmental affiliation. Due to the significance of each individual element, they should be assessed independently of each other as opposed to being combined into a simplified score to be optimized. Herein, we discuss the necessity of such metrics, provide a framework to build upon, evaluate the current landscape through the lens of each key element and publish the findings on a freely available website that enables further evaluation. The OpenAlex database was used to extract the metadata of all papers published from 2000 until August 2022, and Natural language processing was used to identify individual elements. Features were then displayed individually on a static dashboard developed using TableauPublic, which is available at www. equitablescience.com/. In total, 130,721 papers were identified from 7,462 journals where significant underrepresentation of LMIC and Female authors was demonstrated. These findings are pervasive and show no positive correlation with the Journal’s Impact Factor. The systematic collection of the Diversity Factor concept would allow for more detailed analysis, highlight gaps in knowledge, and reflect confidence in the translation of related research. Conversion of this metric to an active pipeline would account for the fact that how we define those most at risk will change over time and quantify responses to particular initiatives. Therefore, continuous measurement of outcomes across groups and those investigating those outcomes will never lose importance. Moving forward, we encourage further revision and improvement by diverse author groups in order to better refine this concept. © 2023 Gallifant et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
PB  - Public Library of Science
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - JOUR
AU  - Schreiner, A.D.
AU  - Livingston, S.
AU  - Zhang, J.
AU  - Gebregziabher, M.
AU  - Marsden, J.
AU  - Koch, D.G.
AU  - Petz, C.A.
AU  - Durkalski-Mauldin, V.L.
AU  - Mauldin, P.D.
AU  - Moran, W.P.
TI  - Identifying Patients at Risk for Fibrosis in a Primary Care NAFLD Cohort
PY  - 2023
T2  - Journal of Clinical Gastroenterology
VL  - 57
IS  - 1
SP  - 89
EP  - 96
DO  - 10.1097/MCG.0000000000001585
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126036935&doi=10.1097%2fMCG.0000000000001585&partnerID=40&md5=d27b9c31769abe06d95a5aa83b07e09b
AB  - Goals and Background: Using natural language processing to create a nonalcoholic fatty liver disease (NAFLD) cohort in primary care, we assessed advanced fibrosis risk with the Fibrosis-4 Index (FIB-4) and NAFLD Fibrosis Score (NFS) and evaluated risk score agreement. Materials and Methods: In this retrospective study of adults with radiographic evidence of hepatic steatosis, we calculated patient-level FIB-4 and NFS scores and categorized them by fibrosis risk. Risk category and risk score agreement was analyzed using weighted κ, Pearson correlation, and Bland-Altman analysis. A multinomial logistic regression model evaluated associations between clinical variables and discrepant FIB-4 and NFS results. Results: Of the 767 patient cohorts, 71% had a FIB-4 or NFS score in the indeterminate-risk or high-risk category for fibrosis. Risk categories disagreed in 43%, and scores would have resulted in different clinical decisions in 30% of the sample. The weighted κ statistic for risk category agreement was 0.41 [95% confidence interval (CI): 0.36-0.46] and the Pearson correlation coefficient for log FIB-4 and NFS was 0.66 (95% CI: 0.62-0.70). The multinomial logistic regression analysis identified black race (odds ratio=2.64, 95% CI: 1.84-3.78) and hemoglobin A1c (odds ratio=1.37, 95% CI: 1.23-1.52) with higher odds of having an NFS risk category exceeding FIB-4. Conclusions: In a primary care NAFLD cohort, many patients had elevated FIB-4 and NFS risk scores and these risk categories were often in disagreement. The choice between FIB-4 and NFS for fibrosis risk assessment can impact clinical decision-making and may contribute to disparities of care.  © 2021 Wolters Kluwer Health, Inc. All rights reserved.
PB  - Lippincott Williams and Wilkins
C2  - 34294656
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - Gode, S.
AU  - Bare, S.
AU  - Raj, B.
AU  - Yoo, H.
TI  - Understanding political polarization using language models: A dataset and method
PY  - 2023
T2  - AI Magazine
VL  - 44
IS  - 3
SP  - 248
EP  - 254
DO  - 10.1002/aaai.12104
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181212210&doi=10.1002%2faaai.12104&partnerID=40&md5=6abcb4859db6c116c4a38e1577f46be8
AB  - Our paper aims to analyze political polarization in US political system using language models, and thereby help candidates make an informed decision. The availability of this information will help voters understand their candidates' views on the economy, healthcare, education, and other social issues. Our main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a language model-based method that helps analyze how polarized a candidate is. Our data are divided into two parts, background information and political information about a candidate, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, alma mater, and so forth. We further split this data into four phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases. To understand the polarization, we begin by showing results from some classical language models in Word2Vec and Doc2Vec. And then use more powerful techniques like the Longformer, a transformer-based encoder, to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background. The code and data for the project will be available here: “https://github.com/samirangode/Understanding_Polarization”. © 2023 The Authors. AI Magazine published by Wiley Periodicals LLC on behalf of the Association for the Advancement of Artificial Intelligence.
PB  - John Wiley and Sons Inc
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Abdelati, A.
AU  - Burns, M.M.
AU  - Chary, M.
TI  - Sublethal toxicities of 2,4-dinitrophenol as inferred from online self-reports
PY  - 2023
T2  - PLoS ONE
VL  - 18
IS  - 9 September
C7  - e0290630
DO  - 10.1371/journal.pone.0290630
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171236809&doi=10.1371%2fjournal.pone.0290630&partnerID=40&md5=f365e50b07011e1c3746111abcaed643
AB  - Introduction 2,4-dinitrophenol (DNP) is a mitochondrial toxin sometimes used as a weight loss agent. Reports of fatalities from DNP have been increasing since 2000, suggesting an increase in use. Our understanding of DNP toxicity in humans comes from reports to Poison Control and postmortem analyses, sources that are biased to more extreme presentations. This leads to a gap in our knowledge about the adverse effects of DNP at nonlethal doses. Here we investigate the doses and effects of DNP as reported online. Methods We analyzed publicly available Internet posts that we collected from 2017–2019. The posts came from anonymous users or users who voluntarily self-identified. We collected data from websites whose terms of use allow for the secondary analysis of data that their users agree to make public. We used natural language processing techniques that we had previously developed to extract doses, effects, and substances mentioned in each post. Results We collected 1,630 posts across 5 online forums and the Reddit forum r/DNP. The posts were from 1,234 unique usernames. The most commonly reported doses were between 150 to 300 mg each day followed by 300 to 450 mg each day. At those doses, the most reported adverse effects were profuse sweating and fatigue. Reports of thermoregulatory (sweating, feeling hot flashes or flushed), fatigue-related, and neurologically related symptoms were statistically significantly more frequent at reported daily doses greater than 150 mg than doses below 150 mg (post-hoc χ2-test with Bonferroni correction). The effects were judged as plausible by two board-certified medical toxicologists. Triiodothyronine, clenbuterol, testosterone, and trenbolone, an androgenic anabolic steroid were the most significantly co-mentioned substances. Conclusions Fatigue, increased body temperature, and paresthesias from DNP are reported more frequently at doses greater than 150 mg each day than at doses less than 150 mg each day. Online discussions of DNP frequently mention androgenic anabolic steroids and other weight loss agents. © 2023 Abdelati et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
PB  - Public Library of Science
C2  - 37703241
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - da Silva, G.S.
AU  - Ulbricht, V.R.
TI  - CHATGPT AND BARD IN EDUCATION: A COMPARATIVE REVIEW
PY  - 2023
T2  - 20th International Conference on Cognition and Exploratory Learning in Digital Age, CELDA 2023
SP  - 369
EP  - 376
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181767468&partnerID=40&md5=b12b484cd62f5af3172fd7e1a8082ee5
AB  - ChatGPT and Bard, two chatbots powered by Large Language Models (LLMs), are propelling the educational sector towards a new era of instructional innovation. Within this educational paradigm, the present investigation conducts a comparative analysis of these groundbreaking chatbots, scrutinizing their distinct operational characteristics and applications as depicted in current scholarly discourse. ChatGPT emerges as an exemplary tool in task-oriented textual interactions, while Bard brandishes unique features such as Text-To-Speech (TTS) functionality, which enhances accessibility and inclusive education, as well as integration with Google Workspace applications. This research critically examines their utilization in various spheres such as pedagogy, academic research, Massive Open Online Courses (MOOCs), mathematics, and software programming. Findings accentuate ChatGPT's superior efficacy in content drafting, code generation, language translation, and providing clinically precise responses, notwithstanding Bard's significant potential encapsulated in its exclusive features. Furthermore, the study traverses’ crucial ethical aspects, including privacy concerns and inherent bias, underscoring the profound implications of these Artificial Intelligence (AI) technologies on literature and advocating against the indiscriminate reliance on such models. © CELDA 2023.All rights reserved.
PB  - IADIS Press
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Handa, P.
AU  - Chhabra, D.
AU  - Goel, N.
AU  - Krishnan, S.
TI  - Exploring the role of ChatGPT in medical image analysis
PY  - 2023
T2  - Biomedical Signal Processing and Control
VL  - 86
C7  - 105292
DO  - 10.1016/j.bspc.2023.105292
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166215609&doi=10.1016%2fj.bspc.2023.105292&partnerID=40&md5=1d955e6767467e7b79a140f295abbdf3
AB  - The rapid advancement of artificial intelligence (AI) in the field of medical image analysis has demonstrated various applications such as computer-aided disease diagnosis and prognosis, image registration, tissue segmentation, image fusion, annotations, etc. However, its real-time use in routine clinical settings is still far-fetched due to numerous factors including lack of clinical trials in AI, development of diverse datasets, generalization of AI models in real-time, their interpretability, and associated biases. The introduction of Chat Generative Pre-trained Transformer (ChatGPT) has further created both an opportunity and havoc in the minds of researchers for its use and applications in this field. This editorial aims to highlight the role of ChatGPT in the field of medical image analysis. An exploratory analysis was conducted on ChatGPT by asking more than a hundred questions related to this field like “what is medical image analysis”, “how can you help me with medical image analysis”, “generate a code for — to perform medical image analysis”, “perform literature survey on — for medical datasets”, “analyze the anomaly in this frame”, “write an editorial on —”, “give road map for — in medical image analysis” etc in February and May 2023. The inferences of the responses generated by ChatGPT, their variation over a two-month gap, our viewpoint, its shortcomings, and concluding remarks have been discussed in this editorial. © 2023 Elsevier Ltd
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - JOUR
AU  - Kelly, P.J.A.
AU  - Snyder, A.M.
AU  - Agénor, M.
AU  - Navalta, C.R.
AU  - Misquith, C.
AU  - Rich, J.D.
AU  - Hughto, J.M.W.
TI  - A Scoping Review of Methodological Approaches to Detect Bias in the Electronic Health Record
PY  - 2023
T2  - Stigma and Health
DO  - 10.1037/sah0000497
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183552162&doi=10.1037%2fsah0000497&partnerID=40&md5=c433e758b4f3a3c3c3f14074d09c0012
AB  - As health systems move to make electronic health records (EHRs) accessible to patients, there is a need to examine if, and the extent to which, bias toward patients may be evident in these records. This scoping review aimed to summarize the scientific literature on methods used to detect biased language about patients in the EHR and the nature and object of the biases detected. A comprehensive literature search was conducted of PubMed, CINAHL, Web of Science, APA PsycInfo, and SOCIndex for peer-reviewed English language studies conducted in the United States published on or before December 22, 2022. Seven studies were included in this review. Four methods were identified: natural language processing methods including machine learning-based (n = 3), Linguistic Inquiry and Word Count (n = 2), and exploratory vocabulary analysis (n = 1), and manual content analysis (n = 2). In four studies, the EHR of Black patients contained significantly greater bias relative to the EHR of White patients. Bias about health conditions (i.e., diabetes, substance use disorder, and chronic pain), women, and preexposure prophylaxis—a medication that prevents HIV infection—were identified. Machine-based learning methods may be best to (a) analyze robust data sampling frames, (b) detect a rare outcome like bias, (c) facilitate inferential analysis, and (d) transcend limitations of manual content analysis. Findings provide an overview of methods that can be used by investigators to analyze EHR records for bias to inform clinical interventions, health policies, and procedures to reduce bias among health care providers. © 2023 American Psychological Association
PB  - American Psychological Association
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Hartzler, A.L.
AU  - Xie, S.J.
AU  - Wedgeworth, P.
AU  - Spice, C.
AU  - Lybarger, K.
AU  - Wood, B.R.
AU  - Duber, H.C.
AU  - Hsieh, G.
AU  - Singh, A.P.
TI  - Integrating patient voices into the extraction of social determinants of health from clinical notes: ethical considerations and recommendations
PY  - 2023
T2  - Journal of the American Medical Informatics Association
VL  - 30
IS  - 8
SP  - 1456
EP  - 1462
DO  - 10.1093/jamia/ocad043
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160194828&doi=10.1093%2fjamia%2focad043&partnerID=40&md5=84667269d6d750d2d866bdf5d2158c34
AB  - Identifying patients’ social needs is a first critical step to address social determinants of health (SDoH)—the conditions in which people live, learn, work, and play that affect health. Addressing SDoH can improve health outcomes, population health, and health equity. Emerging SDoH reporting requirements call for health systems to implement efficient ways to identify and act on patients’ social needs. Automatic extraction of SDoH from clinical notes within the electronic health record through natural language processing offers a promising approach. However, such automated SDoH systems could have unintended consequences for patients, related to stigma, privacy, confidentiality, and mistrust. Using Floridi et al’s “AI4People” framework, we describe ethical considerations for system design and implementation that call attention to patient autonomy, beneficence, nonmaleficence, justice, and explicability. Based on our engagement of clinical and community champions in health equity work at University of Washington Medicine, we offer recommendations for integrating patient voices and needs into automated SDoH systems. © 2023 Oxford University Press. All rights reserved.
PB  - Oxford University Press
C2  - 36944091
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Lituiev, D.S.
AU  - Lacar, B.
AU  - Pak, S.
AU  - Abramowitsch, P.L.
AU  - De Marchis, E.H.
AU  - Peterson, T.A.
TI  - Automatic extraction of social determinants of health from medical notes of chronic lower back pain patients
PY  - 2023
T2  - Journal of the American Medical Informatics Association
VL  - 30
IS  - 8
SP  - 1438
EP  - 1447
DO  - 10.1093/jamia/ocad054
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165517607&doi=10.1093%2fjamia%2focad054&partnerID=40&md5=0aade6104f45ae45c9489c4fa6f5a7c0
AB  - Objective: We applied natural language processing and inference methods to extract social determinants of health (SDoH) information from clinical notes of patients with chronic low back pain (cLBP) to enhance future analyses of the associations between SDoH disparities and cLBP outcomes. Materials and Methods: Clinical notes for patients with cLBP were annotated for 7 SDoH domains, as well as depression, anxiety, and pain scores, resulting in 626 notes with at least one annotated entity for 364 patients. We used a 2-tier taxonomy with these 10 first-level classes (domains) and 52 second-level classes. We developed and validated named entity recognition (NER) systems based on both rule-based and machine learning approaches and validated an entailment model. Results: Annotators achieved a high interrater agreement (Cohen’s kappa of 95.3% at document level). A rule-based system (cTAKES), RoBERTa NER, and a hybrid model (combining rules and logistic regression) achieved performance of F1 ¼ 47.1%, 84.4%, and 80.3%, respectively, for first-level classes. Discussion: While the hybrid model had a lower F1 performance, it matched or outperformed RoBERTa NER model in terms of recall and had lower computational requirements. Applying an untuned RoBERTa entailment model, we detected many challenging wordings missed by NER systems. Still, the entailment model may be sensitive to hypothesis wording. Conclusion: This study developed a corpus of annotated clinical notes covering a broad spectrum of SDoH classes. This corpus provides a basis for training machine learning models and serves as a benchmark for predictive models for NER for SDoH and knowledge extraction from clinical texts. © 2023 Oxford University Press. All rights reserved.
PB  - Oxford University Press
C2  - 37080559
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 14
ER  -

TY  - JOUR
AU  - Meo, S.A.
AU  - Al-Masri, A.A.
AU  - Alotaibi, M.
AU  - Meo, M.Z.S.
AU  - Meo, M.O.S.
TI  - ChatGPT Knowledge Evaluation in Basic and Clinical Medical Sciences: Multiple Choice Question Examination-Based Performance
PY  - 2023
T2  - Healthcare (Switzerland)
VL  - 11
IS  - 14
C7  - 2046
DO  - 10.3390/healthcare11142046
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166553214&doi=10.3390%2fhealthcare11142046&partnerID=40&md5=d2b76e867730f8737a5d05abdf5e61fa
AB  - The Chatbot Generative Pre-Trained Transformer (ChatGPT) has garnered great attention from the public, academicians and science communities. It responds with appropriate and articulate answers and explanations across various disciplines. For the use of ChatGPT in education, research and healthcare, different perspectives exist with some level of ambiguity around its acceptability and ideal uses. However, the literature is acutely lacking in establishing a link to assess the intellectual levels of ChatGPT in the medical sciences. Therefore, the present study aimed to investigate the knowledge level of ChatGPT in medical education both in basic and clinical medical sciences, multiple-choice question (MCQs) examination-based performance and its impact on the medical examination system. In this study, initially, a subject-wise question bank was established with a pool of multiple-choice questions (MCQs) from various medical textbooks and university examination pools. The research team members carefully reviewed the MCQ contents and ensured that the MCQs were relevant to the subject’s contents. Each question was scenario-based with four sub-stems and had a single correct answer. In this study, 100 MCQs in various disciplines, including basic medical sciences (50 MCQs) and clinical medical sciences (50 MCQs), were randomly selected from the MCQ bank. The MCQs were manually entered one by one, and a fresh ChatGPT session was started for each entry to avoid memory retention bias. The task was given to ChatGPT to assess the response and knowledge level of ChatGPT. The first response obtained was taken as the final response. Based on a pre-determined answer key, scoring was made on a scale of 0 to 1, with zero representing incorrect and one representing the correct answer. The results revealed that out of 100 MCQs in various disciplines of basic and clinical medical sciences, ChatGPT attempted all the MCQs and obtained 37/50 (74%) marks in basic medical sciences and 35/50 (70%) marks in clinical medical sciences, with an overall score of 72/100 (72%) in both basic and clinical medical sciences. It is concluded that ChatGPT obtained a satisfactory score in both basic and clinical medical sciences subjects and demonstrated a degree of understanding and explanation. This study’s findings suggest that ChatGPT may be able to assist medical students and faculty in medical education settings since it has potential as an innovation in the framework of medical sciences and education. © 2023 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 36
ER  -

TY  - JOUR
AU  - Goktas, P.
AU  - Karakaya, G.
AU  - Kalyoncu, A.F.
AU  - Damadoglu, E.
TI  - Artificial Intelligence Chatbots in Allergy and Immunology Practice: Where Have We Been and Where Are We Going?
PY  - 2023
T2  - Journal of Allergy and Clinical Immunology: In Practice
VL  - 11
IS  - 9
SP  - 2697
EP  - 2700
DO  - 10.1016/j.jaip.2023.05.042
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163359601&doi=10.1016%2fj.jaip.2023.05.042&partnerID=40&md5=fce2e8b92224cdb45e44351d927d1a27
AB  - Artificial intelligence (AI) is rapidly becoming a valuable tool in healthcare, providing clinicians with a new AI lens perspective for patient care, diagnosis, and treatment. This article explores the potential applications, benefits, and challenges of AI chatbots in clinical settings, with a particular emphasis on ChatGPT 4.0 (OpenAI - Chat generative pretrained transformer 4.0), especially in the field of allergy and immunology. AI chatbots have shown considerable promise in various medical domains, including radiology and dermatology, by improving patient engagement, diagnostic accuracy, and personalized treatment plans. ChatGPT 4.0, developed by OpenAI, is good at understanding and replying to prompts in a way that makes sense. However, it is critical to address the potential biases, data privacy issues, ethical considerations, and the need for verification of AI-generated findings. When used responsibly, AI chatbots can significantly enhance clinical practice in allergy and immunology. However, there are still challenges in using this technology that require ongoing research and collaboration between AI developers and medical specialists. To this end, the ChatGPT 4.0 platform has the potential to enhance patient engagement, improve diagnostic accuracy, and provide personalized treatment plans in allergy and immunology practice. However, limitations and risks must be addressed to ensure their safe and effective use in clinical practice. © 2023 The Authors
PB  - American Academy of Allergy, Asthma and Immunology
C2  - 37301435
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 31
ER  -

TY  - JOUR
AU  - Yi, F.
AU  - Liu, H.
AU  - Wang, Y.
AU  - Wu, S.
AU  - Sun, C.
AU  - Feng, P.
AU  - Zhang, J.
TI  - Medical Named Entity Recognition Fusing Part-of-Speech and Stroke Features
PY  - 2023
T2  - Applied Sciences (Switzerland)
VL  - 13
IS  - 15
C7  - 8913
DO  - 10.3390/app13158913
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167918729&doi=10.3390%2fapp13158913&partnerID=40&md5=7658b2f852dad643c4ce0dea9dc2a4bf
AB  - It is highly significant from a research standpoint and a valuable practice to identify diseases, symptoms, drugs, examinations, and other medical entities in medical text data to support knowledge maps, question and answer systems, and other downstream tasks that can provide the public with knowledgeable answers. However, when contrasted with other languages like English, Chinese words lack a distinct dividing line, and medical entities have problems such as long length and multiple entity types nesting. Therefore, to address these issues, this study suggests a medical named entity recognition (NER) approach that combines part-of-speech and stroke features. First, the text is fed into the BERT pre-training model to get the semantic representation of the text, while the part-of-speech feature vector is obtained using the part-of-speech dictionary, and the stroke feature of the text is extracted through a convolution neural network (CNN). The word vector is then joined with the part-of-speech and stroke feature vectors, respectively, and input into the BiLSTM and CRF layer for training. Additionally, to balance the disparity in data volume across several types of entities, the class-weighted loss function is included in the loss function. According to the experimental findings, our model’s F1 score on the CCKS2019 dataset reaches 78.65%, and the recognition performance exceeds many existing algorithms. © 2023 by the authors.
PB  - Multidisciplinary Digital Publishing Institute (MDPI)
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Han, L.
AU  - Luther, S.L.
AU  - Finch, D.K.
AU  - Dobscha, S.K.
AU  - Skanderson, M.
AU  - Bathulapalli, H.
AU  - Fodeh, S.J.
AU  - Hahm, B.
AU  - Bouayad, L.
AU  - Lee, A.
AU  - Goulet, J.L.
AU  - Brandt, C.A.
AU  - Kerns, R.D.
TI  - Complementary and Integrative Health Approaches and Pain Care Quality in the Veterans Health Administration Primary Care Setting: A Quasi-Experimental Analysis
PY  - 2023
T2  - Journal of Integrative and Complementary Medicine
VL  - 29
IS  - 6-7
SP  - 420
EP  - 429
DO  - 10.1089/jicm.2022.0686
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162235514&doi=10.1089%2fjicm.2022.0686&partnerID=40&md5=d2c3ba8d39609f77549a7c81e8bd5c0c
AB  - Background: Complementary and integrative health (CIH) approaches have been recommended in national and international clinical guidelines for chronic pain management. We set out to determine whether exposure to CIH approaches is associated with pain care quality (PCQ) in the Veterans Health Administration (VHA) primary care setting. Methods: We followed a cohort of 62,721 Veterans with newly diagnosed musculoskeletal disorders between October 2016 and September 2017 over 1-year. PCQ scores were derived from primary care progress notes using natural language processing. CIH exposure was defined as documentation of acupuncture, chiropractic or massage therapies by providers. Propensity scores (PSs) were used to match one control for each Veteran with CIH exposure. Generalized estimating equations were used to examine associations between CIH exposure and PCQ scores, accounting for potential selection and confounding bias. Results: CIH was documented for 14,114 (22.5%) Veterans over 16,015 primary care clinic visits during the follow-up period. The CIH exposure group and the 1:1 PS-matched control group achieved superior balance on all measured baseline covariates, with standardized differences ranging from 0.000 to 0.045. CIH exposure was associated with an adjusted rate ratio (aRR) of 1.147 (95% confidence interval [CI]: 1.142, 1.151) on PCQ total score (mean: 8.36). Sensitivity analyses using an alternative PCQ scoring algorithm (aRR: 1.155; 95% CI: 1.150-1.160) and redefining CIH exposure by chiropractic alone (aRR: 1.118; 95% CI: 1.110-1.126) derived consistent results. Discussion: Our data suggest that incorporating CIH approaches may reflect higher overall quality of care for patients with musculoskeletal pain seen in primary care settings, supporting VHA initiatives and the Declaration of Astana to build comprehensive, sustainable primary care capacity for pain management. Future investigation is warranted to better understand whether and to what degree the observed association may reflect the therapeutic benefits patients actually received or other factors such as empowering provider-patient education and communication about these approaches. Copyright © 2023, Mary Ann Liebert, Inc., publishers 2023.
PB  - Mary Ann Liebert Inc.
C2  - 36971840
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Gangavarapu, A.
TI  - LLMs: A Promising New Tool for Improving Healthcare in Low-Resource Nations
PY  - 2023
T2  - 2023 IEEE Global Humanitarian Technology Conference, GHTC 2023
SP  - 252
EP  - 255
DO  - 10.1109/GHTC56179.2023.10354650
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182732486&doi=10.1109%2fGHTC56179.2023.10354650&partnerID=40&md5=662267d112f6978fea8f10ed99983bef
AB  - This paper explores the potential of large language models (LLMs) in addressing healthcare inequalities, particularly in underserved nations with provider shortages, limited resources, and funding constraints. The UN's Sustainable Development Goal 3 aims to achieve health and well-being for all, but disparities persist. Recent advancements in LLMs, exemplified by OpenAI's GPT-4, demonstrate their ability to surpass human performance on certain medical exams, offering an opportunity to augment the limited number of physicians and meet unmet healthcare needs affordably. By customizing LLMs for healthcare, various applications become possible, including automating patient screening, assisting with diagnosis and treatment, enabling virtual health assistants to track health indicators and educate communities, supporting frontline health workers in addressing basic healthcare needs, translating medical knowledge for accessibility, and aiding multilingual healthcare providers. However, challenges such as inadequate data and infrastructure, risks of bias and privacy breaches, cost and access barriers, and security threats must be addressed to ensure that the benefits of LLMs reach communities facing the greatest challenges and threats. This paper highlights the potential benefits, challenges, and the need for collaboration to harness the power of LLMs effectively in healthcare and contribute to achieving the UN's Sustainable Development Goal 3.  © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Tang, J.
AU  - Arvind, V.
AU  - White, C.A.
AU  - Dominy, C.
AU  - Cho, S.
AU  - Kim, J.S.
TI  - How are Patients Describing You Online? A Natural Language Processing Driven Sentiment Analysis of Online Reviews on CSRS Surgeons
PY  - 2023
T2  - Clinical Spine Surgery
VL  - 36
IS  - 2
SP  - E107
EP  - E113
DO  - 10.1097/BSD.0000000000001372
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149052880&doi=10.1097%2fBSD.0000000000001372&partnerID=40&md5=642eb8e0a6ae176a9c9ef415ae074fa4
AB  - Study Design: A quantitative analysis of written, online reviews of Cervical Spine Research Society (CSRS) surgeons. Objective: This study quantitatively analyzes the written reviews of members of the CSRS to report biases associated with demographic factors and frequently used words in reviews to help aid physician practices. Summary of Background Data: Physician review websites have influence on a patient's selection of a provider, but written reviews are subjective. Sentiment analysis of writing through artificial intelligence can quantify surgeon reviews to provide actionable feedback. Methods: Online written and star-rating reviews of CSRS surgeons were obtained from healthgrades.com. A sentiment analysis package was used to obtain compound scores of each physician's reviews. The relationship between demographic variables and average sentiment score of written reviews were evaluated through t-tests. Positive and negative word and bigram frequency analysis was performed to indicate trends in the reviews' language. Results: In all, 2239 CSRS surgeon's reviews were analyzed. Analysis showed a positive correlation between the sentiment scores and overall average star-rated reviews (r 2=0.60, P<0.01). There was no difference in review sentiment by provider sex. However, the age of surgeons showed a significant difference as those <55 had more positive reviews (mean=+0.50) than surgeons >=55 (mean=+0.37) (P<0.01). The most positive reviews focused both on pain and behavioral factors, whereas the most negative focused mainly on pain. Behavioral attributes increased the odds of receiving positive reviews while pain decreased them. Conclusion: The top-rated surgeons were described as considerate providers and effective at managing pain in their most frequently used words and bigrams. However, the worst-rated ones were mainly described as unable to relieve pain. Through quantitative analysis of physician reviews, pain is a clear factor contributing to both positive and negative reviews of surgeons, reinforcing the need for proper pain expectation management. Level of Evidence: Level 4 - retrospective case-control study.  Copyright © 2022 Wolters Kluwer Health, Inc. All rights reserved.
PB  - Lippincott Williams and Wilkins
C2  - 35945670
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Ng, Q.X.
AU  - Teo, Y.Q.J.
AU  - Kiew, C.Y.
AU  - Lim, B.P.-Y.
AU  - Lim, Y.L.
AU  - Liew, T.M.
TI  - Examining the Prevailing Negative Sentiments Surrounding Measles Vaccination: Unsupervised Deep Learning of Twitter Posts from 2017 to 2022
PY  - 2023
T2  - Cyberpsychology, Behavior, and Social Networking
VL  - 26
IS  - 8
SP  - 621
EP  - 630
DO  - 10.1089/cyber.2023.0025
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166395273&doi=10.1089%2fcyber.2023.0025&partnerID=40&md5=215115e1f77ca6d8b92874cd6f24e0b7
AB  - Despite the proven safety and clinical efficacy of the Measles vaccine, many countries are seeing new heights of vaccine hesitancy or refusal, and are experiencing a resurgence of measles infections as a consequence. With the use of novel machine learning tools, we investigated the prevailing negative sentiments related to Measles vaccination through an analysis of public Twitter posts over a 5-year period. We extracted original tweets using the search terms related to "measles"and "vaccine,"and posted in English from January 1, 2017, to December 15, 2022. Of these, 155,363 tweets were identified to be negative sentiment tweets from unique individuals, through the use of Bidirectional Encoder Representations from Transformers (BERT) Named Entity Recognition and SieBERT, a pretrained sentiment in English analysis model. This was followed by topic modeling and qualitative thematic analysis performed inductively by the study investigators. A total of 11 topics were generated after applying BERTopic. To facilitate a global discussion of results, the topics were grouped into four different themes through iterative thematic analysis. These include (a) the rejection of "anti-vaxxers"or antivaccine sentiments, (b) misbeliefs and misinformation regarding Measles vaccination, (c) negative transference due to COVID-19 related policies, and (d) public reactions to contemporary Measles outbreaks. Theme 1 highlights that the current public discourse may further alienate those who are vaccine hesitant because of the disparaging language often used, while Themes 2 and 3 highlight the typology of misperceptions and misinformation underlying the negative sentiments related to Measles vaccination and the psychological tendency of disconfirmation bias. Nonetheless, the analysis was based solely on Twitter and only tweets in English were included; hence, the findings may not necessarily generalize to non-Western communities. It is important to further understand the thinking and feeling of those who are vaccine hesitant to address the issues at hand.  © Mary Ann Liebert, Inc.
PB  - Mary Ann Liebert Inc.
C2  - 37358808
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Dobscha, S.K.
AU  - Luther, S.L.
AU  - Kerns, R.D.
AU  - Finch, D.K.
AU  - Goulet, J.L.
AU  - Brandt, C.A.
AU  - Skanderson, M.
AU  - Bathulapalli, H.
AU  - Fodeh, S.J.
AU  - Hahm, B.
AU  - Bouayad, L.
AU  - Lee, A.
AU  - Han, L.
TI  - Mental Health Diagnoses are Not Associated With Indicators of Lower Quality Pain Care in Electronic Health Records of a National Sample of Veterans Treated in Veterans Health Administration Primary Care Settings
PY  - 2023
T2  - Journal of Pain
VL  - 24
IS  - 2
SP  - 273
EP  - 281
DO  - 10.1016/j.jpain.2022.08.009
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147049122&doi=10.1016%2fj.jpain.2022.08.009&partnerID=40&md5=9a7f7bc7e1625e908181b68fb1f65cc6
AB  - Prior research has demonstrated disparities in general medical care for patients with mental health conditions, but little is known about disparities in pain care. The objective of this retrospective cohort study was to determine whether mental health conditions are associated with indicators of pain care quality (PCQ) as documented by primary care clinicians in the Veterans Health Administration (VHA). We used natural language processing to analyze electronic health record data from a national sample of Veterans with moderate to severe musculoskeletal pain during primary care visits in the Fiscal Year 2017. Twelve PCQ indicators were annotated from clinician progress notes as present or absent; PCQ score was defined as the sum of these indicators. Generalized estimating equation Poisson models examined associations among mental health diagnosis categories and PCQ scores. The overall mean PCQ score across 135,408 person-visits was 8.4 (SD = 2.3). In the final adjusted model, post-traumatic stress disorder was associated with higher PCQ scores (RR = 1.006, 95%CI 1.002–1.010, P = .007). Depression, alcohol use disorder, other substance use disorder, schizophrenia, and bipolar disorder diagnoses were not associated with PCQ scores. Overall, results suggest that in this patient population, presence of a mental health condition is not associated with lower quality pain care. Perspective: This study used a natural language processing approach to analyze medical records to determine whether mental health conditions are associated with indicators of pain care quality as documented by primary care clinicians. Findings suggest that presence of a diagnosed mental health condition is not associated with lower quality pain care. © 2022
PB  - Elsevier B.V.
C2  - 36167230
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Hoch, C.C.
AU  - Wollenberg, B.
AU  - Lüers, J.-C.
AU  - Knoedler, S.
AU  - Knoedler, L.
AU  - Frank, K.
AU  - Cotofana, S.
AU  - Alfertshofer, M.
TI  - ChatGPT’s quiz skills in different otolaryngology subspecialties: an analysis of 2576 single-choice and multiple-choice board certification preparation questions
PY  - 2023
T2  - European Archives of Oto-Rhino-Laryngology
VL  - 280
IS  - 9
SP  - 4271
EP  - 4278
DO  - 10.1007/s00405-023-08051-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161271395&doi=10.1007%2fs00405-023-08051-4&partnerID=40&md5=84f48bcc2d007f65f0180a79ffd796ba
AB  - Purpose: With the increasing adoption of artificial intelligence (AI) in various domains, including healthcare, there is growing acceptance and interest in consulting AI models to provide medical information and advice. This study aimed to evaluate the accuracy of ChatGPT’s responses to practice quiz questions designed for otolaryngology board certification and decipher potential performance disparities across different otolaryngology subspecialties. Methods: A dataset covering 15 otolaryngology subspecialties was collected from an online learning platform funded by the German Society of Oto-Rhino-Laryngology, Head and Neck Surgery, designed for board certification examination preparation. These questions were entered into ChatGPT, with its responses being analyzed for accuracy and variance in performance. Results: The dataset included 2576 questions (479 multiple-choice and 2097 single-choice), of which 57% (n = 1475) were answered correctly by ChatGPT. An in-depth analysis of question style revealed that single-choice questions were associated with a significantly higher rate (p < 0.001) of correct responses (n = 1313; 63%) compared to multiple-choice questions (n = 162; 34%). Stratified by question categories, ChatGPT yielded the highest rate of correct responses (n = 151; 72%) in the field of allergology, whereas 7 out of 10 questions (n = 65; 71%) on legal otolaryngology aspects were answered incorrectly. Conclusion: The study reveals ChatGPT’s potential as a supplementary tool for otolaryngology board certification preparation. However, its propensity for errors in certain otolaryngology areas calls for further refinement. Future research should address these limitations to improve ChatGPT’s educational use. An approach, with expert collaboration, is recommended for the reliable and accurate integration of such AI models. © 2023, The Author(s).
PB  - Springer Science and Business Media Deutschland GmbH
C2  - 37285018
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 82
ER  -

TY  - JOUR
AU  - Mahtani, A.U.
AU  - Reinstein, I.
AU  - Marin, M.
AU  - Burk-Rafel, J.
TI  - A New Tool for Holistic Residency Application Review: Using Natural Language Processing of Applicant Experiences to Predict Interview Invitation
PY  - 2023
T2  - Academic Medicine
VL  - 98
IS  - 9
SP  - 1018
EP  - 1021
DO  - 10.1097/ACM.0000000000005210
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171185236&doi=10.1097%2fACM.0000000000005210&partnerID=40&md5=421eed33e8f55d3c7498badb58c9f239
AB  - Problem Reviewing residency application narrative components is time intensive and has contributed to nearly half of applications not receiving holistic review. The authors developed a natural language processing (NLP)-based tool to automate review of applicants' narrative experience entries and predict interview invitation. Approach Experience entries (n = 188,500) were extracted from 6,403 residency applications across 3 application cycles (2017-2019) at 1 internal medicine program, combined at the applicant level, and paired with the interview invitation decision (n = 1,224 invitations). NLP identified important words (or word pairs) with term frequency-inverse document frequency, which were used to predict interview invitation using logistic regression with L1 regularization. Terms remaining in the model were analyzed thematically. Logistic regression models were also built using structured application data and a combination of NLP and structured data. Model performance was evaluated on never-before-seen data using area under the receiver operating characteristic and precision-recall curves (AUROC, AUPRC). Outcomes The NLP model had an AUROC of 0.80 (vs chance decision of 0.50) and AUPRC of 0.49 (vs chance decision of 0.19), showing moderate predictive strength. Phrases indicating active leadership, research, or work in social justice and health disparities were associated with interview invitation. The model's detection of these key selection factors demonstrated face validity. Adding structured data to the model significantly improved prediction (AUROC 0.92, AUPRC 0.73), as expected given reliance on such metrics for interview invitation. Next Steps This model represents a first step in using NLP-based artificial intelligence tools to promote holistic residency application review. The authors are assessing the practical utility of using this model to identify applicants screened out using traditional metrics. Generalizability must be determined through model retraining and evaluation at other programs. Work is ongoing to thwart model "gaming," improve prediction, and remove unwanted biases introduced during model training. © 2023 Lippincott Williams and Wilkins. All rights reserved.
PB  - Wolters Kluwer Health
C2  - 36940395
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Minor, K.S.
AU  - Lundin, N.B.
AU  - Myers, E.J.
AU  - Fernández-Villardón, A.
AU  - Lysaker, P.H.
TI  - Automated measures of speech content and speech organization in schizophrenia: Test-retest reliability and generalizability across demographic variables
PY  - 2023
T2  - Psychiatry Research
VL  - 320
C7  - 115048
DO  - 10.1016/j.psychres.2023.115048
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146362347&doi=10.1016%2fj.psychres.2023.115048&partnerID=40&md5=1d566a250e8a181b72bbe8cca78a4f3f
AB  - Technological advances in artificial intelligence and natural language processing have increased efficiency of assessing speech content and speech organization in schizophrenia. Despite these developments, there has been little focus on the psychometrics of these approaches. Using two common assessments, the current study addressed this gap by: 1) measuring test-retest reliability; and 2) assessing whether speech content and/or speech organization generalize across demographics. To test these aims, we examined psychometric properties of the Linguistic Inquiry Word Count (LIWC), a speech content measure, and the Coh-Metrix, a speech organization measure. Across baseline to six month (n = 101) and baseline to one year (n = 47) narrative speech samples, we generally observed fair reliability for speech content measures and fair to good reliability for speech organization measures. Regarding demographics, multiple speech indices varied by race, income, and education. The lack of excellent reliability scores for speech indices holds important implications for examining speech variables in clinical trials and highlights the dynamic nature of speech. This work illustrates the importance of designing speech content and speech organization measures with external validity across demographic factors. Future studies examining speech in schizophrenia should account for potential biases against demographic groups introduced by linguistic analysis tools. © 2023 Elsevier B.V.
PB  - Elsevier Ireland Ltd
C2  - 36645988
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Weaver, A.
AU  - Zhang, A.
AU  - Xiang, X.
AU  - Felsman, P.
AU  - Fischer, D.J.
AU  - Himle, J.A.
TI  - Entertain Me Well: An Entertaining, Tailorable, Online Platform Delivering CBT for Depression
PY  - 2023
T2  - Cognitive and Behavioral Practice
VL  - 30
IS  - 1
SP  - 96
EP  - 115
DO  - 10.1016/j.cbpra.2021.09.003
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119600631&doi=10.1016%2fj.cbpra.2021.09.003&partnerID=40&md5=018f80d6a9f5209754af53666f393746
AB  - Depression prevalence is high, impacting approximately 20% of Americans during their lifetime, and on the rise due to stress and loss associated with the COVID-19 pandemic. Despite the high prevalence of depression, unacceptable treatment access disparities persist. When depression goes untreated, it leads to substantial negative impacts in multiple life domains. Cognitive behavioral therapy (CBT), the gold-standard psychosocial treatment for depression, remains largely unavailable to individuals living with depression, particularly individuals who are members of underrepresented groups in our society. Digital mental health interventions (DMHI) have led to important advances in extending the reach of CBT for depression; however, they are underutilized and treatment engagement remains low. We sought to address some of the current gaps in DMHI by developing an online platform for delivering CBT for depression that is entertaining, simple and straightforward, and tailorable. First, this article introduces our online platform, Entertain Me Well (EMW) and its key innovations, including the use of an engaging, character-driven storyline presented as “episodes” within each session, as well as customizable content that allows for tailoring of text, images, and examples to create content most relevant to the target client population, context, or setting. Next, we describe two EMW depression treatment programs that have been tailored: one for delivery in the rural church setting, called Raising Our Spirits Together, and one tailored for delivery in dialysis centers, called Doing Better on Dialysis. Finally, we discuss future directions for the EMW platform, including the ability to create programs for other common mental health and health conditions, the development of additional character-driven storylines with greater treatment personalization, translation of content in multiple languages, and the use of additional technological innovation, such as artificial intelligence like natural language processing, to enhance platform interactivity. © 2023
PB  - Elsevier Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - CONF
AU  - Harden, G.
AU  - Noe, K.
AU  - Ross, S.
TI  - AMCIS: Using Artificial Intelligence to Identify and Analyze ESG Investments
PY  - 2023
T2  - 29th Annual Americas Conference on Information Systems, AMCIS 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192888297&partnerID=40&md5=735556d84bf3c4f306f13b424a381205
AB  - Predictions about various impacts of artificial intelligence (AI) in ways both positive and negative in diverse industries such as accounting, education, health care, marketing, and even academic research are at a fever pitch. Recent advancements in artificial generative intelligence do lend concern to the potential outcomes of its use with scenarios revealing malicious responses or biased results. What then could cause the topic of AI to be even more controversial? Using that technology to help identify and analyze the risks and opportunities of environmental, social, and governance (ESG) investments. ESG investing has been around for decades, growing in popularity as interest in climate impacts, corporate social responsibility, and ethical behavior of corporations all take turns in the spotlight. There is no question that accounting for ESG has grown in popularity, with 90 percent of S&P 500 companies now publishing ESG reports for stakeholders to review (Perez, 2022). But as with any initiative, ESG also has its share of criticisms, such as being tied to “woke” political ideologies prioritizing liberal agendas rather than a sound return on investment or being difficult to accurately measure with a focus on non-financial factors like staff diversity or the climate impact that are difficult to accurately measure. This research focuses on the latter issue with current decision support systems struggling to accurately measure such benchmarks. AI algorithms can quickly and accurately analyze large datasets with numerous variables related to ESG issues. But if the algorithm or data have any inherent bias, intended or not, consequences to the organization’s reputation can be detrimental. The very trait ESG intends to increase is organizational goodwill, and if the AI used to measure it creates negative results, the harm to the firm can be even worse. Following Dorflietner et al. (2015) who used ASSET4, KLD and Bloomberg as 3 ESG reporting sources, we seek to compare the ESG ratings of chosen companies reported by three different AI systems (Microsoft’s Bing, OpenAI’s ChatGPT and Google’s Bard) to evaluate the abstract concept of ESG accounting and reporting that is based on AI. The AI results will be reported in reference to similarities and differences among the sources. © 2023 29th Annual Americas Conference on Information Systems, AMCIS 2023. All rights reserved.
PB  - Association for Information Systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Angel, M.
AU  - Patel, A.
AU  - Alachkar, A.
AU  - Baldi, P.
TI  - Clinical Knowledge and Reasoning Abilities of Large Language Models in Pharmacy: A Comparative Study on the NAPLEX Exam
PY  - 2023
T2  - Proceedings - 2023 10th International Conference on Social Networks Analysis, Management and Security, SNAMS 2023
DO  - 10.1109/SNAMS60348.2023.10375395
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183464155&doi=10.1109%2fSNAMS60348.2023.10375395&partnerID=40&md5=6ae88dd7f790bc1ce018e7b957f39469
AB  - This study aims to evaluate the capabilities and limitations of three large language models (LLMs)-GPT-3, GPT-4, and Bard, in the field of pharmacy by assessing their reasoning abilities on a sample of the North American Pharmacist Licensure Examination (NAPLEX). Additionally, we explore the potential impacts of LLMs on pharmacy education and practice. To evaluate the LLMs, we utilized the sample of the NAPLEX exam comprising 137 multiple-choice questions. These questions were presented to GPT-3, GPT-4, and Bard through their respective user interfaces, and the answers generated by the LLMs were subsequently compared with the answer key. The results reveal a notable disparity in the performance of the LLMs. GPT-4 emerged as the top performer, accurately answering 78.8% of the questions. This marked a substantial 11% and 27.7% improvement over Bard and GPT-3, respectively. However, when considering questions that required multiple selections, the performance of each LLM decreased significantly. GPT-4, GPT-3, and Bard could only correctly respond to 53.6%, 13.9%, and 21.4% of such questions, respectively. Among the three LLMs evaluated, GPT-4 was the only model capable of passing the NAPLEX exam. Nevertheless, given the continuous evolution of LLMs, it is reasonable to anticipate that future models will effortlessly excel in this context. This highlights the significant potential of LLMs to influence the field of pharmacy. Hence, we must evaluate both the positive and negative implications associated with the integration of LLMs in pharmacy education and practice.  © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Matulis, J.
AU  - McCoy, R.
TI  - Relief in Sight? Chatbots, In-baskets, and the Overwhelmed Primary Care Clinician
PY  - 2023
T2  - Journal of General Internal Medicine
VL  - 38
IS  - 12
SP  - 2808
EP  - 2815
DO  - 10.1007/s11606-023-08271-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163659754&doi=10.1007%2fs11606-023-08271-8&partnerID=40&md5=446c992048766a3197cd81dee5ae7580
AB  - The recent emergence of publically facing artificial intelligence (AI) chatbots has generated vigorous discussion in the lay public around the possibilities, liabilities, and uncertainties of the integration of such technology into everyday life. As primary care clinicians continue to struggle against ever-increasing loads of asynchronous, electronic work, the potential for AI to improve the quality and efficiency of this work looms large. In this essay, we discuss the basic premise of open-access AI chatbots such as CHATGPT, review prior applications of AI in healthcare, and preview some possible AI chatbot–assisted in-basket assistance including scenarios of communicating test results with patients, providing patient education, and clinical decision support in history taking, review of prior diagnostic test characteristics, and common management scenarios. We discuss important concerns related to the future adoption of this technology including the transparency of the training data used in developing these models, the level of oversight and trustworthiness of the information generated, and possible impacts on equity, bias, and patient privacy. A stepwise and balanced approach to simultaneously understand the capabilities and address the concerns associated with these tools will be needed before these tools can improve patient care. © 2023, The Author(s), under exclusive licence to Society of General Internal Medicine.
PB  - Springer
C2  - 37369892
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 9
ER  -

TY  - JOUR
AU  - Anderson, S.S.
TI  - “Places to stand”: Multiple metaphors for framing ChatGPT's corpus
PY  - 2023
T2  - Computers and Composition
VL  - 68
C7  - 102778
DO  - 10.1016/j.compcom.2023.102778
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159870253&doi=10.1016%2fj.compcom.2023.102778&partnerID=40&md5=fa8d5ac42c81bce1da2c2a2de6e2b472
AB  - As a prerequisite for the use of ChatGPT in writing classes, instructors should scaffold students’ (critical) digital literacy of the technology. Part of such scaffolding should include the exploration of relevant frameworks for conceptualizing ChatGPT, including the use of multiple metaphors, like tool and collaborator. By analyzing recent scholarly and news discourse regarding ChatGPT, prompts and outputs from ChatGPT, and the author's own writing process, the essay illustrates the limitations of the tool and collaborator metaphors, while emphasizing the value of multiple metaphors. In particular, the tool metaphor fails to account for ChatGPT's human components – namely its repurposing of thousands of authors’ writing and ideas, from which it draws with no transparency on sources. While the collaborator metaphor appears to address the need to cite ideas that are not one's own, ChatGPT fails to provide the accountability of a human author, even as it includes biased output derived from its training corpus, and while again failing to identify original sources. Medical and surgical metaphors highlight the ways that ChatGPT acts upon both the enormous corpus, or body of human writing, on which it was trained and our social body in our academic communities and beyond. © 2023 Elsevier Inc.
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 17
ER  -

TY  - CONF
AU  - Liu, J.
AU  - Zhou, P.
AU  - Hua, Y.
AU  - Chong, D.
AU  - Tian, Z.
AU  - Liu, A.
AU  - Wang, H.
AU  - You, C.
AU  - Guo, Z.
AU  - Zhu, L.
AU  - Li, M.L.
TI  - Benchmarking Large Language Models on CMExam - A Comprehensive Chinese Medical Exam Dataset
PY  - 2023
T2  - Advances in Neural Information Processing Systems
VL  - 36
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205325083&partnerID=40&md5=55e93ae7da8b45371d12766035c73246
AB  - Recent advancements in large language models (LLMs) have transformed the field of question answering (QA). However, evaluating LLMs in the medical field is challenging due to the lack of standardized and comprehensive datasets. To address this gap, we introduce CMExam, sourced from the Chinese National Medical Licensing Examination. CMExam consists of 60K+ multiple-choice questions for standardized and objective evaluations, as well as solution explanations for model reasoning evaluation in an open-ended manner. For in-depth analyses of LLMs, we invited medical professionals to label five additional question-wise annotations, including disease groups, clinical departments, medical disciplines, areas of competency, and question difficulty levels. Alongside the dataset, we further conducted thorough experiments with representative LLMs and QA algorithms on CMExam. The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great disparity when compared to human accuracy, which stood at 71.6%. For explanation tasks, while LLMs could generate relevant reasoning and demonstrate improved performance after finetuning, they fall short of a desired standard, indicating ample room for improvement. To the best of our knowledge, CMExam is the first Chinese medical exam dataset to provide comprehensive medical annotations. The experiments and findings of LLM evaluation also provide valuable insights into the challenges and potential solutions in developing Chinese medical QA systems and LLM evaluation pipelines. © 2023 Neural information processing systems foundation. All rights reserved.
PB  - Neural information processing systems foundation
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 16
ER  -

TY  - CONF
AU  - Thakur, S.N.
AU  - Sinha, A.
AU  - Singh, M.K.
AU  - Bagaria, M.K.
AU  - Grover, R.
AU  - Shrivastava, K.
TI  - Optimizing Wellness: A Comprehensive Examination of a Conversational AI-Driven Healthcare BOT for Personalized Fitness Guidance
PY  - 2023
T2  - International Conference on Artificial Intelligence for Innovations in Healthcare Industries, ICAIIHI 2023
DO  - 10.1109/ICAIIHI57871.2023.10489319
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191448568&doi=10.1109%2fICAIIHI57871.2023.10489319&partnerID=40&md5=0de08dc6c9e053fb6c003a5cce4a425c
AB  - A specialized fitness advice bot is the subject of this investigation, which looks at the application of conversational artificial intelligence (AI) in healthcare. By utilizing machine learning and natural language processing, the bot offers customized exercise recommendations. Studying the system's capacity to adjust to different users' health profiles and preferences, it looks at its technological architecture, user experience, and personalization features. Robust user data protection is ensured by taking into account privacy and security concerns, including compliance with laws such as HIPAA. A thorough overview of implementing a healthcare bot for fitness advice is provided by the report, which also shows significant obstacles including scalability and algorithmic biases. This study provides insightful information to the rapidly changing field of artificial intelligence in healthcare, assisting educates policymakers and directing developments in this exciting area where technology and wellbeing meet.  © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Samo, G.
AU  - Bonan, C.
TI  - Health-Related Content in Transformer-Based Language Models: Exploring Bias in Domain General vs. Domain Specific Training Sets
PY  - 2023
T2  - Studies in Health Technology and Informatics
VL  - 302
SP  - 743
EP  - 744
DO  - 10.3233/SHTI230252
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159756285&doi=10.3233%2fSHTI230252&partnerID=40&md5=907c80d46682c3c298b2a7334d3a8683
AB  - In this communication, we demonstrate that the bias observed in domain general training sets with health-related content is not improved in domain specific health-communication corpora, contra. © 2023 European Federation for Medical Informatics (EFMI) and IOS Press.
PB  - IOS Press BV
C2  - 37203482
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Elhag, W.
AU  - Bouteldja, D.
AU  - Bouallegue, S.
TI  - A Survey on the Applications, Limitations, and Ethical Considerations of ChatGPT in Various Industries
PY  - 2023
T2  - 2023 IEEE International Conference on Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering, MetroXRAINE 2023 - Proceedings
SP  - 565
EP  - 569
DO  - 10.1109/MetroXRAINE58569.2023.10405608
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185785714&doi=10.1109%2fMetroXRAINE58569.2023.10405608&partnerID=40&md5=22cb9af10273f1d711085ff9b8ca00d3
AB  - This research paper explores the various applications of ChatGPT in various industries, including healthcare, writing, design, content creation, SEO, and sales pages. It also discusses future developments of ChatGPT, including its use in conversational AI, natural language processing tasks, personalization, and the healthcare industry. The article highlights the differences between ChatGPT and GPT-3 and the ethical considerations for using ChatGPT, such as the potential for perpetuating biases and misinformation and the risk of misuse. The limitations of ChatGPT, including its lack of contextual awareness, reinforcement of harmful social norms, and potential for incorrect answers, are also discussed. To ensure the responsible and ethical deployment of ChatGPT, the article highlights the importance of considering and addressing its limitations. © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Cascalheira, C.J.
AU  - Chapagain, S.
AU  - Flinn, R.E.
AU  - Zhao, Y.
AU  - Boubrahimi, S.F.
AU  - Klooster, D.
AU  - Gonzalez, A.
AU  - Lund, E.M.
AU  - Laprade, D.
AU  - Scheer, J.R.
AU  - Hamdi, S.M.
TI  - Predicting Linguistically Sophisticated Social Determinants of Health Disparities with Neural Networks: The Case of LGBTQ+ Minority Stress
PY  - 2023
T2  - Proceedings - 2023 IEEE International Conference on Big Data, BigData 2023
SP  - 1314
EP  - 1321
DO  - 10.1109/BigData59044.2023.10386882
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184981556&doi=10.1109%2fBigData59044.2023.10386882&partnerID=40&md5=0ac27870da20f4ab3c6ee76aa0155d68
AB  - LGBTQ+ minority stress is a pervasive form of anti-LGBTQ+ adverse events and psychological strain that drives health inequities among LGBTQ+ people. Minority stress is also linguistically sophisticated (e.g., composed of cultural idioms, psycholinguistic permutations, and lexical density). Because minority stress is a linguistically sophisticated social determinant of health disparities, it is challenging to detect using natural language processing (NLP). Using 5,789 human-annotated Reddit posts from the LGBTQ+ Minority Stress on Social Media (MiSSoM+) Dataset, we investigated and compared the performance of four neural networks and two traditional machine learning architectures in modeling minority stress at both the factor (i.e., separate components of minority stress) and composite level. A novel hybrid model combining Bidirectional Encoder Representations from Transformers and convolutional neural network (BERT-CNN) improved the prediction of composite minority stress (F1 = 0.84). Our experiments on separate factors of minority stress are the first to demonstrate that hybrid neural network models can detect semantically complex expressions of prejudiced events (F1 = 0.87), expected rejection (F1 = 0.92), internalized stigma (F1 = 0.91), identity concealment (F1 = 0.92), and minority coping (F1 = 0.84). We also substantially improved the prediction of gender dysphoria (F1 = 0.94) - a conceptually new candidate component of minority stress. Big data analytics may not be a panacea for the problem of minority stress, but our work joins a growing literature base to show that deep learning models are remarkable in detecting linguistically sophisticated social determinants of health disparities in big data, thus providing evidence in support of the potential benefit from the innovative use of such technology in eliminating group-specific health inequities.  © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Bisgin, N.
AU  - Bisgin, H.
AU  - Hummel, D.
AU  - Zelner, J.
AU  - Needham, B.L.
TI  - Did the public attribute the Flint Water Crisis to racism as it was happening? Text analysis of Twitter data to examine causal attributions to racism during a public health crisis
PY  - 2023
T2  - Journal of Computational Social Science
VL  - 6
IS  - 1
SP  - 165
EP  - 190
DO  - 10.1007/s42001-022-00192-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143242582&doi=10.1007%2fs42001-022-00192-6&partnerID=40&md5=c14c8854d5b66815df417986f51308f2
AB  - The Flint Water Crisis (FWC) was an avoidable public health disaster that profoundly affected the city’s residents, a majority of whom are Black. Although many scholars and journalists have called attention to the role of racism in the water crisis, little is known about the extent to which the public attributed the FWC to racism as it was unfolding. In this study, we used natural language processing to analyze nearly six million Flint-related tweets posted between April 1, 2014, and June 1, 2016. We found that key developments in the FWC corresponded to increases in the number and percentage of tweets that mentioned terms related to race and racism. Similar patterns were found for other topics hypothesized to be related to the water crisis, including water and politics. Using sentiment analysis, we found that tweets with a negative polarity score were more common in the subset of tweets that mentioned terms related to race and racism when compared to the full set of tweets. Next, we found that word pairs that included terms related to race and racism first appeared after the January 2016 state and federal emergency declarations and a corresponding increase in media coverage of the FWC. We conclude that many Twitter users connected the events of the water crisis to race and racism in real-time. Given growing evidence of negative health effects of second-hand exposure to racism, this may have implications for understanding minority health and health disparities in the US. © 2022, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd.
PB  - Springer
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Koopman, B.
AU  - Zuccon, G.
TI  - Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness
PY  - 2023
T2  - EMNLP 2023 - 2023 Conference on Empirical Methods in Natural Language Processing, Proceedings
SP  - 15012
EP  - 15022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184814606&partnerID=40&md5=447c07792abc057492ba66dc2dbee9b5
AB  - This paper investigates the significant impact different prompts have on the behaviour of ChatGPT when used for health information seeking. As people more and more depend on generative large language models (LLMs) like ChatGPT, it is critical to understand model behaviour under different conditions, especially for domains where incorrect answers can have serious consequences such as health. Using the TREC Misinformation dataset, we empirically evaluate ChatGPT to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness. We show this occurs both for retrieve-then-generate pipelines and based on how a user phrases their question as well as the question type. This work has important implications for the development of more robust and transparent question-answering systems based on generative large language models. Prompts, raw result files and manual analysis are made publicly available at https://github.com/ielab/drchatgpt-health_prompting. ©2023 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - CONF
AU  - Poon, H.
AU  - Naumann, T.
AU  - Zhang, S.
AU  - González Hernández, J.
TI  - Precision Health in the Age of Large Language Models
PY  - 2023
T2  - Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
SP  - 5825
EP  - 5826
DO  - 10.1145/3580305.3599568
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171359706&doi=10.1145%2f3580305.3599568&partnerID=40&md5=fd86ece2d273386a198c3e8793eff6c8
AB  - Medicine today is imprecise. Among the top 20 drugs in the U.S., up to 80% of patients are non-responders. The goal of precision health is to provide the right intervention for the right people at the right time. The key to realize this dream is to develop a data-driven, learning system that can instantly incorporate new health information to optimize care delivery and accelerate biomedical discovery. In reality, however, the health ecosystem is mired in overwhelming unstructured data and excruciating manual processing. For example, in cancer, standard of care often fails, and clinical trials are the last hope. Yet less than 3% of patients could find a matching trial, whereas 40% of trial failures simply stem from insufficient recruitment. Discovery is painfully slow as a new drug may take billions of dollars and over a decade to develop. In this tutorial, we will explore how large language models (LLMs) can serve as a universal structuring tool to democratize biomedical knowledge work and usher in an intelligence revolution in precision health. We first review background for precision health and give a broad overview of the AI revolution that culminated in the development of large language models, highlighting key technical innovations and prominent trends such as consolidation of AI methods across modalities. We then give an in-depth review of biomedical LLMs and precision health applications, with a particular focus on scaling real-world evidence generation and drug discovery. To conclude, we discuss key technical challenges (e.g., bias, hallucination, cost), societal ramifications (e.g., privacy, regulation), as well as exciting research frontiers such as prompt programming, knowledge distillation, multi-modal learning, causal discovery.  © 2023 Owner/Author.
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Currie, G.
AU  - Singh, C.
AU  - Nelson, T.
AU  - Nabasenja, C.
AU  - Al-Hayek, Y.
AU  - Spuur, K.
TI  - ChatGPT in medical imaging higher education
PY  - 2023
T2  - Radiography
VL  - 29
IS  - 4
SP  - 792
EP  - 799
DO  - 10.1016/j.radi.2023.05.011
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160061053&doi=10.1016%2fj.radi.2023.05.011&partnerID=40&md5=c97e782c8cd5a6efd0bbe7752ca9bb0a
AB  - Introduction: Academic integrity among radiographers and nuclear medicine technologists/scientists in both higher education and scientific writing has been challenged by advances in artificial intelligence (AI). The recent release of ChatGPT, a chatbot powered by GPT-3.5 capable of producing accurate and human-like responses to questions in real-time, has redefined the boundaries of academic and scientific writing. These boundaries require objective evaluation. Method: ChatGPT was tested against six subjects across the first three years of the medical radiation science undergraduate course for both exams (n = 6) and written assignment tasks (n = 3). ChatGPT submissions were marked against standardised rubrics and results compared to student cohorts. Submissions were also evaluated by Turnitin for similarity and AI scores. Results: ChatGPT powered by GPT-3.5 performed below the average student performance in all written tasks with an increasing disparity as subjects advanced. ChatGPT performed better than the average student in foundation or general subject examinations where shallow responses meet learning outcomes. For discipline specific subjects, ChatGPT lacked the depth, breadth, and currency of insight to provide pass level answers. Conclusion: ChatGPT simultaneously poses a risk to academic integrity in writing and assessment while affording a tool for enhanced learning environments. These risks and benefits are likely to be restricted to learning outcomes of lower taxonomies. Both risks and benefits are likely to be constrained by higher order taxonomies. Implications for practice: ChatGPT powered by GPT3.5 has limited capacity to support student cheating, introduces errors and fabricated information, and is readily identified by software as AI generated. Lack of depth of insight and appropriateness for professional communication also limits capacity as a learning enhancement tool. © 2023 The College of Radiographers
PB  - W.B. Saunders Ltd
C2  - 37271011
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 71
ER  -

TY  - CONF
AU  - Chen, R.
AU  - Yang, J.
AU  - Xiong, H.
AU  - Bai, J.
AU  - Hu, T.
AU  - Hao, J.
AU  - Feng, Y.
AU  - Zhou, J.T.
AU  - Wu, J.
AU  - Liu, Z.
TI  - Fast Model Debias with Machine Unlearning
PY  - 2023
T2  - Advances in Neural Information Processing Systems
VL  - 36
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188509855&partnerID=40&md5=5810ce4257b7ce2e70f30372a77f49ff
AB  - Recent discoveries have revealed that deep neural networks might behave in a biased manner in many real-world scenarios. For instance, deep networks trained on a large-scale face recognition dataset CelebA tend to predict blonde hair for females and black hair for males. Such biases not only jeopardize the robustness of models but also perpetuate and amplify social biases, which is especially concerning for automated decision-making processes in healthcare, recruitment, etc., as they could exacerbate unfair economic and social inequalities among different groups. Existing debiasing methods suffer from high costs in bias labeling or model re-training, while also exhibiting a deficiency in terms of elucidating the origins of biases within the model. To this respect, we propose a fast model debiasing framework (FMD) which offers an efficient approach to identify, evaluate and remove biases inherent in trained models. The FMD identifies biased attributes through an explicit counterfactual concept and quantifies the influence of data samples with influence functions. Moreover, we design a machine unlearning-based strategy to efficiently and effectively remove the bias in a trained model with a small counterfactual dataset. Experiments on the Colored MNIST, CelebA, and Adult Income datasets along with experiments with large language models demonstrate that our method achieves superior or competing accuracies compared with state-of-the-art methods while attaining significantly fewer biases and requiring much less debiasing cost. Notably, our method requires only a small external dataset and updating a minimal amount of model parameters, without the requirement of access to training data that may be too large or unavailable in practice. © 2023 Neural information processing systems foundation. All rights reserved.
PB  - Neural information processing systems foundation
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 12
ER  -

TY  - JOUR
AU  - Zhao, K.
AU  - Farrell, K.
AU  - Mashiku, M.
AU  - Abay, D.
AU  - Tang, K.
AU  - Oberste, M.S.
AU  - Burns, C.C.
TI  - A search-based geographic metadata curation pipeline to refine sequencing institution information and support public health
PY  - 2023
T2  - Frontiers in Public Health
VL  - 11
C7  - 1254976
DO  - 10.3389/fpubh.2023.1254976
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178031636&doi=10.3389%2ffpubh.2023.1254976&partnerID=40&md5=5e94e15f9f6285942beae317a122f81a
AB  - Background: The National Center for Biotechnology Information (NCBI) Sequence Read Archive (SRA) has amassed a vast reservoir of genetic data since its inception in 2007. These public data hold immense potential for supporting pathogen surveillance and control. However, the lack of standardized metadata and inconsistent submission practices in SRA may impede the data’s utility in public health. Methods: To address this issue, we introduce the Search-based Geographic Metadata Curation (SGMC) pipeline. SGMC utilized Python and web scraping to extract geographic data of sequencing institutions from NCBI SRA in the Cloud and its website. It then harnessed ChatGPT to refine the sequencing institution and location assignments. To illustrate the pipeline’s utility, we examined the geographic distribution of the sequencing institutions and their countries relevant to polio eradication and categorized them. Results: SGMC successfully identified 7,649 sequencing institutions and their global locations from a random selection of 2,321,044 SRA accessions. These institutions were distributed across 97 countries, with strong representation in the United States, the United Kingdom and China. However, there was a lack of data from African, Central Asian, and Central American countries, indicating potential disparities in sequencing capabilities. Comparison with manually curated data for U.S. institutions reveals SGMC’s accuracy rates of 94.8% for institutions, 93.1% for countries, and 74.5% for geographic coordinates. Conclusion: SGMC may represent a novel approach using a generative AI model to enhance geographic data (country and institution assignments) for large numbers of samples within SRA datasets. This information can be utilized to bolster public health endeavors. Copyright © 2023 Zhao, Farrell, Mashiku, Abay, Tang, Oberste and Burns.
PB  - Frontiers Media SA
C2  - 38035280
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Khanom, A.
AU  - Kiesow, D.
AU  - Zdun, M.
AU  - Shyu, C.-R.
TI  - The News Crawler: A Big Data Approach to Local Information Ecosystems
PY  - 2023
T2  - Media and Communication
VL  - 11
IS  - 3
SP  - 318
EP  - 329
DO  - 10.17645/mac.v11i3.6789
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173233497&doi=10.17645%2fmac.v11i3.6789&partnerID=40&md5=5f0980184d574ec471f1b740ae0c02a1
AB  - In the past 20 years, Silicon Valley’s platforms and opaque algorithms have increasingly influenced civic discourse, helping Facebook, Twitter, and others extract and consolidate the revenues generated. That trend has reduced the profitability of local news organizations, but not the importance of locally created news reporting in residents’ day‐to‐day lives. The disruption of the economics and distribution of news has reduced, scattered, and diversified local news sources (digital‐first newspapers, digital‐only newsrooms, and television and radio broadcasters publishing online), making it difficult to inventory and understand the information health of communities, individually and in aggregate. Analysis of this national trend is often based on the geolocation of known news outlets as a proxy for community coverage. This measure does not accurately estimate the quality, scale, or diversity of topics provided to the community. This project is developing a scalable, semi‐automated approach to describe digital news content along journalism‐quality‐focused standards. We propose identifying representative corpora and applying machine learning and natural language processing to estimate the extent to which news articles engage in multiple journalistic dimensions, including geographic relevancy, critical information needs, and equity of coverage. © 2023 by the author(s); licensee Cogitatio Press (Lisbon, Portugal).
PB  - Cogitatio Press
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Žunić, A.
AU  - Corcoran, P.
AU  - Spasić, I.
TI  - The Case of Aspect in Sentiment Analysis: Seeking Attention or Co-Dependency?
PY  - 2022
T2  - Machine Learning and Knowledge Extraction
VL  - 4
IS  - 2
SP  - 474
EP  - 487
DO  - 10.3390/make4020021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141350096&doi=10.3390%2fmake4020021&partnerID=40&md5=f00f90f01ee7794f77436fb346cb2381
AB  - (1) Background: Aspect-based sentiment analysis (SA) is a natural language processing task, the aim of which is to classify the sentiment associated with a specific aspect of a written text. The performance of SA methods applied to texts related to health and well-being lags behind that of other domains. (2) Methods: In this study, we present an approach to aspect-based SA of drug reviews. Specifically, we analysed signs and symptoms, which were extracted automatically using the Unified Medical Language System. This information was then passed onto the BERT language model, which was extended by two layers to fine-tune the model for aspect-based SA. The interpretability of the model was analysed using an axiomatic attribution method. We performed a correlation analysis between the attribution scores and syntactic dependencies. (3) Results: Our fine-tuned model achieved accuracy of approximately (Formula presented.) on a well-balanced test set. It outperformed our previous approach, which used syntactic information to guide the operation of a neural network and achieved an accuracy of approximately (Formula presented.). (4) Conclusions: We demonstrated that a BERT-based model of SA overcomes the negative bias associated with health-related aspects and closes the performance gap against the state-of-the-art in other domains. © 2022 by the authors.
PB  - MDPI
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Wang, Q.
AU  - Liao, J.
AU  - Lapata, M.
AU  - Macleod, M.
TI  - Risk of bias assessment in preclinical literature using natural language processing
PY  - 2022
T2  - Research Synthesis Methods
VL  - 13
IS  - 3
SP  - 368
EP  - 380
DO  - 10.1002/jrsm.1533
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118479839&doi=10.1002%2fjrsm.1533&partnerID=40&md5=9d9469e75d2b17642da716b0713d6d6e
AB  - We sought to apply natural language processing to the task of automatic risk of bias assessment in preclinical literature, which could speed the process of systematic review, provide information to guide research improvement activity, and support translation from preclinical to clinical research. We use 7840 full-text publications describing animal experiments with yes/no annotations for five risk of bias items. We implement a series of models including baselines (support vector machine, logistic regression, random forest), neural models (convolutional neural network, recurrent neural network with attention, hierarchical neural network) and models using BERT with two strategies (document chunk pooling and sentence extraction). We tune hyperparameters to obtain the highest F1 scores for each risk of bias item on the validation set and compare evaluation results on the test set to our previous regular expression approach. The F1 scores of best models on test set are 82.0% for random allocation, 81.6% for blinded assessment of outcome, 82.6% for conflict of interests, 91.4% for compliance with animal welfare regulations and 46.6% for reporting animals excluded from analysis. Our models significantly outperform regular expressions for four risk of bias items. For random allocation, blinded assessment of outcome, conflict of interests and animal exclusions, neural models achieve good performance; for animal welfare regulations, BERT model with a sentence extraction strategy works better. Convolutional neural networks are the overall best models. The tool is publicly available which may contribute to the future monitoring of risk of bias reporting for research improvement activities. © 2021 The Authors. Research Synthesis Methods published by John Wiley & Sons Ltd.
PB  - John Wiley and Sons Ltd
C2  - 34709718
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 12
ER  -

TY  - CONF
AU  - Firoz, N.
AU  - Beresteneva, O.G.
AU  - Aksyonov, S.V.
TI  - Enhancing Depression Detection: Employing Autoencoders and Linguistic Feature Analysis with BERT and LSTM Model
PY  - 2023
T2  - Proceedings - 2023 International Russian Automation Conference, RusAutoCon 2023
SP  - 299
EP  - 304
DO  - 10.1109/RusAutoCon58002.2023.10272795
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174954498&doi=10.1109%2fRusAutoCon58002.2023.10272795&partnerID=40&md5=fc2b0807704511d0e102f3631147672c
AB  - Depression is an abstinent health ailment that affronts several people globally, affecting a consistent decline in mood resulting in a trivial impact on their emotions. The article centers around the utilization of BERT based features and Autoencoders to identify depression from textual input, while also exploring gender identity variations. The study emphasizes the importance of feature engineering for text data obtained from the DAIC_WOZ benchmark dataset. By employing BERT embeddings, which capture the semantic meaning of text, in combination with Autoencoders and additional quantitative characteristics such as PHQ-8 participant feedback, assessment of absolutist word usage, and gender details, the model's performance is significantly improved. We conduct trials with BERT embeddings that encode the textual meaning to extract text characteristics. These features are subsequently combined using Autoencoders. Our method surpasses the performance of the baseline models, achieving an accuracy of 95.5%. Additionally, the mean absolute error (MAE) is measured at 3.75 and the root mean squared error (RMSE) at 5.02, demonstrating the exceptional performance for binary depression classification. These discoveries reveal the capacity of machine learning in mental health investigation, especially when considering gender disparities. © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Wang, C.
AU  - Uh, J.
AU  - Patni, T.
AU  - Merchant, T.
AU  - Li, Y.
AU  - Hua, C.-H.
AU  - Acharya, S.
TI  - Toward MR-only proton therapy planning for pediatric brain tumors: Synthesis of relative proton stopping power images with multiple sequence MRI and development of an online quality assurance tool
PY  - 2022
T2  - Medical Physics
VL  - 49
IS  - 3
SP  - 1559
EP  - 1570
DO  - 10.1002/mp.15479
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124549533&doi=10.1002%2fmp.15479&partnerID=40&md5=07a1adae93ebc6677609a0b8a32616b6
AB  - Purpose: To generate synthetic relative proton stopping power (sRPSP) images from magnetic resonance imaging (MRI) sequence(s) and develop an online quality assurance (QA) tool for sRPSP to facilitate safe integration of magnetic resonance (MR)-only proton planning into clinical practice. Materials and methods: Planning computed tomography (CT) and MR images of 195 pediatric brain tumor patients were utilized (training: 150, testing: 45). Seventeen consistent-cycle generative adversarial network (ccGAN) models were trained separately using paired CT-converted RPSP and MRI datasets to transform a subject's MRI into sRPSP. T1-weighted (T1W), T2-weighted (T2W), and FLAIR MRI were permutated to form 17 combinations, with or without preprocessing, for determining the optimal training sequence(s). For evaluation, sRPSP images were converted to synthetic CT (sCT) and compared to the real CT in terms of mean absolute error (MAE) in Hounsfield units (HU). For QA, sCT was deformed and compared to a reference template built from training dataset to produce a flag map, highlighting pixels that deviate by >100 HU and fall outside the mean ± standard deviation reference intensity. The gamma intensity analysis (10%/3 mm) of the deformed sCT against the QA template on the intensity difference was investigated as a surrogate of sCT accuracy. Results: The sRPSP images generated from a single T1W or T2W sequence outperformed that generated from multi-MRI sequences in terms of MAE (all p < 0.05). Preprocessing with N4 bias and histogram matching reduced MAE of T2W MRI-based sCT (54 ± 21 HU vs. 42 ± 13 HU, p = 0.002). The gamma intensity analysis of sCT against the QA template was highly correlated with the MAE of sCT against the real CT in the testing cohort (r = -0.89 for T1W sCT; r = -0.93 for T2W sCT). Conclusion: Accurate sRPSP images can be generated from T1W/T2W MRI for proton planning. A QA tool highlights regions of inaccuracy, flagging problematic cases unsuitable for clinical use. © 2022 American Association of Physicists in Medicine
PB  - John Wiley and Sons Ltd
C2  - 35075670
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 13
ER  -

TY  - JOUR
AU  - Chin, H.
AU  - Song, H.
AU  - Baek, G.
AU  - Shin, M.
AU  - Jung, C.
AU  - Cha, M.
AU  - Choi, J.
AU  - Cha, C.
TI  - The Potential of Chatbots for Emotional Support and Promoting Mental Well-Being in Different Cultures: Mixed Methods Study
PY  - 2023
T2  - Journal of Medical Internet Research
VL  - 25
C7  - e51712
DO  - 10.2196/51712
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175584595&doi=10.2196%2f51712&partnerID=40&md5=20cb5298c788ec4cfab1326e9058c868
AB  - Background: Artificial intelligence chatbot research has focused on technical advances in natural language processing and validating the effectiveness of human-machine conversations in specific settings. However, real-world chat data remain proprietary and unexplored despite their growing popularity, and new analyses of chatbot uses and their effects on mitigating negative moods are urgently needed. Objective: In this study, we investigated whether and how artificial intelligence chatbots facilitate the expression of user emotions, specifically sadness and depression. We also examined cultural differences in the expression of depressive moods among users in Western and Eastern countries. Methods: This study used SimSimi, a global open-domain social chatbot, to analyze 152,783 conversation utterances containing the terms “depress” and “sad” in 3 Western countries (Canada, the United Kingdom, and the United States) and 5 Eastern countries (Indonesia, India, Malaysia, the Philippines, and Thailand). Study 1 reports new findings on the cultural differences in how people talk about depression and sadness to chatbots based on Linguistic Inquiry and Word Count and n-gram analyses. In study 2, we classified chat conversations into predefined topics using semisupervised classification techniques to better understand the types of depressive moods prevalent in chats. We then identified the distinguishing features of chat-based depressive discourse data and the disparity between Eastern and Western users. Results: Our data revealed intriguing cultural differences. Chatbot users in Eastern countries indicated stronger emotions about depression than users in Western countries (positive: P<.001; negative: P=.01); for example, Eastern users used more words associated with sadness (P=.01). However, Western users were more likely to share vulnerable topics such as mental health (P<.001), and this group also had a greater tendency to discuss sensitive topics such as swear words (P<.001) and death (P<.001). In addition, when talking to chatbots, people expressed their depressive moods differently than on other platforms. Users were more open to expressing emotional vulnerability related to depressive or sad moods to chatbots (74,045/148,590, 49.83%) than on social media (149/1978, 7.53%). Chatbot conversations tended not to broach topics that require social support from others, such as seeking advice on daily life difficulties, unlike on social media. However, chatbot users acted in anticipation of conversational agents that exhibit active listening skills and foster a safe space where they can openly share emotional states such as sadness or depression. Conclusions: The findings highlight the potential of chatbot-assisted mental health support, emphasizing the importance of continued technical and policy-wise efforts to improve chatbot interactions for those in need of emotional assistance. Our data indicate the possibility of chatbots providing helpful information about depressive moods, especially for users who have difficulty communicating emotions to other humans. ©Hyojin Chin, Hyeonho Song, Gumhee Baek, Mingi Shin, Chani Jung, Meeyoung Cha, Junghoi Choi, Chiyoung Cha. Originally published in the Journal of Medical Internet Research (https://www.jmir.org), 20.10.2023.
PB  - JMIR Publications Inc.
C2  - 37862063
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 27
ER  -

TY  - JOUR
AU  - Derton, A.
AU  - Guevara, M.
AU  - Chen, S.
AU  - Moningi, S.
AU  - Kozono, D.E.
AU  - Liu, D.
AU  - Miller, T.A.
AU  - Savova, G.K.
AU  - Mak, R.H.
AU  - Bitterman, D.S.
TI  - Natural Language Processing Methods to Empirically Explore Social Contexts and Needs in Cancer Patient Notes
PY  - 2023
T2  - JCO Clinical Cancer Informatics
VL  - 7
C7  - e2200196
DO  - 10.1200/CCI.22.00196
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160375183&doi=10.1200%2fCCI.22.00196&partnerID=40&md5=864bcd4488d2683d7bd4a58bdfbcdf28
AB  - PURPOSE There is an unmet need to empirically explore and understand drivers of cancer disparities, particularly social determinants of health. We explored natural language processing methods to automatically and empirically extract clinical documentation of social contexts and needs that may underlie disparities. METHODS This was a retrospective analysis of 230,325 clinical notes from 5,285 patients treated with radiotherapy from 2007 to 2019. We compared linguistic features among White versus non-White, low-income insurance versus other insurance, and male versus female patients' notes. Log odds ratios with an informative Dirichlet prior were calculated to compare words over-represented in each group. A variational autoencoder topic model was applied, and topic probability was compared between groups. The presence of machine-learnable bias was explored by developing statistical and neural demographic group classifiers. RESULTS Terms associated with varied social contexts and needs were identified for all demographic group comparisons. For example, notes of non-White and low-income insurance patients were over-represented with terms associated with housing and transportation, whereas notes of White and other insurance patients were over-represented with terms related to physical activity. Topic models identified a social history topic, and topic probability varied significantly between the demographic group comparisons. Classification models performed poorly at classifying notes of non-White and low-income insurance patients (F1 of 0.30 and 0.23, respectively). CONCLUSION Exploration of linguistic differences in clinical notes between patients of different race/ethnicity, insurance status, and sex identified social contexts and needs in patients with cancer and revealed high-level differences in notes. Future work is needed to validate whether these findings may play a role in cancer disparities. © 2023 by American Society of Clinical Oncology.
PB  - Lippincott Williams and Wilkins
C2  - 37235847
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Whitfill, J.T.
AU  - Kalpas, E.
AU  - Garcia-Filion, P.
TI  - Reuniting Long Lost Cousins: a Novel Curriculum in Imaging Informatics for Clinical Informatics Fellows
PY  - 2022
T2  - Journal of Digital Imaging
VL  - 35
IS  - 4
SP  - 876
EP  - 880
DO  - 10.1007/s10278-022-00628-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127668297&doi=10.1007%2fs10278-022-00628-5&partnerID=40&md5=13128196de83fd7c19f3796d64cca396
AB  - We developed a curriculum of imaging informatics for clinical informatics fellows. While imaging informatics and clinical informatics are related fields, they have distinct bodies of knowledge. The aim of this curriculum is to prepare clinical informatics fellows for questions regarding imaging informatics on the clinical informatics board certification examination, prepare fellows to handle issues and requests involving imaging informatics in their future roles as clinical informaticists, and develop sufficient knowledge and skills in order to interface with imaging and radiology domain experts. We mapped ACGME core competencies for clinical informatics and the clinical informatics skills and attributes to topics covered in this curriculum. Topics covered included orders vs. encounter-based workflow, understanding imaging informatics operations and the differences between an IT department leading digital image management and the radiology department, clinical decision support for radiology, procuring and integrating new modalities into a PACS system, troubleshooting slow application performance in a PACS environment, imaging sharing, artificial intelligence (AI) in imaging including AI bias, validation of models within home institution and regulatory issues, and structured reporting vs. Natural Language Processing to mine radiology report data. These topics were covered in interactive didactic sessions as well as a journal club. Future work will expand to include hands-on learning and a formal evaluation of this curriculum with current fellows and recent graduates. © 2022, The Author(s) under exclusive licence to Society for Imaging Informatics in Medicine.
PB  - Institute for Ionics
C2  - 35394222
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Wang, S.
AU  - Šuster, S.
AU  - Baldwin, T.
AU  - Verspoor, K.
TI  - Predicting Publication of Clinical Trials Using Structured and Unstructured Data: Model Development and Validation Study
PY  - 2022
T2  - Journal of Medical Internet Research
VL  - 24
IS  - 12
C7  - e38859
DO  - 10.2196/38859
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144635714&doi=10.2196%2f38859&partnerID=40&md5=d3a6723f9252eefb06cc40bc2a2885ac
AB  - Background: Publication of registered clinical trials is a critical step in the timely dissemination of trial findings. However, a significant proportion of completed clinical trials are never published, motivating the need to analyze the factors behind success or failure to publish. This could inform study design, help regulatory decision-making, and improve resource allocation. It could also enhance our understanding of bias in the publication of trials and publication trends based on the research direction or strength of the findings. Although the publication of clinical trials has been addressed in several descriptive studies at an aggregate level, there is a lack of research on the predictive analysis of a trial’s publishability given an individual (planned) clinical trial description. Objective: We aimed to conduct a study that combined structured and unstructured features relevant to publication status in a single predictive approach. Established natural language processing techniques as well as recent pretrained language models enabled us to incorporate information from the textual descriptions of clinical trials into a machine learning approach. We were particularly interested in whether and which textual features could improve the classification accuracy for publication outcomes. Methods: In this study, we used metadata from ClinicalTrials.gov (a registry of clinical trials) and MEDLINE (a database of academic journal articles) to build a data set of clinical trials (N=76,950) that contained the description of a registered trial and its publication outcome (27,702/76,950, 36% published and 49,248/76,950, 64% unpublished). This is the largest data set of its kind, which we released as part of this work. The publication outcome in the data set was identified from MEDLINE based on clinical trial identifiers. We carried out a descriptive analysis and predicted the publication outcome using 2 approaches: a neural network with a large domain-specific language model and a random forest classifier using a weighted bag-of-words representation of text. Results: First, our analysis of the newly created data set corroborates several findings from the existing literature regarding attributes associated with a higher publication rate. Second, a crucial observation from our predictive modeling was that the addition of textual features (eg, eligibility criteria) offers consistent improvements over using only structured data (F1-score=0.62-0.64 vs F1-score=0.61 without textual features). Both pretrained language models and more basic word-based representations provide high-utility text representations, with no significant empirical difference between the two. Conclusions: Different factors affect the publication of a registered clinical trial. Our approach to predictive modeling combines heterogeneous features, both structured and unstructured. We show that methods from natural language processing can provide effective textual features to enable more accurate prediction of publication success, which has not been explored for this task previously. © Siyang Wang, Simon Šuster, Timothy Baldwin, Karin Verspoor.
PB  - JMIR Publications Inc.
C2  - 36563029
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Currie, G.
AU  - Robbie, S.
AU  - Tually, P.
TI  - ChatGPT and Patient Information in Nuclear Medicine: GPT-3.5 Versus GPT-4
PY  - 2023
T2  - Journal of Nuclear Medicine Technology
VL  - 51
IS  - 4
SP  - 307
EP  - 313
DO  - 10.2967/jnmt.123.266151
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179010630&doi=10.2967%2fjnmt.123.266151&partnerID=40&md5=1347e2f85db6f5ba4169d3b11ad6fbea
AB  - The GPT-3.5–powered ChatGPT was released in late November 2022 powered by the generative pretrained transformer (GPT) version 3.5. It has emerged as a readily accessible source of patient information ahead of medical procedures. Although ChatGPT has purported benefits for supporting patient education and information, actual capability has not been evaluated. Moreover, the March 2023 emergence of paid subscription access to GPT-4 promises further enhanced capabilities requiring evaluation. Methods: ChatGPT was used to generate patient information sheets suitable for gaining informed consent for 7 common procedures in nuclear medicine. Responses were generated independently for both GPT-3.5 and GPT-4 architectures. Specific procedures were selected that had a long-standing history of use to avoid any bias associated with the September 2021 learning cutoff that constrains both GPT-3.5 and GPT-4 architectures. Each information sheet was independently evaluated by 3 expert assessors and ranked on the basis of accuracy, appropriateness, currency, and fitness for purpose. Results: ChatGPT powered by GPT-3.5 provided patient information that was appropriate in terms of being patient-facing but lacked accuracy and currency and omitted important information. GPT-3.5 produced patient information deemed not fit for the purpose. GPT-4 provided patient information enhanced across appropriateness, accuracy, and currency, despite some omission of information. GPT-4 produced patient information that was largely fit for the purpose. Conclusion: Although ChatGPT powered by GPT-3.5 is accessible and provides plausible patient information, inaccuracies and omissions present a risk to patients and informed consent. Conversely, GPT-4 is more accurate and fit for the purpose but, at the time of writing, was available only through a paid subscription. COPYRIGHT ß 2023 by the Society of Nuclear Medicine and Molecular Imaging.
PB  - Society of Nuclear Medicine Inc.
C2  - 37699647
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 21
ER  -

TY  - CONF
AU  - Clapp, M.A.
AU  - Kim, E.
AU  - James, K.E.
AU  - Perlis, R.H.
AU  - Kaimal, A.J.
AU  - McCoy, T.H.
TI  - Natural language processing of admission notes to predict severe maternal morbidity during the delivery encounter
PY  - 2022
T2  - American Journal of Obstetrics and Gynecology
VL  - 227
IS  - 3
SP  - 511.e1
EP  - 511.e8
DO  - 10.1016/j.ajog.2022.04.008
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130435317&doi=10.1016%2fj.ajog.2022.04.008&partnerID=40&md5=63755cdaeed6b2f3bb3f3629ecf20469
AB  - Background: Severe maternal morbidity and mortality remain public health priorities in the United States, given their high rates relative to other high-income countries and the notable racial and ethnic disparities that exist. In general, accurate risk stratification methods are needed to help patients, providers, hospitals, and health systems plan for and potentially avert adverse outcomes. Objective: Our objective was to understand if machine learning methods with natural language processing of history and physical notes could identify a group of patients at high risk of maternal morbidity on admission for delivery without relying on any additional patient information (eg, demographics and diagnosis codes). Study Design: This was a retrospective study of people admitted for delivery at 2 hospitals (hospitals A and B) in a single healthcare system between July 1, 2016, and June 30, 2020. The primary outcome was severe maternal morbidity, as defined by the Centers for Disease Control and Prevention; furthermore, we examined nontransfusion severe maternal morbidity. Clinician documents designated as history and physical notes were extracted from the electronic health record for processing and analysis. A bag-of-words approach was used for this natural language processing analysis (ie, each history or physical note was converted into a matrix of counts of individual words (or phrases) that occurred within the document). The least absolute shrinkage and selection operator models were used to generate prediction probabilities for severe maternal morbidity and nontransfusion severe maternal morbidity for each note. Model discrimination was assessed via the area under the receiver operating curve. Discrimination was compared between models using the DeLong test. Calibration plots were generated to assess model calibration. Moreover, the natural language processing models with history and physical note texts were compared with validated obstetrical comorbidity risk scores based on diagnosis codes. Results: There were 13,572 delivery encounters with history and physical notes from hospital A, split between training (Atrain, n=10,250) and testing (Atest, n=3,322) datasets for model derivation and internal validation. There were 23,397 delivery encounters with history and physical notes from hospital B (Bvalid) used for external validation. For the outcome of severe maternal morbidity, the natural language processing model had an area under the receiver operating curve of 0.67 (95% confidence interval, 0.63–0.72) and 0.72 (95% confidence interval, 0.70–0.74) in the Atest and Bvalid datasets, respectively. For the outcome of nontransfusion severe maternal morbidity, the area under the receiver operating curve was 0.72 (95% confidence interval, 0.65–0.80) and 0.76 (95% confidence interval, 0.73–0.79) in the Atest and Bvalid datasets, respectively. The calibration plots demonstrated the bag-of-words model's ability to distinguish a group of individuals at a substantially higher risk of severe maternal morbidity and nontransfusion severe maternal morbidity, notably those in the top deciles of predicted risk. Areas under the receiver operating curve in the natural language processing–based models were similar to those generated using a validated, retrospectively derived, diagnosis code–based comorbidity score. Conclusion: In this practical application of machine learning, we demonstrated the capabilities of natural language processing for the prediction of severe maternal morbidity based on provider documentation inherently generated at the time of admission. This work should serve as a catalyst for providers, hospitals, and electronic health record systems to explore ways that artificial intelligence can be incorporated into clinical practice and evaluated rigorously for their ability to improve health. © 2022 Elsevier Inc.
PB  - Elsevier Inc.
C2  - 35430230
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 12
ER  -

TY  - JOUR
AU  - Witte, H.
AU  - Blatter, T.U.
AU  - Nagabhushana, P.
AU  - Schär, D.
AU  - Ackermann, J.
AU  - Cadamuro, J.
AU  - Leichtle, A.B.
TI  - Statistical learning and big data applications
PY  - 2023
T2  - Journal of Laboratory Medicine
C7  - labmed-2023-0037
DO  - 10.1515/labmed-2023-0037
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162913747&doi=10.1515%2flabmed-2023-0037&partnerID=40&md5=caf1c1c605b2067a010afa93661c1857
AB  - The amount of data generated in the field of laboratory medicine has grown to an extent that conventional laboratory information systems (LISs) are struggling to manage and analyze this complex, entangled information ("Big Data"). Statistical learning, a generalized framework from machine learning (ML) and artificial intelligence (AI) is predestined for processing "Big Data"and holds the potential to revolutionize the field of laboratory medicine. Personalized medicine may in particular benefit from AI-based systems, especially when coupled with readily available wearables and smartphones which can collect health data from individual patients and offer new, cost-effective access routes to healthcare for patients worldwide. The amount of personal data collected, however, also raises concerns about patient-privacy and calls for clear ethical guidelines for "Big Data"research, including rigorous quality checks of data and algorithms to eliminate underlying bias and enable transparency. Likewise, novel federated privacy-preserving data processing approaches may reduce the need for centralized data storage. Generative AI-systems including large language models such as ChatGPT currently enter the stage to reshape clinical research, clinical decision-support systems, and healthcare delivery. In our opinion, AI-based systems have a tremendous potential to transform laboratory medicine, however, their opportunities should be weighed against the risks carefully. Despite all enthusiasm, we advocate for stringent added-value assessments, just as for any new drug or treatment. Human experts should carefully validate AI-based systems, including patient-privacy protection, to ensure quality, transparency, and public acceptance. In this opinion paper, data prerequisites, recent developments, chances, and limitations of statistical learning approaches are highlighted.  © 2023 the author(s), published by De Gruyter, Berlin/Boston.
PB  - Walter de Gruyter GmbH
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Matic, M.
AU  - Singh, G.
AU  - Carli, F.
AU  - De Oliveira Rosa, N.
AU  - Miglionico, P.
AU  - Magni, L.
AU  - Gutkind, J.S.
AU  - Russell, R.B.
AU  - Inoue, A.
AU  - Raimondi, F.
TI  - PRECOGx: exploring GPCR signaling mechanisms with deep protein representations
PY  - 2022
T2  - Nucleic Acids Research
VL  - 50
IS  - W1
SP  - W598
EP  - W610
DO  - 10.1093/nar/gkac426
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134382489&doi=10.1093%2fnar%2fgkac426&partnerID=40&md5=444adea8c7d270f6288c297b071b0f5c
AB  - In this study we show that protein language models can encode structural and functional information of GPCR sequences that can be used to predict their signaling and functional repertoire. We used the ESM1b protein embeddings as features and the binding information known from publicly available studies to develop PRECOGx, a machine learning predictor to explore GPCR interactions with G protein and β-arrestin, which we made available through a new webserver (https://precogx.bioinfolab.sns.it/). PRECOGx outperformed its predecessor (e.g. PRECOG) in predicting GPCR-transducer couplings, being also able to consider all GPCR classes. The webserver also provides new functionalities, such as the projection of input sequences on a low-dimensional space describing essential features of the human GPCRome, which is used as a reference to track GPCR variants. Additionally, it allows inspection of the sequence and structural determinants responsible for coupling via the analysis of the most important attention maps used by the models as well as through predicted intramolecular contacts. We demonstrate applications of PRECOGx by predicting the impact of disease variants (ClinVar) and alternative splice forms from healthy tissues (GTEX) of human GPCRs, revealing the power to dissect system biasing mechanisms in both health and disease.  © 2022 The Author(s). Published by Oxford University Press on behalf of Nucleic Acids Research.
PB  - Oxford University Press
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 9
ER  -

TY  - JOUR
AU  - Tin, J.
AU  - Stevens, H.
AU  - Rasul, M.E.
AU  - Taylor, L.D.
TI  - Incivility in COVID-19 Vaccine Mandate Discourse and Moral Foundations: Natural Language Processing Approach
PY  - 2023
T2  - JMIR Formative Research
VL  - 7
IS  - 1
C7  - e50367
DO  - 10.2196/50367
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180105681&doi=10.2196%2f50367&partnerID=40&md5=77cc235d39a889e76ff785fd61779400
AB  - Background: Vaccine hesitancy poses a substantial threat to efforts to mitigate the harmful effects of the COVID-19 pandemic. To combat vaccine hesitancy, officials in the United States issued vaccine mandates, which were met with strong antivaccine discourse on social media platforms such as Reddit. The politicized and polarized nature of COVID-19 on social media has fueled uncivil discourse related to vaccine mandates, which is known to decrease confidence in COVID-19 vaccines. Objective: This study examines the moral foundations underlying uncivil COVID-19 vaccine discourse. Moral foundations theory poses that individuals make decisions to express approval or disapproval (ie, uncivil discourse) based on innate moral values. We examine whether moral foundations are associated with dimensions of incivility. Further, we explore whether there are any differences in the presence of incivility between the r/coronaviruscirclejerk and r/lockdownskepticism subreddits. Methods: Natural language processing methodologies were leveraged to analyze the moral foundations underlying uncivil discourse in 2 prominent antivaccine subreddits, r/coronaviruscirclejerk and r/lockdownskepticism. All posts and comments from both of the subreddits were collected since their inception in March 2022. This was followed by filtering the data set for key terms associated with the COVID-19 vaccine (eg, “vaccinate” and “Pfizer”) and mandates (eg, “forced” and “mandating”). These key terms were selected based on a review of existing literature and because of their salience in both of the subreddits. A 10% sample of the filtered key terms was used for the final analysis. Results: Findings suggested that moral foundations play a role in the psychological processes underlying uncivil vaccine mandate discourse. Specifically, we found substantial associations between all moral foundations (ie, care and harm, fairness and cheating, loyalty and betrayal, authority and subversion, and sanctity and degradation) and dimensions of incivility (ie, toxicity, insults, profanity, threat, and identity attack) except for the authority foundation. We also found statistically significant differences between r/coronaviruscirclejerk and r/lockdownskepticism for the presence of the dimensions of incivility. Specifically, the mean of identity attack, insult, toxicity, profanity, and threat in the r/lockdownskepticism subreddit was significantly lower than that in the r/coronaviruscirclejerk subreddit (P < .001). Conclusions: This study shows that moral foundations may play a substantial role in the presence of incivility in vaccine discourse. On the basis of the findings of the study, public health practitioners should tailor messaging by addressing the moral values underlying the concerns people may have about vaccines, which could manifest as uncivil discourse. Another way to tailor public health messaging could be to direct it to parts of social media platforms with increased uncivil discourse. By integrating moral foundations, public health messaging may increase compliance and promote civil discourse surrounding COVID-19. © Jason Tin, Hannah Stevens, Muhammad Ehab Rasul, Laramie D Taylor. Originally published in JMIR Formative Research (https://formative.jmir.org), 29.11.2023. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in JMIR Formative Research, is properly cited. The complete bibliographic information, a link to the original publication on https://formative.jmir.org, as well as this copyright and license information must be included.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Debuse, M.
AU  - Warnick, S.
TI  - Automatic Control of Opinion Dynamics in Social Networks
PY  - 2023
T2  - 2023 IEEE Conference on Control Technology and Applications, CCTA 2023
SP  - 180
EP  - 185
DO  - 10.1109/CCTA54093.2023.10253399
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173837647&doi=10.1109%2fCCTA54093.2023.10253399&partnerID=40&md5=3b906c4184a7c23afe4073ee253c45b8
AB  - This work addresses the issue of how the common interaction dynamics of social media networks enable the creation of "echo chambers,"or self-reinforcing, disjoint communities with distinct opinion biases. We theorize a Strategic Agent as a feedback controller that can dismantle echo chambers and encourage an overall healthier sharing of opinions among agents in the network. We then show how this same controller can then be used to drive opinions of all agents on the network to any desired opinion bias, emphasising the importance of ethical use of automatable discourse-building and enabling technologies, such as large language models like Chat-GPT. © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Qiu, Y.
AU  - Ding, S.
AU  - Tian, D.
AU  - Zhang, C.
AU  - Zhou, D.
TI  - Predicting the quality of answers with less bias in online health question answering communities
PY  - 2022
T2  - Information Processing and Management
VL  - 59
IS  - 6
C7  - 103112
DO  - 10.1016/j.ipm.2022.103112
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141744838&doi=10.1016%2fj.ipm.2022.103112&partnerID=40&md5=b0e16b8fafaa47e505eff0638d65c9de
AB  - Existing approaches in online health question answering (HQA) communities to identify the quality of answers either address it subjectively by human assessment or mainly using textual features. This process may be time-consuming and lose the semantic information of answers. We present an automatic approach for predicting answer quality that combines sentence-level semantics with textual and non-textual features in the context of online healthcare. First, we extend the knowledge adoption model (KAM) theory to obtain the six dimensions of quality measures for textual and non-textual features. Then we apply the Bidirectional Encoder Representations from Transformers (BERT) model for extracting semantic features. Next, the multi-dimensional features are processed for dimensionality reduction using linear discriminant analysis (LDA). Finally, we incorporate the preprocessed features into the proposed BK-XGBoost method to automatically predict the answer quality. The proposed method is validated on a real-world dataset with 48121 question-answer pairs crawled from the most popular online HQA communities in China. The experimental results indicate that our method competes against the baseline models on various evaluation metrics. We found up to 2.9% and 5.7% improvement in AUC value in comparison with BERT and XGBoost models respectively. © 2022 Elsevier Ltd
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 15
ER  -

TY  - JOUR
AU  - Srivastava, R.
AU  - Singh, P.
AU  - Rana, K.P.S.
AU  - Kumar, V.
TI  - A topic modeled unsupervised approach to single document extractive text summarization
PY  - 2022
T2  - Knowledge-Based Systems
VL  - 246
C7  - 108636
DO  - 10.1016/j.knosys.2022.108636
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129545400&doi=10.1016%2fj.knosys.2022.108636&partnerID=40&md5=5b912711fd5128bdb94c47720882f7c6
AB  - Automatic Text Summarization (ATS) is an essential field in natural language processing that attempts to condense large text documents so that users can assimilate information quickly. It finds uses in medical document summarization, review generation, and opinion mining. This work investigated an unsupervised extractive summarization approach that combined clustering with topic modeling to reduce topic bias. Latent Dirichlet Allocation was used for topic modeling, while K-Medoids clustering was employed for summary generation. The approach was evaluated on three datasets—Wikihow, CNN/DailyMail, and the DUC2002 Corpus. The Recall Oriented-Understudy for Gisting Evaluation (ROUGE) metrics were used for comparative analysis against recently reported techniques, specifically ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L). The suggested framework offered scores of 34.80%, 9.13%, and 32.30% on the Wikihow Dataset, 43.90%, 19.01%, and 41.50% on the CNN/DailyMail Dataset, and 49.35%, 31.53%, and 41.72% on the DUC2002 Corpus (R-1, R-2, R-L respectively). These reported metrics are found to be superior when compared to similar recent works. Further, execution time of the proposed method was also recorded and compared with counterparts, which established its superior speed. Based on these promising outcomes, it was concluded that an unsupervised extractive summarization approach with greater subtopic focus significantly improves over generic topic modeling semantic and deep learning approaches. © 2022 Elsevier B.V.
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 52
ER  -

TY  - JOUR
AU  - Bett, E.S.
AU  - Frommeyer, T.C.
AU  - Reddy, T.
AU  - Johnson, J.“.
TI  - Assessment of patient perceptions of technology and the use of machine-based learning in a clinical encounter
PY  - 2023
T2  - Intelligence-Based Medicine
VL  - 7
C7  - 100096
DO  - 10.1016/j.ibmed.2023.100096
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152540895&doi=10.1016%2fj.ibmed.2023.100096&partnerID=40&md5=2a9dd1499cea2629808431b236840e21
AB  - Background: Electronic health records (EHR) were implemented to improve patient care, reduce healthcare disparities, engage patients and families, improve care coordination, and maintain privacy and security. Unfortunately, the mandated use of EHR has also resulted in significantly increased clerical and administrative burden, with physicians spending an estimated three-fourths of their daily time interacting with the EHR, which negatively affects within-clinic processes and contributes to burnout. In-room scribes are associated with improvement in all aspects of physician satisfaction and increased productivity, though less is known about the use of other technologies such as Google Glass (GG), Natural Language Processing (NLP) and Machine-Based Learning (MBL) systems. Given the need to decrease administrative burden on clinicians, particularly in the utilization of the EHR, there is a need to explore the intersection between varying degrees of technology in the clinical encounter and their ability to meet the aforementioned goals of the EHR. Aims: The primary aim is to determine predictors of overall perception of care dependent on varying mechanisms used for documentation and medical decision-making in a routine clinical encounter. Secondary aims include comparing the perception of individual vignettes based on demographics of the participants and investigating any differences in perception questions by demographics of the participants. Methods: Video vignettes were shown to 498 OhioHealth Physician Group patients and to ResearchMatch volunteers during a 15-month period following IRB approval. Data included a baseline survey to gather demographic and background characteristics and then a perceptual survey where patients rated the physician in the video on 5 facets using a 1 to 5 Likert scale. The analysis included summarizing data of all continuous and categorical variables as well as overall perceptions analyzed using multivariate linear regression with perception score as the outcome variable. Results: Univariate modeling identified sex, education, and type of technology as three factors that were statistically significantly related to the overall perception score. Males had higher scores than females (p = 0.03) and those with lower education had higher scores (p < 0.001). In addition, the physician documenting outside of the room encounter had statistically significantly higher overall perception scores (mean = 22.2, p < 0.001) and the physician documenting in the room encounter had statistically significantly lower overall perception scores (mean = 15.3, p < 0.001) when compared to the other vignettes. Multivariable modeling identified all three of the univariably significant factors as independent factors related to overall perception score. Specifically, high school education had higher scores than associate/bachelor education (LSM = 21.6 vs. 19.9, p = 0.0002) and higher scores than master/higher education (LSM = 21.6 vs. 19.5, p < 0.0001). No differences between age groups were found on the individual perception scores. Males had higher scores than females on ‘The doctor clearly explained the diagnosis and treatment to the patient’ and ‘The doctor was sincere and trustworthy’. High school education had higher scores than associate/bachelor and master/higher on all five individual perception scores. Conclusion: The study found sex, education, and type of technology were significant indicators for overall perception of varying technologies used for documentation and medical decision-making in a routine clinical encounter. Importantly, the vignette depicting the least interaction with the EHR received the most positive overall perception score, while the vignette depicting the physician utilizing the EHR during the interaction received the least positive overall perception score. This suggests patients most value having the full attention of the physician and feel less strongly about differentiating the logistics of data transcription and medical decision-making, provided they feel engaged during the interaction. Therefore, the authors suggest maximizing face-to-face time in the integration of technology into the clinical encounter, allowing for increased perceptions of personal attention in the patient-physician interaction. © 2023 The Authors
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Zeng, J.
AU  - Gensheimer, M.F.
AU  - Rubin, D.L.
AU  - Athey, S.
AU  - Shachter, R.D.
TI  - Uncovering interpretable potential confounders in electronic medical records
PY  - 2022
T2  - Nature Communications
VL  - 13
IS  - 1
C7  - 1014
DO  - 10.1038/s41467-022-28546-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125155314&doi=10.1038%2fs41467-022-28546-8&partnerID=40&md5=67a5dd5bfb93f876f3223a2527a54c16
AB  - Randomized clinical trials (RCT) are the gold standard for informing treatment decisions. Observational studies are often plagued by selection bias, and expert-selected covariates may insufficiently adjust for confounding. We explore how unstructured clinical text can be used to reduce selection bias and improve medical practice. We develop a framework based on natural language processing to uncover interpretable potential confounders from text. We validate our method by comparing the estimated hazard ratio (HR) with and without the confounders against established RCTs. We apply our method to four cohorts built from localized prostate and lung cancer datasets from the Stanford Cancer Institute and show that our method shifts the HR estimate towards the RCT results. The uncovered terms can also be interpreted by oncologists for clinical insights. We present this proof-of-concept study to enable more credible causal inference using observational data, uncover meaningful insights from clinical text, and inform high-stakes medical decisions. © 2022, The Author(s).
PB  - Nature Research
C2  - 35197467
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 25
ER  -

TY  - JOUR
AU  - Sedki, M.
AU  - Vidal, N.
AU  - Roux, P.
AU  - Barry, C.
AU  - Speranza, M.
AU  - Falissard, B.
AU  - Brunet-Gouet, E.
TI  - Using a self-attention architecture to automate valence categorization of French teenagers’ free descriptions of their family relationships: a proof of concept
PY  - 2023
T2  - Journal of Medical Artificial Intelligence
VL  - 6
C7  - 15
DO  - 10.21037/jmai-23-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178953104&doi=10.21037%2fjmai-23-8&partnerID=40&md5=fa3a7c3588679eb3c017093a620b6fcd
AB  - Background: When caring for adolescents with mental health problems, family relationships need to be taken into account, whether they are considered supportive or unfavorable by the patients themselves. Recendevelopments in automated natural language processing (NLP) appear to offer solutions to the challenge odesigning tools to evaluate the spontaneous discourse of adolescents on the subject of these relationshipsThis paper proposes a proof of concept of using NLP to categorize valence of family relationships described in free texts written by French teenagers. The proposed study traces the evolution of techniques for word embedding from classical categorization methods to self-attentional architectures. Methods: After decomposing different texts in our possession into short texts composed of sentences and manual labeling, we tested different word embedding scenarios to train a multi-label classification model where a text can take several labels: labels describing the family link between the teenager and the person mentioned in the text and labels describing the teenager’s relationship with them (positive/negative/neutral valence). The natural baseline for word vector representation of our texts is to build a Term Frequency-Inverse-Document-Frequency (TF-IDF) and train classical classifiers (Elasticnet logistic regression, gradient boosting, random forest, support vector classifier) after selecting a model by cross validation in each class of machine learning models. We then studied the strengths of word-vectors embeddings by an advanced language representation technique via the Bidirectional Encoder Representations from Transformers (BERT) CamemBERT transformer model, and, again, used them with classical classifiers to compare their respective performancesThe last scenario consisted in augmenting the CamemBERT with output dense layers (perceptron) representing a classifier adapted to the multi-label classification and fine-tuning the CamemBERT original layers. Results: The optimal fine-tuning depth that achieved a bias-variance trade-off was obtained by a cross-validation procedure. The results of the comparison of the three scenarios on a test dataset showed a clear improvement of the classification performances of the scenario with fine-tuning beyond the baseline and of a simple vectorization using CamemBERT without fine-tuning. Conclusions: Despite the moderate size of the dataset and the input texts, fine-tuning to an optimal depth remains the best solution to build a classifier. © Journal of Medical Artificial Intelligence. All rights reserved.
PB  - AME Publishing Company
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Hryciw, B.N.
AU  - Seely, A.J.E.
AU  - Kyeremanteng, K.
TI  - Guiding principles and proposed classification system for the responsible adoption of artificial intelligence in scientific writing in medicine
PY  - 2023
T2  - Frontiers in Artificial Intelligence
VL  - 6
C7  - 1283353
DO  - 10.3389/frai.2023.1283353
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178227384&doi=10.3389%2ffrai.2023.1283353&partnerID=40&md5=0747a74dfb6f52667355bac49d302b9f
AB  - The integration of large language models (LLMs) and artificial intelligence (AI) into scientific writing, especially in medical literature, presents both unprecedented opportunities and inherent challenges. This manuscript evaluates the transformative potential of LLMs for the synthesis of information, linguistic enhancements, and global knowledge dissemination. At the same time, it raises concerns about unintentional plagiarism, the risk of misinformation, data biases, and an over-reliance on AI. To address these, we propose governing principles for AI adoption that ensure integrity, transparency, validity, and accountability. Additionally, guidelines for reporting AI involvement in manuscript development are delineated, and a classification system to specify the level of AI assistance is introduced. This approach uniquely addresses the challenges of AI in scientific writing, emphasizing transparency in authorship, qualification of AI involvement, and ethical considerations. Concerns regarding access equity, potential biases in AI-generated content, authorship dynamics, and accountability are also explored, emphasizing the human author’s continued responsibility. Recommendations are made for fostering collaboration between AI developers, researchers, and journal editors and for emphasizing the importance of AI’s responsible use in academic writing. Regular evaluations of AI’s impact on the quality and biases of medical manuscripts are also advocated. As we navigate the expanding realm of AI in scientific discourse, it is crucial to maintain the human element of creativity, ethics, and oversight, ensuring that the integrity of scientific literature remains uncompromised. Copyright © 2023 Hryciw, Seely and Kyeremanteng.
PB  - Frontiers Media SA
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - Leung, T.I.
AU  - Sagar, A.
AU  - Shroff, S.
AU  - Henry, T.L.
TI  - Can AI Mitigate Bias in Writing Letters of Recommendation?
PY  - 2023
T2  - JMIR Medical Education
VL  - 9
C7  - e51494
DO  - 10.2196/51494
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170842982&doi=10.2196%2f51494&partnerID=40&md5=d401a758c8467e8576028bfcd7cba5b2
AB  - Letters of recommendation play a significant role in higher education and career progression, particularly for women and underrepresented groups in medicine and science. Already, there is evidence to suggest that written letters of recommendation contain language that expresses implicit biases, or unconscious biases, and that these biases occur for all recommenders regardless of the recommender’s sex. Given that all individuals have implicit biases that may influence language use, there may be opportunities to apply contemporary technologies, such as large language models or other forms of generative artificial intelligence (AI), to augment and potentially reduce implicit biases in the written language of letters of recommendation. In this editorial, we provide a brief overview of existing literature on the manifestations of implicit bias in letters of recommendation, with a focus on academia and medical education. We then highlight potential opportunities and drawbacks of applying this emerging technology in augmenting the focused, professional task of writing letters of recommendation. We also offer best practices for integrating their use into the routine writing of letters of recommendation and conclude with our outlook for the future of generative AI applications in supporting this task. © 2023 JMIR Medical Education. All rights reserved.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - JOUR
AU  - Ahmed, U.
AU  - Srivastava, G.
AU  - Yun, U.
AU  - Lin, J.C.-W.
TI  - EANDC: An explainable attention network based deep adaptive clustering model for mental health treatment
PY  - 2022
T2  - Future Generation Computer Systems
VL  - 130
SP  - 106
EP  - 113
DO  - 10.1016/j.future.2021.12.008
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123205972&doi=10.1016%2fj.future.2021.12.008&partnerID=40&md5=882812bc598513af1ff183614e962c52
AB  - Internet-delivered Psychological Treatment (IDPT) has been shown to be an effective method for improving psychological disorders. Natural language processing (NLP) requires an appropriate set of linguistic features for word representation and emotion segmentation. For psychological applications, models must be trained on extensive and diverse datasets to achieve expert-level performance. Labeling psychological texts authorized by patients is challenging because emotional biases can lead to incorrect segmentation of emotions and labeling emotional data is time consuming. In this paper, we propose an assistance tool for psychologists to explore the emotional aspects of mentally ill individuals. We first use an NLP-based method to create emotional lexicon embeddings, and then apply attention-based deep clustering. The learned representation is then used to visualize the emotional aspect of the text authorized by patients. We expand the patient authored text using synonymous semantic expansion. A latent semantic representation based on context is clustered using EANDC, which is a Explainable Attention Network-based Deep adaptive Clustering model. We use similarity metrics to select a subset of the text and then improve the explainability of learning using a curriculum-based optimization method. The experimental results show that synonym expansion based on the emotion lexicon increases accuracy without affecting the results. The attention method with bidirectional LSTM architecture achieved 0.81 ROC in a blind test. The self-learning based embedding then visualizes the weighted attention words and helps the psychiatrist to improve his explanatory power of the qualitative match for clinical notes and the remedy. The method helps in labeling text and improves the recognition rate of symptoms of mental disorders. © 2021 The Author(s)
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 31
ER  -

TY  - JOUR
AU  - Wigmore, G.
AU  - Deane, A.M.
AU  - Anstey, J.
AU  - Bailey, M.
AU  - Bihari, S.
AU  - Eastwood, G.
AU  - Ghanpur, R.
AU  - Maiden, M.J.
AU  - Presneill, J.J.
AU  - Raman, J.
AU  - Bellomo, R.
TI  - Study protocol and statistical analysis plan for the 20% Human Albumin Solution Fluid Bolus Administration Therapy in Patients after Cardiac Surgery-II (HAS FLAIR-II) trial
PY  - 2022
T2  - Critical Care and Resuscitation
VL  - 24
IS  - 4
SP  - 309
EP  - 318
DO  - 10.51893/2022.4.OA1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143487062&doi=10.51893%2f2022.4.OA1&partnerID=40&md5=6186ad2340b62707e5c41973590f7fff
AB  - Background: Fluid bolus therapy with 20% albumin may shorten the duration of vasopressor therapy in patients after cardiac surgery. Objective: To describe the study protocol and statistical analysis plan for the 20% Human Albumin Solution Fluid Bolus Administration Therapy in Patients after Cardiac Surgery-II (HAS FLAIR-II) trial. Design, setting, participants and intervention: HAS FLAIR-II is a phase 2b, multicentre, parallel group, open-label, randomised controlled trial that will be conducted at six Australian intensive care units. Patients requiring fluid bolus therapy after cardiac surgery will be randomly assigned in a 1:1 ratio to the intervention of fluid bolus therapy with 20% albumin or a comparator of fluid bolus therapy with a crystalloid solution. Main outcome measures: The primary outcome measure is the cumulative duration of vasopressor therapy. Secondary outcomes include vasopressor use, service utilisation, and mortality. All analyses will be conducted on an intention-to-treat basis. Results and conclusion: The study protocol and statistical analysis plan will guide the conduct and analysis of the HAS FLAIR-II trial, such that analytical and reporting biases are minimised. Trial registration: This trial has been registered with the Australian New Zealand Clinical Trials Registry (ACTRN No. 12620000137998). © 2022, College of Intensive Care Medicine. All rights reserved.
PB  - College of Intensive Care Medicine
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Minot, J.R.
AU  - Cheney, N.
AU  - Maier, M.
AU  - Elbers, D.C.
AU  - Danforth, C.M.
AU  - Dodds, P.S.
TI  - Interpretable Bias Mitigation for Textual Data: Reducing Genderization in Patient Notes While Maintaining Classification Performance
PY  - 2022
T2  - ACM Transactions on Computing for Healthcare
VL  - 3
IS  - 4
C7  - 3524887
DO  - 10.1145/3524887
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140732696&doi=10.1145%2f3524887&partnerID=40&md5=7ab3833e79cbb425264751f8b25f60c0
AB  - Medical systems in general, and patient treatment decisions and outcomes in particular, can be affected by bias based on gender and other demographic elements. As language models are increasingly applied to medicine, there is a growing interest in building algorithmic fairness into processes impacting patient care. Much of the work addressing this question has focused on biases encoded in language models - statistical estimates of the relationships between concepts derived from distant reading of corpora. Building on this work, we investigate how differences in gender-specific word frequency distributions and language models interact with regards to bias. We identify and remove gendered language from two clinical-note datasets and describe a new debiasing procedure using BERT-based gender classifiers. We show minimal degradation in health condition classification tasks for low- to medium-levels of dataset bias removal via data augmentation. Finally, we compare the bias semantically encoded in the language models with the bias empirically observed in health records. This work outlines an interpretable approach for using data augmentation to identify and reduce biases in natural language processing pipelines.  © 2022 Copyright held by the owner/author(s).
PB  - Association for Computing Machinery
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 14
ER  -

TY  - JOUR
AU  - Afshar, M.
AU  - Sharma, B.
AU  - Dligach, D.
AU  - Oguss, M.
AU  - Brown, R.
AU  - Chhabra, N.
AU  - Thompson, H.M.
AU  - Markossian, T.
AU  - Joyce, C.
AU  - Churpek, M.M.
AU  - Karnik, N.S.
TI  - Development and multimodal validation of a substance misuse algorithm for referral to treatment using artificial intelligence (SMART-AI): a retrospective deep learning study
PY  - 2022
T2  - The Lancet Digital Health
VL  - 4
IS  - 6
SP  - e426
EP  - e435
DO  - 10.1016/S2589-7500(22)00041-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130558190&doi=10.1016%2fS2589-7500%2822%2900041-3&partnerID=40&md5=e459d18cf20e2f463a399ef88c754f00
AB  - Background: Substance misuse is a heterogeneous and complex set of behavioural conditions that are highly prevalent in hospital settings and frequently co-occur. Few hospital-wide solutions exist to comprehensively and reliably identify these conditions to prioritise care and guide treatment. The aim of this study was to apply natural language processing (NLP) to clinical notes collected in the electronic health record (EHR) to accurately screen for substance misuse. Methods: The model was trained and developed on a reference dataset derived from a hospital-wide programme at Rush University Medical Center (RUMC), Chicago, IL, USA, that used structured diagnostic interviews to manually screen admitted patients over 27 months (between Oct 1, 2017, and Dec 31, 2019; n=54 915). The Alcohol Use Disorder Identification Test and Drug Abuse Screening Tool served as reference standards. The first 24 h of notes in the EHR were mapped to standardised medical vocabulary and fed into single-label, multilabel, and multilabel with auxillary-task neural network models. Temporal validation of the model was done using data from the subsequent 12 months on a subset of RUMC patients (n=16 917). External validation was done using data from Loyola University Medical Center, Chicago, IL, USA between Jan 1, 2007, and Sept 30, 2017 (n=1991 adult patients). The primary outcome was discrimination for alcohol misuse, opioid misuse, or non-opioid drug misuse. Discrimination was assessed by the area under the receiver operating characteristic curve (AUROC). Calibration slope and intercept were measured with the unreliability index. Bias assessments were performed across demographic subgroups. Findings: The model was trained on a cohort that had 3·5% misuse (n=1 921) with any type of substance. 220 (11%) of 1921 patients with substance misuse had more than one type of misuse. The multilabel convolutional neural network classifier had a mean AUROC of 0·97 (95% CI 0·96–0·98) during temporal validation for all types of substance misuse. The model was well calibrated and showed good face validity with model features containing explicit mentions of aberrant drug-taking behaviour. A false-negative rate of 0·18–0·19 and a false-positive rate of 0·03 between non-Hispanic Black and non-Hispanic White groups occurred. In external validation, the AUROCs for alcohol and opioid misuse were 0·88 (95% CI 0·86–0·90) and 0·94 (0·92–0·95), respectively. Interpretation: We developed a novel and accurate approach to leveraging the first 24 h of EHR notes for screening multiple types of substance misuse. Funding: National Institute On Drug Abuse, National Institutes of Health. © 2022 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY-NC-ND 4.0 license
PB  - Elsevier Ltd
C2  - 35623797
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 27
ER  -

TY  - CONF
AU  - Kaur, G.
AU  - Kaur, J.
TI  - Detailed Investigation of the use of Text Mining and Natural Language Processing (NLPs) for Cancer Detection
PY  - 2023
T2  - Proceedings of the 2023 2nd International Conference on Augmented Intelligence and Sustainable Systems, ICAISS 2023
SP  - 1156
EP  - 1162
DO  - 10.1109/ICAISS58487.2023.10250682
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173625352&doi=10.1109%2fICAISS58487.2023.10250682&partnerID=40&md5=dae25c27c8bde723702dfbea271c4c4f
AB  - Clinical decision-making can be improved by analyzing patient characteristics and results. However, most medical data is only presented in narratively written reports, making efficient data collection and analysis impossible. Manual medical record evaluation requires a significant investment of time and labor, and clinical reviewers' differences might result in inconsistent data gathering, leading to biased study results and incorrect assumptions. Using natural language processing (NLP), medical data can automatically and deterministically be extracted from free-text clinical reports. Furthermore, it can potentially increase the pace and scope of clinical research. Comparing machine learning versus non-machine learning approaches, machine learning has proven more effective in solving many issues in various medical fields. This review explains how clinical note databases have been used to apply machine learning-based "Natural Language Processing (ML-NLP)". Examples of optimization algorithms are given to illustrate how straightforward and useful they are in the context of clinical note databases. A secondary data collection method is used for this research to gather relevant data and statistical data related to the research topic. © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Low, B.
AU  - Lavin, D.
AU  - Du, C.R.
AU  - Fang, C.
TI  - Risk-Informed and AI-Based Bias Detection on Gender, Race, and Income Using Gen-Z Survey Data
PY  - 2023
T2  - IEEE Access
VL  - 11
SP  - 88317
EP  - 88328
DO  - 10.1109/ACCESS.2023.3305636
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168276570&doi=10.1109%2fACCESS.2023.3305636&partnerID=40&md5=f356ce8c29a10606594aa839903f12f7
AB  - In the digital world, growing bias in diversity, equity, and inclusion (DEI) is increasingly influencing people's thoughts and behaviors. Though younger generations have a large presence on social media, there is minimal DEI research focusing on Generation Z (Gen-Z). This paper examines the DEI landscape through an online survey targeting the Gen-Z population. We analyzed 675 survey responses to train an AI-based model to assess both the existence and risk of gender, race, and income bias, attempting to foster a healthier online community. We built a Natural Language Processing-based risk assessment model using Word2Vec with a dimensionality reduction algorithm. We then trained the model with Gen-Z's vocabulary using the results from our survey. Our findings show that words traditionally associated with a particular gender are now being used to describe both genders (e.g., strong), and some gender-specific words are fading away (e.g., doll). This suggests that society is moving away from certain gender stereotypes. In the responses to the survey's race-related questions, all racial groups mentioned racial justice, reflecting Gen-Z's awareness of the racial issues that exist in today's society. Also, our K-means clustering algorithm identified social justice issues as a common theme across the gender, race, and income areas studied in our survey, highlighting Gen-Z's understanding of injustice in these areas of society. © 2013 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Yang, Z.
AU  - Zhang, C.
AU  - Wu, M.
AU  - Liu, X.C.
AU  - Jiang, L.Y.
AU  - Cho, K.
AU  - Oermann, E.K.
TI  - Intriguing Effect of the Correlation Prior on ICD-9 Code Assignment
PY  - 2023
T2  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
VL  - 4
SP  - 109
EP  - 118
DO  - 10.18653/v1/2023.acl-srw.19
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173471754&doi=10.18653%2fv1%2f2023.acl-srw.19&partnerID=40&md5=81b5b784a3d916a666fe5a1513fdc6dc
AB  - The Ninth Revision of the International Classification of Diseases (ICD-9) is a standardized coding system used to classify health conditions. It is used for billing, tracking individual patient conditions, and for epidemiology. The highly detailed and technical nature of the codes and their associated medical conditions make it difficult for humans to accurately record them. Researchers have explored the use of neural networks, particularly language models, for automated ICD-9 code assignment. However, the imbalanced distribution of ICD-9 codes leads to poor performance. One solution is to use domain knowledge to incorporate a useful prior. This paper evaluates the usefulness of the correlation bias: we hypothesize that correlations between ICD-9 codes and other medical codes could help improve language models' performance. We showed that while the correlation bias worsens the overall performance, the effect on individual class can be negative or positive. Performance on classes that are more imbalanced and less correlated with other codes is more sensitive to incorporating the correlation bias. This suggests that while the correlation bias has potential to improve ICD-9 code assignment in certain cases, the applicability criteria need to be more carefully studied. © 2023 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Arshad, H.B.
AU  - Butt, S.A.
AU  - Khan, S.U.
AU  - Javed, Z.
AU  - Nasir, K.
TI  - ChatGPT and Artificial Intelligence in Hospital Level Research: Potential, Precautions, and Prospects
PY  - 2023
T2  - Methodist DeBakey Cardiovascular Journal
VL  - 19
IS  - 5
SP  - 77
EP  - 84
DO  - 10.14797/mdcvj.1290
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178642342&doi=10.14797%2fmdcvj.1290&partnerID=40&md5=8c5e430d81dab91ec5ead9e2dfaff4cd
AB  - Rapid advancements in artificial intelligence (AI) have revolutionized numerous sectors, including medical research. Among the various AI tools, OpenAI’s ChatGPT, a state-of-the-art language model, has demonstrated immense potential in aiding and enhancing research processes. This review explores the application of ChatGPT in medical hospital level research, focusing on its capabilities for academic writing assistance, data analytics, statistics handling, and code generation. Notably, it delves into the model’s ability to streamline tasks, support decision making, and improve patient interaction. However, the article also underscores the importance of exercising caution while dealing with sensitive healthcare data and highlights the limitations of ChatGPT, such as its potential for erroneous outputs and biases. Furthermore, the review discusses the ethical considerations that arise with AI use in health care, including data privacy, AI interpretability, and the risk of AI-induced disparities. The article culminates by envisioning the future of AI in medical research, emphasizing the need for robust regulatory frameworks and guidelines that balance the potential of AI with ethical considerations. As AI continues to evolve, it holds promising potential to augment medical research in a manner that is ethical, equitable, and patient-centric. © 2023 The Author(s).
PB  - Houston Methodist Debakey Heart and Vascular Center
C2  - 38028967
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - JOUR
AU  - Li, J.
AU  - Luo, J.
AU  - Pavlov, I.
AU  - Perez, Y.
AU  - Tan, W.
AU  - Roca, O.
AU  - Tavernier, E.
AU  - Kharat, A.
AU  - McNicholas, B.
AU  - Ibarra-Estrada, M.
AU  - Vines, D.L.
AU  - Bosch, N.A.
AU  - Rampon, G.
AU  - Simpson, S.Q.
AU  - Walkey, A.J.
AU  - Fralick, M.
AU  - Verma, A.
AU  - Razak, F.
AU  - Harris, T.
AU  - Laffey, J.G.
AU  - Guerin, C.
AU  - Ehrmann, S.
AU  - Mirza, S.
AU  - Xue, L.
AU  - Pavord, I.D.
AU  - Plamondon, P.
AU  - Jayaraman, D.
AU  - Shahin, J.
AU  - Dahine, J.
AU  - Kulenkamp, A.
AU  - Pacheco, A.
TI  - Awake prone positioning for non-intubated patients with COVID-19-related acute hypoxaemic respiratory failure: a systematic review and meta-analysis
PY  - 2022
T2  - The Lancet Respiratory Medicine
VL  - 10
IS  - 6
SP  - 573
EP  - 583
DO  - 10.1016/S2213-2600(22)00043-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131593484&doi=10.1016%2fS2213-2600%2822%2900043-1&partnerID=40&md5=fe7e1509834e79ff40460d03156754c1
AB  - Background: Awake prone positioning has been broadly utilised for non-intubated patients with COVID-19-related acute hypoxaemic respiratory failure, but the results from published randomised controlled trials (RCTs) in the past year are contradictory. We aimed to systematically synthesise the outcomes associated with awake prone positioning, and evaluate these outcomes in relevant subpopulations. Methods: In this systematic review and meta-analysis, two independent groups of researchers searched MEDLINE, Embase, PubMed, Web of Science, Scopus, MedRxiv, BioRxiv, and ClinicalTrials.gov for RCTs and observational studies (with a control group) of awake prone positioning in patients with COVID-19-related acute hypoxaemic respiratory failure published in English from Jan 1, 2020, to Nov 8, 2021. We excluded trials that included patients intubated before or at enrolment, paediatric patients (ie, younger than 18 years), or trials that did not include the supine position in the control group. The same two independent groups screened studies, extracted the summary data from published reports, and assessed the risk of bias. We used a random-effects meta-analysis to pool individual studies. We used the Grading of Recommendations Assessment, Development, and Evaluation approach to assess the certainty and quality of the evidence. The primary outcome was the reported cumulative intubation risk across RCTs, and effect estimates were calculated as risk ratios (RR;95% CI). The analysis was primarily conducted on RCTs, and observational studies were used for sensitivity analyses. No serious adverse events associated with awake prone positioning were reported. The study protocol was prospectively registered with PROSPERO, CRD42021271285. Findings: A total of 1243 studies were identified, we assessed 138 full-text articles and received the aggregated results of three unpublished RCTs; therefore, after exclusions, 29 studies were included in the study. Ten were RCTs (1985 patients) and 19 were observational studies (2669 patients). In ten RCTs, awake prone positioning compared with the supine position significantly reduced the need for intubation in the overall population (RR 0·84 [95% CI 0·72–0·97]). A reduced need for intubation was shown among patients who received advanced respiratory support (ie, high-flow nasal cannula or non-invasive ventilation) at enrolment (RR 0·83 [0·71–0·97]) and in intensive care unit (ICU) settings (RR 0·83 [0·71–0·97]) but not in patients receiving conventional oxygen therapy (RR 0·87 [0·45–1·69]) or in non-ICU settings (RR 0·88 [0·44–1·76]). No obvious risk of bias and publication bias was found among the included RCTs for the primary outcome. Interpretation: In patients with COVID-19-related acute hypoxaemic respiratory failure, awake prone positioning reduced the need for intubation, particularly among those requiring advanced respiratory support and those in ICU settings. Awake prone positioning should be used in patients who have acute hypoxaemic respiratory failure due to COVID-19 and require advanced respiratory support or are treated in the ICU. Funding: OpenAI, Rice Foundation, National Institute for Health Research, and Oxford Biomedical Research Centre. © 2022 Elsevier Ltd
PB  - Elsevier Ltd
C2  - 35305308
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 115
ER  -

TY  - CONF
AU  - Kim, M.Y.
AU  - Kim, J.
AU  - Johnson, K.M.
TI  - Race, Gender, and Age Biases in Biomedical Masked Language Models
PY  - 2023
T2  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
SP  - 11806
EP  - 11815
DO  - 10.18653/v1/2023.findings-acl.749
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175418038&doi=10.18653%2fv1%2f2023.findings-acl.749&partnerID=40&md5=367f03d6481ac95777b2e629c324ffd0
AB  - Biases cause discrepancies in healthcare services. Race, gender, and age of a patient affect interactions with physicians and the medical treatments one receives. These biases in clinical practices can be amplified following the release of pre-trained language models trained on biomedical corpora. To bring awareness to such repercussions, we examine social biases present in the biomedical masked language models. We curate prompts based on evidence-based practice and compare generated diagnoses based on biases. For a case study, we measure bias in diagnosing coronary artery disease and using cardiovascular procedures based on bias. Our study demonstrates that biomedical models are less biased than BERT in gender, while the opposite is true for race and age. © 2023 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Tong, W.
AU  - Guan, Y.
AU  - Chen, J.
AU  - Huang, X.
AU  - Zhong, Y.
AU  - Zhang, C.
AU  - Zhang, H.
TI  - Artificial intelligence in global health equity: an evaluation and discussion on the application of ChatGPT, in the Chinese National Medical Licensing Examination
PY  - 2023
T2  - Frontiers in Medicine
VL  - 10
C7  - 1237432
DO  - 10.3389/fmed.2023.1237432
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175690719&doi=10.3389%2ffmed.2023.1237432&partnerID=40&md5=ef2e18a46c8189fe33d607e6d1e8e877
AB  - Background: The demand for healthcare is increasing globally, with notable disparities in access to resources, especially in Asia, Africa, and Latin America. The rapid development of Artificial Intelligence (AI) technologies, such as OpenAI’s ChatGPT, has shown promise in revolutionizing healthcare. However, potential challenges, including the need for specialized medical training, privacy concerns, and language bias, require attention. Methods: To assess the applicability and limitations of ChatGPT in Chinese and English settings, we designed an experiment evaluating its performance in the 2022 National Medical Licensing Examination (NMLE) in China. For a standardized evaluation, we used the comprehensive written part of the NMLE, translated into English by a bilingual expert. All questions were input into ChatGPT, which provided answers and reasons for choosing them. Responses were evaluated for “information quality” using the Likert scale. Results: ChatGPT demonstrated a correct response rate of 81.25% for Chinese and 86.25% for English questions. Logistic regression analysis showed that neither the difficulty nor the subject matter of the questions was a significant factor in AI errors. The Brier Scores, indicating predictive accuracy, were 0.19 for Chinese and 0.14 for English, indicating good predictive performance. The average quality score for English responses was excellent (4.43 point), slightly higher than for Chinese (4.34 point). Conclusion: While AI language models like ChatGPT show promise for global healthcare, language bias is a key challenge. Ensuring that such technologies are robustly trained and sensitive to multiple languages and cultures is vital. Further research into AI’s role in healthcare, particularly in areas with limited resources, is warranted. Copyright © 2023 Tong, Guan, Chen, Huang, Zhong, Zhang and Zhang.
PB  - Frontiers Media SA
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - Topaz, M.
AU  - Song, J.
AU  - Davoudi, A.
AU  - McDonald, M.
AU  - Taylor, J.
AU  - Sittig, S.
AU  - Bowles, K.
TI  - Home Health Care Clinicians’ Use of Judgment Language for Black and Hispanic Patients: Natural Language Processing Study
PY  - 2023
T2  - JMIR Nursing
VL  - 6
IS  - 1
C7  - e42552
DO  - 10.2196/42552
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171577913&doi=10.2196%2f42552&partnerID=40&md5=20ed98745f6690e1555da4849aa2e0ef
AB  - Background: A clinician’s biased behavior toward patients can affect the quality of care. Recent literature reviews report on widespread implicit biases among clinicians. Although emerging studies in hospital settings show racial biases in the language used in clinical documentation within electronic health records, no studies have yet investigated the extent of judgment language in home health care. Objective: We aimed to examine racial differences in judgment language use and the relationship between judgment language use and the amount of time clinicians spent on home visits as a reflection of care quality in home health care. Methods: This study is a retrospective observational cohort study. Study data were extracted from a large urban home health care organization in the Northeastern United States. Study data set included patients (N=45,384) who received home health care services between January 1 and December 31, 2019. The study applied a natural language processing algorithm to automatically detect the language of judgment in clinical notes. Results: The use of judgment language was observed in 38% (n=17,141) of the patients. The highest use of judgment language was found in Hispanic (7,167/66,282, 10.8% of all clinical notes), followed by Black (7,010/65,628, 10.7%), White (10,206/107,626, 9.5%), and Asian (1,756/22,548, 7.8%) patients. Black and Hispanic patients were 14% more likely to have notes with judgment language than White patients. The length of a home health care visit was reduced by 21 minutes when judgment language was used. Conclusions: Racial differences were identified in judgment language use. When judgment language is used, clinicians spend less time at patients’ homes. Because the language clinicians use in documentation is associated with the time spent providing care, further research is needed to study the impact of using judgment language on quality of home health care. Policy, education, and clinical practice improvements are needed to address the biases behind judgment language. © Maxim Topaz, Jiyoun Song, Anahita Davoudi, Margaret McDonald, Jacquelyn Taylor, Scott Sittig, Kathryn Bowles.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Ganguly, R.
AU  - Singh, D.
TI  - Explainable Artificial Intelligence (XAI) for the Prediction of Diabetes Management: An Ensemble Approach
PY  - 2023
T2  - International Journal of Advanced Computer Science and Applications
VL  - 14
IS  - 7
SP  - 158
EP  - 163
DO  - 10.14569/IJACSA.2023.0140717
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168808138&doi=10.14569%2fIJACSA.2023.0140717&partnerID=40&md5=876d01980dcb7bd612409a08a4cfe2bc
AB  - Machine learning determines patterns from data to expedite the process of decision making. Fact-based decisions and data-driven decisions are specified by the industry specialist. Due to the continuous growth of machine language models in healthcare, they are breeding continuous complexity and black boxes in ML models. To make the ML model crystal clear and authentically explainable, AI accession came in prevalence. This research scrutinizes the explainable AI and capabilities in the Indian healthcare system to detect diabetes. LIME and SHAP are two libraries and packages that are used to implement explainable AI. The intimated base amalgamates the local and global interpretable methods, which enhances the crystallinity of the complex model and obtains intuition into the equity from the complex model. Moreover, the obtained intuition could also boost clinical data scientists to plan a more felicitous composition of computer-aided diagnosis. Importance of XAI to forecast stubborn disease. In this case, of stubborn diabetes, the correlation between plasma versus insulin, age versus pregnancies, class (diabetic and nondiabetic) versus plasma glucose persisted with a strong relationship. The PIDD (PIMA Indian Diabetic Data set) with the SHAP value is used for concise dependency, and LIME is applicable when anchors and importance of features are both required simultaneously. Dependency plots help physicians visualize independent relationships with predicted disease. To identify dependencies of different attributes, a correlation heatmap is used. From an academic perspective, XAI is very indispensable to mature in the near future. To estimate the presentation of other applicable data set correspondence studies are very much apprenticed. © 2023, Science and Information Organization. All Rights Reserved.
PB  - Science and Information Organization
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Ramadi, K.B.
AU  - Mehta, R.
AU  - He, D.
AU  - Chao, S.
AU  - Chu, Z.
AU  - Atun, R.
AU  - Nguyen, F.T.
TI  - Grass-roots entrepreneurship complements traditional top-down innovation in lung and breast cancer
PY  - 2022
T2  - npj Digital Medicine
VL  - 5
IS  - 1
C7  - 10
DO  - 10.1038/s41746-021-00545-x
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123449036&doi=10.1038%2fs41746-021-00545-x&partnerID=40&md5=fe19b27c16eecd34df7513ff8a942d93
AB  - The majority of biomedical research is funded by public, governmental, and philanthropic grants. These initiatives often shape the avenues and scope of research across disease areas. However, the prioritization of disease-specific funding is not always reflective of the health and social burden of each disease. We identify a prioritization disparity between lung and breast cancers, whereby lung cancer contributes to a substantially higher socioeconomic cost on society yet receives significantly less funding than breast cancer. Using search engine results and natural language processing (NLP) of Twitter tweets, we show that this disparity correlates with enhanced public awareness and positive sentiment for breast cancer. Interestingly, disease-specific venture activity does not correlate with funding or public opinion. We use outcomes from recent early-stage innovation events focused on lung cancer to highlight the complementary mechanism by which bottom-up “grass-roots” initiatives can identify and tackle under-prioritized conditions. © 2022, The Author(s).
PB  - Nature Research
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Welch, D.-M.
TI  - The Impact of AI on Sexual Health Knowledge, Attitude and Behaviour
PY  - 2023
T2  - IET Conference Proceedings
VL  - 2023
IS  - 14
SP  - 190
EP  - 197
DO  - 10.1049/icp.2023.2613
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178570140&doi=10.1049%2ficp.2023.2613&partnerID=40&md5=c2eb37a8662a4f88b6c149aa7444b5d4
AB  - This paper explores the role of Artificial Intelligence (AI) in influencing knowledge, attitudes, and behaviour in sexual health. In recent years, the capability and application of AI have grown exponentially. AI has recently gained wider public attention due to the so-called democratisation of AI, such as the public accessibility of tools in the form of ChatGPT. With new evolving technologies, there is always the possibility for them to influence us both positively and negatively. There is a plethora of evidence and research analysing the development and application of AI, with a particular focus on behaviour, attitudes, and knowledge, especially within the field of health. However, there is a limited exploration of the impact of AI on sexual health knowledge, attitude, and behaviour. It has been recently widely documented that the UK is on the brink of a sexual health crisis, resulting in an increase in risky sexual behaviour and poorer sexual health outcomes, according to the Sex Education Forum (SEF). Sexual health provisions continue to face increased financial pressures, with face-to-face services being dramatically reduced along with a stalled and inconsistent rollout of compulsory Relationship and Sex Education (RSE) resulting in a disparity in sexual health knowledge impacting behaviour and attitudes. To improve sexual health outcomes, there is scope to consider a new approach that incorporates the inclusion of AI. The paper discusses the potential for positive and negative AI influences on sexual health knowledge, attitude, and behaviour, identifies current gaps in sexual health and how these could be addressed, and presents future directions for research. © The Institution of Engineering & Technology 2023.
PB  - Institution of Engineering and Technology
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Kaleybar, J.M.
AU  - Khaloo, H.
AU  - Naghipour, A.
TI  - Efficient Vision Transformer for Accurate Traffic Sign Detection
PY  - 2023
T2  - 2023 13th International Conference on Computer and Knowledge Engineering, ICCKE 2023
SP  - 36
EP  - 41
DO  - 10.1109/ICCKE60553.2023.10326242
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179757374&doi=10.1109%2fICCKE60553.2023.10326242&partnerID=40&md5=7ddc5894af9242fa540174bcccaccda6
AB  - This research paper addresses the challenges associated with traffic sign detection in self-driving vehicles and driver assistance systems. The development of reliable and highly accurate algorithms is crucial for the widespread adoption of traffic sign recognition and detection (TSRD) in diverse real-life scenarios. However, this task is complicated by suboptimal traffic images affected by factors such as camera movement, adverse weather conditions, and inadequate lighting. This study specifically focuses on traffic sign detection methods and introduces the application of the Transformer model, particularly the Vision Transformer variants, to tackle this task. The Transformer's attention mechanism, originally designed for natural language processing, offers improved parallel efficiency. Vision Transformers have demonstrated success in various domains, including autonomous driving, object detection, healthcare, and defense-related applications. To enhance the efficiency of the Transformer model, the research proposes a novel strategy that integrates a locality inductive bias and a transformer module. This includes the introduction of the Efficient Convolution Block and the Local Transformer Block, which effectively capture short-term and long-term dependency information, thereby improving both detection speed and accuracy. Experimental evaluations demonstrate the significant advancements achieved by this approach, particularly when applied to the GTSDB dataset.  © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Banabilah, S.
AU  - Aloqaily, M.
AU  - Alsayed, E.
AU  - Malik, N.
AU  - Jararweh, Y.
TI  - Federated learning review: Fundamentals, enabling technologies, and future applications
PY  - 2022
T2  - Information Processing and Management
VL  - 59
IS  - 6
C7  - 103061
DO  - 10.1016/j.ipm.2022.103061
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136606514&doi=10.1016%2fj.ipm.2022.103061&partnerID=40&md5=5244fdba096f23260dda11f14cb60009
AB  - Federated Learning (FL) has been foundational in improving the performance of a wide range of applications since it was first introduced by Google. Some of the most prominent and commonly used FL-powered applications are Android's Gboard for predictive text and Google Assistant. FL can be defined as a setting that makes on-device, collaborative Machine Learning possible. A wide range of literature has studied FL technical considerations, frameworks, and limitations with several works presenting a survey of the prominent literature on FL. However, prior surveys have focused on technical considerations and challenges of FL, and there has been a limitation in more recent work that presents a comprehensive overview of the status and future trends of FL in applications and markets. In this survey, we introduce the basic fundamentals of FL, describing its underlying technologies, architectures, system challenges, and privacy-preserving methods. More importantly, the contribution of this work is in scoping a wide variety of FL current applications and future trends in technology and markets today. We present a classification and clustering of literature progress in FL in application to technologies including Artificial Intelligence, Internet of Things, blockchain, Natural Language Processing, autonomous vehicles, and resource allocation, as well as in application to market use cases in domains of Data Science, healthcare, education, and industry. We discuss future open directions and challenges in FL within recommendation engines, autonomous vehicles, IoT, battery management, privacy, fairness, personalization, and the role of FL for governments and public sectors. By presenting a comprehensive review of the status and prospects of FL, this work serves as a reference point for researchers and practitioners to explore FL applications under a wide range of domains. © 2022 Elsevier Ltd
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 301
ER  -

TY  - JOUR
AU  - Li, Z.
AU  - Kormilitzin, A.
AU  - Fernandes, M.
AU  - Vaci, N.
AU  - Liu, Q.
AU  - Newby, D.
AU  - Goodday, S.
AU  - Smith, T.
AU  - Nevado-Holgado, A.J.
AU  - Winchester, L.
TI  - Validation of UK Biobank data for mental health outcomes: A pilot study using secondary care electronic health records
PY  - 2022
T2  - International Journal of Medical Informatics
VL  - 160
C7  - 104704
DO  - 10.1016/j.ijmedinf.2022.104704
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124408917&doi=10.1016%2fj.ijmedinf.2022.104704&partnerID=40&md5=503802b9382b60ad92b61499e01c6ddf
AB  - UK Biobank (UKB) is widely employed to investigate mental health disorders and related exposures; however, its applicability and relevance in a clinical setting and the assumptions required have not been sufficiently and systematically investigated. Here, we present the first validation study using secondary care mental health data with linkage to UKB from Oxford - Clinical Record Interactive Search (CRIS) focusing on comparison of demographic information, diagnostic outcome, medication record and cognitive test results, with missing data and the implied bias from both resources depicted. We applied a natural language processing model to extract information embedded in unstructured text from clinical notes and attachments. Using a contingency table we compared the demographic information recorded in UKB and CRIS. We calculated the positive predictive value (PPV, proportion of true positives cases detected) for mental health diagnosis and relevant medication. Amongst the cohort of 854 subjects, PPVs for any mental health diagnosis for dementia, depression, bipolar disorder and schizophrenia were 41.6%, and were 59.5%, 12.5%, 50.0% and 52.6%, respectively. Self-reported medication records in UKB had general PPV of 47.0%, with the prevalence of frequently prescribed medicines to each typical mental health disorder considerably different from the information provided by CRIS. UKB is highly multimodal, but with limited follow-up records, whereas CRIS offers a longitudinal high-resolution clinical picture with more than ten years of observations. The linkage of both datasets will reduce the self-report bias and synergistically augment diverse modalities into a unified resource to facilitate more robust research in mental health. © 2022 The Authors
PB  - Elsevier Ireland Ltd
C2  - 35168089
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 11
ER  -

TY  - JOUR
AU  - Yeomans, M.
TI  - The straw man effect: Partisan misrepresentation in natural language
PY  - 2022
T2  - Group Processes and Intergroup Relations
VL  - 25
IS  - 7
SP  - 1905
EP  - 1924
DO  - 10.1177/13684302211014582
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111056797&doi=10.1177%2f13684302211014582&partnerID=40&md5=413145a4528a8646dd58588d65653051
AB  - Political discourse often seems divided not just by different preferences, but by entirely different representations of the debate. Are partisans able to accurately describe their opponents’ position, or do they instead generate unrepresentative “straw man” arguments? In this research we examined an (incentivized) political imitation game by asking partisans on both sides of the U.S. health care debate to describe the most common arguments for and against ObamaCare. We used natural language-processing algorithms to benchmark the biases and blind spots of our participants. Overall, partisans showed a limited ability to simulate their opponents’ perspective, or to distinguish genuine from imitation arguments. In general, imitations were less extreme than their genuine counterparts. Individual difference analyses suggest that political sophistication only improves the representations of one’s own side but not of an opponent’s side, exacerbating the straw man effect. Our findings suggest that false beliefs about partisan opponents may be pervasive. © The Author(s) 2021.
PB  - SAGE Publications Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Ali, H.
AU  - Qadir, J.
AU  - Alam, T.
AU  - Househ, M.
AU  - Shah, Z.
TI  - ChatGPT and Large Language Models in Healthcare: Opportunities and Risks
PY  - 2023
T2  - 2023 IEEE International Conference on Artificial Intelligence, Blockchain, and Internet of Things, AIBThings 2023 - Proceedings
DO  - 10.1109/AIBThings58340.2023.10291020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178510215&doi=10.1109%2fAIBThings58340.2023.10291020&partnerID=40&md5=ac2a865199a23e8348a16527d330c337
AB  - ChatGPT, a pre-Trained large language model (LLM), has the potential to transform healthcare by providing valid clinical insights and reducing doctors' workload. There are already signs that such tools can be useful for automating the generation of patient discharge reports, clinical vignettes, and radiology reports. Such tools can also capture the vast medical knowledge base as demonstrated by ChatGPT clearing the United States Medical Licensing Examination (USMLE). Such tools promise to make healthcare more accessible, scalable, and efficient, leading to better patient outcomes. However, such tools are far from perfect and well-known to be susceptible to error, misinformation, and bias. In this paper, we review the potential applications of ChatGPT in healthcare and also identify potentials risks that must be addressed before ChatGPT and other LLM tools can be safely adopted in healthcare. First, we offer case studies on using ChatGPT for passing USMLE, identifying prevention methods for cardiovascular disease, generating patient discharge reports, generating clinical vignettes, and generating radiology reports. Second, we present the opportunities that ChatGPT offers in healthcare. By leveraging its language generation and processing capabilities, ChatGPT can streamline and improve a range of healthcare tasks, from digitizing clinical notes and improving the accuracy of diagnosis to revolutionizing medical education and empowering patients with personalized healthcare information. Finally, we reflect on the associated risks and conclude that caution is advised in interpreting the results of ChatGPT as these studies are preliminary and not entirely error-free.  © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Zhu, Y.
AU  - Wang, J.
AU  - Meng, B.
AU  - Ji, H.
AU  - Wang, S.
AU  - Zhi, G.
AU  - Liu, J.
AU  - Shi, C.
TI  - Quantifying Spatiotemporal Heterogeneities in PM2.5-Related Health and Associated Determinants Using Geospatial Big Data: A Case Study in Beijing
PY  - 2022
T2  - Remote Sensing
VL  - 14
IS  - 16
C7  - 4012
DO  - 10.3390/rs14164012
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137802222&doi=10.3390%2frs14164012&partnerID=40&md5=fe0f8daa29cbdaaa6524edc4994be820
AB  - Air pollution has brought about serious challenges to public health. With the limitations of available data, previous studies overlooked spatiotemporal heterogeneities in PM2.5-related health (PM2.5-RH) and multiple associated factors at the subdistrict scale. In this research, social media Weibo data was employed to extract PM2.5-RH based on the Bidirectional Encoder Representations from Transformers (BERT) model, in Beijing, China. Then, the relationship between PM2.5-RH and eight associated factors was qualified based on multi-source geospatial big data using Geographically Weighted Regression (GWR) models. The results indicate that the PM2.5-RH in the study area showed a spatial pattern of agglomeration to the city center and seasonal variation in the spatially non-stationary effects. The impacts of varied factors on PM2.5-RH were also spatiotemporally heterogeneous. Specifically, nighttime light (NTL), population density (PD) and the normalized difference built-up index (NDBI) had outstanding effects on PM2.5-RH in the four seasons, but with spatial disparities. The impact of the normalized difference vegetation index (NDVI) on PM2.5-RH was significant in summer, especially in the central urban areas, while in winter, the contribution of the air quality index (AQI) was increased. This research further demonstrates the feasibility of using social media data to indicate the effect of air pollution on public health and provides new insights into the seasonal impacts of associated driving factors on the health effects of air pollution. © 2022 by the authors.
PB  - MDPI
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - CONF
AU  - Youn, J.
AU  - Bowen, A.
TI  - PEARC '22: Practice and Experience in Advanced Research Computing Proceedings
PY  - 2022
T2  - PEARC 2022 Conference Series - Practice and Experience in Advanced Research Computing 2022 - Revolutionary: Computing, Connections, You
C7  - 91
DO  - 10.1145/3491418.3535185
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135232943&doi=10.1145%2f3491418.3535185&partnerID=40&md5=3f4fa078be928836260bfad082ab1523
AB  - The growing prevalence of online hate speech is concerning, given the massive growth of online platforms. Hate speech is defined as language that attacks, humiliates, or incites violence against specific groups. According to research, there is a link between online hate speech and real-world crimes, as well as victims' deteriorating mental health. To combat the online prevalence of abusive speech, hate speech detection models based on machine learning and natural language processing are being developed to automatically detect the toxicity of online content. However, current models tend to mislabel African American English (AAE) text as hate speech at a significantly higher rate than texts written in Standard American English (SAE). To confirm the existence of systematic racism within these models, I evaluate a logical regression model and a BERT model. Then, I determine the efficacy of the bias reduction method for the BERT model and the correlation between model performance and reduced bias. © 2022 Owner/Author.
PB  - Association for Computing Machinery, Inc
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Khurshid, S.
AU  - Reeder, C.
AU  - Harrington, L.X.
AU  - Singh, P.
AU  - Sarma, G.
AU  - Friedman, S.F.
AU  - Di Achille, P.
AU  - Diamant, N.
AU  - Cunningham, J.W.
AU  - Turner, A.C.
AU  - Lau, E.S.
AU  - Haimovich, J.S.
AU  - Al-Alusi, M.A.
AU  - Wang, X.
AU  - Klarqvist, M.D.R.
AU  - Ashburner, J.M.
AU  - Diedrich, C.
AU  - Ghadessi, M.
AU  - Mielke, J.
AU  - Eilken, H.M.
AU  - McElhinney, A.
AU  - Derix, A.
AU  - Atlas, S.J.
AU  - Ellinor, P.T.
AU  - Philippakis, A.A.
AU  - Anderson, C.D.
AU  - Ho, J.E.
AU  - Batra, P.
AU  - Lubitz, S.A.
TI  - Cohort design and natural language processing to reduce bias in electronic health records research
PY  - 2022
T2  - npj Digital Medicine
VL  - 5
IS  - 1
C7  - 47
DO  - 10.1038/s41746-022-00590-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128208489&doi=10.1038%2fs41746-022-00590-0&partnerID=40&md5=f2850a1e4fe20e09bd471e673d1c0049
AB  - Electronic health record (EHR) datasets are statistically powerful but are subject to ascertainment bias and missingness. Using the Mass General Brigham multi-institutional EHR, we approximated a community-based cohort by sampling patients receiving longitudinal primary care between 2001-2018 (Community Care Cohort Project [C3PO], n = 520,868). We utilized natural language processing (NLP) to recover vital signs from unstructured notes. We assessed the validity of C3PO by deploying established risk models for myocardial infarction/stroke and atrial fibrillation. We then compared C3PO to Convenience Samples including all individuals from the same EHR with complete data, but without a longitudinal primary care requirement. NLP reduced the missingness of vital signs by 31%. NLP-recovered vital signs were highly correlated with values derived from structured fields (Pearson r range 0.95–0.99). Atrial fibrillation and myocardial infarction/stroke incidence were lower and risk models were better calibrated in C3PO as opposed to the Convenience Samples (calibration error range for myocardial infarction/stroke: 0.012–0.030 in C3PO vs. 0.028–0.046 in Convenience Samples; calibration error for atrial fibrillation 0.028 in C3PO vs. 0.036 in Convenience Samples). Sampling patients receiving regular primary care and using NLP to recover missing data may reduce bias and maximize generalizability of EHR research. © 2022, The Author(s).
PB  - Nature Research
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 47
ER  -

TY  - CONF
AU  - Noaeen, M.
AU  - Amini, S.
AU  - Bhasker, S.
AU  - Ghezelsefli, Z.
AU  - Ahmed, A.
AU  - Jafarinezhad, O.
AU  - Abad, Z.S.H.
TI  - Unlocking the Power of EHRs: Harnessing Unstructured Data for Machine Learning-based Outcome Predictions
PY  - 2023
T2  - Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS
DO  - 10.1109/EMBC40787.2023.10340232
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179647916&doi=10.1109%2fEMBC40787.2023.10340232&partnerID=40&md5=5e1eb8d4c6f5ea6a5c5032b1ba6c57de
AB  - The integration of Electronic Health Records (EHRs) with Machine Learning (ML) models has become imperative in examining patient outcomes due to the vast amounts of clinical data they provide. However, critical information regarding social and behavioral factors that affect health, such as social isolation, stress, and mental health complexities, is often recorded in unstructured clinical notes, hindering its accessibility. This has resulted in an over-reliance on clinical data in current EHR-based research, potentially leading to disparities in health outcomes. This study aims to evaluate the impact of incorporating patient-specific context from unstructured EHR data on the accuracy and stability of ML algorithms for predicting mortality, using the MIMIC III database. Results from the study confirmed the significance of incorporating patient-specific information into prediction models, leading to a notable improvement in the discriminatory power and robustness of the ML algorithms. Furthermore, the findings underline the importance of considering non-clinical factors related to a patient's daily life, in addition to clinical factors, when making predictions about patient outcomes. The advent of advanced generative models, such as GPT-4, presents new opportunities for effectively extracting social and behavioral factors from unstructured clinical notes, further enhancing the accuracy and stability of ML algorithms in predicting patient outcomes. The results of our study have significant ramifications for improving ML in clinical decision support and patient outcome predictions, specifically highlighting the potential role of generative models like GPT-4 in advancing ML-based outcome predictions. © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
C2  - 38083058
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - LeBaron, V.
AU  - Flickinger, T.
AU  - Ling, D.
AU  - Lee, H.
AU  - Edwards, J.
AU  - Tewari, A.
AU  - Wang, Z.
AU  - Barnes, L.E.
TI  - Feasibility and acceptability testing of CommSense: A novel communication technology to enhance health equity in clinician–patient interactions
PY  - 2023
T2  - Digital Health
VL  - 9
DO  - 10.1177/20552076231184991
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164658745&doi=10.1177%2f20552076231184991&partnerID=40&md5=5673c063b0076b38731dabaf35ce2ca7
AB  - Background: Quality patient–clinician communication is paramount to achieving safe and compassionate healthcare, but evaluating communication performance during real clinical encounters is challenging. Technology offers novel opportunities to provide clinicians with actionable feedback to enhance their communication skills. Methods: This pilot study evaluated the acceptability and feasibility of CommSense, a novel natural language processing (NLP) application designed to record and extract key metrics of communication performance and provide real-time feedback to clinicians. Metrics of communication performance were established from a review of the literature and technical feasibility verified. CommSense was deployed on a wearable (smartwatch), and participants were recruited from an academic medical center to test the technology. Participants completed a survey about their experience; results were exported to SPSS (v.28.0) for descriptive analysis. Results: Forty (n = 40) healthcare participants (nursing students, medical students, nurses, and physicians) pilot tested CommSense. Over 90% of participants “strongly agreed” or “agreed” that CommSense could improve compassionate communication (n = 38, 95%) and help healthcare organizations deliver high-quality care (n = 39, 97.5%). Most participants (n = 37, 92.5%) “strongly agreed” or “agreed” they would be willing to use CommSense in the future; 100% (n = 40) “strongly agreed” or “agreed” they were interested in seeing information analyzed by CommSense about their communication performance. Metrics of most interest were medical jargon, interruptions, and speech dominance. Conclusion: Participants perceived significant benefits of CommSense to track and improve communication skills. Future work will deploy CommSense in the clinical setting with a more diverse group of participants, validate data fidelity, and explore optimal ways to share data analyzed by CommSense with end-users. © The Author(s) 2023.
PB  - SAGE Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Markowitz, D.M.
TI  - Gender and ethnicity bias in medicine: a text analysis of 1.8 million critical care records
PY  - 2022
T2  - PNAS Nexus
VL  - 1
IS  - 4
C7  - pgac157
DO  - 10.1093/pnasnexus/pgac157
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150722595&doi=10.1093%2fpnasnexus%2fpgac157&partnerID=40&md5=0d463e74c18c2c66f47beb36b1cbd1ce
AB  - Gender and ethnicity biases are pervasive across many societal domains including politics, employment, and medicine. Such biases will facilitate inequalities until they are revealed and mitigated at scale. To this end, over 1.8 million caregiver notes (502 million words) from a large US hospital were evaluated with natural language processing techniques in search of gender and ethnicity bias indicators. Consistent with nonlinguistic evidence of bias in medicine, physicians focused more on the emotions of women compared to men and focused more on the scientific and bodily diagnoses of men compared to women. Content patterns were relatively consistent across genders. Physicians also attended to fewer emotions for Black/African and Asian patients compared to White patients, and physicians demonstrated the greatest need to work through diagnoses for Black/African women compared to other patients. Content disparities were clearer across ethnicities, as physicians focused less on the pain of Black/African and Asian patients compared to White patients in their critical care notes. This research provides evidence of gender and ethnicity biases in medicine as communicated by physicians in the field and requires the critical examination of institutions that perpetuate bias in social systems.  © The Author(s) 2022. Published by Oxford University Press on behalf of the National Academy of Sciences.
PB  - National Academy of Sciences
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 21
ER  -

TY  - CONF
AU  - Tataei Sarshar, N.
AU  - Ranjbarzadeh, R.
AU  - Jafarzadeh Ghoushchi, S.
AU  - de Oliveira, G.G.
AU  - Anari, S.
AU  - Parhizkar, M.
AU  - Bendechache, M.
TI  - Glioma Brain Tumor Segmentation in Four MRI Modalities Using a Convolutional Neural Network and Based on a Transfer Learning Method
PY  - 2023
T2  - Smart Innovation, Systems and Technologies
VL  - 207 SIST
SP  - 386
EP  - 402
DO  - 10.1007/978-3-031-04435-9_39
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135014652&doi=10.1007%2f978-3-031-04435-9_39&partnerID=40&md5=41c41c7eea7cf78550c80c3be8c122f3
AB  - Accurate segmentation of brain tumors from magnetic resonance imaging (MRI) images is still a challenging task for many applications in the medical domain. To gain a better segmentation outcome, it is vital to employ more information from brain tissue, which is available in different modalities. So, in this study, to segment brain tumors in MRI images, a new four different feature extraction Convolutional Neural Network (CNN) architecture is utilized. Also, we used four image modalities, including T1, T2, T1-c, and FLAIR. Weights and biases from a ResNet-50 network were imported to our structure to increase the segmentation accuracy. We designed two main building blocks to extract additional features from each modality. Our model uses both local and global features by defining two different sizes of patches. The proposed framework was evaluated on the BRATS 2018 dataset and showed that our architecture has a competitive dice score (0.9223, 0.8993, and 0.9211 for Enhanced, Whole, and Core tumor areas, respectively). © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 20
ER  -

TY  - JOUR
AU  - Hadley, E.
AU  - Marcial, L.H.
AU  - Quattrone, W.
AU  - Bobashev, G.
TI  - Text Analysis of Trends in Health Equity and Disparities From the Internal Revenue Service Tax Documentation Submitted by US Nonprofit Hospitals Between 2010 and 2019: Exploratory Study
PY  - 2023
T2  - Journal of Medical Internet Research
VL  - 25
C7  - e44330
DO  - 10.2196/44330
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160210292&doi=10.2196%2f44330&partnerID=40&md5=56be967aa5bbc0e06e54bf836fbf40a8
AB  - Background: Many US hospitals are classified as nonprofits and receive tax-exempt status partially in exchange for providing benefits to the community. Proof of compliance is collected with the Schedule H form submitted as part of the annual Internal Revenue Service Form 990 (F990H), including a free-response text section that is known for being ambiguous and difficult to audit. This research is among the first to use natural language processing approaches to evaluate this text section with a focus on health equity and disparities. Objective: This study aims to determine the extent to which the free-response text in F990H reveals how nonprofit hospitals address health equity and disparities, including alignment with public priorities. Methods: We used free-response text submitted by hospital reporting entities in Part V and VI of the Internal Revenue Service Form 990 Schedule H between 2010 and 2019. We identified 29 main themes connected to health equity and disparities, and 152 related key phrases. We tallied occurrences of these phrases through term frequency analysis, calculated the Moran I statistic to assess geographic variation in 2018, analyzed Google Trends use for the same terms during the same period, and used semantic search with Sentence-BERT in Python to understand contextual use. Results: We found increased use from 2010 to 2019 across all the 29 phrase themes related to health equity and disparities. More than 90% of hospital reporting entities used terms in 2018 and 2019 related to affordability (2018: 2117/2131, 99.34%; 2019: 1620/1627, 99.57%), government organizations (2018: 2053/2131, 96.33%; 2019: 1577/1627, 96.93%), mental health (2018: 1937/2131, 90.9%; 2019: 1517/1627, 93.24%), and data collection (2018: 1947/2131, 91.37%; 2019: 1502/1627, 92.32%). The themes with the largest relative increase were LGBTQ (lesbian, gay, bisexual, transgender, and queer; 1676%; 2010: 12/2328, 0.51%; 2019: 149/1627, 9.16%) and social determinants of health (958%; 2010: 68/2328, 2.92%; 2019: 503/1627, 30.92%). Terms related to homelessness varied geographically from 2010 to 2018, and terms related to equity, health IT, immigration, LGBTQ, oral health, rural, social determinants of health, and substance use showed statistically significant (P < .05) geographic variation in 2018. The largest percentage point increase was for terms related to substance use (2010: 403/2328, 17.31%; 2019: 1149/1627, 70.62%). However, use in themes such as LGBTQ, disability, oral health, and race and ethnicity ranked lower than public interest in these topics, and some increased mentions of themes were to explicitly say that no action was taken. Conclusions: Hospital reporting entities demonstrate an increasing awareness of health equity and disparities in community benefit tax documentation, but these do not necessarily correspond with general population interests or additional action. We propose further investigation of alignment with community health needs assessments and make suggestions for improvements to F990H reporting requirements. © Emily Hadley, Laura Haak Marcial, Wes Quattrone, Georgiy Bobashev. Originally published in the Journal of Medical Internet Research (https://www.jmir.org), 24.05.2023. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on https://www.jmir.org/, as well as this copyright and license information must be included.
PB  - JMIR Publications Inc.
C2  - 37223985
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Göllner, S.
AU  - Tropmann-Frick, M.
TI  - Bridging the Gap between Theory and Practice: Towards Responsible AI Evaluation
PY  - 2023
T2  - CEUR Workshop Proceedings
VL  - 3580
SP  - 68
EP  - 76
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180011251&partnerID=40&md5=1c9837f017f11c2e9e1148c14930db5b
AB  - The growing integration of artificial intelligence (AI) in diverse sectors underscores the need for comprehensive and standardized approaches to ensure AI responsibility. However, the absence of a holistic framework to evaluate the fairness, privacy-preserving, secure, explainable, and human-centered facets of AI systems poses a challenge. Addressing this gap, this research paper presents a novel approach to assessing Responsible AI by combining insights from a systematic literature review with a practical evaluation framework. The paper provides a concise overview of the key aspects of Responsible AI and highlights the findings from the literature review. Furthermore, the paper introduces a set of evaluation metrics specifically designed for the current state of the art, using different model types and data from the healthcare domain. The framework supports the evaluation of Natural Language Processing (NLP), Computer Vision (CV), and tabular data models for classification tasks. Additionally, the paper briefly demonstrates VERIFAI, an example implementation of the framework, which serves as a comprehensive tool for assessing the responsibility of AI systems. The overall objective of this research is to make a meaningful contribution to the Responsible AI discourse, providing researchers and practitioners with a valuable resource to enhance the overall responsibility of their AI systems. © 2023 Copyright for this paper by its authors.
PB  - CEUR-WS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Klöser, L.
AU  - Büsgen, A.
AU  - Kohl, P.
AU  - Kraft, B.
AU  - Zündorf, A.
TI  - Explaining Relation Classification Models with Semantic Extents
PY  - 2023
T2  - Communications in Computer and Information Science
VL  - 1875 CCIS
SP  - 189
EP  - 208
DO  - 10.1007/978-3-031-39059-3_13
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172735976&doi=10.1007%2f978-3-031-39059-3_13&partnerID=40&md5=203fc1379aabae1f378a11dd13af75c2
AB  - In recent years, the development of large pretrained language models, such as BERT and GPT, significantly improved information extraction systems on various tasks, including relation classification. State-of-the-art systems are highly accurate on scientific benchmarks. A lack of explainability is currently a complicating factor in many real-world applications. Comprehensible systems are necessary to prevent biased, counterintuitive, or harmful decisions. We introduce semantic extents, a concept to analyze decision patterns for the relation classification task. Semantic extents are the most influential parts of texts concerning classification decisions. Our definition allows similar procedures to determine semantic extents for humans and models. We provide an annotation tool and a software framework to determine semantic extents for humans and models conveniently and reproducibly. Comparing both reveals that models tend to learn shortcut patterns from data. These patterns are hard to detect with current interpretability methods, such as input reductions. Our approach can help detect and eliminate spurious decision patterns during model development. Semantic extents can increase the reliability and security of natural language processing systems. Semantic extents are an essential step in enabling applications in critical areas like healthcare or finance. Moreover, our work opens new research directions for developing methods to explain deep learning models. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Wang, X.
AU  - Gupta, D.
AU  - Killian, M.
AU  - He, Z.
TI  - Benchmarking Transformer-Based Models for Identifying Social Determinants of Health in Clinical Notes
PY  - 2023
T2  - Proceedings - 2023 IEEE 11th International Conference on Healthcare Informatics, ICHI 2023
SP  - 570
EP  - 574
DO  - 10.1109/ICHI57859.2023.00102
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181557095&doi=10.1109%2fICHI57859.2023.00102&partnerID=40&md5=8b3c29febe0f6bb9a636237422c836ab
AB  - Electronic health records (EHR) have been widely used in building machine learning models for health outcomes prediction. However, many EHR-based models are inherently biased due to lack of risk factors on social determinants of health (SDoH), which are responsible for up to 40% preventive deaths. As SDoH information is often captured in clinical notes, recent efforts have been made to extract such information from notes with natural language processing and append it to other structured data. In this work, we benchmark 7 pre-trained transformer-based models, including BERT, ALBERT, BioBERT, BioClinicalBERT, RoBERTa, ELECTRA, and RoBERTa-MIMIC-Trial, for recognizing SDoH terms using a previously annotated corpus of MIMIC-III clinical notes. Our study shows that BioClinicalBERT model performs best on F-1 scores (0.911, 0.923) under both strict and relaxed criteria. This work shows the promise of using transformer-based models for recognizing SDoH information from clinical notes. © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Boren, R.A.
TI  - A case of neglect
PY  - 2022
T2  - Cortex
VL  - 154
SP  - 254
EP  - 258
DO  - 10.1016/j.cortex.2022.06.003
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133609919&doi=10.1016%2fj.cortex.2022.06.003&partnerID=40&md5=632e6af298e838544487250c77051539
AB  - In 1875 William Andrew Johnson, who had been formerly enslaved by Andrew Johnson and who subsequently served as his valet after being emancipated, was present when the former president suffered his fatal stroke. William's description of his deficits, as told decades later to journalist Ernie Pyle, appears to represent one of the earliest known cases of asomatognosia. The limited description of the symptoms provides a backdrop for a discussion of the evolution of knowledge regarding disorders of body awareness. This case also highlights the importance of caregivers as sources of clinical information and serves as a cautionary tale regarding the risk of marginalizing them due to cultural bias. © 2022 Elsevier Ltd
PB  - Masson SpA
C2  - 35810499
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Toliyat, A.
AU  - Levitan, S.I.
AU  - Peng, Z.
AU  - Etemadpour, R.
TI  - Asian hate speech detection on Twitter during COVID-19
PY  - 2022
T2  - Frontiers in Artificial Intelligence
VL  - 5
C7  - 932381
DO  - 10.3389/frai.2022.932381
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136906550&doi=10.3389%2ffrai.2022.932381&partnerID=40&md5=02732321406ccc50064fa06feea705b7
AB  - Coronavirus disease 2019 (COVID-19) started in Wuhan, China, in late 2019, and after being utterly contagious in Asian countries, it rapidly spread to other countries. This disease caused governments worldwide to declare a public health crisis with severe measures taken to reduce the speed of the spread of the disease. This pandemic affected the lives of millions of people. Many citizens that lost their loved ones and jobs experienced a wide range of emotions, such as disbelief, shock, concerns about health, fear about food supplies, anxiety, and panic. All of the aforementioned phenomena led to the spread of racism and hate against Asians in western countries, especially in the United States. An analysis of official preliminary police data by the Center for the Study of Hate & Extremism at California State University shows that Anti-Asian hate crime in 16 of America's largest cities increased by 149% in 2020. In this study, we first chose a baseline of Americans' hate crimes against Asians on Twitter. Then we present an approach to balance the biased dataset and consequently improve the performance of tweet classification. We also have downloaded 10 million tweets through the Twitter API V-2. In this study, we have used a small portion of that, and we will use the entire dataset in the future study. In this article, three thousand tweets from our collected corpus are annotated by four annotators, including three Asian and one Asian-American. Using this data, we built predictive models of hate speech using various machine learning and deep learning methods. Our machine learning methods include Random Forest, K-nearest neighbors (KNN), Support Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), Logistic Regression, Decision Tree, and Naive Bayes. Our Deep Learning models include Basic Long-Term Short-Term Memory (LSTM), Bidirectional LSTM, Bidirectional LSTM with Drop out, Convolution, and Bidirectional Encoder Representations from Transformers (BERT). We also adjusted our dataset by filtering tweets that were ambiguous to the annotators based on low Fleiss Kappa agreement between annotators. Our final result showed that Logistic Regression achieved the best statistical machine learning performance with an F1 score of 0.72, while BERT achieved the best performance of the deep learning models, with an F1-Score of 0.85. Copyright © 2022 Toliyat, Levitan, Peng and Etemadpour.
PB  - Frontiers Media S.A.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - CONF
AU  - Cai, W.
AU  - Encarnacion, R.
AU  - Chern, B.
AU  - Corbett-Davies, S.
AU  - Bogen, M.
AU  - Bergman, S.
AU  - Goel, S.
TI  - Adaptive Sampling Strategies to Construct Equitable Training Datasets
PY  - 2022
T2  - ACM International Conference Proceeding Series
SP  - 1467
EP  - 1478
DO  - 10.1145/3531146.3533203
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133004540&doi=10.1145%2f3531146.3533203&partnerID=40&md5=1ccd1cdd50ba294e2aa7065ed2c39013
AB  - In domains ranging from computer vision to natural language processing, machine learning models have been shown to exhibit stark disparities, often performing worse for members of traditionally underserved groups. One factor contributing to these performance gaps is a lack of representation in the data the models are trained on. It is often unclear, however, how to operationalize representativeness in specific applications. Here we formalize the problem of creating equitable training datasets, and propose a statistical framework for addressing this problem. We consider a setting where a model builder must decide how to allocate a fixed data collection budget to gather training data from different subgroups. We then frame dataset creation as a constrained optimization problem, in which one maximizes a function of group-specific performance metrics based on (estimated) group-specific learning rates and costs per sample. This flexible approach incorporates preferences of model-builders and other stakeholders, as well as the statistical properties of the learning task. When data collection decisions are made sequentially, we show that under certain conditions this optimization problem can be efficiently solved even without prior knowledge of the learning rates. To illustrate our approach, we conduct a simulation study of polygenic risk scores on synthetic genomic data - an application domain that often suffers from non-representative data collection. When optimizing policies for overall or group-specific average health, we find that our adaptive approach outperforms heuristic strategies, including equal and representative sampling. In this sense, equal treatment with respect to sampling decisions does not guarantee equal or equitable outcomes. © 2022 ACM.
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 17
ER  -

TY  - JOUR
AU  - Poulsen, M.N.
AU  - Freda, P.J.
AU  - Troiani, V.
AU  - Davoudi, A.
AU  - Mowery, D.L.
TI  - Classifying Characteristics of Opioid Use Disorder From Hospital Discharge Summaries Using Natural Language Processing
PY  - 2022
T2  - Frontiers in Public Health
VL  - 10
C7  - 850619
DO  - 10.3389/fpubh.2022.850619
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130698561&doi=10.3389%2ffpubh.2022.850619&partnerID=40&md5=910c253e57a4ca8b1dc7f8b3ba20f554
AB  - Background: Opioid use disorder (OUD) is underdiagnosed in health system settings, limiting research on OUD using electronic health records (EHRs). Medical encounter notes can enrich structured EHR data with documented signs and symptoms of OUD and social risks and behaviors. To capture this information at scale, natural language processing (NLP) tools must be developed and evaluated. We developed and applied an annotation schema to deeply characterize OUD and related clinical, behavioral, and environmental factors, and automated the annotation schema using machine learning and deep learning-based approaches. Methods: Using the MIMIC-III Critical Care Database, we queried hospital discharge summaries of patients with International Classification of Diseases (ICD-9) OUD diagnostic codes. We developed an annotation schema to characterize problematic opioid use, identify individuals with potential OUD, and provide psychosocial context. Two annotators reviewed discharge summaries from 100 patients. We randomly sampled patients with their associated annotated sentences and divided them into training (66 patients; 2,127 annotated sentences) and testing (29 patients; 1,149 annotated sentences) sets. We used the training set to generate features, employing three NLP algorithms/knowledge sources. We trained and tested prediction models for classification with a traditional machine learner (logistic regression) and deep learning approach (Autogluon based on ELECTRA's replaced token detection model). We applied a five-fold cross-validation approach to reduce bias in performance estimates. Results: The resulting annotation schema contained 32 classes. We achieved moderate inter-annotator agreement, with F1-scores across all classes increasing from 48 to 66%. Five classes had a sufficient number of annotations for automation; of these, we observed consistently high performance (F1-scores) across training and testing sets for drug screening (training: 91–96; testing: 91–94) and opioid type (training: 86–96; testing: 86–99). Performance dropped from training and to testing sets for other drug use (training: 52–65; testing: 40–48), pain management (training: 72–78; testing: 61–78) and psychiatric (training: 73–80; testing: 72). Autogluon achieved the highest performance. Conclusion: This pilot study demonstrated that rich information regarding problematic opioid use can be manually identified by annotators. However, more training samples and features would improve our ability to reliably identify less common classes from clinical text, including text from outpatient settings. Copyright © 2022 Poulsen, Freda, Troiani, Davoudi and Mowery.
PB  - Frontiers Media S.A.
C2  - 35615042
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 13
ER  -

TY  - CONF
AU  - Li, H.
AU  - Wu, Y.
AU  - Schlegel, V.
AU  - Batista-Navarro, R.
AU  - Nguyen, T.-T.
AU  - Kashyap, A.R.
AU  - Zeng, X.
AU  - Beck, D.
AU  - Winkler, S.
AU  - Nenadic, G.
TI  - PULSAR: Pre-training with Extracted Healthcare Terms for Summarising Patients’ Problems and Data Augmentation with Black-box Large Language Models
PY  - 2023
T2  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
SP  - 503
EP  - 509
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174529501&partnerID=40&md5=cdb6b15590753352c31b842cec71614b
AB  - Medical progress notes play a crucial role in documenting a patient’s hospital journey, including his or her condition, treatment plan, and any updates for healthcare providers. Automatic summarisation of a patient’s problems in the form of a “problem list” can aid stakeholders in understanding a patient’s condition, reducing workload and cognitive bias. BioNLP 2023 Shared Task 1A focuses on generating a list of diagnoses and problems from the provider’s progress notes during hospitalisation. In this paper, we introduce our proposed approach to this task, which integrates two complementary components ‡. One component employs large language models (LLMs) for data augmentation; the other is an abstractive summarisation LLM with a novel pre-training objective for generating the patients’ problems summarised as a list. Our approach was ranked second among all submissions to the shared task. The performance of our model on the development and test datasets shows that our approach is more robust on unknown data, with an improvement of up to 3.1 points over the same size of the larger model. © 2023 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - JOUR
AU  - Rados, M.
AU  - Mouthaan, B.
AU  - Barsi, P.
AU  - Carmichael, D.
AU  - Heckemann, R.A.
AU  - Kelemen, A.
AU  - Kobulashvili, T.
AU  - Kuchukhidze, G.
AU  - Marusic, P.
AU  - Minkin, K.
AU  - Tisdall, M.
AU  - Trinka, E.
AU  - Veersema, T.
AU  - Vos, S.B.
AU  - Wagner, J.
AU  - Braun, K.
AU  - van Eijsden, P.
TI  - Diagnostic value of MRI in the presurgical evaluation of patients with epilepsy: influence of field strength and sequence selection: a systematic review and meta-analysis from the E-PILEPSY Consortium
PY  - 2022
T2  - Epileptic Disorders
VL  - 24
IS  - 2
SP  - 323
EP  - 342
DO  - 10.1684/epd.2021.1399
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130864289&doi=10.1684%2fepd.2021.1399&partnerID=40&md5=dbd19f570cc22b1258ed120ce7562202
AB  - Objective MRI is a cornerstone in presurgical evaluation of epilepsy. Despite guidelines, clinical practice varies. In light of the E-PILEPSY pilot reference network, we conducted a systematic review and meta-analysis on the diagnostic value of MRI in the presurgical evaluation of epilepsy patients. Methods We included original research articles on diagnostic value of higher MRI field strength and guideline-recommended and additional MRI sequences in detecting an epileptogenic lesion in adult or paediatric epilepsy surgery candidates. Lesion detection rate was used as a metric in meta-analysis. Results Eighteen studies were included for MRI field strength and 25 for MRI sequences, none were free from bias. In patients with normal MRI at lower-field strength, 3T improved lesion detection rate by 18% and 7T by 23%. Field strengths higher than 1.5T did not have higher lesion detection rates in patients with hippocampal sclerosis (HS). The lesion detection rate of epilepsy-specific MRI protocols was 83% for temporal lobe epilepsy (TLE) patients. Dedicated MRI protocols and evaluation by an experienced epilepsy neuroradiologist increased lesion detection. For HS, 3DT1, T2, and FLAIR each had a lesion detection rate at around 90%. Apparent diffusion coefficient indices had a lateralizing value of 33% for TLE. DTI fractional anisotropy and mean diffusivity had a localizing value of 8% and 34%. Significance A dedicated MRI protocol and expert evaluation benefits lesion detection rate in epilepsy surgery candidates. If patients remain MRI negative, imaging at higher-field strength may reveal lesions. In HS, apparent diffusion coefficient indices may aid lateralization and localization more than increasing field strength. DTI can add further diagnostic information. For other additional sequences, the quality and number of studies is insufficient to draw solid conclusions. Our findings may be used as evidence base for developing new high-quality MRI studies and clinical guidelines. © 2022 Epileptic Disorders.
PB  - John Libbey
C2  - 34961746
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 15
ER  -

TY  - CONF
AU  - Kumar, Y.
AU  - Delgado, J.
AU  - Kupershtein, E.
AU  - Hannon, B.
AU  - Gordon, Z.
AU  - Li, J.J.
AU  - Morreale, P.
TI  - AssureAIDoctor- A Bias-Free AI Bot
PY  - 2023
T2  - 2023 International Symposium on Networks, Computers and Communications, ISNCC 2023
DO  - 10.1109/ISNCC58260.2023.10323978
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179845872&doi=10.1109%2fISNCC58260.2023.10323978&partnerID=40&md5=3324d552e3c5f864df447cadff63224a
AB  - The researchers introduce the AssureAIDoctor (AAID) App - a pioneering application that aims to revolutionize healthcare by integrating the latest artificial intelligence (AI) features into a mobile-native product. The app leverages the OpenAI API to simulate virtual doctor-patient interactions, offering users potential remedies for their symptoms. The distinguishing feature of AAID is its use of DALL-E, an advanced image generator, and the forthcoming OpenAI's Code interpreter. This allows users to enhance their interactions with the AI by uploading images, thereby personalizing their healthcare experience. The app's user interface is designed to support this advanced functionality. Preliminary tests show promising results, with AAID accurately responding to various symptom inputs. While scalability is a key focus, the app addresses potential challenges such as increased operational costs associated with Microsoft Azure AI cloud and OpenAI API services. Despite these challenges, AAID is committed to making healthcare accessible for underrepresented and uninsured individuals. The app embodies the potential of AI in healthcare, promising to make healthcare more equitable and accessible. © 2023 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Christodoulakis, N.
AU  - Abdelkader, W.
AU  - Lokker, C.
AU  - Cotterchio, M.
AU  - Griffith, L.E.
AU  - Vanderloo, L.M.
AU  - Anderson, L.N.
TI  - Public Health Surveillance of Behavioral Cancer Risk Factors During the COVID-19 Pandemic: Sentiment and Emotion Analysis of Twitter Data
PY  - 2023
T2  - JMIR Formative Research
VL  - 7
IS  - 1
C7  - e46874
DO  - 10.2196/46874
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177845293&doi=10.2196%2f46874&partnerID=40&md5=d4f8cdce0eb16a424543c79f6bba25a7
AB  - Background: The COVID-19 pandemic and its associated public health mitigation strategies have dramatically changed patterns of daily life activities worldwide, resulting in unintentional consequences on behavioral risk factors, including smoking, alcohol consumption, poor nutrition, and physical inactivity. The infodemic of social media data may provide novel opportunities for evaluating changes related to behavioral risk factors during the pandemic. Objective: We explored the feasibility of conducting a sentiment and emotion analysis using Twitter data to evaluate behavioral cancer risk factors (physical inactivity, poor nutrition, alcohol consumption, and smoking) over time during the first year of the COVID-19 pandemic. Methods: Tweets during 2020 relating to the COVID-19 pandemic and the 4 cancer risk factors were extracted from the George Washington University Libraries Dataverse. Tweets were defined and filtered using keywords to create 4 data sets. We trained and tested a machine learning classifier using a prelabeled Twitter data set. This was applied to determine the sentiment (positive, negative, or neutral) of each tweet. A natural language processing package was used to identify the emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, and trust) based on the words contained in the tweets. Sentiments and emotions for each of the risk factors were evaluated over time and analyzed to identify keywords that emerged. Results: The sentiment analysis revealed that 56.69% (51,479/90,813) of the tweets about physical activity were positive, 16.4% (14,893/90,813) were negative, and 26.91% (24,441/90,813) were neutral. Similar patterns were observed for nutrition, where 55.44% (27,939/50,396), 15.78% (7950/50,396), and 28.79% (14,507/50,396) of the tweets were positive, negative, and neutral, respectively. For alcohol, the proportions of positive, negative, and neutral tweets were 46.85% (34,897/74,484), 22.9% (17,056/74,484), and 30.25% (22,531/74,484), respectively, and for smoking, they were 41.2% (11,628/28,220), 24.23% (6839/28,220), and 34.56% (9753/28,220), respectively. The sentiments were relatively stable over time. The emotion analysis suggests that the most common emotion expressed across physical activity and nutrition tweets was trust (69,495/320,741, 21.67% and 42,324/176,564, 23.97%, respectively); for alcohol, it was joy (49,147/273,128, 17.99%); and for smoking, it was fear (23,066/110,256, 20.92%). The emotions expressed remained relatively constant over the observed period. An analysis of the most frequent words tweeted revealed further insights into common themes expressed in relation to some of the risk factors and possible sources of bias. Conclusions: This analysis provided insight into behavioral cancer risk factors as expressed on Twitter during the first year of the COVID-19 pandemic. It was feasible to extract tweets relating to all 4 risk factors, and most tweets had a positive sentiment with varied emotions across the different data sets. Although these results can play a role in promoting public health, a deeper dive via qualitative analysis can be conducted to provide a contextual examination of each tweet. ©Nicolette Christodoulakis, Wael Abdelkader, Cynthia Lokker, Michelle Cotterchio, Lauren E Griffith, Leigh M Vanderloo, Laura N Anderson.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Radović, N.
AU  - Špero, M.
AU  - Hrkać Pustahija, A.
AU  - Almahariq, F.
AU  - Srdoč, D.
TI  - T2-Fluid-Attenuated Inversion Recovery Mismatch Sign in Grade II and III Gliomas: Is There a Coexisting T2-Diffusion-Weighted Imaging Mismatch?
PY  - 2022
T2  - Journal of Computer Assisted Tomography
VL  - 46
IS  - 2
SP  - 251
EP  - 256
DO  - 10.1097/RCT.0000000000001267
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126738654&doi=10.1097%2fRCT.0000000000001267&partnerID=40&md5=8f8c7fd8bd1551eaa92860eccba2e8f3
AB  - Objective To determine whether the T2 fluid-attenuated inversion recovery (T2-FLAIR) mismatch sign in diffuse gliomas is associated with an equivalent pattern of disparity in signal intensities when comparing T2- and diffusion-weighted imaging (DWI). Methods The level of correspondence between T2-FLAIR and T2-DWI evaluations in 34 World Health Organization grade II/III gliomas and interreader agreement among 3 neuroradiologists were assessed by calculating intraclass correlation coefficient and κ statistics, respectively. Tumoral apparent diffusion coefficient values were compared using t test. Results There was an almost perfect correspondence between the 2 mismatch signs (intraclass correlation coefficient = 0.824 [95% confidence interval, 0.68-0.91]) that were associated with higher mean tumoral apparent diffusion coefficient (P < 0.01). Interreader agreement was substantial for T2-FLAIR (Fleiss κ = 0.724) and moderate for T2-DWI comparisons (Fleiss κ = 0.589) (P < 0.001). Conclusions The T2-FLAIR mismatch sign is usually reflected by a distinct microstructural pattern on DWI. The management of this tumor subtype may benefit from specifically tailored imaging assessments.  © 2022 Wolters Kluwer Health, Inc. All rights reserved.
PB  - Lippincott Williams and Wilkins
C2  - 35297581
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Nawaz, F.A.
AU  - Barr, A.A.
AU  - Desai, M.Y.
AU  - Tsagkaris, C.
AU  - Singh, R.
AU  - Klager, E.
AU  - Eibensteiner, F.
AU  - Parvanov, E.D.
AU  - Hribersek, M.
AU  - Kletecka-Pulker, M.
AU  - Willschke, H.
AU  - Atanasov, A.G.
TI  - Promoting Research, Awareness, and Discussion on AI in Medicine Using #MedTwitterAI: A Longitudinal Twitter Hashtag Analysis
PY  - 2022
T2  - Frontiers in Public Health
VL  - 10
C7  - 856571
DO  - 10.3389/fpubh.2022.856571
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134231765&doi=10.3389%2ffpubh.2022.856571&partnerID=40&md5=7b90414fa3e358d579a8822325146fd0
AB  - Background: Artificial intelligence (AI) has the potential to reshape medical practice and the delivery of healthcare. Online discussions surrounding AI's utility in these domains are increasingly emerging, likely due to considerable interest from healthcare practitioners, medical technology developers, and other relevant stakeholders. However, many practitioners and medical students report limited understanding and familiarity with AI. Objective: To promote research, events, and resources at the intersection of AI and medicine for the online medical community, we created a Twitter-based campaign using the hashtag #MedTwitterAI. Methods: In the present study, we analyze the use of #MedTwitterAI by tracking tweets containing this hashtag posted from 26th March, 2019 to 26th March, 2021, using the Symplur Signals hashtag analytics tool. The full text of all #MedTwitterAI tweets was also extracted and subjected to a natural language processing analysis. Results: Over this time period, we identified 7,441 tweets containing #MedTwitterAI, posted by 1,519 unique Twitter users which generated 59,455,569 impressions. The most common identifiable locations for users including this hashtag in tweets were the United States (378/1,519), the United Kingdom (80/1,519), Canada (65/1,519), India (46/1,519), Spain (29/1,519), France (24/1,519), Italy (16/1,519), Australia (16/1,519), Germany (16/1,519), and Brazil (15/1,519). Tweets were frequently enhanced with links (80.2%), mentions of other accounts (93.9%), and photos (56.6%). The five most abundant single words were AI (artificial intelligence), patients, medicine, data, and learning. Sentiment analysis revealed an overall majority of positive single word sentiments (e.g., intelligence, improve) with 230 positive and 172 negative sentiments with a total of 658 and 342 mentions of all positive and negative sentiments, respectively. Most frequently mentioned negative sentiments were cancer, risk, and bias. Most common bigrams identified by Markov chain depiction were related to analytical methods (e.g., label-free detection) and medical conditions/biological processes (e.g., rare circulating tumor cells). Conclusion: These results demonstrate the generated considerable interest of using #MedTwitterAI for promoting relevant content and engaging a broad and geographically diverse audience. The use of hashtags in Twitter-based campaigns can be an effective tool to raise awareness of interdisciplinary fields and enable knowledge-sharing on a global scale. Copyright © 2022 Nawaz, Barr, Desai, Tsagkaris, Singh, Klager, Eibensteiner, Parvanov, Hribersek, Kletecka-Pulker, Willschke and Atanasov.
PB  - Frontiers Media S.A.
C2  - 35844878
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - JOUR
AU  - Powell, K.R.
AU  - Popescu, M.
AU  - Lee, S.
AU  - Mehr, D.R.
AU  - Alexander, G.L.
TI  - Examining the Use of Text Messages Among Multidisciplinary Care Teams to Reduce Avoidable Hospitalization of Nursing Home Residents with Dementia: Protocol for a Secondary Analysis
PY  - 2023
T2  - JMIR Research Protocols
VL  - 12
C7  - e50231
DO  - 10.2196/50231
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169793382&doi=10.2196%2f50231&partnerID=40&md5=bc04f8eb45b2633c59f1d54be3c32d5b
AB  - Background: Reducing avoidable nursing home (NH)–to-hospital transfers of residents with Alzheimer disease or a related dementia (ADRD) has become a national priority due to the physical and emotional toll it places on residents and the high costs to Medicare and Medicaid. Technologies supporting the use of clinical text messages (TMs) could improve communication among health care team members and have considerable impact on reducing avoidable NH-to-hospital transfers. Although text messaging is a widely accepted mechanism of communication, clinical models of care using TMs are sparsely reported in the literature, especially in NHs. Protocols for assessing technologies that integrate TMs into care delivery models would be beneficial for end users of these systems. Without evidence to support clinical models of care using TMs, users are left to design their own methods and protocols for their use, which can create wide variability and potentially increase disparities in resident outcomes. Objective: Our aim is to describe the protocol of a study designed to understand how members of the multidisciplinary team communicate using TMs and how salient and timely communication can be used to avert poor outcomes for NH residents with ADRD, including hospitalization. Methods: This project is a secondary analysis of data collected from a Centers for Medicare & Medicaid Services (CMS)–funded demonstration project designed to reduce avoidable hospitalizations for long-stay NH residents. We will use two data sources: (1) TMs exchanged among the multidisciplinary team across the 7-year CMS study period (August 2013-September 2020) and (2) an adapted acute care transfer tool completed by advanced practice registered nurses to document retrospective details about NH-to-hospital transfers. The study is guided by an age-friendly model of care called the 4Ms (What Matters, Medications, Mentation, and Mobility) framework. We will use natural language processing, statistical methods, and social network analysis to generate a new ontology and to compare communication patterns found in TMs occurring around the time NH-to-hospital transfer decisions were made about residents with and without ADRD. Results: After accounting for inclusion and exclusion criteria, we will analyze over 30,000 TMs pertaining to over 3600 NH-to-hospital transfers. Development of the 4M ontology is in progress, and the 3-year project is expected to run until mid-2025. Conclusions: To our knowledge, this project will be the first to explore the content of TMs exchanged among a multidisciplinary team of care providers as they make decisions about NH-to-hospital resident transfers. Understanding how the presence of evidence-based elements of high-quality care relate to avoidable hospitalizations among NH residents with ADRD will generate knowledge regarding the future scalability of behavioral interventions. Without this knowledge, NHs will continue to rely on ineffective and outdated communication methods that fail to account for evidence-based elements of age-friendly care. © Kimberly R Powell, Mihail Popescu, Suhwon Lee, David R Mehr, Gregory L Alexander.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Terasawa, Y.
AU  - Shimomura, R.
AU  - Sato, K.
AU  - Himeno, T.
AU  - Inoue, T.
AU  - Kohriyama, T.
TI  - The efficacy and safety of alteplase treatment in patients with acute ischemic stroke with unknown time of onset: -Real world data-
PY  - 2023
T2  - Journal of Clinical Neuroscience
VL  - 107
SP  - 124
EP  - 128
DO  - 10.1016/j.jocn.2022.11.018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144380480&doi=10.1016%2fj.jocn.2022.11.018&partnerID=40&md5=487cab51f3b781e72db090bc22bedfa0
AB  - Introduction: Treatment with alteplase for acute ischemic stroke patients with an unknown time of onset is safe and effective. However, clinical trials have some selection bias. The purpose of this study was to clarify the efficacy and safety of alteplase treatment in patients with unknown time of onset in a real-world clinical setting. Methods: We included consecutive patients with acute ischemic stroke visited within 4.5 h of onset or symptom recognition. We divided patients into two groups: onset clear group (C-group) and unknown time of onset group (U-group). We treated patients with an unknown time of onset if the DWI-FLAIR mismatch was positive. We calculated the prevalence of alteplase treatment in each group and compared prognosis between the two groups. Results: Six hundred thirty-two patients arrived within 4.5 h of onset or symptom recognition. Of these, 446 patients (71 %) were in the C-group and 186 (29 %) in the U group. Alteplase treatment was performed in 35 % of patients in the C group and in 18 % in the U group (p < 0.001). Favorable outcomes at 90 days in patients treated with alteplase were comparable between the C group (52 %) and the U group (53 %) (p = 0.887). All hemorrhagic complications, including non-symptomatic hemorrhagic transformation, occurred in 11 of 157 patients (7 %) in the C-group and one of 34 patients (3 %) in the U-group (p = 0.696). Conclusion: In a real-world clinical setting, alteplase treatment was performed safe in 18% of patients with an unknown time of stroke onset based on patient selection using the DWI-FLAIR mismatch. © 2022 Elsevier Ltd
PB  - Churchill Livingstone
C2  - 36535219
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Abd-Alrazaq, A.
AU  - AlSaad, R.
AU  - Alhuwail, D.
AU  - Ahmed, A.
AU  - Healy, P.M.
AU  - Latifi, S.
AU  - Aziz, S.
AU  - Damseh, R.
AU  - Alrazak, S.A.
AU  - Sheikh, J.
TI  - Large Language Models in Medical Education: Opportunities, Challenges, and Future Directions
PY  - 2023
T2  - JMIR Medical Education
VL  - 9
C7  - e48291
DO  - 10.2196/48291
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164475756&doi=10.2196%2f48291&partnerID=40&md5=a943ad6c011474b6501d18ed38963838
AB  - The integration of large language models (LLMs), such as those in the Generative Pre-trained Transformers (GPT) series, into medical education has the potential to transform learning experiences for students and elevate their knowledge, skills, and competence. Drawing on a wealth of professional and academic experience, we propose that LLMs hold promise for revolutionizing medical curriculum development, teaching methodologies, personalized study plans and learning materials, student assessments, and more. However, we also critically examine the challenges that such integration might pose by addressing issues of algorithmic bias, overreliance, plagiarism, misinformation, inequity, privacy, and copyright concerns in medical education. As we navigate the shift from an information-driven educational paradigm to an artificial intelligence (AI)–driven educational paradigm, we argue that it is paramount to understand both the potential and the pitfalls of LLMs in medical education. This paper thus offers our perspective on the opportunities and challenges of using LLMs in this context. We believe that the insights gleaned from this analysis will serve as a foundation for future recommendations and best practices in the field, fostering the responsible and effective use of AI technologies in medical education. ©Alaa Abd-alrazaq, Rawan AlSaad, Dari Alhuwail, Arfan Ahmed, Padraig Mark Healy, Syed Latifi, Sarah Aziz, Rafat Damseh, Sadam Alabed Alrazak, Javaid Sheikh.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 196
ER  -

TY  - JOUR
AU  - Park, S.H.
AU  - Cheng, C.P.
AU  - Buehler, N.J.
AU  - Sanford, T.
AU  - Torrey, W.
TI  - A sentiment analysis on online psychiatrist reviews to identify clinical attributes of psychiatrists that shape the therapeutic alliance
PY  - 2023
T2  - Frontiers in Psychiatry
VL  - 14
C7  - 1174154
DO  - 10.3389/fpsyt.2023.1174154
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164392741&doi=10.3389%2ffpsyt.2023.1174154&partnerID=40&md5=2bc20fd7c81f5e9560065e6d918dfda2
AB  - Background: While online reviews from physician rating websites are increasingly utilized by healthcare providers to better understand patient needs, it remains difficult to objectively identify areas for improvement in providing psychiatric care. Objectives: To quantitatively characterize the sentiment of online written reviews of psychiatrists to determine clinical attributes that can be strengthened to improve psychiatrists’ therapeutic alliance with their patients. Materials and methods: Sentiment scores of 6,400 written reviews of 400 US-based psychiatrists on a US-based online physician rating website were obtained through a natural-language-processing-based sentiment analysis. Relationships among sentiment scores, average star ratings, and demographics were examined. Linguistic analyses determined words and bigrams that were highly associated with reviews with the most positive and negative sentiment. Findings: Sentiment scores were significantly correlated with average star ratings of the psychiatrists (R = 0.737, p < 0.001). Psychiatrists who were younger (< 56 years old) and/or practiced in the Northeast had significantly higher average star ratings than those older and/or practicing in the Southwest. Frequency analysis showed that positive reviews most frequently contained “time” (N = 1,138) and “caring” (N = 784) while negative reviews most frequently contained “medication” (N = 495) and “time” (N = 379). Logistic regression analysis revealed that reviews were more likely to be considered positive when they included “great listener” (OR = 16.89) and “comfortable” (OR = 10.72) and more likely to be negative when they included “meds” (OR = 0.55) and “side effect” (OR = 0.59). Conclusion: Psychiatrists who are younger and located in the Northeast receive more positive reviews; there may be potential for demographic bias among patient reviewers. Patients positively rate psychiatrists who make them feel heard and comfortable but negatively rate encounters centered around medications and their side effects. Our study lends quantitative evidence to support the importance of thorough and empathetic communication of psychiatrists in building a strong therapeutic alliance. Copyright © 2023 Park, Cheng, Buehler, Sanford and Torrey.
PB  - Frontiers Media SA
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Khoury, P.
AU  - Srinivasan, R.
AU  - Kakumanu, S.
AU  - Ochoa, S.
AU  - Keswani, A.
AU  - Sparks, R.
AU  - Rider, N.L.
TI  - A Framework for Augmented Intelligence in Allergy and Immunology Practice and Research—A Work Group Report of the AAAAI Health Informatics, Technology, and Education Committee
PY  - 2022
T2  - Journal of Allergy and Clinical Immunology: In Practice
VL  - 10
IS  - 5
SP  - 1178
EP  - 1188
DO  - 10.1016/j.jaip.2022.01.047
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126388265&doi=10.1016%2fj.jaip.2022.01.047&partnerID=40&md5=1936a5bd2c23603806755d133d0aad11
AB  - Artificial and augmented intelligence (AI) and machine learning (ML) methods are expanding into the health care space. Big data are increasingly used in patient care applications, diagnostics, and treatment decisions in allergy and immunology. How these technologies will be evaluated, approved, and assessed for their impact is an important consideration for researchers and practitioners alike. With the potential of ML, deep learning, natural language processing, and other assistive methods to redefine health care usage, a scaffold for the impact of AI technology on research and patient care in allergy and immunology is needed. An American Academy of Asthma Allergy and Immunology Health Information Technology and Education subcommittee workgroup was convened to perform a scoping review of AI within health care as well as the specialty of allergy and immunology to address impacts on allergy and immunology practice and research as well as potential challenges including education, AI governance, ethical and equity considerations, and potential opportunities for the specialty. There are numerous potential clinical applications of AI in allergy and immunology that range from disease diagnosis to multidimensional data reduction in electronic health records or immunologic datasets. For appropriate application and interpretation of AI, specialists should be involved in the design, validation, and implementation of AI in allergy and immunology. Challenges include incorporation of data science and bioinformatics into training of future allergists-immunologists. © 2022
PB  - American Academy of Allergy, Asthma and Immunology
C2  - 35300959
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 24
ER  -

TY  - JOUR
AU  - Ujah, O.I.
AU  - Olaore, P.
AU  - Nnorom, O.C.
AU  - Ogbu, C.E.
AU  - Kirby, R.S.
TI  - Examining ethno-racial attitudes of the public in Twitter discourses related to the United States Supreme Court Dobbs vs. Jackson Women's Health Organization ruling: A machine learning approach
PY  - 2023
T2  - Frontiers in Global Women's Health
VL  - 4
C7  - 1149441
DO  - 10.3389/fgwh.2023.1149441
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159900964&doi=10.3389%2ffgwh.2023.1149441&partnerID=40&md5=48089e5ca502669bdf9a005edb7f3ec9
AB  - Background: The decision of the US Supreme Court to repeal Roe vs. Wade sparked significant media attention. Although primarily related to abortion, opinions are divided about how this decision would impact disparities, especially for Black, Indigenous, and people of color. We used advanced natural language processing (NLP) techniques to examine ethno-racial contents in Twitter discourses related to the overturn of Roe vs. Wade. Methods: We screened approximately 3 million tweets posted to Roe vs. Wade discussions and identified unique tweets in English-language that had mentions related to race, ethnicity, and racism posted between June 24 and July 10, 2022. We performed lexicon-based sentiment analysis to identify sentiment polarity and the emotions expressed in the Twitter discourse and conducted structural topic modeling to identify and examine latent themes. Results: Of the tweets retrieved, 0.7% (n = 23,044) had mentions related to race, ethnicity, and racism. The overall sentiment polarity was negative (mean = −0.41, SD = 1.48). Approximately 60.0% (n = 12,092) expressed negative sentiments, while 39.0% (n = 81,45) expressed positive sentiments, and 3.0% (n = 619) expressed neutral sentiments. There were 20 latent themes which emerged from the topic model. The predominant topics in the discourses were related to “racial resentment” (topic 2, 11.3%), “human rights” (topic 2, 7.9%), and “socioeconomic disadvantage” (topic 16, 7.4%). Conclusions: Our study demonstrates wide ranging ethno-racial concerns following the reversal of Roe and supports the need for active surveillance of racial and ethnic disparities in abortion access in the post-Roe era. 2023 Ujah, Olaore, Nnorom, Ogbu and Kirby.
PB  - Frontiers Media S.A.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Scoles, B.
AU  - Nicodemo, C.
TI  - Doctors’ attitudes toward specific medical conditions
PY  - 2022
T2  - Journal of Economic Behavior and Organization
VL  - 204
SP  - 182
EP  - 199
DO  - 10.1016/j.jebo.2022.09.023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140712685&doi=10.1016%2fj.jebo.2022.09.023&partnerID=40&md5=d8e18274bbe2f0d10e8ec9675c71b1b3
AB  - This study uses machine learning and natural language processing tools to examine the language used by healthcare professionals on a global online forum. It contributes to an underdeveloped area of knowledge, that of physician attitudes toward their patients. Using comments left by physicians on Reddit's ”Medicine” subreddit (r/medicine), we test if the language from online discussions can reveal doctors’ attitudes toward specific medical conditions. We focus on a set of chronic conditions that usually are more stigmatized and compare them to ones well accepted by the medical community. We discovered that when comparing diseases with similar traits, doctors discussed some conditions with more negative attitudes. These results show bias does not occur only along the dimensions traditionally analyzed in the economics literature of gender and race, but also along the dimension of disease type. This is meaningful because the emotions associated with beliefs impact physicians’ decision making, prescribing behavior, and quality of care. First, we run a binomial LASSO-logistic regression to compare a range of 21 diseases against myalgic encephalomyelitis/chronic fatigue syndrome (ME/CFS), depression, and the autoimmune diseases multiple sclerosis and rheumatoid arthritis. Next, we use dictionary methods to compare five more chronic diseases: Lyme disease, Ehlers-Danlos syndrome (EDS), Alzheimer's disease, osteoporosis, and lupus. The results show physicians discuss ME/CFS, depression, and Lyme disease with more negative language than the other diseases in the set. The results for ME/CFS included over four times more negative words than the results for depression. © 2022 The Author(s)
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Karabacak, M.
AU  - Ozkara, B.B.
AU  - Margetis, K.
AU  - Wintermark, M.
AU  - Bisdas, S.
TI  - The Advent of Generative Language Models in Medical Education
PY  - 2023
T2  - JMIR Medical Education
VL  - 9
C7  - e48163
DO  - 10.2196/48163
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164319374&doi=10.2196%2f48163&partnerID=40&md5=e8f247be4f6accbca21d0be0ee988fa0
AB  - Artificial intelligence (AI) and generative language models (GLMs) present significant opportunities for enhancing medical education, including the provision of realistic simulations, digital patients, personalized feedback, evaluation methods, and the elimination of language barriers. These advanced technologies can facilitate immersive learning environments and enhance medical students' educational outcomes. However, ensuring content quality, addressing biases, and managing ethical and legal concerns present obstacles. To mitigate these challenges, it is necessary to evaluate the accuracy and relevance of AI-generated content, address potential biases, and develop guidelines and policies governing the use of AI-generated content in medical education. Collaboration among educators, researchers, and practitioners is essential for developing best practices, guidelines, and transparent AI models that encourage the ethical and responsible use of GLMs and AI in medical education. By sharing information about the data used for training, obstacles encountered, and evaluation methods, developers can increase their credibility and trustworthiness within the medical community. In order to realize the full potential of AI and GLMs in medical education while mitigating potential risks and obstacles, ongoing research and interdisciplinary collaboration are necessary. By collaborating, medical professionals can ensure that these technologies are effectively and responsibly integrated, contributing to enhanced learning experiences and patient care. © 2023 Authors. All rights reserved.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 98
ER  -

TY  - JOUR
AU  - Mohsan, M.M.
AU  - Akram, M.U.
AU  - Rasool, G.
AU  - Alghamdi, N.S.
AU  - Baqai, M.A.A.
AU  - Abbas, M.
TI  - Vision Transformer and Language Model Based Radiology Report Generation
PY  - 2023
T2  - IEEE Access
VL  - 11
SP  - 1814
EP  - 1824
DO  - 10.1109/ACCESS.2022.3232719
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146219653&doi=10.1109%2fACCESS.2022.3232719&partnerID=40&md5=93f2e7b328b8f5e7327282345afd3712
AB  - Recent advancements in transformers exploited computer vision problems which results in state-of-The-Art models. Transformer-based models in various sequence prediction tasks such as language translation, sentiment classification, and caption generation have shown remarkable performance. Auto report generation scenarios in medical imaging through caption generation models is one of the applied scenarios for language models and have strong social impact. In these models, convolution neural networks have been used as encoder to gain spatial information and recurrent neural networks are used as decoder to generate caption or medical report. However, using transformer architecture as encoder and decoder in caption or report writing task is still unexplored. In this research, we explored the effect of losing spatial biasness information in encoder by using pre-Trained vanilla image transformer architecture and combine it with different pre-Trained language transformers as decoder. In order to evaluate the proposed methodology, the Indiana University Chest X-Rays dataset is used where ablation study is also conducted with respect to different evaluations. The comparative analysis shows that the proposed methodology has represented remarkable performance when compared with existing techniques in terms of different performance parameters.  © 2013 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 21
ER  -

TY  - CONF
AU  - Xu, Y.
AU  - Yang, K.
AU  - Zhang, C.
AU  - Zou, P.
AU  - Wang, Z.
AU  - Ding, H.
AU  - Zhao, J.
AU  - Wang, Y.
AU  - Xie, B.
TI  - VecoCare: Visit Sequences-Clinical Notes Joint Learning for Diagnosis Prediction in Healthcare Data
PY  - 2023
T2  - IJCAI International Joint Conference on Artificial Intelligence
VL  - 2023-August
SP  - 4921
EP  - 4929
DO  - 10.24963/ijcai.2023/547
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170374705&doi=10.24963%2fijcai.2023%2f547&partnerID=40&md5=b7071e7b06f88d5fe8b48164f81048a2
AB  - Due to the insufficiency of electronic health records (EHR) data utilized in practical diagnosis prediction scenarios, most works are devoted to learning powerful patient representations either from structured EHR data (e.g., temporal medical events, lab test results, etc.) or unstructured data (e.g., clinical notes, etc.). However, synthesizing rich information from both of them still needs to be explored. Firstly, the heterogeneous semantic biases across them heavily hinder the synthesis of representation spaces, which is critical for diagnosis prediction. Secondly, the intermingled quality of partial clinical notes leads to inadequate representations of to-be-predicted patients. Thirdly, typical attention mechanisms mainly focus on aggregating information from similar patients, ignoring important auxiliary information from others. To tackle these challenges, we propose a novel visit sequences-clinical notes joint learning approach, dubbed VecoCare. It performs a Gromov-Wasserstein Distance (GWD)-based contrastive learning task and an adaptive masked language model task in a sequential pre-training manner to reduce heterogeneous semantic biases. After pre-training, VecoCare further aggregates information from both similar and dissimilar patients through a dual-channel retrieval mechanism. We conduct diagnosis prediction experiments on two real-world datasets, which indicates that VecoCare outperforms state-of-the-art approaches. Moreover, the findings discovered by VecoCare are consistent with the medical researches. © 2023 International Joint Conferences on Artificial Intelligence. All rights reserved.
PB  - International Joint Conferences on Artificial Intelligence
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - JOUR
AU  - Abuyaman, O.
TI  - Strengths and Weaknesses of ChatGPT Models for Scientific Writing About Medical Vitamin B12: Mixed Methods Study
PY  - 2023
T2  - JMIR Formative Research
VL  - 7
IS  - 1
C7  - e49459
DO  - 10.2196/49459
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178337537&doi=10.2196%2f49459&partnerID=40&md5=2fc9c1b637e7942948e6cf8da14a552b
AB  - Background: ChatGPT is a large language model developed by OpenAI designed to generate human-like responses to prompts. Objective: This study aims to evaluate the ability of GPT-4 to generate scientific content and assist in scientific writing using medical vitamin B12 as the topic. Furthermore, the study will compare the performance of GPT-4 to its predecessor, GPT-3.5. Methods: The study examined responses from GPT-4 and GPT-3.5 to vitamin B12–related prompts, focusing on their quality and characteristics and comparing them to established scientific literature. Results: The results indicated that GPT-4 can potentially streamline scientific writing through its ability to edit language and write abstracts, keywords, and abbreviation lists. However, significant limitations of ChatGPT were revealed, including its inability to identify and address bias, inability to include recent information, lack of transparency, and inclusion of inaccurate information. Additionally, it cannot check for plagiarism or provide proper references. The accuracy of GPT-4’s answers was found to be superior to GPT-3.5. Conclusions: ChatGPT can be considered a helpful assistant in the writing process but not a replacement for a scientist’s expertise. Researchers must remain aware of its limitations and use it appropriately. The improvements in consecutive ChatGPT versions suggest the possibility of overcoming some present limitations in the near future. © Omar Abuyaman.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - JOUR
AU  - Ghosh, C.C.
AU  - McVicar, D.
AU  - Davidson, G.
AU  - Shannon, C.
AU  - Armour, C.
TI  - What can we learn about the psychiatric diagnostic categories by analysing patients' lived experiences with Machine-Learning?
PY  - 2022
T2  - BMC Psychiatry
VL  - 22
IS  - 1
C7  - 427
DO  - 10.1186/s12888-022-03984-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132880241&doi=10.1186%2fs12888-022-03984-2&partnerID=40&md5=704d7d838c68fb6f26d2ca027f281f30
AB  - Background: To deliver appropriate mental healthcare interventions and support, it is imperative to be able to distinguish one person from the other. The current classification of mental illness (e.g., DSM) is unable to do that well, indicating the problem of diagnostic heterogeneity between disorders (i.e., the disorder categories have many common symptoms). As a result, the same person might be diagnosed with two different disorders by two independent clinicians. We argue that this problem might have resulted because these disorders were created by a group of humans (APA taskforce members) who relied on more intuition and consensus than data. Literature suggests that human-led decisions are prone to biases, group-thinking, and other factors (such as financial conflict of interest) that can enormously influence creating diagnostic and treatment guidelines. Therefore, in this study, we inquire that if we prevent such human intervention (and thereby their associated biases) and use Artificial Intelligence (A.I.) to form those disorder structures from the data (patient-reported symptoms) directly, then can we come up with homogenous clusters or categories (representing disorders/syndromes: a group of co-occurring symptoms) that are adequately distinguishable from each other for them to be clinically useful. Additionally, we inquired how these A.I.-created categories differ (or are similar) from human-created categories. Finally, to the best of our knowledge, this is the first study, that demonstrated how to use narrative qualitative data from patients with psychopathology and group their experiences using an A.I. Therefore, the current study also attempts to serve as a proof-of-concept. Method: We used secondary data scraped from online communities and consisting of 10,933 patients’ narratives about their lived experiences. These patients were diagnosed with one or more DSM diagnoses for mental illness. Using Natural Language Processing techniques, we converted the text data into a numeric form. We then used an Unsupervised Machine Learning algorithm called K-Means Clustering to group/cluster the symptoms. Results: Using the data mining approach, the A.I. found four categories/clusters formed from the data. We presented ten symptoms or experiences under each cluster to demonstrate the practicality of application and understanding. We also identified the transdiagnostic factors and symptoms that were unique to each of these four clusters. We explored the extent of similarities between these clusters and studied the difference in data density in them. Finally, we reported the silhouette score of + 0.046, indicating that the clusters are poorly distinguishable from each other (i.e., they have high overlapping symptoms). Discussion: We infer that whether humans attempt to categorise mental illnesses or an A.I., the result is that the categories of mental disorders will not be unique enough to be able to distinguish one service seeker from another. Therefore, the categorical approach of diagnosing mental disorders can be argued to fall short of its purpose. We need to search for a classification system beyond the categorical approaches even if there are secondary merits (such as ease of communication and black-and-white (binary) decision making). However, using our A.I. based data mining approach had several meritorious findings. For example, we found that some symptoms are more exclusive or unique to one cluster. In contrast, others are shared by most other clusters (i.e., identification of transdiagnostic experiences). Such differences are interesting objects of inquiry for future studies. For example, in clear contrast to the traditional diagnostic systems, while some experiences, such as auditory hallucinations, are present in all four clusters, others, such as trouble with eating, are exclusive to one cluster (representing a syndrome: a group of co-occurring symptoms). We argue that trans-diagnostic conditions (e.g., auditory hallucinations) might be prime targets for symptom-level interventions. For syndrome-level grouping and intervention, however, we argue that exclusive symptoms are the main targets. Conclusion: Categorical approach to mental disorders is not a way forward because the categories are not unique enough and have several shared symptoms. We argue that the same symptoms can be present in more than one syndrome, although dimensionally different. However, we need additional studies to test this hypothesis. Future directions and implications were discussed. © 2022, The Author(s).
PB  - BioMed Central Ltd
C2  - 35751077
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - CONF
AU  - Li, W.
AU  - Zhao, J.
AU  - Gao, H.
TI  - A Prompt Tuning Method for Chinese Medical Text Classification
PY  - 2023
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 14178 LNAI
SP  - 151
EP  - 166
DO  - 10.1007/978-3-031-46671-7_11
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177477905&doi=10.1007%2f978-3-031-46671-7_11&partnerID=40&md5=4e69b1e5b4d36ff339793cd746522542
AB  - The field of clinical medicine involves complex data analysis and mining, and text classification task in natural language processing can assist in case screening, etiology analysis, disease prediction, and other aspects in the medical field, thereby improving research efficiency and accuracy. However, obtaining supervised data is difficult due to the fact that medical texts such as cases often contain sensitive patient information. This difficulty partially explains why existing text classification methods do not perform well in medical tasks. Although the prompt tuning performs better than traditional neural networks and fine tuning methods in few-shot learning and interpretability. Chinese requires at least two characters to express complex semantics, which makes it challenging to apply the prompt tuning method that is primarily designed for English in Chinese text classification tasks. To address this issue, we propose Knowledge Enhanced Multi-Token Prompt Tuning (KMPT). KMPT first uses multiple tokens as label words to have complete Chinese semantics, and then uses external knowledge to expand the label words set, improving coverage and reducing bias. The experimental results on the Chinese medical dataset CHIP-CTC show that KMPT outperforms baseline methods in Chinese medical text classification tasks and has better interpretability and convergence speed than fine tuning. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Enamoto, L.M.
AU  - Weigang, L.
AU  - Filho, G.P.R.
AU  - Costa, P.C.
TI  - Generic Multimodal Gradient-based Meta Learner Framework
PY  - 2023
T2  - 2023 26th International Conference on Information Fusion, FUSION 2023
DO  - 10.23919/FUSION52260.2023.10224143
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171550519&doi=10.23919%2fFUSION52260.2023.10224143&partnerID=40&md5=403cab0047819726bb5ab75cd989da29
AB  - Research in Natural Language Processing, bio-medicine, and computer vision achieved excellent results in machine learning due to the success of the Transformer-based models. However, these excellent results depend on the labeled high-quality and large-scale datasets. If one of these requirements is not met, the model may lack generalization ability, and its performance will be unsatisfactory. To address these issues, this research proposes a Generic Multimodal Gradient-Based Meta Framework (GeMGF) trained from scratch to avoid language bias, learns from a few data, and reduces the model degradation trained on a finite dataset. GeMGF was evaluated using the benchmark dataset CUB-200-2011 for the text and image classification tasks. The results show that GeMGF outperforms the state-of-the-art models with 93.2% accuracy. GeMGF is simple, efficient, and adaptable to other data modalities and fields.  © 2023 International Society of Information Fusion.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Salem, M.
AU  - Ryan, M.A.
AU  - Oliver, A.
AU  - Hussain, K.F.
AU  - Lladó, X.
TI  - Improving the detection of new lesions in multiple sclerosis with a cascaded 3D fully convolutional neural network approach
PY  - 2022
T2  - Frontiers in Neuroscience
VL  - 16
C7  - 1007619
DO  - 10.3389/fnins.2022.1007619
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143406068&doi=10.3389%2ffnins.2022.1007619&partnerID=40&md5=3fff36a2042677598cb2402de4688309
AB  - Longitudinal magnetic resonance imaging (MRI) has an important role in multiple sclerosis (MS) diagnosis and follow-up. Specifically, the presence of new lesions on brain MRI scans is considered a robust predictive biomarker for the disease progression. New lesions are a high-impact prognostic factor to predict evolution to MS or risk of disability accumulation over time. However, the detection of this disease activity is performed visually by comparing the follow-up and baseline scans. Due to the presence of small lesions, misregistration, and high inter-/intra-observer variability, this detection of new lesions is prone to errors. In this direction, one of the last Medical Image Computing and Computer Assisted Intervention (MICCAI) challenges was dealing with this automatic new lesion quantification. The MSSEG-2: MS new lesions segmentation challenge offers an evaluation framework for this new lesion segmentation task with a large database (100 patients, each with two-time points) compiled from the OFSEP (Observatoire français de la sclérose en plaques) cohort, the French MS registry, including 3D T2-w fluid-attenuated inversion recovery (T2-FLAIR) images from different centers and scanners. Apart from a change in centers, MRI scanners, and acquisition protocols, there are more challenges that hinder the automated detection process of new lesions such as the need for large annotated datasets, which may be not easily available, or the fact that new lesions are small areas producing a class imbalance problem that could bias trained models toward the non-lesion class. In this article, we present a novel automated method for new lesion detection of MS patient images. Our approach is based on a cascade of two 3D patch-wise fully convolutional neural networks (FCNNs). The first FCNN is trained to be more sensitive revealing possible candidate new lesion voxels, while the second FCNN is trained to reduce the number of misclassified voxels coming from the first network. 3D T2-FLAIR images from the two-time points were pre-processed and linearly co-registered. Afterward, a fully CNN, where its inputs were only the baseline and follow-up images, was trained to detect new MS lesions. Our approach obtained a mean segmentation dice similarity coefficient of 0.42 with a detection F1-score of 0.5. Compared to the challenge participants, we obtained one of the highest precision scores (PPVL = 0.52), the best PPVL rate (0.53), and a lesion detection sensitivity (SensL of 0.53). Copyright © 2022 Salem, Ryan, Oliver, Hussain and Lladó.
PB  - Frontiers Media S.A.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Dave, A.D.
AU  - Ruaño, G.
AU  - Kost, J.
AU  - Wang, X.
TI  - Automated Extraction of Pain Symptoms: A Natural Language Approach using Electronic Health Records
PY  - 2022
T2  - Pain Physician
VL  - 25
IS  - 2
SP  - E245
EP  - E254
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127071815&partnerID=40&md5=3e6f02dca8c58a126261cea7125d5d0f
AB  - Background: Pain costs more than $600 billion annually and affects more than 100 million Americans, but is still a poorly understood problem and one for which there is very often limited effective treatment. Electronic health records (EHRs) are the only databases with a high volume of granular pain information that allows for documentation of detailed clinical notes on a patient’s subjective experience. Objectives: This study applied natural language processing (NLP) technology to an EHR dataset as part of a pilot study to capture pain information from clinical notes and prove its feasibility as an efficient method. Study Design: Retrospective study Setting: All data were from UConn Health John Dempsey Hospital (JDH) in Farmington, CT. Methods: The JDH EHR dataset contains 611,355 clinical narratives from 359,854 patients from diverse demographic backgrounds from 2010 through 2019. These data were processed through a customized NLP pipeline. A training set of 100 notes was annotated based on focus group-generated ontology and used to generate and evaluate an NLP model that was later tested on the remaining notes. Validation of the model was evaluated externally and performance was analyzed. Results: The model identified back pain as the most common location of experienced pain with 40,369 term frequencies. Patients most commonly experienced decreased mobility with their pain with 7,375 term frequencies. Pain was most commonly found to be radiating with 26,967 term frequencies and patients most commonly rated their pain as 8/10 with 2,375 term frequencies. All parameters studied had statistical F-scores greater than 0.85. Limitations: A single-center, pilot study subject to reporting bias, recording bias, and missing patient data. Conclusions: Our customized NLP model demonstrated good and successful performance in extracting granular pain information from clinical notes in electronic health records. © 2022, American Society of Interventional Pain Physicians. All rights reserved.
PB  - American Society of Interventional Pain Physicians
C2  - 35322976
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - Gadson, D.S.
AU  - Wesley, D.B.
AU  - van der Stelt, C.M.
AU  - Lacey, E.
AU  - DeMarco, A.T.
AU  - Snider, S.F.
AU  - Turkeltaub, P.E.
TI  - Aphasia severity is modulated by race and lesion size in chronic survivors: A retrospective study
PY  - 2022
T2  - Journal of Communication Disorders
VL  - 100
C7  - 106270
DO  - 10.1016/j.jcomdis.2022.106270
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139371771&doi=10.1016%2fj.jcomdis.2022.106270&partnerID=40&md5=c9ee9683e9801a174dae3b7019bafc21
AB  - Introduction: In stroke survivors with aphasia (SWA), differences in behavioral language performance have been observed between Black and White Americans. These racial differences in aphasia outcomes may reflect biological stroke severity, disparities in access to care, potential assessment bias, or interactions between these factors and race. Understanding the origin of disparities in aphasia outcomes is critical to any efforts to promote health equity among SWA. In this study, we explore aphasia outcomes by examining the relationship between race, socioeconomic status, and neurological factors in SWA. Method: Eighty-five chronic left-hemisphere SWA (31 Black, 54 White) participated in the study. The primary aphasia outcome measure was the Western Aphasia Battery-Revised (WAB-R). Lesion size was measured based on manual lesion segmentations. FLAIR and T2 images were scored for severity of white matter disease. Independent sample t-tests were used to determine differences by race in education, age, income, aphasia severity, white matter disease, and lesion size. A linear regression model was used to explore factors that predicted aphasia severity on the WAB-R. Result: Level of education and estimated income differed by race in our sample. For predictors of aphasia severity, the regression model revealed a significant effect of lesion size on WAB Aphasia Quotient and an interaction of race x lesion size, such that Black and White participants with small lesions had similar WAB scores, but in individuals with larger lesions, Black participants had lower WAB scores than White participants. Conclusion: We suggest two explanations for the difference between Black and White SWA in the relationship between lesion size and aphasia severity. First, the impact of disparities in access to rehabilitation after stroke may be more evident when a stroke is larger and causes significant aphasia. Additionally, an assessment bias in aphasia outcome measures may be more evident with increasing severity of aphasia. Future studies should further discern the drivers of observed disparities in aphasia outcomes in order to identify opportunities to improve equity in aphasia care. © 2022
PB  - Elsevier Inc.
C2  - 36215784
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - Hadar-Shoval, D.
AU  - Elyoseph, Z.
AU  - Lvovsky, M.
TI  - The plasticity of ChatGPT’s mentalizing abilities: personalization for personality structures
PY  - 2023
T2  - Frontiers in Psychiatry
VL  - 14
C7  - 1234397
DO  - 10.3389/fpsyt.2023.1234397
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171384832&doi=10.3389%2ffpsyt.2023.1234397&partnerID=40&md5=67d32007a50151845495b7f9ddd42bc0
AB  - This study evaluated the potential of ChatGPT, a large language model, to generate mentalizing-like abilities that are tailored to a specific personality structure and/or psychopathology. Mentalization is the ability to understand and interpret one’s own and others’ mental states, including thoughts, feelings, and intentions. Borderline Personality Disorder (BPD) and Schizoid Personality Disorder (SPD) are characterized by distinct patterns of emotional regulation. Individuals with BPD tend to experience intense and unstable emotions, while individuals with SPD tend to experience flattened or detached emotions. We used ChatGPT’s free version 23.3 and assessed the extent to which its responses akin to emotional awareness (EA) were customized to the distinctive personality structure-character characterized by Borderline Personality Disorder (BPD) and Schizoid Personality Disorder (SPD), employing the Levels of Emotional Awareness Scale (LEAS). ChatGPT was able to accurately describe the emotional reactions of individuals with BPD as more intense, complex, and rich than those with SPD. This finding suggests that ChatGPT can generate mentalizing-like responses consistent with a range of psychopathologies in line with clinical and theoretical knowledge. However, the study also raises concerns regarding the potential for stigmas or biases related to mental diagnoses to impact the validity and usefulness of chatbot-based clinical interventions. We emphasize the need for the responsible development and deployment of chatbot-based interventions in mental health, which considers diverse theoretical frameworks. Copyright © 2023 Hadar-Shoval, Elyoseph and Lvovsky.
PB  - Frontiers Media SA
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 19
ER  -

TY  - JOUR
AU  - Chaunzwa, T.L.
AU  - Del Rey, M.Q.
AU  - Bitterman, D.S.
TI  - Clinical Informatics Approaches to Understand and Address Cancer Disparities
PY  - 2022
T2  - Yearbook of Medical Informatics
VL  - 31
IS  - 1
SP  - 121
EP  - 130
DO  - 10.1055/s-0042-1742511
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143352621&doi=10.1055%2fs-0042-1742511&partnerID=40&md5=8de3c4b8226b83ad1b20c81eabe71018
AB  - Objectives: Disparities in cancer incidence and outcomes across race, ethnicity, gender, socioeconomic status, and geography are well-documented, but their etiologies are often poorly understood and multifactorial. Clinical informatics can provide tools to better understand and address these disparities by enabling high-throughput analysis of multiple types of data. Here, we review recent efforts in clinical informatics to study and measure disparities in cancer. Methods: We carried out a narrative review of clinical informatics studies related to cancer disparities and bias published from 2018-2021, with a focus on domains such as real-world data (RWD) analysis, natural language processing (NLP), radiomics, genomics, proteomics, metabolomics, and metagenomics. Results: Clinical informatics studies that investigated cancer disparities across race, ethnicity, gender, and age were identified. Most cancer disparities work within clinical informatics used RWD analysis, NLP, radiomics, and genomics. Emerging applications of clinical informatics to understand cancer disparities, including proteomics, metabolomics, and metagenomics, were less well represented in the literature but are promising future research avenues. Algorithmic bias was identified as an important consideration when developing and implementing cancer clinical informatics techniques, and efforts to address this bias were reviewed. Conclusions: In recent years, clinical informatics has been used to probe a range of data sources to understand cancer disparities across different populations. As informatics tools become integrated into clinical decision-making, attention will need to be paid to ensure that algorithmic bias does not amplify existing disparities. In our increasingly interconnected medical systems, clinical informatics is poised to untap the full potential of multi-platform health data to address cancer disparities.  © 2022 IMIA and Georg Thieme Verlag KG.
PB  - Thieme Medical Publishers, Inc.
C2  - 36463869
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Zengul, F.D.
AU  - Oner, N.
AU  - Ozaydin, B.
AU  - Hall, A.G.
AU  - Berner, E.S.
AU  - Cimino, J.J.
AU  - Lemak, C.H.
TI  - Mapping 2 Decades of Research in Health Services Research, Health Policy, and Health Economics Journals
PY  - 2022
T2  - Medical Care
VL  - 60
IS  - 3
SP  - 264
EP  - 272
DO  - 10.1097/MLR.0000000000001685
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124636223&doi=10.1097%2fMLR.0000000000001685&partnerID=40&md5=386d90d7b1500bbd7a206c02bfb55e02
AB  - Objective: To identify major research topics and exhibit trends in these topics in 15 health services research, health policy, and health economics journals over 2 decades. Data Sources: The study sample of 35,159 abstracts (1999-2020) were collected from PubMed for 15 journals. Study Design: The study used a 3-phase approach for text analyses: (1) developing the corpus of 40,618 references from PubMed (excluding 5459 of those without abstract or author information); (2) preprocessing and generating the term list using natural language processing to eliminate irrelevant textual data and identify important terms and phrases; (3) analyzing the preprocessed text data using latent semantic analysis, topic analyses, and multiple correspondence analysis. Principal Findings: Application of analyses generated 16 major research topics: (1) implementation/intervention science; (2) HIV and women's health; (3) outcomes research and quality; (4) veterans/military studies; (5) provider/primary-care interventions; (6) geriatrics and formal/informal care; (7) policies and health outcomes; (8) medication treatment/therapy; (9) patient interventions; (10) health insurance legislation and policies; (11) public health policies; (12) literature reviews; (13) cost-effectiveness and economic evaluation; (14) cancer care; (15) workforce issues; and (16) socioeconomic status and disparities. The 2-dimensional map revealed that some journals have stronger associations with specific topics. Findings were not consistent with previous studies based on user perceptions. Conclusion: Findings of this study can be used by the stakeholders of health services research, policy, and economics to develop future research agendas, target journal submissions, and generate interdisciplinary solutions by examining overlapping journals for particular topics. © 2022 Lippincott Williams and Wilkins. All rights reserved.
PB  - Lippincott Williams and Wilkins
C2  - 34984990
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Maimone, C.
AU  - Dolan, B.M.
AU  - Green, M.M.
AU  - Sanguino, S.M.
AU  - Garcia, P.M.
AU  - O’brien, C.L.
TI  - Utilizing Natural Language Processing of Narrative Feedback to Develop a Predictive Model of PreClerkship Performance: Lessons Learned
PY  - 2023
T2  - Perspectives on Medical Education
VL  - 12
IS  - 1
SP  - 141
EP  - 148
DO  - 10.5334/pme.40
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158022899&doi=10.5334%2fpme.40&partnerID=40&md5=3b3c42dffa4ce08d0d0ba17c35a797cb
AB  - Background: Natural language processing is a promising technique that can be used to create efficiencies in the review of narrative feedback to learners. The Feinberg School of Medicine has implemented formal review of pre-clerkship narrative feedback since 2014 through its portfolio assessment system but this process requires considerable time and effort. This article describes how natural language processing was used to build a predictive model of pre-clerkship student performance that can be utilized to assist competency committee reviews. Approach: The authors took an iterative and inductive approach to the analysis, which allowed them to identify characteristics of narrative feedback that are both predictive of performance and useful to faculty reviewers. Words and phrases were manually grouped into topics that represented concepts illustrating student performance. Topics were reviewed by experienced reviewers, tested for consistency across time, and checked to ensure they did not demonstrate bias. Outcomes: Sixteen topic groups of words and phrases were found to be predictive of performance. The best-fitting model used a combination of topic groups, word counts, and categorical ratings. The model had an AUC value of 0.92 on the training data and 0.88 on the test data. Reflection: A thoughtful, careful approach to using natural language processing was essential. Given the idiosyncrasies of narrative feedback in medical education, standard natural language processing packages were not adequate for predicting student outcomes. Rather, employing qualitative techniques including repeated member checking and iterative revision resulted in a useful and salient predictive model. © 2023 The Author(s).
PB  - Ubiquity Press
C2  - 37151853
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 9
ER  -

TY  - JOUR
AU  - Beranová, L.
AU  - Joachimiak, M.P.
AU  - Kliegr, T.
AU  - Rabby, G.
AU  - Sklenák, V.
TI  - Why was this cited? Explainable machine learning applied to COVID-19 research literature
PY  - 2022
T2  - Scientometrics
VL  - 127
IS  - 5
SP  - 2313
EP  - 2349
DO  - 10.1007/s11192-022-04314-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127726326&doi=10.1007%2fs11192-022-04314-9&partnerID=40&md5=e3a5260786747850c987fdfa161efd8a
AB  - Multiple studies have investigated bibliometric factors predictive of the citation count a research article will receive. In this article, we go beyond bibliometric data by using a range of machine learning techniques to find patterns predictive of citation count using both article content and available metadata. As the input collection, we use the CORD-19 corpus containing research articles—mostly from biology and medicine—applicable to the COVID-19 crisis. Our study employs a combination of state-of-the-art machine learning techniques for text understanding, including embeddings-based language model BERT, several systems for detection and semantic expansion of entities: ConceptNet, Pubtator and ScispaCy. To interpret the resulting models, we use several explanation algorithms: random forest feature importance, LIME, and Shapley values. We compare the performance and comprehensibility of models obtained by “black-box” machine learning algorithms (neural networks and random forests) with models built with rule learning (CORELS, CBA), which are intrinsically explainable. Multiple rules were discovered, which referred to biomedical entities of potential interest. Of the rules with the highest lift measure, several rules pointed to dipeptidyl peptidase4 (DPP4), a known MERS-CoV receptor and a critical determinant of camel to human transmission of the camel coronavirus (MERS-CoV). Some other interesting patterns related to the type of animal investigated were found. Articles referring to bats and camels tend to draw citations, while articles referring to most other animal species related to coronavirus are lowly cited. Bat coronavirus is the only other virus from a non-human species in the betaB clade along with the SARS-CoV and SARS-CoV-2 viruses. MERS-CoV is in a sister betaC clade, also close to human SARS coronaviruses. Thus both species linked to high citation counts harbor coronaviruses which are more phylogenetically similar to human SARS viruses. On the other hand, feline (FIPV, FCOV) and canine coronaviruses (CCOV) are in the alpha coronavirus clade and more distant from the betaB clade with human SARS viruses. Other results include detection of apparent citation bias favouring authors with western sounding names. Equal performance of TF-IDF weights and binary word incidence matrix was observed, with the latter resulting in better interpretability. The best predictive performance was obtained with a “black-box” method—neural network. The rule-based models led to most insights, especially when coupled with text representation using semantic entity detection methods. Follow-up work should focus on the analysis of citation patterns in the context of phylogenetic trees, as well on patterns referring to DPP4, which is currently considered as a SARS-Cov-2 therapeutic target. © 2022, Akadémiai Kiadó, Budapest, Hungary.
PB  - Springer Science and Business Media B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 15
ER  -

TY  - CONF
AU  - Bautista, Y.J.P.
AU  - Theran, C.
AU  - Aló, R.
AU  - Lima, V.
TI  - Health Disparities Through Generative AI Models: A Comparison Study Using a Domain Specific Large Language Model
PY  - 2023
T2  - Lecture Notes in Networks and Systems
VL  - 813 LNNS
SP  - 220
EP  - 232
DO  - 10.1007/978-3-031-47454-5_17
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177067963&doi=10.1007%2f978-3-031-47454-5_17&partnerID=40&md5=418cd3a6a4c69227019ec535ae78e9f7
AB  - Health disparities are differences in health outcomes and access to healthcare between different groups, including racial and ethnic minorities, low-income people, and rural residents. An artificial intelligence (AI) program called large language models (LLMs) can understand and generate human language, improving health communication and reducing health disparities. There are many challenges in using LLMs in human-doctor interaction, including the need for diverse and representative data, privacy concerns, and collaboration between healthcare providers and technology experts. We introduce the comparative investigation of domain-specific large language models such as SciBERT with a multi-purpose LLMs BERT. We used cosine similarity to analyze text queries about health disparities in exam rooms when factors such as race are used alone. Using text queries, SciBERT fails when it doesn’t differentiate between queries text: “race” alone and “perpetuates health disparities.” We believe clinicians can use generative AI to create a draft response when communicating asynchronously with patients. However, careful attention must be paid to ensure they are developed and implemented ethically and equitably. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Waisberg, E.
AU  - Ong, J.
AU  - Kamran, S.A.
AU  - Masalkhi, M.
AU  - Zaman, N.
AU  - Sarker, P.
AU  - Lee, A.G.
AU  - Tavakkoli, A.
TI  - Bridging artificial intelligence in medicine with generative pre-trained transformer (GPT) technology
PY  - 2023
T2  - Journal of Medical Artificial Intelligence
VL  - 6
C7  - 13
DO  - 10.21037/jmai-23-36
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172797304&doi=10.21037%2fjmai-23-36&partnerID=40&md5=9f1ab793fe6607a5ab9208b07ce73b2c
AB  - Since its public release in November 2022, the usage of ChatGPT (Open AI, USA) has been unprecedented. This large language model (LLM) can produce human-like text from deep-learning techniques. LLMs are rapidly approaching human-level performance. ChatGPT can potentially help democratize the ability to code, by allowing clinicians to be able to develop basic artificial intelligence (AI) techniques. By leveraging AI models, these clinicians can expand the scope of their research abilities, and this can potentially lead to an AI in medicine revolution, where clinicians are able to generate clinically-focused AI techniques with the goal of improving patient outcomes across all domains. In this paper, we examine the performance of ChatGPT at developing an AI program for medicine and its associated limitations and challenges. Similar to the majority of AI models, the ethical concerns surrounding its application in medicine remains, which includes biases, patient autonomy, and confidentiality, transparency, and accuracy of data. ChatGPT must also be used in accordance with local healthcare regulations, such as the Health Insurance Portability and Accountability Act (HIPAA) in the United States. All things considered, ChatGPT and future generative AI technologies will democratize the ability to code and develop AI, likely leading to breakthroughs in the medical AI sector. © Journal of Medical Artificial Intelligence. All rights reserved.
PB  - AME Publishing Company
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 11
ER  -

TY  - CONF
AU  - Jain, M.
AU  - Rai, C.S.
AU  - Jain, J.
TI  - RadiomicsGAN: Image Augmentation and Translation using the Conditional Gan Framework for Enhanced Prediction of Brain Degeneration
PY  - 2023
T2  - Proceedings of the 17th INDIACom; 2023 10th International Conference on Computing for Sustainable Global Development, INDIACom 2023
SP  - 507
EP  - 513
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159605001&partnerID=40&md5=b26340010117a3cdb9baa823405ab671
AB  - Machine learning and deep learning are effective techniques to predict alarming medical conditions in advance. The event prediction models require high quality and bigger datasets for rigorous training and testing. Medical data acquisition, annotations, pre-processing, and de-identification are cumbersome and sluggish jobs. CNN models require enormous big data to predict accurately. These models face under-fitting and over-fitting phenomenon due to redundancy and biasing in the dataset. In this study we proposed a novel framework Radiomics GAN to translate different MRI formats e.g. T1 to T2 and T1 to FLAIR. This translated data are used in a 3DCNN model to predict early detection of brain degeneration. We experimented to predict brain degeneration, by applying 3DCNN on data augmented and processed using GANs, which shows a very promising result as compared to the data augmented through geometric and photometric transformations. We found that data augmentation using novel GAN helped to achieve a sturdy prediction model whose accuracy is much better than on the datasets augmented using geometrical and photographic augmentation. GAN augmented datasets in T1 formats achieved an accuracy of 95.3%, while geometric transformation based augmented dataset achieved 92.2% in 3DCNN model. The RadiomicsGAN framework achieved very high prediction accuracy on T2 transferred dataset i.e. 98.2% and FLAIR formats achieved an accuracy of 97.4% in the same 3DCNN model. This study is very helpful for the medical community. Radiologists do not need to refer to different MRI formats for accurate decisions. © 2023 Bharati Vidyapeeth, New Delhi.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Roszik, J.
AU  - Lee, J.J.
AU  - Wu, Y.-H.
AU  - Liu, X.
AU  - Kawakami, M.
AU  - Kurie, J.M.
AU  - Belouali, A.
AU  - Boca, S.M.
AU  - Gupta, S.
AU  - Beckman, R.A.
AU  - Madhavan, S.
AU  - Dmitrovsky, E.
TI  - Real-world Studies Link NSAID Use to Improved Overall Lung Cancer Survival
PY  - 2022
T2  - Cancer Research Communications
VL  - 2
IS  - 7
SP  - 590
EP  - 601
DO  - 10.1158/2767-9764.CRC-22-0179
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194900054&doi=10.1158%2f2767-9764.CRC-22-0179&partnerID=40&md5=0b2c3f9c7e47fcd3ed78011e4a240bf9
AB  - Inflammation is a cancer hallmark. NSAIDs improve overall survival (OS) in certain cancers. Real-world studies explored here whether NSAIDs improve non–small cell lung cancer (NSCLC) OS. Analyses independently interrogated clinical databases from The University of Texas MD Anderson Cancer Center (MDACC cohort, 1987 to 2015; 33,162 NSCLCs and 3,033 NSAID users) and Georgetown-MedStar health system (Georgetown cohort, 2000 to 2019; 4,497 NSCLCs and 1,993 NSAID users). Structured and unstructured clinical data were extracted from electronic health records using natural language processing (NLP). Associations were made between NSAID use and NSCLC prognostic features (tobacco use, gender, race, and body mass index, BMI). NSAIDs were statistically significantly (P < 0.0001) associated with increased NSCLC survival (5-year OS 29.7% for NSAID users vs. 13.1% for nonusers) in the MDACC cohort. NSAID users gained 11.6 months over nonusers in 5-year restricted mean survival time. Stratified analysis by stage, histopathology, and multicovariable assessment substantiated benefits. NSAID users were pooled independent of NSAID type and by NSAID type. Landmark analysis excluded immortal time bias. Survival improvements (P < 0.0001) were confirmed in the Georgetown cohort. Thus, real-world NSAID usage was independently associated with increased NSCLC survival in the MDACC and Georgetown cohorts. Findings were confirmed by landmark analyses and NSAID type. The OS benefits persisted despite tobacco use and did not depend on gender, race, or BMI (MDACC cohort, P < 0.0001). These real-world findings could guide future NSAID lung cancer randomized trials. Significance: NLP and real-world studies conducted in large cohorts explored whether NSAIDs improved survival across NSCLC stages, histopathology, gender, smoking history, or demographic groups. A statistically significant association between NSAID use and NSCLC survival was found. This provides a rationale for future NSAID randomized NSCLC trials. © 2022 The Authors; Published by the American Association for Cancer Research.
PB  - American Association for Cancer Research Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Salahat, M.
AU  - Ali, L.
AU  - Ghazal, T.M.
AU  - Alzoubi, H.M.
TI  - Personality Assessment Based on Natural Stream of Thoughts Empowered with Machine Learning
PY  - 2023
T2  - Computers, Materials and Continua
VL  - 76
IS  - 1
SP  - 1
EP  - 17
DO  - 10.32604/cmc.2023.036019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164275386&doi=10.32604%2fcmc.2023.036019&partnerID=40&md5=9bd806fffa1379cbee0ac6ddf2aea6a1
AB  - Knowing each other is obligatory in a multi-agent collaborative environment. Collaborators may develop the desired know-how of each other in various aspects such as habits, job roles, status, and behaviors. Among different distinguishing characteristics related to a person, personality traits are an effective predictive tool for an individual’s behavioral pattern. It has been observed that when people are asked to share their details through questionnaires, they intentionally or unintentionally become biased. They knowingly or unknowingly provide enough information in much-unbiased comportment in open writing about themselves. Such writings can effectively assess an individual’s personality traits that may yield enormous possibilities for applications such as forensic departments, job interviews, mental health diagnoses, etc. Stream of consciousness, collected by James Pennbaker and Laura King, is one such way of writing, referring to a narrative technique where the emotions and thoughts of the writer are presented in a way that brings the reader to the fluid through the mental states of the narrator. Moreover, computationally, various attempts have been made in an individual’s personality traits assessment through deep learning algorithms; however, the effectiveness and reliability of results vary with varying word embedding techniques. This article proposes an empirical approach to assessing personality by applying convolutional networks to text documents. Bidirectional Encoder Representations from Transformers (BERT) word embedding technique is used for word vector generation to enhance the contextual meanings. © 2023 Tech Science Press. All rights reserved.
PB  - Tech Science Press
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - JOUR
AU  - Al-Garadi, M.A.
AU  - Kim, S.
AU  - Guo, Y.
AU  - Warren, E.
AU  - Yang, Y.-C.
AU  - Lakamana, S.
AU  - Sarker, A.
TI  - Natural language model for automatic identification of Intimate Partner Violence reports from Twitter
PY  - 2022
T2  - Array
VL  - 15
C7  - 100217
DO  - 10.1016/j.array.2022.100217
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135331101&doi=10.1016%2fj.array.2022.100217&partnerID=40&md5=d03560dbd07ba285dd42be835e2f5a0c
AB  - Intimate partner violence (IPV) is a preventable public health problem that affects millions of people worldwide. Approximately one in four women are estimated to be or have been victims of severe violence at some point in their lives, irrespective of age, ethnicity, and economic status. Victims often report IPV experiences on social media, and automatic detection of such reports via machine learning may enable improved surveillance and targeted distribution of support and/or interventions for those in need. However, no artificial intelligence systems for automatic detection currently exists, and we attempted to address this research gap. We collected posts from Twitter using a list of IPV-related keywords, manually reviewed subsets of retrieved posts, and prepared annotation guidelines to categorize tweets into IPV-report or non-IPV-report. We annotated 6,348 tweets in total, with the inter-annotator agreement (IAA) of 0.86 (Cohen's kappa) among 1,834 double-annotated tweets. The class distribution in the annotated dataset was highly imbalanced, with only 668 posts (∼11%) labeled as IPV-report. We then developed an effective natural language processing model to identify IPV-reporting tweets automatically. The developed model achieved classification F1-scores of 0.76 for the IPV-report class and 0.97 for the non-IPV-report class. We conducted post-classification analyses to determine the causes of system errors and to ensure that the system did not exhibit biases in its decision making, particularly with respect to race and gender. Our automatic model can be an essential component for a proactive social media-based intervention and support framework, while also aiding population-level surveillance and large-scale cohort studies. © 2022 The Authors
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 25
ER  -

TY  - JOUR
AU  - LeBaron, V.
AU  - Boukhechba, M.
AU  - Edwards, J.
AU  - Flickinger, T.
AU  - Ling, D.
AU  - Barnes, L.E.
TI  - Exploring the Use of Wearable Sensors and Natural Language Processing Technology to Improve Patient-Clinician Communication: Protocol for a Feasibility Study
PY  - 2022
T2  - JMIR Research Protocols
VL  - 11
IS  - 5
C7  - e37975
DO  - 10.2196/37975
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130614973&doi=10.2196%2f37975&partnerID=40&md5=0ae37d904e147d834016d45a8384887b
AB  - Background: Effective communication is the bedrock of quality health care, but it continues to be a major problem for patients, family caregivers, health care providers, and organizations. Although progress related to communication skills training for health care providers has been made, clinical practice and research gaps persist, particularly regarding how to best monitor, measure, and evaluate the implementation of communication skills in the actual clinical setting and provide timely feedback about communication effectiveness and quality. Objective: Our interdisciplinary team of investigators aims to develop, and pilot test, a novel sensing system and associated natural language processing algorithms (CommSense) that can (1) be used on mobile devices, such as smartwatches; (2) reliably capture patient-clinician interactions in a clinical setting; and (3) process these communications to extract key markers of communication effectiveness and quality. The long-term goal of this research is to use CommSense in a variety of health care contexts to provide real-time feedback to end users to improve communication and patient health outcomes. Methods: This is a 1-year pilot study. During Phase I (Aim 1), we will identify feasible metrics of communication to extract from conversations using CommSense. To achieve this, clinical investigators will conduct a thorough review of the recent health care communication and palliative care literature to develop an evidence-based “ideal and optimal” list of communication metrics. This list will be discussed collaboratively within the study team and consensus will be reached regarding the included items. In Phase II (Aim 2), we will develop the CommSense software by sharing the “ideal and optimal” list of communication metrics with engineering investigators to gauge technical feasibility. CommSense will build upon prior work using an existing Android smartwatch platform (SWear) and will include sensing modules that can collect (1) physiological metrics via embedded sensors to measure markers of stress (eg, heart rate variability), (2) gesture data via embedded accelerometer and gyroscope sensors, and (3) voice and ultimately textual features via the embedded microphone. In Phase III (Aim 3), we will pilot test the ability of CommSense to accurately extract identified communication metrics using simulated clinical scenarios with nurse and physician participants. Results: Development of the CommSense platform began in November 2021, with participant recruitment expected to begin in summer 2022. We anticipate that preliminary results will be available in fall 2022. Conclusions: CommSense is poised to make a valuable contribution to communication science, ubiquitous computing technologies, and natural language processing. We are particularly eager to explore the ability of CommSense to support effective virtual and remote health care interactions and reduce disparities related to patient-clinician communication in the context of serious illness. ©Virginia LeBaron, Mehdi Boukhechba, James Edwards, Tabor Flickinger, David Ling, Laura E Barnes.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Alexander, G.
AU  - Bahja, M.
AU  - Butt, G.F.
TI  - Automating Large-scale Health Care Service Feedback Analysis: Sentiment Analysis and Topic Modeling Study
PY  - 2022
T2  - JMIR Medical Informatics
VL  - 10
IS  - 4
C7  - e29385
DO  - 10.2196/29385
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128458371&doi=10.2196%2f29385&partnerID=40&md5=b43e8a61a8012ba84c7318994fcaaa33
AB  - Background: Obtaining patient feedback is an essential mechanism for health care service providers to assess their quality and effectiveness. Unlike assessments of clinical outcomes, feedback from patients offers insights into their lived experiences. The Department of Health and Social Care in England via National Health Service Digital operates a patient feedback web service through which patients can leave feedback of their experiences in structured and free-text report forms. Free-text feedback, compared with structured questionnaires, may be less biased by the feedback collector and, thus, more representative; however, it is harder to analyze in large quantities and challenging to derive meaningful, quantitative outcomes. Objective: The aim of this study is to build a novel data analysis and interactive visualization pipeline accessible through an interactive web application to facilitate the interrogation of and provide unique insights into National Health Service patient feedback. Methods: This study details the development of a text analysis tool that uses contemporary natural language processing and machine learning models to analyze free-text clinical service reviews to develop a robust classification model and interactive visualization web application. The methodology is based on the design science research paradigm and was conducted in three iterations: a sentiment analysis of the patient feedback corpus in the first iteration, topic modeling (unigram and bigram)-based analysis for topic identification in the second iteration, and nested topic modeling in the third iteration that combines sentiment analysis and topic modeling methods. An interactive data visualization web application for use by the general public was then created, presenting the data on a geographic representation of the country, making it easily accessible. Results: Of the 11,103 possible clinical services that could be reviewed across England, 2030 (18.28%) different services received a combined total of 51,845 reviews between October 1, 2017, and September 30, 2019. Dominant topics were identified for the entire corpus followed by negative- and positive-sentiment topics in turn. Reviews containing high- and low-sentiment topics occurred more frequently than reviews containing less polarized topics. Time-series analysis identified trends in topic and sentiment occurrence frequency across the study period. Conclusions: Using contemporary natural language processing techniques, unstructured text data were effectively characterized for further analysis and visualization. An efficient pipeline was successfully combined with a web application, making automated analysis and dissemination of large volumes of information accessible. This study represents a significant step in efforts to generate and visualize useful, actionable, and unique information from free-text patient reviews. © 2022 JMIR Publications Inc.. All right reserved.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 14
ER  -

TY  - JOUR
AU  - Cohen, I.G.
TI  - What Should ChatGPT Mean for Bioethics?
PY  - 2023
T2  - American Journal of Bioethics
VL  - 23
IS  - 10
SP  - 8
EP  - 16
DO  - 10.1080/15265161.2023.2233357
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165217907&doi=10.1080%2f15265161.2023.2233357&partnerID=40&md5=5cb8826c3130a0265017619e2b0f08fa
AB  - In the last several months, several major disciplines have started their initial reckoning with what ChatGPT and other Large Language Models (LLMs) mean for them–law, medicine, business among other professions. With a heavy dose of humility, given how fast the technology is moving and how uncertain its social implications are, this article attempts to give some early tentative thoughts on what ChatGPT might mean for bioethics. I will first argue that many bioethics issues raised by ChatGPT are similar to those raised by current medical AI–built into devices, decision support tools, data analytics, etc. These include issues of data ownership, consent for data use, data representativeness and bias, and privacy. I describe how these familiar issues appear somewhat differently in the ChatGPT context, but much of the existing bioethical thinking on these issues provides a strong starting point. There are, however, a few “new-ish” issues I highlight–by new-ish I mean issues that while perhaps not truly new seem much more important for it than other forms of medical AI. These include issues about informed consent and the right to know we are dealing with an AI, the problem of medical deepfakes, the risk of oligopoly and inequitable access related to foundational models, environmental effects, and on the positive side opportunities for the democratization of knowledge and empowering patients. I also discuss how races towards dominance (between large companies and between the U.S. and geopolitical rivals like China) risk sidelining ethics. © 2023 Taylor & Francis Group, LLC.
PB  - Taylor and Francis Ltd.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 49
ER  -

TY  - JOUR
AU  - Ito, N.
AU  - Kadomatsu, S.
AU  - Fujisawa, M.
AU  - Fukaguchi, K.
AU  - Ishizawa, R.
AU  - Kanda, N.
AU  - Kasugai, D.
AU  - Nakajima, M.
AU  - Goto, T.
AU  - Tsugawa, Y.
TI  - The Accuracy and Potential Racial and Ethnic Biases of GPT-4 in the Diagnosis and Triage of Health Conditions: Evaluation Study
PY  - 2023
T2  - JMIR Medical Education
VL  - 9
IS  - 1
C7  - e47532
DO  - 10.2196/47532
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178079652&doi=10.2196%2f47532&partnerID=40&md5=38c109330bae06014f7a5789aabaa016
AB  - Background: Whether GPT-4, the conversational artificial intelligence, can accurately diagnose and triage health conditions and whether it presents racial and ethnic biases in its decisions remain unclear. Objective: We aim to assess the accuracy of GPT-4 in the diagnosis and triage of health conditions and whether its performance varies by patient race and ethnicity. Methods: We compared the performance of GPT-4 and physicians, using 45 typical clinical vignettes, each with a correct diagnosis and triage level, in February and March 2023. For each of the 45 clinical vignettes, GPT-4 and 3 board-certified physicians provided the most likely primary diagnosis and triage level (emergency, nonemergency, or self-care). Independent reviewers evaluated the diagnoses as "correct" or "incorrect." Physician diagnosis was defined as the consensus of the 3 physicians. We evaluated whether the performance of GPT-4 varies by patient race and ethnicity, by adding the information on patient race and ethnicity to the clinical vignettes. Results: The accuracy of diagnosis was comparable between GPT-4 and physicians (the percentage of correct diagnosis was 97.8% (44/45; 95% CI 88.2%-99.9%) for GPT-4 and 91.1% (41/45; 95% CI 78.8%-97.5%) for physicians; P=.38). GPT-4 provided appropriate reasoning for 97.8% (44/45) of the vignettes. The appropriateness of triage was comparable between GPT-4 and physicians (GPT-4: 30/45, 66.7%; 95% CI 51.0%-80.0%; physicians: 30/45, 66.7%; 95% CI 51.0%-80.0%; P=.99). The performance of GPT-4 in diagnosing health conditions did not vary among different races and ethnicities (Black, White, Asian, and Hispanic), with an accuracy of 100% (95% CI 78.2%-100%). P values, compared to the GPT-4 output without incorporating race and ethnicity information, were all.99. The accuracy of triage was not significantly different even if patients' race and ethnicity information was added. The accuracy of triage was 62.2% (95% CI 46.5%-76.2%; P=.50) for Black patients; 66.7% (95% CI 51.0%-80.0%; P=.99) for White patients; 66.7% (95% CI 51.0%-80.0%; P=.99) for Asian patients, and 62.2% (95%CI 46.5%-76.2%; P=.69) for Hispanic patients. P values were calculated by comparing the outputs with and without conditioning on race and ethnicity. Conclusions: GPT-4's ability to diagnose and triage typical clinical vignettes was comparable to that of board-certified physicians. The performance of GPT-4 did not vary by patient race and ethnicity. These findings should be informative for health systems looking to introduce conversational artificial intelligence to improve the efficiency of patient diagnosis and triage. © 2023 The Author(s).
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 27
ER  -

TY  - CONF
AU  - Morales-Garzón, A.
AU  - Sánchez-Pérez, G.M.
AU  - Sierra, J.C.
AU  - Martin-Bautista, M.J.
TI  - The Promise of Query Answering Systems in Sexuality Studies: Current State, Challenges and Limitations
PY  - 2023
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 14113 LNAI
SP  - 39
EP  - 49
DO  - 10.1007/978-3-031-42935-4_4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172089045&doi=10.1007%2f978-3-031-42935-4_4&partnerID=40&md5=44339c71bcf0455f914f7d055e328727
AB  - Sexuality is a field of study that attempts to comprehend human behaviour, improve sexual health and understand culture and gender, among others. Recent advances and developments in artificial intelligence, specifically in query answering and natural language processing, can help to study the social relationship between population and sexuality. They are powerful tools to cope with crucial problems in the field, such as subjectivity, social desirability and social opinion biases. In this work, we review the state-of-the-art of AI-based methods in sexuality-related studies. Focusing on the psychological perspective, we analyse the role of query answering in this area of research. We discuss the necessary foundations, challenges, and limitations a query answering system must cover in this specialised and complex field. © 2023, Springer Nature Switzerland AG.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Lin, C.-C.
AU  - Akuhata-Huntington, Z.
AU  - Hsu, C.-W.
TI  - Comparing ChatGPT's ability to rate the degree of stereotypes and the consistency of stereotype attribution with those of medical students in New Zealand in developing a similarity rating test: a methodological study
PY  - 2023
T2  - Journal of Educational Evaluation for Health Professions
VL  - 20
IS  - 17
DO  - 10.3352/jeehp.2023.20.17
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162902398&doi=10.3352%2fjeehp.2023.20.17&partnerID=40&md5=14b650d6660405b9d1ab0f0b66da5782
AB  - This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Learning about one's implicit bias is crucial for improving one's cultural competency and thereby reducing health inequity. To evaluate bias among medical students following a previously developed cultural training program targeting New Zealand Maori, we developed a text-based, self-evaluation tool called the Similarity Rating Test (SRT). The development process of the SRT was resource-intensive, limiting its generalizability and applicability. Here, we explored the potential of ChatGPT, an automated chatbot, to assist in the development process of the SRT by comparing ChatGPT's and students' evaluations of the SRT. Despite results showing non-significant equivalence and difference between ChatGPT's and students' ratings, ChatGPT's ratings were more consistent than students' ratings. The consistency rate was higher for non-stereotypical than for stereotypical statements, regardless of rater type. Further studies are warranted to validate ChatGPT's potential for assisting in SRT development for implementation in medical education and evaluation of ethnic stereotypes and related topics.  © 2023 Korea.
PB  - Korea Health Personnel Licensing Examination Institute
C2  - 37385685
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Wang, L.
AU  - Li, Y.
AU  - Miller, T.
AU  - Bethard, S.
AU  - Savova, G.
TI  - Two-Stage Fine-Tuning for Improved Bias and Variance for Large Pretrained Language Models
PY  - 2023
T2  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
VL  - 1
SP  - 15746
EP  - 15761
DO  - 10.18653/v1/2023.acl-long.877
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174394561&doi=10.18653%2fv1%2f2023.acl-long.877&partnerID=40&md5=a9caa24921de8773c97c5c99df35cc33
AB  - The bias-variance tradeoff is the idea that learning methods need to balance model complexity with data size to minimize both under-fitting and over-fitting. Recent empirical work and theoretical analyses with over-parameterized neural networks challenge the classic bias-variance trade-off notion suggesting that no such trade-off holds: as the width of the network grows, bias monotonically decreases while variance initially increases followed by a decrease. In this work, we first provide a variance decomposition-based justification criteria to examine whether large pretrained neural models in a fine-tuning setting are generalizable enough to have low bias and variance. We then perform theoretical and empirical analysis using ensemble methods explicitly designed to decrease variance due to optimization. This results in essentially a two-stage fine-tuning algorithm that first ratchets down bias and variance iteratively, and then uses a selected fixed-bias model to further reduce variance due to optimization by ensembling. We also analyze the nature of variance change with the ensemble size in low- and high-resource classes. Empirical results show that this two-stage method obtains strong results on SuperGLUE tasks and clinical information extraction tasks. Code and settings are available: https://github.com/christa60/bias-var-fine-tuning-plms.git. © 2023 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - CONF
AU  - Tsai, C.-H.
AU  - Kadire, S.
AU  - Sreeramdas, T.
AU  - VanOrmer, M.
AU  - Thoene, M.
AU  - Hanson, C.
AU  - Berry, A.A.
AU  - Khazanchi, D.
TI  - Generating Personalized Pregnancy Nutrition Recommendations with GPT-Powered AI Chatbot
PY  - 2023
T2  - Proceedings of the International ISCRAM Conference
VL  - 2023-text
SP  - 263
EP  - 271
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171754630&partnerID=40&md5=9482557d6856c377c67965470515c408
AB  - Low socioeconomic status (SES) and inadequate nutrition during pregnancy are linked to health disparities and adverse outcomes, including an increased risk of preterm birth, low birth weight, and intrauterine growth restriction. AI-powered computational agents have enormous potential to address this challenge by providing nutrition guidelines or advice to patients with different health literacy and demographics. This paper presents our preliminary exploration of creating a GPT-powered AI chatbot called NutritionBot and investigates the implications for pregnancy nutrition recommendations. We used a user-centered design approach to define the target user persona and collaborated with medical professionals to co-design the chatbot. We integrated our proposed chatbot with ChatGPT to generate pregnancy nutrition recommendations tailored to patients’ lifestyles. Our contributions include introducing a design persona of a pregnant woman from an underserved population, co-designing a nutrition advice chatbot with healthcare experts, and sharing design implications for future GPT-based nutrition chatbots based on our preliminary findings. © 2023 Information Systems for Crisis Response and Management, ISCRAM. All rights reserved.
PB  - Information Systems for Crisis Response and Management, ISCRAM
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Baldwin, P.
AU  - Yaneva, V.
AU  - Mee, J.
AU  - Clauser, B.E.
AU  - Ha, L.A.
TI  - Using Natural Language Processing to Predict Item Response Times and Improve Test Construction
PY  - 2021
T2  - Journal of Educational Measurement
VL  - 58
IS  - 1
SP  - 4
EP  - 30
DO  - 10.1111/jedm.12264
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102484910&doi=10.1111%2fjedm.12264&partnerID=40&md5=c607a7ea8212680fd8196c00e6031865
AB  - In this article, it is shown how item text can be represented by (a) 113 features quantifying the text's linguistic characteristics, (b) 16 measures of the extent to which an information-retrieval-based automatic question-answering system finds an item challenging, and (c) through dense word representations (word embeddings). Using a random forests algorithm, these data then are used to train a prediction model for item response times and predicted response times then are used to assemble test forms. Using empirical data from the United States Medical Licensing Examination, we show that timing demands are more consistent across these specially assembled forms than across forms comprising randomly-selected items. Because an exam's timing conditions affect examinee performance, this result has implications for exam fairness whenever examinees are compared with each other or against a common standard. © 2020 by the National Council on Measurement in Education
PB  - Blackwell Publishing Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 12
ER  -

TY  - JOUR
AU  - Chang, A.
AU  - Schulz, P.J.
AU  - Tu, S.
AU  - Liu, M.T.
TI  - Communicative blame in online communication of the COVID-19 pandemic: Computational approach of stigmatizing cues and negative sentiment gauged with automated analytic techniques
PY  - 2020
T2  - Journal of Medical Internet Research
VL  - 22
IS  - 11
C7  - e21504
DO  - 10.2196/21504
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096889901&doi=10.2196%2f21504&partnerID=40&md5=dee70b2353b08f90c1800607c06a6798
AB  - Background: Information about a new coronavirus emerged in 2019 and rapidly spread around the world, gaining significant public attention and attracting negative bias. The use of stigmatizing language for the purpose of blaming sparked a debate. Objective: This study aims to identify social stigma and negative sentiment toward the blameworthy agents in social communities. Methods: We enabled a tailored text-mining platform to identify data in their natural settings by retrieving and filtering online sources, and constructed vocabularies and learning word representations from natural language processing for deductive analysis along with the research theme. The data sources comprised of ten news websites, eleven discussion forums, one social network, and two principal media sharing networks in Taiwan. A synthesis of news and social networking analytics was present from December 30, 2019, to March 31, 2020. Results: We collated over 1.07 million Chinese texts. Almost two-thirds of the texts on COVID-19 came from news services (n=683,887, 63.68%), followed by Facebook (n=297,823, 27.73%), discussion forums (n=62,119, 5.78%), and Instagram and YouTube (n=30,154, 2.81%). Our data showed that online news served as a hotbed for negativity and for driving emotional social posts. Online information regarding COVID-19 associated it with China-and a specific city within China through references to the “Wuhan pneumonia”-potentially encouraging xenophobia. The adoption of this problematic moniker had a high frequency, despite the World Health Organization guideline to avoid biased perceptions and ethnic discrimination. Social stigma is disclosed through negatively valenced responses, which are associated with the most blamed targets. Conclusions: Our sample is sufficiently representative of a community because it contains a broad range of mainstream online media. Stigmatizing language linked to the COVID-19 pandemic shows a lack of civic responsibility that encourages bias, hostility, and discrimination. Frequently used stigmatizing terms were deemed offensive, and they might have contributed to recent backlashes against China by directing blame and encouraging xenophobia. The implications ranging from health risk communication to stigma mitigation and xenophobia concerns amid the COVID-19 outbreak are emphasized. Understanding the nomenclature and biased terms employed in relation to the COVID-19 outbreak is paramount. We propose solidarity with communication professionals in combating the COVID-19 outbreak and the infodemic. Finding solutions to curb the spread of virus bias, stigma, and discrimination is imperative. © Angela Chang, Peter Johannes Schulz, ShengTsung Tu, Matthew Tingchi Liu. Originally published in the Journal of Medical Internet Research (http://www.jmir.org), 25.11.2020. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on http://www.jmir.org/, as well as this copyright and license information must be included.
PB  - JMIR Publications Inc.
C2  - 33108306
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 31
ER  -

TY  - JOUR
AU  - Wang, H.
AU  - Li, Y.
AU  - Hutch, M.
AU  - Naidech, A.
AU  - Luo, Y.
TI  - Using tweets to understand how COVID-19–Related health beliefs are affected in the age of social media: Twitter data analysis study
PY  - 2021
T2  - Journal of Medical Internet Research
VL  - 23
IS  - 2
C7  - e26302
DO  - 10.2196/26302
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101873559&doi=10.2196%2f26302&partnerID=40&md5=26633272f953bb06d50761bb701d1667
AB  - Background: The emergence of SARS-CoV-2 (ie, COVID-19) has given rise to a global pandemic affecting 215 countries and over 40 million people as of October 2020. Meanwhile, we are also experiencing an infodemic induced by the overabundance of information, some accurate and some inaccurate, spreading rapidly across social media platforms. Social media has arguably shifted the information acquisition and dissemination of a considerably large population of internet users toward higher interactivities. Objective: This study aimed to investigate COVID-19-related health beliefs on one of the mainstream social media platforms, Twitter, as well as potential impacting factors associated with fluctuations in health beliefs on social media. Methods: We used COVID-19-related posts from the mainstream social media platform Twitter to monitor health beliefs. A total of 92,687,660 tweets corresponding to 8,967,986 unique users from January 6 to June 21, 2020, were retrieved. To quantify health beliefs, we employed the health belief model (HBM) with four core constructs: perceived susceptibility, perceived severity, perceived benefits, and perceived barriers. We utilized natural language processing and machine learning techniques to automate the process of judging the conformity of each tweet with each of the four HBM constructs. A total of 5000 tweets were manually annotated for training the machine learning architectures. Results: The machine learning classifiers yielded areas under the receiver operating characteristic curves over 0.86 for the classification of all four HBM constructs. Our analyses revealed a basic reproduction number R0 of 7.62 for trends in the number of Twitter users posting health belief–related content over the study period. The fluctuations in the number of health belief–related tweets could reflect dynamics in case and death statistics, systematic interventions, and public events. Specifically, we observed that scientific events, such as scientific publications, and nonscientific events, such as politicians’ speeches, were comparable in their ability to influence health belief trends on social media through a Kruskal-Wallis test (P=.78 and P=.92 for perceived benefits and perceived barriers, respectively). Conclusions: As an analogy of the classic epidemiology model where an infection is considered to be spreading in a population with an R0 greater than 1, we found that the number of users tweeting about COVID-19 health beliefs was amplifying in an epidemic manner and could partially intensify the infodemic. It is “unhealthy” that both scientific and nonscientific events constitute no disparity in impacting the health belief trends on Twitter, since nonscientific events, such as politicians’ speeches, might not be endorsed by substantial evidence and could sometimes be misleading. © Hanyin Wang, Yikuan Li, Meghan Hutch, Andrew Naidech, Yuan Luo.
PB  - JMIR Publications Inc.
C2  - 33529155
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 41
ER  -

TY  - CONF
AU  - Ahmed, M.A.
AU  - Chatterjee, M.
AU  - Dadure, P.
AU  - Pakray, P.
TI  - The Role of Biased Data in Computerized Gender Discrimination
PY  - 2022
T2  - Proceedings - 3rd Workshop on Gender Equality, Diversity, and Inclusion in Software Engineering, GEICSE 2022
SP  - 6
EP  - 11
DO  - 10.1145/3524501.3527599
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138406970&doi=10.1145%2f3524501.3527599&partnerID=40&md5=c6a81196e14f8d6fb6df056669fcbfa6
AB  - Gender bias is prevalent in all walks of life from schools to colleges, corporate as well as government offices. This has led to the under-representation of the female gender in many professions. Most of the Artificial Intelligence-Natural Language Processing (AI-NLP) models learning from these underrepresented real world datasets amplify the bias in many cases, resulting in traditional biases being reinforced. In this paper, we have discussed how gender bias became ingrained in our society and how it results in the underrepresentation of the female gender in several fields such as education, healthcare, STEM, film industry, food industry, and sports. We shed some light on how traditional gender bias is reflected in AI-NLP systems such as automated resume screening, machine translation, text generation, etc. Future prospects of these AI-NLP applications need to include possible solutions to these existing biased AI-NLP applications, such as debiasing the word embeddings and having guidelines for more ethical and transparent standards. ACM Reference Format: Md. Arshad Ahmed, Madhura Chatterjee, Pankaj Dadure, and Partha Pakray. 2022. The Role of Biased Data in Computerized Gender Discrimination. In Third Workshop on Gender Equality, Diversity, and Inclusion in Software Engineering (GE@ICSE'22), May 20, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/3524501.3527599  © 2022 ACM.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - JOUR
AU  - Wang, N.
AU  - Wang, C.
AU  - Hou, L.
AU  - Fang, B.
TI  - Investigating young employee stressors in contemporary society based on user-generated contents
PY  - 2021
T2  - International Journal of Environmental Research and Public Health
VL  - 18
IS  - 24
C7  - 13109
DO  - 10.3390/ijerph182413109
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120945567&doi=10.3390%2fijerph182413109&partnerID=40&md5=77f4f11486e806f9433f8dbbec2ab607
AB  - Understanding stressors is an effective measure to decrease employee stress and improve employee mental health. The extant literature mainly focuses on a singular stressor among various aspects of their work or life. In addition, the extant literature generally uses questionnaires or interviews to obtain data. Data obtained in such ways are often subjective and lack authenticity. We propose a novel machine–human hybrid approach to conduct qualitative content analysis of user-generated online content to explore the stressors of young employees in contemporary society. The user-generated online contents were collected from a famous Q&A platform in China and we adopted natural language processing and deep learning technology to discover knowledge. Our results identified three kinds of new stressors, that is, affection from leaders, affection from the social circle, and the gap between dream and reality. These new identified stressors were due to the lack of social security and regulation, frequent occurrences of social media fearmongering, and subjective cognitive bias, respectively. In light of our findings, we offer valuable practical insights and policy recommendations to relieve stress and improve mental health of young employees. The primary contributions of our work are two-fold, as follows. First, we propose a novel approach to explore the stressors of young employees in contemporary society, which is applicable not only in China, but also in other countries and regions. Second, we expand the scope of job demands-resources (JD-R) theory, which is an important framework for the classification of employee stressors. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
PB  - MDPI
C2  - 34948729
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - David, C.V.
AU  - MacAllister, W.S.
TI  - Fine motor impairment in children with epilepsy: Relations with seizure severity and lateralizing value
PY  - 2022
T2  - Epilepsy and Behavior
VL  - 127
C7  - 108518
DO  - 10.1016/j.yebeh.2021.108518
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122527575&doi=10.1016%2fj.yebeh.2021.108518&partnerID=40&md5=9378a23220c75aecd4af9fcedbdee6bf
AB  - Motor skill deficits are common in epilepsy. The Grooved Pegboard Test (GPT) is the most commonly used fine motor task and is included in the NIH Common Data Elements Battery for the assessment of epilepsy. However, there are limited data on its utility in children and adolescents. The present study investigated the effectiveness of this task in children and adolescents with epilepsy clinically referred for neuropsychological evaluation in a tertiary medical center. Two hundred and two children and adolescents (ages 6–16, 104 males, 98 females) completed the GPT. Base rates of impairment were calculated, correlational analyses determined relations with clinical variables, and ANOVAs and t-tests assessed for differences by seizure type, gender, and lateralized deficits in those with lateralized focal epilepsy. The GPT was sensitive to fine motor impairment in these children and adolescents, with over 60% having impaired performances. Further, performance was significantly correlated with IQ, age of epilepsy onset, number of medications, and seizure frequency. At the group level, those with lateralized focal epilepsy did not show significant differences between left and right hands, though the GPT correctly lateralized 63% of those with large between-hand performance disparities (i.e., one standard deviation or greater). In sum, the GPT is sensitive to fine motor deficits in pediatric epilepsy and is related to known epilepsy severity factors. However, the ability of the task to lateralize epilepsy onset is not robust. © 2021
PB  - Academic Press Inc.
C2  - 35016052
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Oberlin, L.E.
AU  - Respino, M.
AU  - Victoria, L.
AU  - Abreu, L.
AU  - Hoptman, M.J.
AU  - Alexopoulos, G.S.
AU  - Gunning, F.M.
TI  - Late-life depression accentuates cognitive weaknesses in older adults with small vessel disease
PY  - 2022
T2  - Neuropsychopharmacology
VL  - 47
IS  - 2
SP  - 580
EP  - 587
DO  - 10.1038/s41386-021-00973-z
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100727843&doi=10.1038%2fs41386-021-00973-z&partnerID=40&md5=5d98c5305c16b941deeeacef0211f336
AB  - Neuroimaging features of small vessel disease (SVD) are highly prevalent in older adulthood and associated with significant variability in clinical symptoms, yet the factors predicting these symptom disparities are poorly understood. We employed a novel metric of SVD, peak width of skeletonized mean diffusivity (PSMD), to elucidate the relationship of late-life depression (LLD) to the cognitive presentation of vascular pathology. A total of 109 older adults without a diagnosis of a neurocognitive disorder were enrolled in the study; 44 with major depressive disorder and 65 age-matched controls. Subjects completed neuropsychological testing and magnetic resonance imaging including FLAIR and diffusion tensor imaging sequences, from which white matter hyperintensity volume and diffusion metrics (fractional anisotropy, mean diffusivity, PSMD) were quantified. In hierarchical models, the relationship between vascular burden and cognitive performance varied as a function of diagnostic status, such that the negative association between PSMD and processing speed was significantly stronger in participants with LLD compared to controls. Greater PSMD also predicted poorer performance on delayed memory and executive function tasks specifically among those with LLD, while there were no associations between PSMD and task performance among controls. PSMD outperformed conventional SVD and diffusion markers in predicting cognitive performance and dysexecutive behaviors in participants with LLD. These data suggest that LLD may confer a vulnerability to the cognitive manifestations of white matter abnormalities in older adulthood. PSMD, a novel biomarker of diffuse microstructural changes in SVD, may be a more sensitive marker of subtle cognitive deficits stemming from vascular pathology in LLD. © 2021, The Author(s).
PB  - Springer Nature
C2  - 33564103
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 23
ER  -

TY  - JOUR
AU  - Thompson, H.M.
AU  - Sharma, B.
AU  - Bhalla, S.
AU  - Boley, R.
AU  - McCluskey, C.
AU  - Dligach, D.
AU  - Churpek, M.M.
AU  - Karnik, N.S.
AU  - Afshar, M.
TI  - Bias and fairness assessment of a natural language processing opioid misuse classifier: Detection and mitigation of electronic health record data disadvantages across racial subgroups
PY  - 2021
T2  - Journal of the American Medical Informatics Association
VL  - 28
IS  - 11
SP  - 2393
EP  - 2403
DO  - 10.1093/jamia/ocab148
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118598233&doi=10.1093%2fjamia%2focab148&partnerID=40&md5=7226041ecd9b2379a5941403c5e3515a
AB  - Objectives: To assess fairness and bias of a previously validated machine learning opioid misuse classifier. Materials & Methods: Two experiments were conducted with the classifier's original (n = 1000) and external validation (n = 53 974) datasets from 2 health systems. Bias was assessed via testing for differences in type II error rates across racial/ethnic subgroups (Black, Hispanic/Latinx, White, Other) using bootstrapped 95% confidence intervals. A local surrogate model was estimated to interpret the classifier's predictions by race and averaged globally from the datasets. Subgroup analyses and post-hoc recalibrations were conducted to attempt to mitigate biased metrics. Results: We identified bias in the false negative rate (FNR = 0.32) of the Black subgroup compared to the FNR (0.17) of the White subgroup. Top features included "heroin"and "substance abuse"across subgroups. Post-hoc recalibrations eliminated bias in FNR with minimal changes in other subgroup error metrics. The Black FNR subgroup had higher risk scores for readmission and mortality than the White FNR subgroup, and a higher mortality risk score than the Black true positive subgroup (P <. 05). Discussion: The Black FNR subgroup had the greatest severity of disease and risk for poor outcomes. Similar features were present between subgroups for predicting opioid misuse, but inequities were present. Post-hoc mitigation techniques mitigated bias in type II error rate without creating substantial type I error rates. From model design through deployment, bias and data disadvantages should be systematically addressed. Conclusion: Standardized, transparent bias assessments are needed to improve trustworthiness in clinical machine learning models.  © 2021 The Author(s) 2021. Published by Oxford University Press on behalf of the American Medical Informatics Association.
PB  - Oxford University Press
C2  - 34383925
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 47
ER  -

TY  - CONF
AU  - Nakov, P.
AU  - Da San Martino, G.
AU  - Alam, F.
TI  - Fact-checking, fake news, propaganda, media bias, and the covid-19 infodemic
PY  - 2022
T2  - WSDM 2022 - Proceedings of the 15th ACM International Conference on Web Search and Data Mining
SP  - 1632
EP  - 1634
DO  - 10.1145/3488560.3501395
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125757302&doi=10.1145%2f3488560.3501395&partnerID=40&md5=669ccad3ab61813409876ff796884ef3
AB  - Social media have democratized content creation and have made it easy for anybody to spread information online. However, stripping traditional media from their gate-keeping role has left the public unprotected against biased, deceptive and disinformative content, which could now travel online at breaking-news speed and influence major public events. For example, during the COVID-19 pandemic, a new blending of medical and political disinformation has given rise to the first global infodemic. We offer an overview of the emerging and inter-connected research areas of fact-checking, disinformation, "fake news'', propaganda, and media bias detection. We explore the general fact-checking pipeline and important elements thereof such as check-worthiness estimation, spotting previously fact-checked claims, stance detection, source reliability estimation, detection of persuasion techniques, and detecting malicious users in social media. We also cover large-scale pre-trained language models, and the challenges and opportunities they offer for generating and for defending against neural fake news. Finally, we discuss the ongoing COVID-19 infodemic. © 2022 ACM.
PB  - Association for Computing Machinery, Inc
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Rice, M.L.
AU  - Leung, J.G.
AU  - Mara, K.C.
AU  - Leung, S.B.
TI  - Assessment of gender differences in letters of recommendation for pharmacy residency applicants
PY  - 2021
T2  - American Journal of Health-System Pharmacy
VL  - 78
IS  - 12
SP  - 1118
EP  - 1125
DO  - 10.1093/ajhp/zxab150
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108020698&doi=10.1093%2fajhp%2fzxab150&partnerID=40&md5=7a351281535d0f633f9bd636431c5b90
AB  - Letters of recommendation (LORs) are highly regarded components of pharmacy residency applications, as they provide insight into an applicant's character and capabilities. In other medical fields, differences in language have been reported for letters written for female and male applicants; however, data on gender differences in LORs for pharmacy residency applications are currently lacking. Methods: LORs for applicants to our institution's postgraduate year 1 pharmacy residency program for the 2019-2020 academic year were extracted and processed by a natural language processing service. Words within 18 categories were identified and counted for each LOR. Total word count was also compared. Results: Of the 473 LORs included for analysis, 320 (67.7%) were written for female applicants and 153 (32.3%) were written for male applicants. Approximately two-thirds of all writers were women for both female and male applicants. In comparing letters for women and men, there was a statistically significant difference in the percentage of LORs that contained terms in categories described as gendered, solitary/reserved, and desire. There was no statistically significant difference in total word count or in the presence of words in other categories such as grindstone, standout, agentic, or communal. When controlling for grade point average, writer gender, duration that the writer knew the applicant, and the writer's professional position, there were no changes to the statistical findings. Conclusion: Letters written for female and male applicants were largely similar with regard to length and word categories utilized. While no clear gender bias was found when evaluating pharmacy residency LORs, writers must continue to assess their implicit biases and how those biases might affect a candidate's application. © 2021 American Society of Health-System Pharmacists 2021. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com.
PB  - Oxford University Press
C2  - 33821930
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Abler, D.
AU  - Andrearczyk, V.
AU  - Oreiller, V.
AU  - Garcia, J.B.
AU  - Vuong, D.
AU  - Tanadini-Lang, S.
AU  - Guckenberger, M.
AU  - Reyes, M.
AU  - Depeursinge, A.
TI  - Comparison of MR Preprocessing Strategies and Sequences for Radiomics-Based MGMT Prediction
PY  - 2022
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 12963 LNCS
SP  - 367
EP  - 380
DO  - 10.1007/978-3-031-09002-8_33
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135197765&doi=10.1007%2f978-3-031-09002-8_33&partnerID=40&md5=42f8fbcfdb46e3fbc4b49f143602de7a
AB  - Hypermethylation of the O6-methylguanine-DNA-methyltransferase (MGMT) promoter in glioblastoma (GBM) is a predictive biomarker associated with improved treatment outcome. In clinical practice, MGMT methylation status is determined by biopsy or after surgical removal of the tumor. This study aims to investigate the feasibility of non-invasive medical imaging based “radio-genomic” surrogate markers of MGMT methylation status. The imaging dataset of the RSNA-ASNR-MICCAI Brain Tumor Segmentation (BraTS) challenge allows exploring radiomics strategies for MGMT prediction in a large and very heterogeneous dataset that represents a variety of real-world imaging conditions including different imaging protocols and devices. To characterize and optimize MGMT prediction strategies under these conditions, we examined different image preprocessing approaches and their effect on the average prediction performance of simple radiomics models. We found features derived from FLAIR images to be most informative for MGMT prediction, particularly if aggregated over the entire (enhancing and non-enhancing) tumor with or without inclusion of the edema. Our results also indicate that the imaging characteristics of the tumor region can distort MR-bias-field correction in a way that negatively affects the prediction performance of the derived models. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Mahendra, M.
AU  - Luo, Y.
AU  - Mills, H.
AU  - Schenk, G.
AU  - Butte, A.J.
AU  - Dudley, R.A.
TI  - Impact of Different Approaches to Preparing Notes for Analysis With Natural Language Processing on the Performance of Prediction Models in Intensive Care
PY  - 2021
T2  - Critical Care Explorations
VL  - 3
IS  - 6
SP  - E0450
DO  - 10.1097/CCE.0000000000000450
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124315082&doi=10.1097%2fCCE.0000000000000450&partnerID=40&md5=2b36dafdc65043297cc5613ee4827dd2
AB  - OBJECTIVES: To evaluate whether different approaches in note text preparation (known as preprocessing) can impact machine learning model performance in the case of mortality prediction ICU. DESIGN: Clinical note text was used to build machine learning models for adults admitted to the ICU. Preprocessing strategies studied were none (raw text), cleaning text, stemming, term frequency-inverse document frequency vectorization, and creation of n-grams. Model performance was assessed by the area under the receiver operating characteristic curve. Models were trained and internally validated on University of California San Francisco data using 10-fold cross validation. These models were then externally validated on Beth Israel Deaconess Medical Center data. SETTING: ICUs at University of California San Francisco and Beth Israel Deaconess Medical Center. SUBJECTS: Ten thousand patients in the University of California San Francisco training and internal testing dataset and 27,058 patients in the external validation dataset, Beth Israel Deaconess Medical Center. INTERVENTIONS: None. MEASUREMENTS AND MAIN RESULTS: Mortality rate at Beth Israel Deaconess Medical Center and University of California San Francisco was 10.9% and 7.4%, respectively. Data are presented as area under the receiver operating characteristic curve (95% CI) for models validated at University of California San Francisco and area under the receiver operating characteristic curve for models validated at Beth Israel Deaconess Medical Center. Models built and trained on University of California San Francisco data for the prediction of inhospital mortality improved from the raw note text model (AUROC, 0.84; CI, 0.80-0.89) to the term frequency-inverse document frequency model (AUROC, 0.89; CI, 0.85-0.94). When applying the models developed at University of California San Francisco to Beth Israel Deaconess Medical Center data, there was a similar increase in model performance from raw note text (area under the receiver operating characteristic curve at Beth Israel Deaconess Medical Center: 0.72) to the term frequency-inverse document frequency model (area under the receiver operating characteristic curve at Beth Israel Deaconess Medical Center: 0.83). CONCLUSIONS: Differences in preprocessing strategies for note text impacted model discrimination. Completing a preprocessing pathway including cleaning, stemming, and term frequency-inverse document frequency vectorization resulted in the preprocessing strategy with the greatest improvement in model performance. Further study is needed, with particular emphasis on how to manage author implicit bias present in note text, before natural language processing algorithms are implemented in the clinical setting. © The Author(s) 2021.
PB  - Lippincott Williams and Wilkins
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - CONF
AU  - Trovati, M.
AU  - Awan, S.
TI  - Semantic Network: A Brief Review of its Datasets
PY  - 2022
T2  - Lecture Notes in Networks and Systems
VL  - 527 LNNS
SP  - 224
EP  - 233
DO  - 10.1007/978-3-031-14627-5_21
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137020013&doi=10.1007%2f978-3-031-14627-5_21&partnerID=40&md5=24fef351d2cdb01d142e6ab439771ae7
AB  - Semantic networks are graphical representations, in terms of nodes and edges, of words and phrases from the linguistics vocabulary to form a meaning and relation between them to describe the intended target or object. It is part of the computer science field utilising Natural Language Processing, text mining, psychology, and sociology. The edges may describe the relation between the nodes as weak or strong, or one way or both ways, or if the node is stand alone. Semantic networks use datasets like ConceptNet and WordNet which uses English vocabulary like nouns, verbs, adjectives, adverbs to form common meanings and relations. Sometimes the linguistics vocabulary is not present or not clear e.g., examining a patient electronic medical records or physician’s handwritten notes will reveal several terminologies which are only specific to medical professionals in their field. Similarly, when the intended target is an opinion or a subjective state which may or may not be true and represent the author personal views, biases, or prejudices. Instances like these make it difficult to establish a meaningful relation for data analysis purposes. In that case, need arises to develop a database from the ground up for the semantic network to be established. This paper briefly discusses the major semantic network datasets which are open source and available for semantic analysis. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Varol, A.E.
AU  - Kocaman, V.
AU  - Haq, H.U.
AU  - Talby, D.
TI  - Understanding COVID-19 News Coverage using Medical NLP
PY  - 2022
T2  - CEUR Workshop Proceedings
VL  - 3117
SP  - 45
EP  - 53
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128740318&partnerID=40&md5=b2c1befae0fef10ec517fb1c54ab0057
AB  - Being a global pandemic, the COVID-19 outbreak received global media attention. In this study, we analyze news publications from CNN and The Guardian - two of the world's most influential media organizations. The dataset includes more than 36,000 articles, analyzed using the clinical and biomedical Natural Language Processing (NLP) models from the Spark NLP for Healthcare library, which enables a deeper analysis of medical concepts than previously achieved. The analysis covers key entities and phrases, observed biases, and change over time in news coverage by correlating mined medical symptoms, procedures, drugs, and guidance with commonly mentioned demographic and occupational groups. Another analysis is of extracted Adverse Drug Events about drug and vaccine manufacturers, which when reported by major news outlets has an impact on vaccine hesitancy. © 2021 Copyright for this paper by its authors
PB  - CEUR-WS
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Lewandrowski, K.-U.
AU  - Muraleedharan, N.
AU  - Eddy, S.A.
AU  - Sobti, V.
AU  - Reece, B.D.
AU  - León, J.F.R.
AU  - Shah, S.
TI  - Feasibility of deep learning algorithms for reporting in routine spine magnetic resonance imaging
PY  - 2020
T2  - International Journal of Spine Surgery
VL  - 14
SP  - S86
EP  - S97
DO  - 10.14444/7131
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097678666&doi=10.14444%2f7131&partnerID=40&md5=4ffd3ad3b1473f3ff651193509f17f6e
AB  - Background: Artificial intelligence is gaining traction in automated medical imaging analysis. Development of more accurate magnetic resonance imaging (MRI) predictors of successful clinical outcomes is necessary to better define indications for surgery, improve clinical outcomes with targeted minimally invasive and endoscopic procedures, and realize cost savings by avoiding more invasive spine care. Objective: To demonstrate the ability for deep learning neural network models to identify features in MRI DICOM datasets that represent varying intensities or severities of common spinal pathologies and injuries and to demonstrate the feasibility of generating automated verbal MRI reports comparable to those produced by reading radiologists. Methods: A 3-dimensional (3D) anatomical model of the lumbar spine was fitted to each of the patient’s MRIs by a team of technicians. MRI T1, T2, sagittal, axial, and transverse reconstruction image series were used to train segmentation models by the intersection of the 3D model through these image sequences. Class definitions were extracted from the radiologist report for the central canal: (0) no disc bulge/protrusion/canal stenosis, (1) disc bulge without canal stenosis, (2) disc bulge resulting in canal stenosis, and (3) disc herniation/protrusion/extrusion resulting in canal stenosis. Both the left and right neural foramina were assessed with either (0) neural foraminal stenosis absent, or (1) neural foramina stenosis present. Reporting criteria for the pathologies at each disc level and, when available, the grading of severity were extracted, and a natural language processing model was used to generate a verbal and written report. These data were then used to train a set of very deep convolutional neural network models, optimizing for minimal binary cross-entropy for each classification. Results: The initial prediction validation of the implemented deep learning algorithm was done on 20% of the dataset, which was not used for artificial intelligence training. Of the 17,800 total disc locations for which MRI images and radiology reports were available, 14,720 were used to train the model, and 3560 were used to validate against. The convergence of validation accuracy achieved with the deep learning algorithm for the foraminal stenosis detector was 81% (sensitivity ¼ 72.4.4%, specificity ¼ 83.1%) after 25 complete iterations through the entire training dataset (epoch). The accuracy was 86.2% (sensitivity ¼ 91.1%, specificity ¼ 82.5%) for the central stenosis detector and 85.2% (sensitivity ¼ 81.8%, specificity ¼ 87.4%) for the disc herniation detector. Conclusions: Deep learning algorithms may be used for routine reporting in spine MRI. There was a minimal disparity among accuracy, sensitivity, and specificity, indicating that the data were not overfitted to the training set. We concluded that variability in the training data tends to reduce overfitting and overtraining as the deep neural network models learn to focus on the common pathologies. Future studies should demonstrate the accuracy of deep neural network models and the predictive value of favorable clinical outcomes with intervention and surgery. © International Society for the Advancement of Spine Surgery
PB  - ISASS
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 38
ER  -

TY  - JOUR
AU  - Mounsef, J.
AU  - Hasib, M.
AU  - Raza, A.
TI  - Building an Arabic Dialectal Diagnostic Dataset for Healthcare
PY  - 2022
T2  - International Journal of Advanced Computer Science and Applications
VL  - 13
IS  - 7
SP  - 859
EP  - 868
DO  - 10.14569/IJACSA.2022.01307100
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135791080&doi=10.14569%2fIJACSA.2022.01307100&partnerID=40&md5=b91afe725a457867e5ae9bec6af3a5f9
AB  - Accurate diagnosis of patient conditions becomes challenging for medical practitioners in urban metropolitan cities. A variety of languages and spoken dialects impedes the diagnosis achieved through the exploratory journey a medical practitioner and patient go through. Natural language processing has been used in well-known applications, such as Google Translate, as a solution to reduce language barriers. Languages typically encountered in these applications provide the most commonly known, used or standardized dialect. The Arabic language can benefit from the common dialect, which is available in such applications. However, given the diversity of dialects in Arabic in the healthcare domain, there is a risk associated with incorrect interpretation of a dialect, which can impact the diagnosis or treatment of patients. Arabic language dialect corpuses published in recent research work can be applied to rule-based natural language applications. Our study aims to develop an approach to support medical practitioners by ensuring that the diagnosis is not impeded based on the misinterpretation of patient responses. Our initial approach reported in this work adopts the methods used by practitioners in the diagnosis carried out within the scope of the Emirati and Egyptian Arabic dialects. In this paper, we develop and provide a public Arabic Dialect Dataset (ADD), which is a corpus of audio samples related to healthcare. In order to train machine learning models, the dataset development is designed with multi-class labelling. Our work indicates that there is a clear risk of bias in datasets, which may come about when a large number of classes do not have enough training samples. Our crowd sourcing solution presented in this work may be an approach to overcome the sourcing of audio samples. Models trained with this dataset may be used to support the diagnosis made by medical practitioners. © 2022. International Journal of Advanced Computer Science and Applications. All Rights Reserved.
PB  - Science and Information Organization
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Nuechterlein, N.
AU  - Li, B.
AU  - Feroze, A.
AU  - Holland, E.C.
AU  - Shapiro, L.
AU  - Haynor, D.
AU  - Fink, J.
AU  - Cimino, P.J.
TI  - Radiogenomic modeling predicts survival-associated prognostic groups in glioblastoma
PY  - 2021
T2  - Neuro-Oncology Advances
VL  - 3
IS  - 1
C7  - vdab004
DO  - 10.1093/noajnl/vdab004
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120855599&doi=10.1093%2fnoajnl%2fvdab004&partnerID=40&md5=6cb80eee9a323abe2eb653ef840bd0a3
AB  - Background. Combined whole-exome sequencing (WES) and somatic copy number alteration (SCNA) information can separate isocitrate dehydrogenase (IDH)1/2-wildtype glioblastoma into two prognostic molecular subtypes, which cannot be distinguished by epigenetic or clinical features. The potential for radiographic features to discriminate between these molecular subtypes has yet to be established. Methods. Radiologic features (n = 35 340) were extracted from 46 multisequence, pre-operative magnetic resonance imaging (MRI) scans of IDH1/2-wildtype glioblastoma patients from The Cancer Imaging Archive (TCIA), all of whom have corresponding WES/SCNA data. We developed a novel feature selection method that leverages the structure of extracted MRI features to mitigate the dimensionality challenge posed by the disparity between a large number of features and the limited patients in our cohort. Six traditional machine learning classifiers were trained to distinguish molecular subtypes using our feature selection method, which was compared to least absolute shrinkage and selection operator (LASSO) feature selection, recursive feature elimination, and variance thresholding. Results. We were able to classify glioblastomas into two prognostic subgroups with a cross-validated area under the curve score of 0.80 (±0.03) using ridge logistic regression on the 15-dimensional principle component analysis (PCA) embedding of the features selected by our novel feature selection method. An interrogation of the selected features suggested that features describing contours in the T2 signal abnormality region on the T2-weighted fluidattenuated inversion recovery (FLAIR) MRI sequence may best distinguish these two groups from one another. Conclusions. We successfully trained a machine learning model that allows for relevant targeted feature extraction from standard MRI to accurately predict molecularly-defined risk-stratifying IDH1/2-wildtype glioblastoma patient groups.  © The Author(s) 2021.
PB  - Oxford University Press
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - CONF
AU  - Kazemi, A.
AU  - Li, Z.
AU  - Peréz-Rosas, V.
AU  - Mihalcea, R.
TI  - Extractive and Abstractive Explanations for Fact-Checking and Evaluation of News
PY  - 2021
T2  - NLP4IF 2021 - NLP for Internet Freedom: Censorship, Disinformation, and Propaganda, Proceedings of the 4th Workshop
SP  - 45
EP  - 50
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119245585&partnerID=40&md5=1e616eab407758361fbe16f43ac38a93
AB  - In this paper, we explore the construction of natural language explanations for news claims, with the goal of assisting fact-checking and news evaluation applications. We experiment with two methods: (1) an extractive method based on Biased TextRank – a resource-effective unsupervised graph-based algorithm for content extraction; and (2) an abstractive method based on the GPT-2 language model. We perform comparative evaluations on two misinformation datasets in the political and health news domains, and find that the extractive method shows the most promise. © 2021 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Scaccia, J.P.
AU  - Scott, V.C.
TI  - 5335 days of Implementation Science: using natural language processing to examine publication trends and topics
PY  - 2021
T2  - Implementation Science
VL  - 16
IS  - 1
C7  - 47
DO  - 10.1186/s13012-021-01120-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104865376&doi=10.1186%2fs13012-021-01120-4&partnerID=40&md5=f50c8d30820c892e802400a805e9351b
AB  - Introduction: Moving evidence-based practices into the hands of practitioners requires the synthesis and translation of research literature. However, the growing pace of scientific publications across disciplines makes it increasingly difficult to stay abreast of research literature. Natural language processing (NLP) methods are emerging as a valuable strategy for conducting content analyses of academic literature. We sought to apply NLP to identify publication trends in the journal Implementation Science, including key topic clusters and the distribution of topics over time. A parallel study objective was to demonstrate how NLP can be used in research synthesis. Methods: We examined 1711 Implementation Science abstracts published from February 22, 2006, to October 1, 2020. We retrieved the study data using PubMed’s Application Programming Interface (API) to assemble a database. Following standard preprocessing steps, we use topic modeling with Latent Dirichlet allocation (LDA) to cluster the abstracts following a minimization algorithm. Results: We examined 30 topics and computed topic model statistics of quality. Analyses revealed that published articles largely reflect (i) characteristics of research, or (ii) domains of practice. Emergent topic clusters encompassed key terms both salient and common to implementation science. HIV and stroke represent the most commonly published clinical areas. Systematic reviews have grown in topic prominence and coherence, whereas articles pertaining to knowledge translation (KT) have dropped in prominence since 2013. Articles on HIV and implementation effectiveness have increased in topic exclusivity over time. Discussion: We demonstrated how NLP can be used as a synthesis and translation method to identify trends and topics across a large number of (over 1700) articles. With applicability to a variety of research domains, NLP is a promising approach to accelerate the dissemination and uptake of research literature. For future research in implementation science, we encourage the inclusion of more equity-focused studies to expand the impact of implementation science on disadvantaged communities. © 2021, The Author(s).
PB  - BioMed Central Ltd
C2  - 33902657
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 21
ER  -

TY  - CONF
AU  - Rathi, H.K.
AU  - Dawande, P.
AU  - Kane, S.
AU  - Gaikwad, A.
AU  - Narlawar, M.S.
TI  - Artificial Intelligence, Machine Learning And Deep Learning In Health Care
PY  - 2022
T2  - ECS Transactions
VL  - 107
IS  - 1
SP  - 15981
EP  - 15987
DO  - 10.1149/10701.15981ecst
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130578596&doi=10.1149%2f10701.15981ecst&partnerID=40&md5=217668f90cf5c48ac34d24abe3c86834
AB  - BACKGROUND: Artificial Intelligence (AI) and related technologies are widely used in business and community, and are integrated into health care. This technology has the potential to transform many aspects of patient care, as well as management functions within providers, payers, and pharmaceutical companies. A growing number of research shows that artificial intelligence (AI) can work better or better than humans in important health care tasks such as diagnostics. As machine learning (ML) models become more common in our daily lives, concerns about their potential for injury are growing. In medicine, optimism in the human-health function of ML is influenced by ethical concerns about the potential of tools to magnify existing health disparities. Recent research, for example, has found that when applied to women, minority races and ethnic groups, as well as those with social insurance, modern clinical prediction algorithms do well. When popular language language models are trained in science textbooks, they complete the templates of clinical notes to compliment hospitals for violent white fathers, according to other studies. CONCLUSION: While in-depth learning as a means of analysis and modelling has some advantages at the moment, it also highlights a major trend: the integration of health sciences with data. Obstacles to access to health care will be higher than many other industries, where billions of people use in-depth learning and other forms of electronic learning software every day. © The Electrochemical Society
PB  - Institute of Physics
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 11
ER  -

TY  - JOUR
AU  - Rowe, F.
AU  - Mahony, M.
AU  - Graells-Garrido, E.
AU  - Rango, M.
AU  - Sievers, N.
TI  - Using Twitter to track immigration sentiment during early stages of the COVID-19 pandemic
PY  - 2021
T2  - Data and Policy
VL  - 3
IS  - 12
C7  - e36
DO  - 10.1017/dap.2021.38
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136555330&doi=10.1017%2fdap.2021.38&partnerID=40&md5=65f7509c368eb051684a5b28efdef2f5
AB  - Large-scale coordinated efforts have been dedicated to understanding the global health and economic implications of the COVID-19 pandemic. Yet, the rapid spread of discrimination and xenophobia against specific populations has largely been neglected. Understanding public attitudes toward migration is essential to counter discrimination against immigrants and promote social cohesion. Traditional data sources to monitor public opinion are often limited, notably due to slow collection and release activities. New forms of data, particularly from social media, can help overcome these limitations. While some bias exists, social media data are produced at an unprecedented temporal frequency, geographical granularity, are collected globally and accessible in real-time. Drawing on a data set of 30.39 million tweets and natural language processing, this article aims to measure shifts in public sentiment opinion about migration during early stages of the COVID-19 pandemic in Germany, Italy, Spain, the United Kingdom, and the United States. Results show an increase of migration-related Tweets along with COVID-19 cases during national lockdowns in all five countries. Yet, we found no evidence of a significant increase in anti-immigration sentiment, as rises in the volume of negative messages are offset by comparable increases in positive messages. Additionally, we presented evidence of growing social polarization concerning migration, showing high concentrations of strongly positive and strongly negative sentiments.  © The Author(s), 2021. Published by Cambridge University Press.
PB  - Cambridge University Press
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 34
ER  -

TY  - JOUR
AU  - Schillinger, D.
AU  - Balyan, R.
AU  - Crossley, S.A.
AU  - McNamara, D.S.
AU  - Liu, J.Y.
AU  - Karter, A.J.
TI  - Employing computational linguistics techniques to identify limited patient health literacy: Findings from the ECLIPPSE study
PY  - 2021
T2  - Health Services Research
VL  - 56
IS  - 1
SP  - 132
EP  - 144
DO  - 10.1111/1475-6773.13560
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091296602&doi=10.1111%2f1475-6773.13560&partnerID=40&md5=759654e0fcb8eba50f4336aebad109cb
AB  - Objective: To develop novel, scalable, and valid literacy profiles for identifying limited health literacy patients by harnessing natural language processing. Data Source: With respect to the linguistic content, we analyzed 283 216 secure messages sent by 6941 diabetes patients to physicians within an integrated system's electronic portal. Sociodemographic, clinical, and utilization data were obtained via questionnaire and electronic health records. Study Design: Retrospective study used natural language processing and machine learning to generate five unique “Literacy Profiles” by employing various sets of linguistic indices: Flesch-Kincaid (LP_FK); basic indices of writing complexity, including lexical diversity (LP_LD) and writing quality (LP_WQ); and advanced indices related to syntactic complexity, lexical sophistication, and diversity, modeled from self-reported (LP_SR), and expert-rated (LP_Exp) health literacy. We first determined the performance of each literacy profile relative to self-reported and expert-rated health literacy to discriminate between high and low health literacy and then assessed Literacy Profiles’ relationships with known correlates of health literacy, such as patient sociodemographics and a range of health-related outcomes, including ratings of physician communication, medication adherence, diabetes control, comorbidities, and utilization. Principal Findings: LP_SR and LP_Exp performed best in discriminating between high and low self-reported (C-statistics: 0.86 and 0.58, respectively) and expert-rated health literacy (C-statistics: 0.71 and 0.87, respectively) and were significantly associated with educational attainment, race/ethnicity, Consumer Assessment of Provider and Systems (CAHPS) scores, adherence, glycemia, comorbidities, and emergency department visits. Conclusions: Since health literacy is a potentially remediable explanatory factor in health care disparities, the development of automated health literacy indicators represents a significant accomplishment with broad clinical and population health applications. Health systems could apply literacy profiles to efficiently determine whether quality of care and outcomes vary by patient health literacy; identify at-risk populations for targeting tailored health communications and self-management support interventions; and inform clinicians to promote improvements in individual-level care. © 2020 The Authors. Health Services Research published by Wiley Periodicals LLC on behalf of Health Research and Educational Trust
PB  - Blackwell Publishing Inc.
C2  - 32966630
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - CONF
AU  - Samuel, H.
AU  - Zaiane, O.
AU  - Bolduc, F.
TI  - Evaluation of Applied Machine Learning for Health Misinformation Detection via Survey of Medical Professionals on Controversial Topics in Pediatrics
PY  - 2021
T2  - ACM International Conference Proceeding Series
SP  - 1
EP  - 6
DO  - 10.1145/3472813.3472814
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118651440&doi=10.1145%2f3472813.3472814&partnerID=40&md5=f56446bafd41d378dd9ebf00a5680bc9
AB  - In this research, we present an evaluation of a system for detection of health misinformation using applied machine learning. The system incorporates computing automation, information retrieval, and natural language processing in conjunction with evidence-based medicine to generate a veracity score based on consensus from trusted medical knowledge bases. For our study, we pre-computed the veracity scores of controversial topics in pediatrics with our proposed system, and then also solicited evaluations of these topics from medical professionals in the neurodevelopmental field via a quantitative survey. Hence, this work provides a double-blind comparison on the veracity of medical claims between our proposed system's results and medical professionals' responses. The results showed that our system's automated assessment matched professional opinions of medical personnel with 80% precision. The survey also demonstrated the inherent challenge with health misinformation detection, as there was no consensus among the medical professionals for 50% of the controversial statements. Nevertheless, this evaluation shows promising results for using objective trust metrics such as the veracity score, in contrast with subjective trust metrics that rely on potentially biased crowdsourcing, ratings, and pre-trained labelling of data. © 2021 ACM.
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Borgese, M.
AU  - Joyce, C.
AU  - Anderson, E.E.
AU  - Churpek, M.M.
AU  - Afshar, M.
TI  - Bias Assessment and Correction in Machine Learning Algorithms: A Use-Case in a Natural Language Processing Algorithm to Identify Hospitalized Patients with Unhealthy Alcohol Use
PY  - 2021
T2  - AMIA ... Annual Symposium proceedings. AMIA Symposium
VL  - 2021
SP  - 247
EP  - 254
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126716495&partnerID=40&md5=ffd489e56cb8b92d34f367a20ee72da0
AB  - Unhealthy alcohol use represents a major economic burden and cause of morbidity and mortality in the United States. Implementation of interventions for unhealthy alcohol use depends on the availability and accuracy of screening tools. Our group previously applied methods in natural language processing and machine learning to build a classifier for unhealthy alcohol use. In this study, we sought to evaluate and address bias through the use-case of our classifier. We demonstrated the presence of biased unhealthy alcohol use risk underestimation among Hispanic compared to Non-Hispanic White trauma inpatients, 18- to 44-year-old compared to 45 years and older medical/surgical inpatients, and Non-Hispanic Black compared to Non-Hispanic White medical/surgical inpatients. We further showed that intercept, slope, and concurrent intercept and slope recalibration resulted in minimal or no improvements in bias-indicating metrics within these subgroups. Our results exemplify the importance of integrating bias assessment early into the classifier development pipeline. ©2021 AMIA - All rights reserved.
PB  - NLM (Medline)
C2  - 35308909
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - CONF
AU  - Liang, P.P.
AU  - Wu, C.
AU  - Morency, L.-P.
AU  - Salakhutdinov, R.
TI  - Towards Understanding and Mitigating Social Biases in Language Models
PY  - 2021
T2  - Proceedings of Machine Learning Research
VL  - 139
SP  - 6565
EP  - 6576
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161338144&partnerID=40&md5=ecb2b6d1c87e16c106a42282ee0d49a0
AB  - Warning: this paper contains model outputs that may be offensive or upsetting. As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier. Copyright © 2021 by the author(s)
PB  - ML Research Press
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 198
ER  -

TY  - JOUR
AU  - Oliveira, T.
AU  - Evangelista, S.
AU  - Alves, M.
AU  - Quinan, R.
TI  - “Those on the Right Take Chloroquine”: The Illiberal Instrumentalisation of Scientific Debates during the COVID-19 Pandemic in Brasil
PY  - 2021
T2  - Javnost
VL  - 28
IS  - 2
SP  - 165
EP  - 184
DO  - 10.1080/13183222.2021.1921521
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110939182&doi=10.1080%2f13183222.2021.1921521&partnerID=40&md5=708df325ddfb1c23151088c9f9ba131c
AB  - Since the beginning of the COVID-19 pandemic, the Brasilian government has undergone a significant political dispute over the use of hydroxychloroquine as a measure to confront the disease and contested scientific and healthcare organisations findings related to the drug's effectiveness. In this article, we seek to understand the manner in which an illiberal populist government and the supporters thereof refer to scientific discourse during the pandemic, with a focus on the debates on Brasilian far-right networks on Twitter. Using a mixed methodology with statistical methods, social media analysis, natural language processing and qualitative content analysis, this study seeks to investigate which sources and stakeholders were referenced and the narratives that structured the arguments of far-right supporters who defended the use of hydroxychloroquine. The results highlight the use of sources that are ideologically aligned to the right and a reconfiguration of scientific authority that was supported by illiberal values. Among the main discourses, we observed an epistemic challenge with a partisan bias, which led to the scientific authority legitimising some arguments and discrediting others. We also identified the spread of conspiracy theories that reflected the epistemic challenge, in addition to conservative, revivalist and individualistic postures. © 2021 EURICOM.
PB  - Taylor and Francis Ltd.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 18
ER  -

TY  - JOUR
AU  - Cui, C.
AU  - Hong, Y.
AU  - Bao, J.
AU  - He, L.
AU  - Tusconi, M.
TI  - The diagnostic reliability and validity of noninvasive imaging modalities to assess leptomeningeal collateral flow for ischemic stroke patients: A systematic review and meta-analysis
PY  - 2021
T2  - Medicine (United States)
VL  - 100
IS  - 18
SP  - E25543
DO  - 10.1097/MD.0000000000025543
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105488330&doi=10.1097%2fMD.0000000000025543&partnerID=40&md5=058b1e756ef7bd00a37920386a25e460
AB  - Leptomeningeal collateral flow (LMF) is associated with infarct area and clinical outcome for ischemic stroke patients. Although LMF can be detected by multiple imaging methods, but their diagnostic performance is uncertain.The aim of this study was to evaluate the diagnostic validity or reliability of noninvasive image methods in assessing LMF.Databases included PubMed, Web of Science, Embase, and Cochrane Library.Original observational cohort studies.Ischemic stroke patients.Different noninvasive image methods to assess LMF.Newcastle-Ottawa Scale to evaluate the quality of the studies; forest plot to show pooled results; I2and Egger test to evaluate the heterogeneity and publication bias.Thirty of the 126 selected studies were eligible. For CT angiography, the interobserver agreement ranged from 0.494 to 0.93 and weighted kappa was 0.888; for patients receiving thrombolysis or endovascular treatment, 0.68 to 0.91; 0.494 to 0.89 for the 2-point system, 0.60 to 0.93 for the 3-point system, 0.68 to 0.87 for the system of >4 points; area under the curve (AUC) was 0.78. For perfusion computed tomography (CTP), the interobserver agreement ranged from 0.724 to 0.872; for patients receiving thrombolysis or endovascular treatment, 0.74 to 0.872; 0.724 for the 2-point system, 0.783 to 0.953 for the 3-point system; the intraobserver agreement was 0.884; AUC was 0.826. For MRI-fluid attenuated inversion recovery (FLAIR), the interobserver agreement ranged from 0.58 to 0.86; for patients receiving thrombolysis or endovascular treatment, 0.75 to 0.86; 0.86 for the two-point system, 0.77 to 0.87 for the system of more than 5 points; AUC was 0.82.No pooled data of CTP and FLAIR. The difference cohort study had difference bias. The unpublished data were not included.CT angiography is a good tool for assessing LMF. CTP shows a good validity and reliability, but its diagnostic value needs more evidence. FLAIR is a good modality to assess LMF. These image methods had better validity and reliability to evaluate LMF of patients receiving thrombolysis or endovascular treatment than all ischemic stroke patients. © 2021 Lippincott Williams and Wilkins. All rights reserved.
PB  - Lippincott Williams and Wilkins
C2  - 33950927
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Gogleva, A.
AU  - Papa, E.
AU  - Jansson, E.
AU  - De Baets, G.
TI  - Drug discovery as a recommendation problem: Challenges and complexities in biological decisions
PY  - 2021
T2  - RecSys 2021 - 15th ACM Conference on Recommender Systems
SP  - 548
EP  - 550
DO  - 10.1145/3460231.3474598
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115638246&doi=10.1145%2f3460231.3474598&partnerID=40&md5=a4f962672d963e58089b2db7d02cf00c
AB  - Drug discovery is notorious for its low success rates [5]. Despite best research efforts, the majority of drugs fail at early stages of development, even before they enter clinical trials. This phenomenon stems from the inherent complexity of biological systems and our poor understanding of human diseases. To improve that understanding, swaths of data have been generated in recent years. Still, data does not easily translate into knowledge or actionable insights. Here we explore how approaches from the recommendation system domain could help scientists comprehend the ever-growing amount of biomedical facts. The aim of these efforts is to make better drug development decisions, which ultimately result in safe and efficient treatments for patients [3]. Recommendation systems are well established in e-commerce, streaming and social media platforms, however in the biomedical domain their usage is limited to a few recent studies [1, 6, 7, 8]. Direct transfer of classic recommendation approaches to the biomedical domain is not trivial. Specifics of the problem space impose numerous challenges for a recommendation system practitioner, to name a few: Regardless of the challenges, the adoption of recommendation systems presents numerous opportunities to support and accelerate drug discovery. Even a slight increase in success rate of drug pipelines will result in a vast number of patients gaining access to safe and effective treatments. Recommendation systems could play a leading role in this process. Adding context to experimental data is one class of problems that could benefit from recommenders. In this process new data is integrated with prior evidence to produce a new hypothesis. In a typical scenario, thousands of genes need to be ranked by their relevance to a disease given new and existing data. As a case study we focused on finding out why some lung cancer patients develop resistance to treatments. Current protocol to find resistance markers starts with high-throughput genomic screens resulting in an initial list of potential gene candidates, followed by tedious manual curation by several experts to reduce the list to a manageable number for further follow-up. To find resistance markers faster and to reduce bias we built a hybrid recommendation system on top of a heterogeneous biomedical knowledge graph [2]. In the absence of continuous feedback and training data, we approached recommendations as a multi-objective optimization problem [4]. Genes were ranked by trading off diverse types of evidence that link them to potential mechanisms of resistance in lung cancer. We used a knowledge graph as the primary source of features, so that the relevance of a gene could be expressed via properties of a graph. Our hybrid feature set also included clinical and pre-clinical data as well as metrics of literature support obtained with natural language processing techniques. This hybrid approach helped to identify novel resistance mechanisms that could have been overlooked by experts due to inherent bias or limited integration of data. Most importantly, our method reduced the time required to prioritise resistance markers from months to minutes and became a standard procedure for processing genomic screens. Another class of problems exists around target identification tasks. The idea here is to find a molecular target, often a gene or a protein, that could be modulated with a drug to treat a disease. As the number of potential targets is large, the search space can be reduced using network propagation on a dedicated subgraph that captures the functional relationship between genes. This approach also requires a set of seed genes, defined based on high confidence associations with diseases. Disease preferences are then propagated through the network resulting in a preference distribution for the complete set of genes which is used to reduce the search space. In contrast to adding context to experiments, a considerable amount of training data is available to support target identification. For instance, both successful and failed clinical trials can act as a useful source of data for target identification. Such a setting warrants use of supervised recommendation systems. A supervised approach, however introduces another machine learning hurdle-trust. Since supervised models are typically "black boxes", their quality must be ascertained indirectly, for example using train-test split and estimating model's performance on the test set. Such quantitative performance metrics often are of little value to a biological expert looking for relevant gene targets. Instead, experts instinctively assess model quality by checking if a list of recommendations contains a handful of expected genes [9]. To simultaneously use biologists' intuitions as training data, while avoiding an overly optimistic trust in model output, we used an ensemble modeling approach. We partitioned training data among multiple models such that each available training gene was omitted from one model's training data. The model was then permitted to assess this previously unseen gene in constructing its final list of recommendations, while training genes were removed from consideration. Each model therefore produced a list of recommendations based on an incomplete set of genes. A final set of recommendations was then constructed by collating each individual model's output list. Because these output lists were constructed with biologist input through supervised training, biologists placed a higher degree of trust in the recommendations. This allowed roughly two dozen genes to be fast-tracked for manual assessment and experimental screening. In summary, accumulation of large amounts of biomedical data coupled with a need to comprehend and reason about it makes drug discovery an attractive field to apply recommendation techniques. Specifics of the problem space and complexity of biological systems call for efficient recommendation solutions that could operate in unsupervised or weakly supervised settings. At the same time, a strong emphasis on explainability is essential to gain trust of biomedical experts.  © 2021 Owner/Author.
PB  - Association for Computing Machinery, Inc
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Fagernäs, S.
AU  - Hamilton, W.
AU  - Espinoza, N.
AU  - Miloff, A.
AU  - Carlbring, P.
AU  - Lindner, P.
TI  - What do users think about Virtual Reality relaxation applications? A mixed methods study of online user reviews using natural language processing
PY  - 2021
T2  - Internet Interventions
VL  - 24
C7  - 100370
DO  - 10.1016/j.invent.2021.100370
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100705676&doi=10.1016%2fj.invent.2021.100370&partnerID=40&md5=b0552bdc3027a3d5f36c1da07b25e41c
AB  - The advent of affordable Virtual Reality (VR) technology has spurred consumer and commercial interest in VR relaxation applications, which has quickly grown into a popular non-gaming genre on digital marketplaces. While laboratory studies have demonstrated efficacy of VR relaxation for mental health purposes, little is known about how users experience this type of intervention and no study has examined the reception of consumer versions among regular users in everyday life. Studying published user reviews offers a unique window into naturalistic user experiences that complements traditional qualitative methods by circumventing the sampling bias of interview studies, and allowing analyses on full samples, unconstrained by coding resources. Using an innovative, semi-automated Natural Language Processing technique, the current study analyzed 1379 published reviews (including star ratings) of 30 different VR relaxation applications available for the Oculus Go and Gear VR. The uncovered topic structure and sentiment analysis thereof suggests that users have an overall positive view of VR relaxation applications, describing them as successful in inducing immersion and relaxation, and having appreciated gamification elements. However, perceived quality varied substantially between applications that explained more variance in star ratings than specific features. Critical issues raised were both technical (e.g. “overheating”) in nature and related to specific design elements and use. Implications for the design of consumer VR applications and future research are discussed. © 2021 The Authors
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 23
ER  -

TY  - JOUR
AU  - Satti, F.A.
AU  - Hussain, M.
AU  - Hussain, J.
AU  - Ali, S.I.
AU  - Ali, T.
AU  - Bilal, H.S.M.
AU  - Chung, T.
AU  - Lee, S.
TI  - Unsupervised Semantic Mapping for Healthcare Data Storage Schema
PY  - 2021
T2  - IEEE Access
VL  - 9
C7  - 9499053
SP  - 107267
EP  - 107278
DO  - 10.1109/ACCESS.2021.3100686
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111555920&doi=10.1109%2fACCESS.2021.3100686&partnerID=40&md5=8f81f8cd82717ab43d33c58abb409522
AB  - Data, information, and knowledge processing systems, in the domain of healthcare, are currently plagued by heterogeneity at various levels. Current solutions have focused on developing a standard-based, manual intervention mechanism, which requires a large number of human resources and necessitates the realignment of existing systems. State-of-the-art methodologies in the field of natural language processing and machine learning can help to partially automate this process, reducing the resource requirements and providing a relatively good multi-class-based classification algorithm. We present a novel methodology for bridging the gap between various healthcare data management solutions by leveraging the strength of transformer-based machine learning models, to create mappings between the data elements. Additionally, the annotated data, collected against five medical schemas and labeled by four annotators is made available for helping future researchers. Our results indicate, that for biased, dependent multi-class text classification, transformer-based models provide better results than linguistic and other classical models. In particular, the Robustly Optimized BERT Pretraining Approach (RoBERTa) provides the best schema matching performance by achieving a Cohen's kappa score of 0.47 and Matthews Correlation Coefficient (MCC) score of 0.48, with human-annotated data.  © 2013 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Scaccia, J.P.
TI  - Examining the concept of equity in community psychology with natural language processing
PY  - 2021
T2  - Journal of Community Psychology
VL  - 49
IS  - 6
SP  - 1718
EP  - 1731
DO  - 10.1002/jcop.22603
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105948949&doi=10.1002%2fjcop.22603&partnerID=40&md5=c0fccbf8155b8aa60c30f17f3a9dd592
AB  - Large amounts of text-based data, like study abstracts, often go unanalyzed because the task is laborious. Natural language processing (NLP) uses computer-based algorithms not traditionally implemented in community psychology to effectively and efficiently process text. These methods include examining the frequency of words and phrases, the clustering of topics, and the interrelationships of words. This article applied NLP to explore the concept of equity in community psychology. The COVID-19 crisis has made pre-existing health equity gaps even more salient. Community psychology has a specific interest in working with organizations, systems, and communities to address social determinants that perpetuate inequities by refocusing interventions around achieving health and wellness for all. This article examines how community psychology has discussed equity thus far to identify strengths and gaps for future research and practice. The results showed the prominence of community-based participatory research and the diversity of settings researchers work in. However, the total number of abstracts with equity concepts was lower than expected, which suggests there is a need for a continued focus on equity. © 2021 Wiley Periodicals LLC.
PB  - John Wiley and Sons Inc
C2  - 34004017
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Emmert-Streib, F.
TI  - From the Digital Data Revolution toward a Digital Society: Pervasiveness of Artificial Intelligence
PY  - 2021
T2  - Machine Learning and Knowledge Extraction
VL  - 3
IS  - 1
SP  - 284
EP  - 298
DO  - 10.3390/make3010014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113529814&doi=10.3390%2fmake3010014&partnerID=40&md5=e22c1a9e3f596d5d19a955dc18f72120
AB  - Technological progress has led to powerful computers and communication technologies that penetrate nowadays all areas of science, industry and our private lives. As a consequence, all these areas are generating digital traces of data amounting to big data resources. This opens unprecedented opportunities but also challenges toward the analysis, management, interpretation and responsible usage of such data. In this paper, we discuss these developments and the fields that have been particularly effected by the digital revolution. Our discussion is AI-centered showing domain-specific prospects but also intricacies for the method development in artificial intelligence. For instance, we discuss recent breakthroughs in deep learning algorithms and artificial intelligence as well as advances in text mining and natural language processing, e.g., word-embedding methods that enable the processing of large amounts of text data from diverse sources such as governmental reports, blog entries in social media or clinical health records of patients. Furthermore, we discuss the necessity of further improving general artificial intelligence approaches and for utilizing advanced learning paradigms. This leads to arguments for the establishment of statistical artificial intelligence. Finally, we provide an outlook on important aspects of future challenges that are of crucial importance for the development of all fields, including ethical AI and the influence of bias on AI systems. As potential end-point of this development, we define digital society as the asymptotic limiting state of digital economy that emerges from fully connected information and communication technologies enabling the pervasiveness of AI. Overall, our discussion provides a perspective on the elaborate relatedness of digital data and AI systems. © 2021 by the author.
PB  - MDPI
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 27
ER  -

TY  - CONF
AU  - Subramanian, S.
AU  - Hrinchuk, O.
AU  - Adams, V.
AU  - Kuchaiev, O.
TI  - NVIDIA NeMo's Neural Machine Translation Systems for English ↔ German and English ↔ Russian News and Biomedical Tasks at WMT21
PY  - 2021
T2  - WMT 2021 - 6th Conference on Machine Translation, Proceedings
SP  - 197
EP  - 204
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127181583&partnerID=40&md5=fe3ff9330eb8d28c9d4c5284ca0435a2
AB  - This paper provides an overview of NVIDIA NeMo's neural machine translation systems for the constrained data track of the WMT21 News and Biomedical Shared Translation Tasks. Our news task submissions for English ↔ German (En ↔ De) and English ↔ Russian (En ↔ Ru) are built on top of a baseline transformer-based sequence-to-sequence model (Vaswani et al., 2017). Specifically, we use a combination of 1) checkpoint averaging 2) model scaling 3) data augmentation with backtranslation and knowledge distillation from right-to-left factorized models 4) finetuning on test sets from previous years 5) model ensembling 6) shallow fusion decoding with transformer language models and 7) noisy channel re-ranking. Additionally, our biomedical task submission for English ↔ Russian uses a biomedically biased vocabulary and is trained from scratch on news task data, medically relevant text curated from the news task dataset, and biomedical data provided by the shared task. Our news system achieves a sacreBLEU score of 39.5 on the WMT'20 En → De test set outperforming the best submission from last year's task of 38.8. Our biomedical task Ru → En and En → Ru systems reach BLEU scores of 43.8 and 40.3 respectively on the WMT'20 Biomedical Task Test set, outperforming the previous year's best submissions. © 2021 Association for Computational Linguistics
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - CONF
AU  - Shounak, R.
AU  - Roy, S.
AU  - Kumar, V.
AU  - Tiwari, V.
TI  - Reddit Comment Toxicity Score Prediction through BERT via Transformer Based Architecture
PY  - 2022
T2  - 2022 IEEE 13th Annual Information Technology, Electronics and Mobile Communication Conference, IEMCON 2022
SP  - 353
EP  - 358
DO  - 10.1109/IEMCON56893.2022.9946574
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143613777&doi=10.1109%2fIEMCON56893.2022.9946574&partnerID=40&md5=75a233f7d0fb543fa8e93db5eb026137
AB  - Hateful and offensive language on social media platforms has a severe influence on users' mental health and engagement of people from various backgrounds. Automatic detection of foul language has traditionally relied heavily on datasets with categorical data. However, the degree of offensiveness of comments varies. The proposed model uses tfidf followed by Ridge Regression,Catboost Regression and BERT followed by dense layers. The study uses a dataset containing Reddit-comments written in English language with precise and calculated values ranging from -1 to 1. Best-Worst Scaling was used to annotate the dataset, a type of comparative annotation that has been found to reduce the biases associated with rating scales. It has been demonstrated that the technique gives extremely accurate offensiveness scores. The proposed method offers user to customize their own threshold of offensiveness. The experiments has been conducted with different n-gram ranges. The result reveals better performance than state of the art.  © 2022 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - CONF
AU  - Chakraborty, M.
TI  - Does reusing pre-trained NLP model propagate bugs?
PY  - 2021
T2  - ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering
SP  - 1686
EP  - 1688
DO  - 10.1145/3468264.3473494
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116200524&doi=10.1145%2f3468264.3473494&partnerID=40&md5=02eef32dc4f71477e365c0660b2f559f
AB  - In this digital era, the textual content has become a seemingly ubiquitous part of our life. Natural Language Processing (NLP) empowers machines to comprehend the intricacies of textual data and eases human-computer interaction. Advancement in language modeling, continual learning, availability of a large amount of linguistic data, and large-scale computational power have made it feasible to train models for downstream tasks related to text analysis, including safety-critical ones, e.g., medical, airlines, etc. Compared to other deep learning (DL) models, NLP-based models are widely reused for various tasks. However, the reuse of pre-trained models in a new setting is still a complex task due to the limitations of the training dataset, model structure, specification, usage, etc. With this motivation, we study BERT, a vastly used language model (LM), from the direction of reusing in the code. We mined 80 posts from Stack Overflow related to BERT and found 4 types of bugs observed in clients' code. Our results show that 13.75% are fairness, 28.75% are parameter, 15% are token, and 16.25% are version-related bugs.  © 2021 Owner/Author.
PB  - Association for Computing Machinery, Inc
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Zapata, M.C.
AU  - Barreneche, J.G.
AU  - Nieto Aristizabal, J.K.
TI  - Automation of Study Design Classification and Clinical Evidence Ranking for Health Technology Assessment of Medical Devices
PY  - 2021
T2  - Communications in Computer and Information Science
VL  - 1431 CCIS
SP  - 190
EP  - 201
DO  - 10.1007/978-3-030-86702-7_17
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116738217&doi=10.1007%2f978-3-030-86702-7_17&partnerID=40&md5=839f89d66a24221da499b011954afe7c
AB  - The Health Technology Assessment (HTA) is an important tool to support the health technology incorporation and selection process in a whole health system. In the specific case of medical devices, evaluation is a challenge due to factor such as rapid innovation, the learning curve of users, among others. In most Colombian hospitals, where the HTA process exists, the clinical evaluation (safety, effectiveness and efficacy) of health technology is based on data generated through literature searching (or literature systematic review) and clinical experience, this process can be complex and time-consuming. The reduction in workload, time, and risk of bias in the literature review process is possible through automation techniques and Natural Language Processing (NLP), not only in the medical domain but also where big volumes of text must be analyzed. This work aims to compare different techniques to classify Random Controlled Clinical Trials (RCT) and Systematic Reviews (SR), and to explore NLP methods for prioritizing title and abstracts screening for literature review in the HTA process. A ranking strategy was proposed and applied in an HTA report, the total articles for the screening process were halved. The best performance for the classification was obtained by using a Support Vector Machine, specifically, sensitivity and specificity were 0.98 and 0.91 (RCT) and 0.93 and 0.97 (SR). Even though a good search strategy is a fundamental part to obtain the clinical evidence for the assessment process, where screening task is almost as important, and prioritizing methods as proposed can reduce the processing time and the researchers can focus on the analysis of relevant information. In the last years, many efforts have been made to guide the good practices in clinical studies reports and researchers have been receptive but there are still publications with unclear study design, in which the automatic classification can lighten this task for the review process. © 2021, Springer Nature Switzerland AG.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Lin, I.W.
AU  - Njoo, L.
AU  - Field, A.
AU  - Sharma, A.
AU  - Reinecke, K.
AU  - Althoff, T.
AU  - Tsvetkov, Y.
TI  - Gendered Mental Health Stigma in Masked Language Models
PY  - 2022
T2  - Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022
SP  - 2152
EP  - 2170
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149444118&partnerID=40&md5=03b47a3fe29bc6d974c28591bd2e6e61
AB  - Mental health stigma prevents many individuals from receiving the appropriate care, and social psychology studies have shown that mental health tends to be overlooked in men. In this work, we investigate gendered mental health stigma in masked language models. In doing so, we operationalize mental health stigma by developing a framework grounded in psychology research: we use clinical psychology literature to curate prompts, then evaluate the models' propensity to generate gendered words. We find that masked language models capture societal stigma about gender in mental health: models are consistently more likely to predict female subjects than male in sentences about having a mental health condition (32% vs. 19%), and this disparity is exacerbated for sentences that indicate treatment-seeking behavior. Furthermore, we find that different models capture dimensions of stigma differently for men and women, associating stereotypes like anger, blame, and pity more with women with mental health conditions than with men. In showing the complex nuances of models' gendered mental health stigma, we demonstrate that context and overlapping dimensions of identity are important considerations when assessing computational models' social biases. © 2022 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 15
ER  -

TY  - JOUR
AU  - Igwe, K.C.
AU  - Lao, P.J.
AU  - Vorburger, R.S.
AU  - Banerjee, A.
AU  - Rivera, A.
AU  - Chesebro, A.
AU  - Laing, K.
AU  - Manly, J.J.
AU  - Brickman, A.M.
TI  - Automatic quantification of white matter hyperintensities on T2-weighted fluid attenuated inversion recovery magnetic resonance imaging
PY  - 2022
T2  - Magnetic Resonance Imaging
VL  - 85
SP  - 71
EP  - 79
DO  - 10.1016/j.mri.2021.10.007
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118739097&doi=10.1016%2fj.mri.2021.10.007&partnerID=40&md5=a72abf2506066ef97ee296e3faf0b559
AB  - White matter hyperintensities (WMH) are areas of increased signal visualized on T2-weighted fluid attenuated inversion recovery (FLAIR) brain magnetic resonance imaging (MRI) sequences. They are typically attributed to small vessel cerebrovascular disease in the context of aging. Among older adults, WMH are associated with risk of cognitive decline and dementia, stroke, and various other health outcomes. There has been increasing interest in incorporating quantitative WMH measurement as outcomes in clinical trials, observational research, and clinical settings. Here, we present a novel, fully automated, unsupervised detection algorithm for WMH segmentation and quantification. The algorithm uses a robust preprocessing pipeline, including brain extraction and a sample-specific mask that incorporates spatial information for automatic false positive reduction, and a half Gaussian mixture model (HGMM). The method was evaluated in 24 participants with varying degrees of WMH (4.9–78.6 cm3) from a community-based study of aging and dementia with dice coefficient, sensitivity, specificity, correlation, and bias relative to the ground truth manual segmentation approach performed by two expert raters. Results were compared with those derived from commonly used available WMH segmentation packages, including SPM lesion probability algorithm (LPA), SPM lesion growing algorithm (LGA), and Brain Intensity AbNormality Classification Algorithm (BIANCA). The HGMM algorithm derived WMH values that had a dice score of 0.87, sensitivity of 0.89, and specificity of 0.99 compared to ground truth. White matter hyperintensity volumes derived with HGMM were strongly correlated with ground truth values (r = 0.97, p = 3.9e-16), with no observable bias (−1.1 [−2.6, 0.44], p-value = 0.16). Our novel algorithm uniquely uses a robust preprocessing pipeline and a half-Gaussian mixture model to segment WMH with high agreement with ground truth for large scale studies of brain aging. © 2021 Elsevier Inc.
PB  - Elsevier Inc.
C2  - 34662699
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Jayasinghe, L.
AU  - Velupillai, S.
AU  - Stewart, R.
TI  - Quoted text in the mental healthcare electronic record: An analysis of the distribution and content of single-word quotations
PY  - 2021
T2  - BMJ Open
VL  - 11
IS  - 12
C7  - e049249
DO  - 10.1136/bmjopen-2021-049249
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122634168&doi=10.1136%2fbmjopen-2021-049249&partnerID=40&md5=4762d597add41b8b7a356133fd97def2
AB  - Objective To investigate the distribution and content of quoted text within the electronic health records (EHRs) using a previously developed natural language processing tool to generate a database of quotations. Design χ 2 and logistic regression were used to assess the profile of patients receiving mental healthcare for whom quotations exist. K-means clustering using pre-trained word embeddings developed on general discharge summaries and psychosis specific mental health records were used to group one-word quotations into semantically similar groups and labelled by human subjective judgement. Setting EHRs from a large mental healthcare provider serving a geographic catchment area of 1.3 million residents in South London. Participants For analysis of distribution, 33 499 individuals receiving mental healthcare on 30 June 2019 in South London and Maudsley. For analysis of content, 1587 unique lemmatised words, appearing a minimum of 20 times on the database of quotations created on 16 January 2020. Results The strongest individual indicator of quoted text is inpatient care in the preceding 12 months (OR 9.79, 95% CI 7.84 to 12.23). Next highest indicator is ethnicity with those with a black background more likely to have quoted text in comparison to white background (OR 2.20, 95% CI 2.08 to 2.33). Both are attenuated slightly in the adjusted model. Early psychosis intervention word embeddings subjectively produced categories pertaining to: mental illness, verbs, negative sentiment, people/relationships, mixed sentiment, aggression/violence and negative connotation. Conclusions The findings that inpatients and those from a black ethnic background more commonly have quoted text raise important questions around where clinical attention is focused and whether this may point to any systematic bias. Our study also shows that word embeddings trained on early psychosis intervention records are useful in categorising even small subsets of the clinical records represented by one-word quotations. © Author(s) (or their employer(s)) 2021. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.
PB  - BMJ Publishing Group
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Sezgin, E.
AU  - Sirrianni, J.
AU  - Linwood, S.L.
TI  - Operationalizing and Implementing Pretrained, Large Artificial Intelligence Linguistic Models in the US Health Care System: Outlook of Generative Pretrained Transformer 3 (GPT-3) as a Service Model
PY  - 2022
T2  - JMIR Medical Informatics
VL  - 10
IS  - 2
C7  - e32875
DO  - 10.2196/32875
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124941694&doi=10.2196%2f32875&partnerID=40&md5=2d3d99188157ec630d8e33486415f73f
AB  - Generative pretrained transformer models have been popular recently due to their enhanced capabilities and performance. In contrast to many existing artificial intelligence models, generative pretrained transformer models can perform with very limited training data. Generative pretrained transformer 3 (GPT-3) is one of the latest releases in this pipeline, demonstrating human-like logical and intellectual responses to prompts. Some examples include writing essays, answering complex questions, matching pronouns to their nouns, and conducting sentiment analyses. However, questions remain with regard to its implementation in health care, specifically in terms of operationalization and its use in clinical practice and research. In this viewpoint paper, we briefly introduce GPT-3 and its capabilities and outline considerations for its implementation and operationalization in clinical practice through a use case. The implementation considerations include (1) processing needs and information systems infrastructure, (2) operating costs, (3) model biases, and (4) evaluation metrics. In addition, we outline the following three major operational factors that drive the adoption of GPT-3 in the US health care system: (1) ensuring Health Insurance Portability and Accountability Act compliance, (2) building trust with health care providers, and (3) establishing broader access to the GPT-3 tools. This viewpoint can inform health care practitioners, developers, clinicians, and decision makers toward understanding the use of the powerful artificial intelligence tools integrated into hospital systems and health care. © 2022 JMIR Medical Informatics. All rights reserved.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 79
ER  -

TY  - CONF
AU  - Padhee, S.
AU  - Swygert, K.
AU  - Micir, I.
TI  - Exploring Language Patterns in a Medical Licensure Exam Item Bank
PY  - 2021
T2  - Proceedings - 2021 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2021
SP  - 503
EP  - 508
DO  - 10.1109/BIBM52615.2021.9669681
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125203301&doi=10.1109%2fBIBM52615.2021.9669681&partnerID=40&md5=70be69f93f0f48b5bccd33e95f7101f9
AB  - This study examines the use of natural language processing (NLP) models to evaluate whether language patterns used by item writers in a medical licensure exam might contain evidence of biased or stereotypical language. This type of bias in item language choices can be particularly impactful for items in a medical licensure assessment, as it could pose a threat to content validity and defensibility of test score validity evidence. To the best of our knowledge, this is the first attempt using machine learning (ML) and NLP to explore language bias on a large item bank. Using a prediction algorithm trained on clusters of similar item stems, we demonstrate that our approach can be used to review large item banks for potential biased language or stereotypical patient characteristics in clinical science vignettes. The findings may guide the development of methods to address stereotypical language patterns found in test items and enable an efficient updating of those items, if needed, to reflect contemporary norms, thereby improving the evidence to support the validity of the test scores.  © 2021 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Samo, G.
AU  - Bonan, C.
AU  - Si, F.
TI  - Health-Related Content in Transformer-Based Deep Neural Network Language Models: Exploring Cross-Linguistic Syntactic Bias
PY  - 2022
T2  - Studies in Health Technology and Informatics
VL  - 295
SP  - 221
EP  - 225
DO  - 10.3233/SHTI220702
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133218677&doi=10.3233%2fSHTI220702&partnerID=40&md5=0480be9b8306c433f0996853a1b36146
AB  - This paper explores a methodology for bias quantification in transformer-based deep neural network language models for Chinese, English, and French. When queried with health-related mythbusters on COVID-19, we observe a bias that is not of a semantic/encyclopaedical knowledge nature, but rather a syntactic one, as predicted by theoretical insights of structural complexity. Our results highlight the need for the creation of health-communication corpora as training sets for deep learning. © 2022 The authors and IOS Press.
PB  - IOS Press BV
C2  - 35773848
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Rzycki, M.
AU  - Kraszewski, S.
AU  - Gładysiewicz-Kudrawiec, M.
TI  - Diptool—A novel numerical tool for membrane interactions analysis, applying to antimicrobial detergents and drug delivery aids
PY  - 2021
T2  - Materials
VL  - 14
IS  - 21
C7  - 6455
DO  - 10.3390/ma14216455
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118595351&doi=10.3390%2fma14216455&partnerID=40&md5=1877c2c4276184d1b5a58c2915e99b27
AB  - The widespread problem of resistance development in bacteria has become a critical issue for modern medicine. To limit that phenomenon, many compounds have been extensively studied. Among them were derivatives of available drugs, but also alternative novel detergents such as Gemini surfactants. Over the last decade, they have been massively synthesized and studied to obtain the most effective antimicrobial agents, as well as the most selective aids for nanoparticles drug delivery. Various protocols and distinct bacterial strains used in Minimal Inhibitory Concentration experimental studies prevented performance benchmarking of different surfactant classes over these last years. Motivated by this limitation, we designed a theoretical methodology implemented in custom fast screening software to assess the surfactant activity on model lipid membranes. Experimentally based QSAR (quantitative structure-activity relationship) prediction delivered a set of parameters underlying the Diptool software engine for high-throughput agent-membrane interactions analysis. We validated our software by comparing score energy profiles with Gibbs free energy from the Adaptive Biasing Force approach on octenidine and chlorhexidine, popular antimicrobials. Results from Diptool can reflect the molecule behavior in the lipid membrane and correctly predict free energy of translocation much faster than classic molecular dynamics. This opens a new venue for searching novel classes of detergents with sharp biologic activity. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
PB  - MDPI
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Cappelle, S.
AU  - Pareto, D.
AU  - Sunaert, S.
AU  - Smets, I.
AU  - Laenen, A.
AU  - Dubois, B.
AU  - Demaerel, P.
TI  - T1w/FLAIR ratio standardization as a myelin marker in MS patients
PY  - 2022
T2  - NeuroImage: Clinical
VL  - 36
C7  - 103248
DO  - 10.1016/j.nicl.2022.103248
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140792305&doi=10.1016%2fj.nicl.2022.103248&partnerID=40&md5=3f41e60071b69bd71a5e36d57770f0af
AB  - Introduction: Calculation of a T1w/T2w ratio was introduced as a proxy for myelin integrity in the brain of multiple sclerosis (MS) patients. Since nowadays 3D FLAIR is commonly used for lesion detection instead of T2w images, we introduce a T1w/FLAIR ratio as an alternative for the T1w/T2w ratio. Objectives: Bias and intensity variation are widely present between different scanners, between subjects and within subjects over time in T1w, T2w and FLAIR images. We present a standardized method for calculating a histogram calibrated T1w/FLAIR ratio to reduce bias and intensity variation in MR sequences from different scanners and at different time-points. Material and methods: 207 Relapsing Remitting MS patients were scanned on 4 different 3 T scanners with a protocol including 3D T1w, 2D T2w and 3D FLAIR images. After bias correction, T1w/FLAIR ratio maps and T1w/T2w ratio maps were calculated in 4 different ways: without calibration, with linear histogram calibration as described by Ganzetti et al. (2014), and by using 2 methods of non-linear histogram calibration. The first nonlinear calibration uses a template of extra-cerebral tissue and cerebrospinal fluid (CSF) brought from Montreal Neurological Institute (MNI) space to subject space; for the second nonlinear method we used an extra-cerebral tissue and CSF template of our own subjects. Additionally, we segmented several brain structures such as Normal Appearing White Matter (NAWM), Normal Appearing Grey Matter (NAGM), corpus callosum, thalami and MS lesions using Freesurfer and Samseg. Results: The coefficient of variation of T1w/FLAIR ratio in NAWM for the no calibrated, linear, and 2 nonlinear calibration methods were respectively 24, 19.1, 9.5, 13.8. The nonlinear methods of calibration showed the best results for calculating the T1w/FLAIR ratio with a smaller dispersion of the data and a smaller overlap of T1w/FLAIR ratio in the different segmented brain structures. T1w/T2w and T1w/FLAIR ratios showed a wider range of values compared to MTR values. Conclusions: Calibration of T1w/T2w and T1w/FLAIR ratio maps is imperative to account for the sources of variation described above. The nonlinear calibration methods showed the best reduction of between-subject and within-subject variability. The T1w/T2w and T1w/FLAIR ratio seem to be more sensitive to smaller changes in tissue integrity than MTR. Future work is needed to determine the exact substrate of T1w/FLAIR ratio and to obtain correlations with clinical outcome. © 2022 The Authors
PB  - Elsevier Inc.
C2  - 36451354
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Gao, Y.
AU  - Zhou, M.
AU  - Metaxas, D.N.
TI  - UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation
PY  - 2021
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 12903 LNCS
SP  - 61
EP  - 71
DO  - 10.1007/978-3-030-87199-4_6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116499365&doi=10.1007%2f978-3-030-87199-4_6&partnerID=40&md5=addfb1053b37c421c95edc763ee18014
AB  - Transformer architecture has emerged to be successful in a number of natural language processing tasks. However, its applications to medical vision remain largely unexplored. In this study, we present UTNet, a simple yet powerful hybrid Transformer architecture that integrates self-attention into a convolutional neural network for enhancing medical image segmentation. UTNet applies self-attention modules in both encoder and decoder for capturing long-range dependency at different scales with minimal overhead. To this end, we propose an efficient self-attention mechanism along with relative position encoding that reduces the complexity of self-attention operation significantly from O(n2) to approximate O(n). A new self-attention decoder is also proposed to recover fine-grained details from the skipped connections in the encoder. Our approach addresses the dilemma that Transformer requires huge amounts of data to learn vision inductive bias. Our hybrid layer design allows the initialization of Transformer into convolutional networks without a need of pre-training. We have evaluated UTNet on the multi-label, multi-vendor cardiac magnetic resonance imaging cohort. UTNet demonstrates superior segmentation performance and robustness against the state-of-the-art approaches, holding the promise to generalize well on other medical image segmentations. © 2021, Springer Nature Switzerland AG.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 422
ER  -

TY  - JOUR
AU  - Sukhera, J.
AU  - Ahmed, H.
TI  - Leveraging Machine Learning to Understand How Emotions Influence Equity Related Education: Quasi-Experimental Study
PY  - 2022
T2  - JMIR Medical Education
VL  - 8
IS  - 1
C7  - e33934
DO  - 10.2196/33934
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128153340&doi=10.2196%2f33934&partnerID=40&md5=35e66ec459feb48b5f68efd3f1743be8
AB  - Background: Teaching and learning about topics such as bias are challenging due to the emotional nature of bias-related discourse. However, emotions can be challenging to study in health professions education for numerous reasons. With the emergence of machine learning and natural language processing, sentiment analysis (SA) has the potential to bridge the gap. Objective: To improve our understanding of the role of emotions in bias-related discourse, we developed and conducted a SA of bias-related discourse among health professionals. Methods: We conducted a 2-stage quasi-experimental study. First, we developed a SA (algorithm) within an existing archive of interviews with health professionals about bias. SA refers to a mechanism of analysis that evaluates the sentiment of textual data by assigning scores to textual components and calculating and assigning a sentiment value to the text. Next, we applied our SA algorithm to an archive of social media discourse on Twitter that contained equity-related hashtags to compare sentiment among health professionals and the general population. Results: When tested on the initial archive, our SA algorithm was highly accurate compared to human scoring of sentiment. An analysis of bias-related social media discourse demonstrated that health professional tweets (n=555) were less neutral than the general population (n=6680) when discussing social issues on professionally associated accounts (x2 [2, n=555)]=35.455; P<.001), suggesting that health professionals attach more sentiment to their posts on Twitter than seen in the general population. Conclusions: The finding that health professionals are more likely to show and convey emotions regarding equity-related issues on social media has implications for teaching and learning about sensitive topics related to health professions education. Such emotions must therefore be considered in the design, delivery, and evaluation of equity and bias-related education. © 2022 JMIR Publications Inc.. All Rights Reserved.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Elisevich, K.
AU  - Davoodi-Bojd, E.
AU  - Heredia, J.G.
AU  - Soltanian-Zadeh, H.
TI  - Prospective Quantitative Neuroimaging Analysis of Putative Temporal Lobe Epilepsy
PY  - 2021
T2  - Frontiers in Neurology
VL  - 12
C7  - 747580
DO  - 10.3389/fneur.2021.747580
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119449594&doi=10.3389%2ffneur.2021.747580&partnerID=40&md5=c77aa98115d4de8076d9259bdb83045d
AB  - Purpose: A prospective study of individual and combined quantitative imaging applications for lateralizing epileptogenicity was performed in a cohort of consecutive patients with a putative diagnosis of mesial temporal lobe epilepsy (mTLE). Methods: Quantitative metrics were applied to MRI and nuclear medicine imaging studies as part of a comprehensive presurgical investigation. The neuroimaging analytics were conducted remotely to remove bias. All quantitative lateralizing tools were trained using a separate dataset. Outcomes were determined after 2 years. Of those treated, some underwent resection, and others were implanted with a responsive neurostimulation (RNS) device. Results: Forty-eight consecutive cases underwent evaluation using nine attributes of individual or combinations of neuroimaging modalities: 1) hippocampal volume, 2) FLAIR signal, 3) PET profile, 4) multistructural analysis (MSA), 5) multimodal model analysis (MMM), 6) DTI uncertainty analysis, 7) DTI connectivity, and 9) fMRI connectivity. Of the 24 patients undergoing resection, MSA, MMM, and PET proved most effective in predicting an Engel class 1 outcome (>80% accuracy). Both hippocampal volume and FLAIR signal analysis showed 76% and 69% concordance with an Engel class 1 outcome, respectively. Conclusion: Quantitative multimodal neuroimaging in the context of a putative mTLE aids in declaring laterality. The degree to which there is disagreement among the various quantitative neuroimaging metrics will judge whether epileptogenicity can be confined sufficiently to a particular temporal lobe to warrant further study and choice of therapy. Prediction models will improve with continued exploration of combined optimal neuroimaging metrics. © Copyright © 2021 Elisevich, Davoodi-Bojd, Heredia and Soltanian-Zadeh.
PB  - Frontiers Media S.A.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Lee, M.H.
AU  - Kyung, R.
TI  - Mental Health Stigma and Natural Language Processing: Two Enigmas Through the Lens of a Limited Corpus
PY  - 2022
T2  - 2022 IEEE World AI IoT Congress, AIIoT 2022
SP  - 688
EP  - 691
DO  - 10.1109/AIIoT54504.2022.9817362
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134879612&doi=10.1109%2fAIIoT54504.2022.9817362&partnerID=40&md5=849476c742c001cc80a81016385bce8a
AB  - Mental health stigma is an elephant in the room. It exacerbates one's illness, impedes approaches to treatment, and ultimately contributes to the persistence of a 'mental health epidemic.' A definitive solution for managing stigmatized language is yet to be discovered, especially on the internet, where stigma is virtually ubiquitous in the forms of user posts, text messages, and biased articles. This study proposes text classification, a subset of natural language processing (NLP), as a solution to identify stigma in context. NLP is frequently used to detect human sentiments and emotions to eradicate hate speech, racism, and personal attacks; however, it has not been thoroughly explored in the field of mental health stigma, and the lack of preexisting data presents a challenge. Facing limited resources, the study hypothesized that the BERT model's fine-Tuning method allowed for a small corpus to provide satisfactory results. The model returned surprisingly impressive results (0.94 accuracies, 0.91 F1-Score). The study not only confirms that NLP can be used as an effective solution to detect and later reduce stigma but also that the BERT model is still proficient with a limited corpus. Therefore, NLP tasks historically focused on thoroughly researched fields with an abundance of data, can also be used effectively in underdeveloped, unexplored fields of research that currently lack the datasets needed for training.  © 2022 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Hsu, J.W.-C.
AU  - Christensen, P.
AU  - Ge, Y.
AU  - Long, S.W.
TI  - Classification of cervical biopsy free-text diagnoses through linear-classifier based natural language processing
PY  - 2022
T2  - Journal of Pathology Informatics
VL  - 13
C7  - 100123
DO  - 10.1016/j.jpi.2022.100123
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133903560&doi=10.1016%2fj.jpi.2022.100123&partnerID=40&md5=a2fe0a8f712df32745287caf0dca9c8a
AB  - Routine cervical cancer screening has significantly decreased the incidence and mortality of cervical cancer. As selection of proper screening modalities depends on well-validated clinical decision algorithms, retrospective review correlating cytology and HPV test results with cervical biopsy diagnosis is essential for validating and revising these algorithms to changing technologies, demographics, and optimal clinical practices. However, manual categorization of the free-text biopsy diagnosis into discrete categories is extremely laborious due to the overwhelming number of specimens, which may lead to significant error and bias. Advances in machine learning and natural language processing (NLP), particularly over the last decade, have led to significant accomplishments and impressive performance in computer-based classification tasks. In this work, we apply an efficient version of an NLP framework, FastText™, to an annotated cervical biopsy dataset to create a supervised classifier that can assign accurate biopsy categories to free-text biopsy interpretations with high concordance to manually annotated data (>99.6%). We present cases where the machine-learning classifier disagrees with previous annotations and examine these discrepant cases after referee review by an expert pathologist. We also show that the classifier is robust on an untrained external dataset, achieving a concordance of 97.7%. In conclusion, we demonstrate a useful application of NLP to a real-world pathology classification task and highlight the benefits and limitations of this approach. © 2022
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Gandhi, D.
AU  - Shah, V.
AU  - Chawan, P.M.
TI  - A Vision Transformer Approach for Classification an A Small-Sized Medical Image Dataset
PY  - 2022
T2  - 5th IEEE International Conference on Advances in Science and Technology, ICAST 2022
SP  - 519
EP  - 524
DO  - 10.1109/ICAST55766.2022.10039593
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149180018&doi=10.1109%2fICAST55766.2022.10039593&partnerID=40&md5=ce36236dc4969f2a66b45d2da3e8cea6
AB  - In natural language processing, transformers have supplanted RNN models as more advanced technology. The vision transformer (ViT) model which is built upon the transformer architecture brought the concept of self-attention to images. Though the ViT model has given a better performance than CNNs on huge datasets, it still gives a poor performance on small-sized datasets when trained from scratch. This is because of its low locality inductive bias problem. This paper proposes a modified vision transformer architecture that would consist of three additional layers. A modified tokenization layer, a token learner layer, and a stochastic depth layer. The model is trained from scratch on a small-sized medical image dataset which is HAMS10000. The model is then compared with inception ResNet V2, a Vgg16 model, and a conventional ViT model trained on the same dataset. We see a more than 10% improvement in the ViT model and a significant increase in accuracy compared to the CNN models.  © 2022 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Parikh, J.R.
AU  - Genetti, C.A.
AU  - Aykanat, A.
AU  - Brownstein, C.A.
AU  - Schmitz-Abe, K.
AU  - Danowski, M.
AU  - Quitadomo, A.
AU  - Madden, J.A.
AU  - Yacoubian, C.
AU  - Gain, R.
AU  - Williams, T.
AU  - Meskell, M.
AU  - Brown, A.
AU  - Frith, A.
AU  - Rockowitz, S.
AU  - Sliz, P.
AU  - Agrawal, P.B.
AU  - Defay, T.
AU  - McDonagh, P.
AU  - Reynders, J.
AU  - Lefebvre, S.
AU  - Beggs, A.H.
TI  - A data-driven architecture using natural language processing to improve phenotyping efficiency and accelerate genetic diagnoses of rare disorders
PY  - 2021
T2  - Human Genetics and Genomics Advances
VL  - 2
IS  - 3
C7  - 100035
DO  - 10.1016/j.xhgg.2021.100035
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120474170&doi=10.1016%2fj.xhgg.2021.100035&partnerID=40&md5=ff503e9cb18ccae82c6be9f8f7e65768
AB  - Effective genetic diagnosis requires the correlation of genetic variant data with detailed phenotypic information. However, manual encoding of clinical data into machine-readable forms is laborious and subject to observer bias. Natural language processing (NLP) of electronic health records has great potential to enhance reproducibility at scale but suffers from idiosyncrasies in physician notes and other medical records. We developed methods to optimize NLP outputs for automated diagnosis. We filtered NLP-extracted Human Phenotype Ontology (HPO) terms to more closely resemble manually extracted terms and identified filter parameters across a three-dimensional space for optimal gene prioritization. We then developed a tiered pipeline that reduces manual effort by prioritizing smaller subsets of genes to consider for genetic diagnosis. Our filtering pipeline enabled NLP-based extraction of HPO terms to serve as a sufficient replacement for manual extraction in 92% of prospectively evaluated cases. In 75% of cases, the correct causal gene was ranked higher with our applied filters than without any filters. We describe a framework that can maximize the utility of NLP-based phenotype extraction for gene prioritization and diagnosis. The framework is implemented within a cloud-based modular architecture that can be deployed across health and research institutions. © 2021 The Author(s)
PB  - Cell Press
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - CONF
AU  - Kumar, S.
AU  - Jayant, R.
AU  - Charagulla, N.
TI  - Sentiment Analysis on the News to Improve Mental Health
PY  - 2021
T2  - 2021 IEEE MIT Undergraduate Research Technology Conference, URTC 2021
DO  - 10.1109/URTC54388.2021.9701632
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127372909&doi=10.1109%2fURTC54388.2021.9701632&partnerID=40&md5=fe694d2de32c5739391a5b470124e3d8
AB  - The popularization of the internet created a revitalized digital media. With monetization driven by clicks, journalists have reprioritized their content for the highly competitive atmosphere of online news. The resulting negativity bias is harmful and can lead to anxiety and mood disturbance. We utilized a pipeline of 4 sentiment analysis models trained on various datasets-using Sequential, LSTM, BERT, and SVM models. When combined, the application, a mobile app, solely displays uplifting and positive stories for users to read. Results have been successful-1,300 users rate the app at 4.9 stars, and 85% report improved mental health by using it.  © 2021 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Agaronnik, N.D.
AU  - El-Jawahri, A.
AU  - Lindvall, C.
AU  - Iezzoni, L.I.
TI  - Exploring the Process of Cancer Care for Patients With Pre-Existing Mobility Disability
PY  - 2021
T2  - JCO oncology practice
VL  - 17
IS  - 1
SP  - e53
EP  - e61
DO  - 10.1200/OP.20.00378
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100070575&doi=10.1200%2fOP.20.00378&partnerID=40&md5=401333f919b10324b2d5d3af19de8339
AB  - PURPOSE: Approximately 13% of the US population report mobility disability. People with mobility disability experience healthcare disparities, including lower rates of cancer screening and substandard cancer care compared with nondisabled people. We explored clinicians' reports of aspects of diagnosing and treating three common cancer types among persons with pre-existing mobility disability. METHODS: We used standard diagnosis codes and natural language processing to screen electronic health records (EHR) in the Research Patient Data Repository for patients with pre-existing chronic mobility impairment who were newly diagnosed with one of three common cancers (colorectal, prostate, and non-Hodgkin lymphoma) between 2005 and 2017. We eliminated numerous cases whose EHRs lacked essential information. We reviewed EHRs of 27 cases, using conventional content analysis to identify themes concerning their cancer diagnoses and treatments. RESULTS: Clinicians' notations coalesced around four major themes: (1) patients' health risks raise concerns about diagnostic processes; (2) cancer signs or symptoms can be erroneously attributed to the patient's underlying disabling condition, delaying diagnosis; (3) disability complicates cancer treatment decisions; and (4) problems with equipment accessibility and disability accommodations impede cancer diagnoses. DISCUSSION: Clinicians view patients with pre-existing mobility disability as often clinically complex, presenting challenges for diagnosing and treating their cancer. Nonetheless, these patients may experience substandard care because of disability-related problems. Given the growing population of people with mobility disability, further efforts to improve care quality and timeliness of diagnosis are warranted.
PB  - NLM (Medline)
C2  - 33351675
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 9
ER  -

TY  - CONF
AU  - Miles, S.
AU  - Yao, L.
AU  - Meng, W.
AU  - Black, C.M.
AU  - Miled, Z.B.
TI  - Topic Extraction from A Cancer Health Forum
PY  - 2021
T2  - Proceedings - 2021 IEEE 9th International Conference on Healthcare Informatics, ISCHI 2021
SP  - 491
EP  - 492
DO  - 10.1109/ICHI52183.2021.00085
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118189530&doi=10.1109%2fICHI52183.2021.00085&partnerID=40&md5=f2f3f2fdb08e22ecf108885afd3a2c16
AB  - This paper presents a methodology for topic extraction from a corpus of unstructured posts submitted to the online health forum r/Cancer. Topic extraction is important in many fields. It can provide an understanding of how patients and their caregivers manage the disease and related treatments. Reduced vector embeddings are generated for each post using a combination of a pre-trained language model and dimensionality reduction. These embeddings are then clustered with particle swarm optimization (PSO). An embedding size of 300 produced a topic model with a quality of 0.44. This quality level was obtained without biasing the words in the vocabulary towards a specific topic as currently practiced. © 2021 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Agmon, S.
AU  - Gillis, P.
AU  - Horvitz, E.
AU  - Radinsky, K.
TI  - Gender-sensitive word embeddings for healthcare
PY  - 2022
T2  - Journal of the American Medical Informatics Association
VL  - 29
IS  - 3
SP  - 415
EP  - 423
DO  - 10.1093/jamia/ocab279
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123901022&doi=10.1093%2fjamia%2focab279&partnerID=40&md5=a4c671a13f908e1bcfff1f05b3f8a6cb
AB  - Objective: To analyze gender bias in clinical trials, to design an algorithm that mitigates the effects of biases of gender representation on natural-language (NLP) systems trained on text drawn from clinical trials, and to evaluate its performance. Materials and Methods: We analyze gender bias in clinical trials described by 16 772 PubMed abstracts (2008-2018). We present a method to augment word embeddings, the core building block of NLP-centric representations, by weighting abstracts by the number of women participants in the trial. We evaluate the resulting gender-sensitive embeddings performance on several clinical prediction tasks: comorbidity classification, hospital length of stay prediction, and intensive care unit (ICU) readmission prediction. Results: For female patients, the gender-sensitive model area under the receiver-operator characteristic (AUROC) is 0.86 versus the baseline of 0.81 for comorbidity classification, mean absolute error 4.59 versus the baseline of 4.66 for length of stay prediction, and AUROC 0.69 versus 0.67 for ICU readmission. All results are statistically significant. Discussion: Women have been underrepresented in clinical trials. Thus, using the broad clinical trials literature as training data for statistical language models could result in biased models, with deficits in knowledge about women. The method presented enables gender-sensitive use of publications as training data for word embeddings. In experiments, the gender-sensitive embeddings show better performance than baseline embeddings for the clinical tasks studied. The results highlight opportunities for recognizing and addressing gender and other representational biases in the clinical trials literature. Conclusion: Addressing representational biases in data for training NLP embeddings can lead to better results on downstream tasks for underrepresented populations. © 2021 The Author(s).
PB  - Oxford University Press
C2  - 34918101
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - CONF
AU  - Sun, Z.
AU  - Harit, A.
AU  - Cristea, A.I.
AU  - Yu, J.
AU  - Moubayed, N.A.
AU  - Shi, L.
TI  - Is Unimodal Bias Always Bad for Visual Question Answering? A Medical Domain Study with Dynamic Attention
PY  - 2022
T2  - Proceedings - 2022 IEEE International Conference on Big Data, Big Data 2022
SP  - 5352
EP  - 5360
DO  - 10.1109/BigData55660.2022.10020791
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147957708&doi=10.1109%2fBigData55660.2022.10020791&partnerID=40&md5=81d872af977f15c8a1894bc24e043e31
AB  - Medical visual question answering (Med-VQA) is to answer medical questions based on clinical images provided. This field is still in its infancy due to the complexity of the trio formed of questions, multimodal features and expert knowledge. In this paper, we tackle, a 'myth' in the Natural Language Processing area - that unimodal bias is always considered undesirable in learning models. Additionally, we study the effect of integrating a novel dynamic attention mechanism into such models, inspired by a recent graph deep learning study.Unlike traditional attention, dynamic attention scores are conditioned on different query words in a question and thus enhance the representation learning ability of texts. We propose that some questions are answered more accurately with a reinforcement of question embedding after fusing multimodal features. Extensive experiments have been implemented on the VQA-RAD datasets and demonstrate that our proposed model, reinforCe unimOdal dynamiC Attention (COCA), outperforms the state-of-the-art methods overall and performs competitively at open-ended question answering. © 2022 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Olusanya, O.A.
AU  - Ammar, N.
AU  - Davis, R.L.
AU  - Bednarczyk, R.A.
AU  - Shaban-Nejad, A.
TI  - A Digital Personal Health Library for Enabling Precision Health Promotion to Prevent Human Papilloma Virus-Associated Cancers
PY  - 2021
T2  - Frontiers in Digital Health
VL  - 3
C7  - 683161
DO  - 10.3389/fdgth.2021.683161
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113337645&doi=10.3389%2ffdgth.2021.683161&partnerID=40&md5=0ddfa01c260324c06d9af8f05e721398
AB  - Human papillomavirus (HPV) causes the most prevalent sexually transmitted infection (STI) in the United States. Sexually active young adults are susceptible to HPV, accounting for approximately 50% of new STIs. Oncogenic HPV subtypes 16 and 18 are associated with squamous intraepithelial lesions and cancers and are mostly preventable through prophylactic HPV vaccination. Accordingly, this study's objectives are to (1) summarize SDoH barriers and implication for low HPV vaccination rates among young adults (18–26 years), (2) propose a digital health solution that utilizes the PHL to collect, integrate, and manage personalized sexual and health information, and (3) describe the features of the PHL-based app. Through the application of novel techniques from artificial intelligence, specifically knowledge representation, semantic web, and natural language processing, this proposed PHL-based application will compile clinical, biomedical, and SDoH data from multi-dimensional sources. Therefore, this application will provide digital health interventions that are customized to individuals' specific needs and capacities. The PHL-based application could promote management and usage of personalized digital health information to facilitate precision health promotion thereby, informing health decision-making regarding HPV vaccinations, routine HPV/STI testing, cancer screenings, vaccine safety/efficacy/side effects, and safe sexual practices. In addition to detecting vaccine hesitancy, disparities and perceived barriers, this application could address participants' specific needs/challenges with navigating health literacy, technical skills, peer influence, education, language, cultural and spiritual beliefs. Precision health promotion focused on improving knowledge acquisition and information-seeking behaviors, promoting safe sexual practices, increasing HPV vaccinations, and facilitating cancer screenings could be effective in preventing HPV-associated cancers. © Copyright © 2021 Olusanya, Ammar, Davis, Bednarczyk and Shaban-Nejad.
PB  - Frontiers Media SA
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 9
ER  -

TY  - CONF
AU  - Howell, K.
AU  - Barnes, M.
AU  - Randall Curtis, J.
AU  - Engelberg, R.A.
AU  - Lee, R.Y.
AU  - Lober, W.B.
AU  - Sibley, J.
AU  - Cohen, T.
TI  - Controlling for Confounding Variables: Accounting for Dataset Bias in Classifying Patient-Provider Interactions
PY  - 2021
T2  - Studies in Computational Intelligence
VL  - 914
SP  - 271
EP  - 282
DO  - 10.1007/978-3-030-53352-6_25
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097099627&doi=10.1007%2f978-3-030-53352-6_25&partnerID=40&md5=9d56e369756c8f1ed3207f93e073c2b5
AB  - Natural Language Processing (NLP) is a key enabling technology for re-use of information in free-text clinical notes. However, a barrier to deployment is the availability of labeled corpora for supervised machine learning, which are expensive to acquire as they must be annotated by experienced clinicians. Where corpora are available, they may be opportunistically collected and thus vulnerable to bias. Here we evaluate an approach for accounting for dataset bias in the context of identifying specific patient-provider interactions. In this context, bias is the result of a phenomenon being over or under-represented in a particular type of clinical note as a result of the way a dataset was curated. Using a clinical dataset which represents a great deal of variation in terms of author and setting, we control for confounding variables using a backdoor adjustment approach[1, 2], which to our knowledge has not been previously applied the clinical domain. This approach improves precision by up to 5% and the adjusted models’ scores for false positives are generally lower, resulting in a more generalizable model with the potential to enhance the downstream utility of models trained using opportunistically collected clinical corpora. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Braghetto, I.
AU  - Korn, O.
AU  - Gutiérrez, L.
AU  - Torrealba, A.
AU  - Rojas, J.
TI  - GASTROESOPHAGEAL SYMPTOMS AFTER LAPAROSCOPIC GASTRIC BYPASS: MISTAKES IN PERFORMING THE PROCEDURE?
ST  - SINTOMAS GASTROESOFÁGICOS APÓS BYPASS GÁSTRICO LAPAROSCÓPICO: EQUÍVOCO NA EXECUÇÃO  DO PROCEDIMENTO?
PY  - 2022
T2  - Arquivos Brasileiros de Cirurgia Digestiva
VL  - 35
C7  - e1657
DO  - 10.1590/0102-672020210002e1657
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132307071&doi=10.1590%2f0102-672020210002e1657&partnerID=40&md5=31333a07154a6e79ae622206908b9a54
AB  - Walter de Biase SILVA-NETO1, Claudemiro QUIRESE1, Eduardo GuiABSTRACT – BACKGROUND: Laparoscopic Roux-en-Y gastric bypass (LGB) is the recommended procedure for morbidly obese patients with gastroesophageal reflux disease (GERD). However, there Fabriciohave beenFerreirareported COELHOgastroesophageal3, Pauloreflux symptomsHERMANor esophagitis3 after LGB. Few functional esophageal studies have been reported to date. AIM: To evaluate the anatomic and physiologic factors contributing to the appearance of these problems in patients who underwent LGB. METHODS: This prospective study included 38 patients with postoperative gastroesophageal RESUMOreflux symptoms-Racional:submittedO tratamentoto LGB. deTheyescolhawere subjectedpara pacientesto clinical,comendoscopic,hipertensãoradiologic,portal esquistossomóticamanometric, and 24-hcompH-monitoringsangramentoevaluations.de varizesRESULTS:é a desconexãoEighteen (47.4%)ázigo-portalof 38 patientsmais esplenectomiapresented with (DAPE)heartburnassociadaor regurgitation,à terapia7 presentedendoscópica.withPorém,pain, andestudos4 presentedmostramwith aumentodysphagia. doErosivecalibreesophagitisdas varizeswasemobservedalguns pacientesin 11 (28.9%)durantepatients,o seguimentoand Barrett’sem longoesophagusprazo. Objetivo:(5.7%) and jejunitis (10.5%) were also observed. Hiatal hernia was the most frequent finding observed in 15 Avaliar o impacto da DAPE e tratamento endoscópico pós-operatório no comportamento (39.5%) patients, and most (10.5%) of these patients appeared with concomitant anastomotic dasstrictures.varizesAesofágicaslong blind jejunale recidivaloophemorrágica,was detected indeonepacientes(2.6%) patient.esquistossomóticos.Nearly 75% of theMétodos:patients Foramhad hypotensiveestudadoslower36 pacientesesophagealcomsphincterseguimento(9.61±4.05superiormmHg),a cinco17.4% anos,had hypomotilitydistribuídosofemthe doisesophagealgrupos:body,quedaandda64.7%pressãohadportalpathologicabaixoacidde 30%refluxe(%acimatimedepH30%<4=6.98±5.5;comparadosDeMeester’scom o calibrescore=32.4±21.15).das varizes esofágicasCONCLUSION:no pós-operatórioAlthough rare, itprecoceis possiblee tardioto observealémgastroesophagealdo índice de recidivareflux and other important postoperative symptoms after LGB, which are associated with anatomic and hemorrágica. Resultados physiologic abnormalities at the esophagogastric junction and proximal gastric pouch. © 2022, Colegio Brasileiro de Cirurgia Digestiva. All rights reserved.
PB  - Colegio Brasileiro de Cirurgia Digestiva
C2  - 35730886
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - De Angeli, K.
AU  - Gao, S.
AU  - Danciu, I.
AU  - Durbin, E.B.
AU  - Wu, X.-C.
AU  - Stroup, A.
AU  - Doherty, J.
AU  - Schwartz, S.
AU  - Wiggins, C.
AU  - Damesyn, M.
AU  - Coyle, L.
AU  - Penberthy, L.
AU  - Tourassi, G.D.
AU  - Yoon, H.-J.
TI  - Class imbalance in out-of-distribution datasets: Improving the robustness of the TextCNN for the classification of rare cancer types
PY  - 2022
T2  - Journal of Biomedical Informatics
VL  - 125
C7  - 103957
DO  - 10.1016/j.jbi.2021.103957
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120156672&doi=10.1016%2fj.jbi.2021.103957&partnerID=40&md5=8ca81874bd9ee8ad8af28a6cf3f5e10a
AB  - In the last decade, the widespread adoption of electronic health record documentation has created huge opportunities for information mining. Natural language processing (NLP) techniques using machine and deep learning are becoming increasingly widespread for information extraction tasks from unstructured clinical notes. Disparities in performance when deploying machine learning models in the real world have recently received considerable attention. In the clinical NLP domain, the robustness of convolutional neural networks (CNNs) for classifying cancer pathology reports under natural distribution shifts remains understudied. In this research, we aim to quantify and improve the performance of the CNN for text classification on out-of-distribution (OOD) datasets resulting from the natural evolution of clinical text in pathology reports. We identified class imbalance due to different prevalence of cancer types as one of the sources of performance drop and analyzed the impact of previous methods for addressing class imbalance when deploying models in real-world domains. Our results show that our novel class-specialized ensemble technique outperforms other methods for the classification of rare cancer types in terms of macro F1 scores. We also found that traditional ensemble methods perform better in top classes, leading to higher micro F1 scores. Based on our findings, we formulate a series of recommendations for other ML practitioners on how to build robust models with extremely imbalanced datasets in biomedical NLP applications. © 2021
PB  - Academic Press Inc.
C2  - 34823030
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 44
ER  -

TY  - CONF
AU  - Yang, H.
AU  - Zhang, S.
AU  - Han, X.
AU  - Zhao, B.
AU  - Ren, Y.
AU  - Sheng, Y.
AU  - Zhang, X.-Y.
TI  - Denoising of 3D MR Images Using a Voxel-Wise Hybrid Residual MLP-CNN Model to Improve Small Lesion Diagnostic Confidence
PY  - 2022
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 13433 LNCS
SP  - 292
EP  - 302
DO  - 10.1007/978-3-031-16437-8_28
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139064213&doi=10.1007%2f978-3-031-16437-8_28&partnerID=40&md5=42fa4b37557a44e3284639b44321a1d4
AB  - Small lesions in magnetic resonance imaging (MRI) images are crucial for clinical diagnosis of many kinds of diseases. However, the MRI quality can be easily degraded by various noise, which can greatly affect the accuracy of diagnosis of small lesion. Although some methods for denoising MR images have been proposed, task-specific denoising methods for improving the diagnosis confidence of small lesions are lacking. In this work, we propose a voxel-wise hybrid residual MLP-CNN model to denoise three-dimensional (3D) MR images with small lesions. We combine basic deep learning architecture, MLP and CNN, to obtain an appropriate inherent bias for the image denoising and integrate each output layers in MLP and CNN by adding residual connections to leverage long-range information. We evaluate the proposed method on 720 T2-FLAIR brain images with small lesions at different noise levels. The results show the superiority of our method in both quantitative and visual evaluations on testing dataset compared to state-of-the-art methods. Moreover, two experienced radiologists agreed that at moderate and high noise levels, our method outperforms other methods in terms of recovery of small lesions and overall image denoising quality. The implementation of our method is available at https://github.com/laowangbobo/Residual_MLP_CNN_Mixer. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Ivanov, O.
AU  - Wolf, L.
AU  - Brecher, D.
AU  - Lewis, E.
AU  - Masek, K.
AU  - Montgomery, K.
AU  - Andrieiev, Y.
AU  - McLaughlin, M.
AU  - Liu, S.
AU  - Dunne, R.
AU  - Klauer, K.
AU  - Reilly, C.
TI  - Improving ED Emergency Severity Index Acuity Assignment Using Machine Learning and Clinical Natural Language Processing
PY  - 2021
T2  - Journal of Emergency Nursing
VL  - 47
IS  - 2
SP  - 265
EP  - 278.e7
DO  - 10.1016/j.jen.2020.11.001
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098143496&doi=10.1016%2fj.jen.2020.11.001&partnerID=40&md5=61ead4c692ec7e5858fb85435ef28cb8
AB  - Introduction: Triage is critical to mitigating the effect of increased volume by determining patient acuity, need for resources, and establishing acuity-based patient prioritization. The purpose of this retrospective study was to determine whether historical EHR data can be used with clinical natural language processing and machine learning algorithms (KATE) to produce accurate ESI predictive models. Methods: The KATE triage model was developed using 166,175 patient encounters from two participating hospitals. The model was tested against a random sample of encounters that were correctly assigned an acuity by study clinicians using the Emergency Severity Index (ESI) standard as a guide. Results: At the study sites, KATE predicted accurate ESI acuity assignments 75.7% of the time compared with nurses (59.8%) and the average of individual study clinicians (75.3%). KATE's accuracy was 26.9% higher than the average nurse accuracy (P <.001). On the boundary between ESI 2 and ESI 3 acuity assignments, which relates to the risk of decompensation, KATE's accuracy was 93.2% higher, with 80% accuracy compared with triage nurses 41.4% accuracy (P <.001). Discussion: KATE provides a triage acuity assignment more accurate than the triage nurses in this study sample. KATE operates independently of contextual factors, unaffected by the external pressures that can cause under triage and may mitigate biases that can negatively affect triage accuracy. Future research should focus on the impact of KATE providing feedback to triage nurses in real time, on mortality and morbidity, ED throughput, resource optimization, and nursing outcomes. © 2020 Emergency Nurses Association
PB  - Mosby Inc.
C2  - 33358394
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 48
ER  -

TY  - JOUR
AU  - Beach, M.C.
AU  - Saha, S.
AU  - Park, J.
AU  - Taylor, J.
AU  - Drew, P.
AU  - Plank, E.
AU  - Cooper, L.A.
AU  - Chee, B.
TI  - Testimonial Injustice: Linguistic Bias in the Medical Records of Black Patients and Women
PY  - 2021
T2  - Journal of General Internal Medicine
VL  - 36
IS  - 6
SP  - 1708
EP  - 1714
DO  - 10.1007/s11606-021-06682-z
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103196162&doi=10.1007%2fs11606-021-06682-z&partnerID=40&md5=8663821533a6f3ab216dde9402f5d468
AB  - Background: Black Americans and women report feeling doubted or dismissed by health professionals. Objective: To identify linguistic mechanisms by which physicians communicate disbelief of patients in medical records and then to explore racial and gender differences in the use of such language. Design: Cross-sectional. Setting/Participants: All notes for patients seen in an academic ambulatory internal medicine practice in 2017. Main Measures: A content analysis of 600 clinic notes revealed three linguistic features suggesting disbelief: (1) quotes (e.g., had a “reaction” to the medication); (2) specific “judgment words” that suggest doubt (e.g., “claims” or “insists”); and (3) evidentials, a sentence construction in which patients’ symptoms or experience is reported as hearsay. We used natural language processing to evaluate the prevalence of these features in the remaining notes and tested differences by race and gender, using mixed-effects regression to account for clustering of notes within patients and providers. Key Results: Our sample included 9251 notes written by 165 physicians about 3374 unique patients. Most patients were identified as Black (74%) and female (58%). Notes written about Black patients had higher odds of containing at least one quote (OR 1.48, 95% CI 1.20–1.83) and at least one judgment word (OR 1.25, 95% CI 1.02–1.53), and used more evidentials (β 0.32, 95% CI 0.17–0.47), compared to notes of White patients. Notes about female vs. male patients did not differ in terms of judgment words or evidentials but had a higher odds of containing at least one quote (OR 1.22, 95% CI 1.05–1.44). Conclusions: Black patients may be subject to systematic bias in physicians’ perceptions of their credibility, a form of testimonial injustice. This is another potential mechanism for racial disparities in healthcare quality that should be further investigated and addressed. © 2021, This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply.
PB  - Springer
C2  - 33754318
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 89
ER  -

TY  - JOUR
AU  - van der Werff, S.D.
AU  - Thiman, E.
AU  - Tanushi, H.
AU  - Valik, J.K.
AU  - Henriksson, A.
AU  - Ul Alam, M.
AU  - Dalianis, H.
AU  - Ternhag, A.
AU  - Nauclér, P.
TI  - The accuracy of fully automated algorithms for surveillance of healthcare-associated urinary tract infections in hospitalized patients
PY  - 2021
T2  - Journal of Hospital Infection
VL  - 110
SP  - 139
EP  - 147
DO  - 10.1016/j.jhin.2021.01.023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101944631&doi=10.1016%2fj.jhin.2021.01.023&partnerID=40&md5=5ba618f2f8ce1c4a37fa1e61302f26e6
AB  - Background: Surveillance for healthcare-associated infections such as healthcare-associated urinary tract infections (HA-UTI) is important for directing resources and evaluating interventions. However, traditional surveillance methods are resource-intensive and subject to bias. Aim: To develop and validate a fully automated surveillance algorithm for HA-UTI using electronic health record (EHR) data. Methods: Five algorithms were developed using EHR data from 2979 admissions at Karolinska University Hospital from 2010 to 2011: (1) positive urine culture (UCx); (2) positive UCx + UTI codes (International Statistical Classification of Diseases and Related Health Problems, 10th revision); (3) positive UCx + UTI-specific antibiotics; (4) positive UCx + fever and/or UTI symptoms; (5) algorithm 4 with negation for fever without UTI symptoms. Natural language processing (NLP) was used for processing free-text medical notes. The algorithms were validated in 1258 potential UTI episodes from January to March 2012 and results extrapolated to all UTI episodes within this period (N = 16,712). The reference standard for HA-UTIs was manual record review according to the European Centre for Disease Prevention and Control (and US Centers for Disease Control and Prevention) definitions by trained healthcare personnel. Findings: Of the 1258 UTI episodes, 163 fulfilled the ECDC HA-UTI definition and the algorithms classified 391, 150, 189, 194, and 153 UTI episodes, respectively, as HA-UTI. Algorithms 1, 2, and 3 had insufficient performances. Algorithm 4 achieved better performance and algorithm 5 performed best for surveillance purposes with sensitivity 0.667 (95% confidence interval: 0.594–0.733), specificity 0.997 (0.996–0.998), positive predictive value 0.719 (0.624–0.807) and negative predictive value 0.997 (0.996–0.997). Conclusion: A fully automated surveillance algorithm based on NLP to find UTI symptoms in free-text had acceptable performance to detect HA-UTI compared to manual record review. Algorithms based on administrative and microbiology data only were not sufficient. © 2021
PB  - W.B. Saunders Ltd
C2  - 33548370
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - CONF
AU  - Qian, Z.
TI  - Applications, Risks and Countermeasures of Artificial Intelligence in Education
PY  - 2021
T2  - Proceedings - 2021 2nd International Conference on Artificial Intelligence and Education, ICAIE 2021
SP  - 89
EP  - 92
DO  - 10.1109/ICAIE53562.2021.00026
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116170983&doi=10.1109%2fICAIE53562.2021.00026&partnerID=40&md5=676fa445c28d90ff0c307e91880c3ebb
AB  - The application of artificial intelligence in education has greatly affected school management, teachers' teaching and students' learning, and has an important influence on the mechanism and mechanism of learning process. The concept of artificial intelligence is to simulate human intelligence by machine, thus completing specific roles and tasks, and has the characteristics of convenience, intelligence and interaction. It is mainly applied to computer vision, natural language processing, biometric recognition, speech recognition, human-computer interaction and other technologies in the field of education, which brings space for the development of architecture, medicine and chemistry. However, AI also brings about the risk of decisionmaking mistakes, career substitution, privacy leakage, information cocoon house and data bias. In order to avoid these risks effectively, this paper puts forward solutions from five perspectives: the state, product developers, educational managers, teachers and students, so as to correctly understand the relationship between artificial intelligence and education, explore the application mode of artificial intelligence in the field of education, and ensure the deep integration of artificial intelligence and education. © 2021 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - CONF
AU  - Gong, C.
AU  - Du, X.
AU  - Choudhary, D.
AU  - Bhushanam, B.
AU  - Liu, Q.
AU  - Kejariwal, A.
TI  - Harmless Transfer Learning for Item Embeddings
PY  - 2022
T2  - Findings of the Association for Computational Linguistics: NAACL 2022 - Findings
SP  - 504
EP  - 516
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137367787&partnerID=40&md5=9b800f10f9e7f1e834184871602655d2
AB  - Learning embedding layers (for classes, words, items, etc.) is a key component of lots of applications, ranging from natural language processing, recommendation systems to electronic health records, etc. However, the frequency of real-world items follows a long-tail distribution in these applications, causing naive training methods perform poorly on the rare items. A line of previous works address this problem by transferring the knowledge from the frequent items to rare items by introducing an auxiliary transfer loss. However, when defined improperly, the transfer loss may introduce harmful biases and deteriorate the performance. In this work, we propose a harmless transfer learning framework that limits the impact of the potential biases in both the definition and optimization of the transfer loss. On the definition side, we reduce the bias in transfer loss by focusing on the items to which information from high-frequency items can be efficiently transferred. On the optimization side, we leverage a lexicographic optimization framework to efficiently incorporate the information of the transfer loss without hurting the minimization of the main prediction loss function. Our method serves as a plug-in module and significantly boosts the performance on a variety of NLP and recommendation system tasks. © Findings of the Association for Computational Linguistics: NAACL 2022 - Findings.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Mayank, M.
AU  - Sharma, S.
AU  - Sharma, R.
TI  - DEAP-FAKED: Knowledge Graph based Approach for Fake News Detection
PY  - 2022
T2  - Proceedings of the 2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2022
SP  - 47
EP  - 51
DO  - 10.1109/ASONAM55673.2022.10068653
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143797458&doi=10.1109%2fASONAM55673.2022.10068653&partnerID=40&md5=019e0e3740a49613d321f8d1a5e98d4a
AB  - Fake News on social media platforms has attracted a lot of attention in recent times, primarily for events related to politics (2016 US Presidential elections), and healthcare (infodemic during COVID-19), to name a few. Various methods have been proposed for detecting Fake News. The approaches span from exploiting techniques related to network analysis, Natural Language Processing (NLP), and the usage of Graph Neural Networks (GNNs). In this work, we propose DEAP-FAKED, a knowleDgE grAPh FAKe nEws Detection framework for identifying Fake News. Our approach combines natural language processing (NLP) and tensor decomposition model to encode news content and embed Knowledge Graph (KG) entities, respectively. A variety of these encodings provides a complementary advantage to our detector. We evaluate our framework using two publicly available datasets containing articles from domains such as politics, business, technology, and healthcare. As part of dataset pre-processing, we also remove the bias, such as the source of the articles, which could impact the performance of the models. DEAP-FAKED obtains an F1-score of 88% and 78% for the two datasets, which is an improvement of 21 %, and 3%, respectively, which shows the effectiveness of the approach.  © 2022 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 31
ER  -

TY  - JOUR
AU  - Robertson, C.
AU  - Thomas, A.
AU  - Koyama, A.
AU  - Middlebrooks, L.
AU  - Kandaswamy, S.
AU  - Orenstein, E.
AU  - Gooding, H.
TI  - Missed Opportunities for Sexual History Documentation and Sexually Transmitted Infection Testing in the Pediatric Emergency Department
PY  - 2022
T2  - Journal of Adolescent Health
VL  - 70
IS  - 3
SP  - 429
EP  - 434
DO  - 10.1016/j.jadohealth.2021.10.002
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119905185&doi=10.1016%2fj.jadohealth.2021.10.002&partnerID=40&md5=44d5ba60ff06bd36d658a5fcda32a307
AB  - Purpose: Sexually transmitted infections (STIs) are disproportionally prevalent in adolescents, and adolescents often present to the pediatric emergency department (PED) for STI care. Prior studies have found low rates of sexual history documentation and STI testing in the PED. However, these studies have had limited sample sizes because of the burden of manual chart review. We aimed to estimate the rate of sexual history documentation and identify factors associated with STI testing in a large cohort of adolescents using natural language processing (NLP). Methods: We applied a validated NLP algorithm to all adolescent visits over a three-year period to the PED at a single large children's health care organization with a chief complaint potentially related to an STI. We utilized NLP to determine the prevalence of sexual history documentation in these patients. We applied logistic regression models to determine associations between sexual history documentation, patient demographic factors, and STI testing. Results: Of the 1,987 patient encounters included, only 56% had a sexual history documented, and only 40% of all patients were tested for STIs. Patients were more likely to have a sexual history documented and to be tested for STIs if they were of non-Hispanic black race/ethnicity, were >15 years of age, and had nonprivate insurance. Patients with a sexual history documented were seven times more likely to have STI testing ordered. Of patients tested (n = 728), 25% were positive for an STI. Conclusions: Despite presenting to the PED with symptoms potentially related to an STI, many adolescents are not receiving recommended sexual health care. Rates of sexual history documentation and STI testing varied by demographic factors including race, age, and insurance status. Utilizing NLP technology allowed us to examine a larger sample size than previously documented in the adolescent sexual history and PED literature. This study highlights critical opportunities to improve sexual health provision and equity of care provided in the PED. © 2021 Society for Adolescent Health and Medicine
PB  - Elsevier Inc.
C2  - 34836803
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Wellesley Wesley, E.
AU  - Patel, I.
AU  - Kadra-Scalzo, G.
AU  - Pritchard, M.
AU  - Shetty, H.
AU  - Broadbent, M.
AU  - Segev, A.
AU  - Patel, R.
AU  - Downs, J.
AU  - MacCabe, J.H.
AU  - Hayes, R.D.
AU  - de Freitas, D.F.
TI  - Gender disparities in clozapine prescription in a cohort of treatment-resistant schizophrenia in the South London and Maudsley case register
PY  - 2021
T2  - Schizophrenia Research
VL  - 232
SP  - 68
EP  - 76
DO  - 10.1016/j.schres.2021.05.006
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108742139&doi=10.1016%2fj.schres.2021.05.006&partnerID=40&md5=a7b07c9d250f7f253e457d02b9816662
AB  - Background: Gender disparities in treatment are apparent across many areas of healthcare. There has been little research into whether clozapine prescription, the first-line treatment for treatment-resistant schizophrenia (TRS), is affected by patient gender. Methods: This retrospective cohort study identified 2244 patients with TRS within the South London and Maudsley NHS Trust, by using a bespoke method validated against a gold-standard, manually coded, dataset of TRS cases. The outcome and exposures were identified from the free-text using natural language processing applications (including machine learning and rules-based approaches) and from information entered in structured fields. Multivariable logistic regression was carried out to calculate the odds ratios for clozapine prescription according to patients' gender, and adjusting for numerous potential confounders including sociodemographic, clinical (e.g., psychiatric comorbidities and substance use), neutropenia, functional factors (e.g., problems with occupation), and clinical monitoring. Results: Clozapine was prescribed to 77% of the women and 85% of the men with TRS. Women had reduced odds of being prescribed clozapine as compared to men after adjusting for all factors included in the present study (adjusted OR: 0.66; 95% CI 0.44–0.97; p = 0.037). Conclusion: Women with TRS are less likely to be prescribed clozapine than men with TRS, even when considering the effects of multiple clinical and functional factors. This finding suggests there could be gender bias in clozapine prescription, which carries ramifications for the relatively poorer care of women with TRS regarding many outcomes such as increased hospitalisation, mortality, and poorer quality of life. © 2021 The Authors
PB  - Elsevier B.V.
C2  - 34022618
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 19
ER  -

TY  - CONF
AU  - Jawale, S.
AU  - Sawarkar, S.D.
TI  - Interpretable Sentiment Analysis based on Deep Learning: An overview
PY  - 2020
T2  - 2020 IEEE Pune Section International Conference, PuneCon 2020
C7  - 9362361
SP  - 65
EP  - 70
DO  - 10.1109/PuneCon50868.2020.9362361
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102659814&doi=10.1109%2fPuneCon50868.2020.9362361&partnerID=40&md5=193f84bb2463de8576c4c9be54d04afe
AB  - Sentiment analysis (SA) or emotion AI or opinion mining uses natural language processing (NLP). Sentiment Analysis identify, study, quantify, obtain, tacit states and subject related information. Broad spectrum of areas influenced due to Sentiment Analysis such as policy making by the government, finding mental health of individuals, finding misuse of drugs in healthcare, fraud detection in the financial sector, covid-19 awareness and impact, Cyber-crime etc. As the amplitude of social media data increases day by day, there is a need to automatically address sentiment analysis. Deep learning handles it very well. It gives very good accuracy but incomprehensibility in decision strategy. For better decision-making trust, believe, fairness, reliability, and unbiasing is important. This paper explores the work done in this area along with popular techniques to address interpretability in sentiment analysis and its evaluation criteria. © 2020 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Žunić, A.
AU  - Corcoran, P.
AU  - Spasić, I.
TI  - Aspect-based sentiment analysis with graph convolution over syntactic dependencies
PY  - 2021
T2  - Artificial Intelligence in Medicine
VL  - 119
C7  - 102138
DO  - 10.1016/j.artmed.2021.102138
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112331602&doi=10.1016%2fj.artmed.2021.102138&partnerID=40&md5=99b165579da34549e990a96b1f55c3a3
AB  - Aspect-based sentiment analysis is a natural language processing task whose aim is to automatically classify the sentiment associated with a specific aspect of a written text. In this study, we propose a novel model for aspect-based sentiment analysis, which exploits the dependency parse tree of a sentence using graph convolution to classify the sentiment of a given aspect. To evaluate this model in the domain of health and well-being, where this task is biased toward negative sentiment, we used a corpus of drug reviews. Specific aspects were grounded in the Unified Medical Language System, a large repository of inter-related biomedical concepts and the corresponding terminology. Our experiments demonstrated that graph convolution approach outperforms standard deep learning architectures on the task of aspect-based sentiment analysis. Moreover, graph convolution over dependency parse trees (F-score of 0.8179) outperforms the same approach over a flat sequence representation of sentences (F-score of 0.7332). These results bring the performance of sentiment analysis in health and well-being in line with the state of the art in other domains. © 2021
PB  - Elsevier B.V.
C2  - 34531007
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 20
ER  -

TY  - CONF
AU  - Liu, E.
AU  - Tessler, M.H.
AU  - Dubosh, N.
AU  - Hiller, K.M.
AU  - Levy, R.P.
TI  - Assessing Group-level Gender Bias in Professional Evaluations: The Case of Medical Student End-of-Shift Feedback
PY  - 2022
T2  - GeBNLP 2022 - 4th Workshop on Gender Bias in Natural Language Processing, Proceedings of the Workshop
SP  - 86
EP  - 93
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137577136&partnerID=40&md5=ffe7f25fb3fce021c51cb42af0326dfc
AB  - Although approximately 50% of medical school graduates today are women, female physicians tend to be underrepresented in senior positions, make less money than their male counterparts and receive fewer promotions. There is a growing body of literature demonstrating gender bias in various forms of evaluation in medicine, but this work was mainly conducted by looking for specific words using fixed dictionaries such as LIWC and focused on recommendation letters. We use a dataset of written and quantitative assessments of medical student performance on individual shifts of work, collected across multiple institutions, to investigate the extent to which gender bias exists in a day-to-day context for medical students. We investigate differences in the narrative comments given to male and female students by both male or female faculty assessors, using a fine-tuned BERT model. This allows us to examine whether groups are written about in systematically different ways, without relying on hand-crafted wordlists or topic models. We compare these results to results from the traditional LIWC method and find that, although we find no evidence of group-level gender bias in this dataset, terms related to family and children are used more in feedback given to women. © 2022 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - JOUR
AU  - Himmelstein, G.
AU  - Bates, D.
AU  - Zhou, L.
TI  - Examination of Stigmatizing Language in the Electronic Health Record
PY  - 2022
T2  - JAMA Network Open
VL  - 5
IS  - 1
C7  - 44967
DO  - 10.1001/jamanetworkopen.2021.44967
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123815131&doi=10.1001%2fjamanetworkopen.2021.44967&partnerID=40&md5=e94b47935bc2a8100dea0ffc8c3b2bf9
AB  - Importance: Stigmatizing language in the electronic health record (EHR) may alter treatment plans, transmit biases between clinicians, and alienate patients. However, neither the frequency of stigmatizing language in hospital notes, nor whether clinicians disproportionately use it in describing patients in particular demographic subgroups are known. Objective: To examine the prevalence of stigmatizing language in hospital admission notes and the patient and clinician characteristics associated with the use of such language. Design, Setting, and Participants: This cross-sectional study of admission notes used natural language processing on 48651 admission notes written about 29783 unique patients by 1932 clinicians at a large, urban academic medical center between January to December 2018. The admission notes included 8738 notes about 4309 patients with diabetes written by 1204 clinicians; 6197 notes about 3058 patients with substance use disorder by 1132 clinicians; and 5176 notes about 2331 patients with chronic pain by 1056 clinicians. Statistical analyses were performed between May and September 2021. Exposures: Patients' demographic characteristics (age, race and ethnicity, gender, and preferred language); clinicians' characteristics (gender, postgraduate year [PGY], and credential [physician vs advanced practice clinician]). Main Outcome and Measures: Binary indicator for any vs no stigmatizing language; frequencies of specific stigmatizing words. Linear probability models were the main measure, and logistic regression and odds ratios were used for sensitivity analyses and further exploration. Results: The sample included notes on 29783 patients with a mean (SD) age of 46.9 (27.6) years. Of these patients, 1033 (3.5%) were non-Hispanic Asian, 2498 (8.4%) were non-Hispanic Black, 18956 (63.6%) were non-Hispanic White, 17334 (58.2%) were female, and 2939 (9.9%) preferred a language other than English. Of all admission notes, 1197 (2.5%) contained stigmatizing language. The diagnosis-specific stigmatizing language was present in 599 notes (6.9%) for patients with diabetes, 209 (3.4%) for patients with substance use disorders, and 37 (0.7%) for patients with chronic pain. In the whole sample, notes about non-Hispanic Black patients vs non-Hispanic White patients had a 0.67 (95% CI, 0.15 to 1.18) percentage points greater probability of containing stigmatizing language, with similar disparities in all 3 diagnosis-specific subgroups. Greater diabetes severity and the physician-author being less advanced in their training was associated with more stigmatizing language. A 1 point increase in the diabetes severity index was associated with a 1.23 (95% CI,23 to 2.23) percentage point greater probability of a note containing stigmatizing language. In the sample restricted to physicians, a higher PGY was associated with less use of stigmatizing language overall (-0.05 percentage points/PGY [95% CI, -0.09 to -0.01]). Conclusions and Relevance: In this cross-sectional study, stigmatizing language in hospital notes varied by medical condition and was more often used to describe non-Hispanic Black patients. Training clinicians to minimize stigmatizing language in the EHR might improve patient-clinician relationships and reduce the transmission of bias between clinicians.. © 2022 BMJ Publishing Group. All rights reserved.
PB  - American Medical Association
C2  - 35084481
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 99
ER  -

TY  - JOUR
AU  - Ormerod, M.
AU  - del Rincón, J.M.
AU  - Devereux, B.
TI  - Predicting semantic similarity between clinical sentence pairs using transformer models: Evaluation and representational analysis
PY  - 2021
T2  - JMIR Medical Informatics
VL  - 9
IS  - 5
C7  - e23099
DO  - 10.2196/23099
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106949779&doi=10.2196%2f23099&partnerID=40&md5=65365efa2acbb29429798bd919851ea0
AB  - Background: Semantic textual similarity (STS) is a natural language processing (NLP) task that involves assigning a similarity score to 2 snippets of text based on their meaning. This task is particularly difficult in the domain of clinical text, which often features specialized language and the frequent use of abbreviations. Objective: We created an NLP system to predict similarity scores for sentence pairs as part of the Clinical Semantic Textual Similarity track in the 2019 n2c2/OHNLP Shared Task on Challenges in Natural Language Processing for Clinical Data. We subsequently sought to analyze the intermediary token vectors extracted from our models while processing a pair of clinical sentences to identify where and how representations of semantic similarity are built in transformer models. Methods: Given a clinical sentence pair, we take the average predicted similarity score across several independently fine-tuned transformers. In our model analysis we investigated the relationship between the final model’s loss and surface features of the sentence pairs and assessed the decodability and representational similarity of the token vectors generated by each model. Results: Our model achieved a correlation of 0.87 with the ground-truth similarity score, reaching 6th place out of 33 teams (with a first-place score of 0.90). In detailed qualitative and quantitative analyses of the model’s loss, we identified the system’s failure to correctly model semantic similarity when both sentence pairs contain details of medical prescriptions, as well as its general tendency to overpredict semantic similarity given significant token overlap. The token vector analysis revealed divergent representational strategies for predicting textual similarity between bidirectional encoder representations from transformers (BERT)–style models and XLNet. We also found that a large amount information relevant to predicting STS can be captured using a combination of a classification token and the cosine distance between sentence-pair representations in the first layer of a transformer model that did not produce the best predictions on the test set. Conclusions: We designed and trained a system that uses state-of-the-art NLP models to achieve very competitive results on a new clinical STS data set. As our approach uses no hand-crafted rules, it serves as a strong deep learning baseline for this task. Our key contribution is a detailed analysis of the model’s outputs and an investigation of the heuristic biases learned by transformer models. We suggest future improvements based on these findings. In our representational analysis we explore how different transformer models converge or diverge in their representation of semantic signals as the tokens of the sentences are augmented by successive layers. This analysis sheds light on how these “black box” models integrate semantic similarity information in intermediate layers, and points to new research directions in model distillation and sentence embedding extraction for applications in clinical NLP. ©Mark Ormerod, Jesús Martínez del Rincón, Barry Devereux.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 21
ER  -

TY  - CONF
AU  - Abbasi, A.
AU  - Dobolyi, D.
AU  - Lalor, J.P.
AU  - Netemeyer, R.
AU  - Smith, K.
AU  - Yang, Y.
TI  - Constructing a Psychometric Testbed for Fair Natural Language Processing
PY  - 2021
T2  - EMNLP 2021 - 2021 Conference on Empirical Methods in Natural Language Processing, Proceedings
SP  - 3748
EP  - 3758
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123339534&partnerID=40&md5=dbcbf8a483f43d8d0a66414a0503d3fd
AB  - Psychometric measures of ability, attitudes, perceptions, and beliefs are crucial for understanding user behavior in various contexts including health, security, e-commerce, and finance. Traditionally, psychometric dimensions have been measured and collected using survey-based methods. Inferring such constructs from user-generated text could allow timely, unobtrusive collection and analysis. In this work we construct a corpus for psychometric natural language processing (NLP) related to important dimensions such as trust, anxiety, numeracy, and literacy, in the health domain. We discuss our multi-step process to align user text with their survey-based response items and provide an overview of the resulting testbed, which encompasses survey-based psychometric measures and accompanying user-generated text from 8,502 respondents. Our testbed also encompasses self-reported demographic information, including race, sex, age, income, and education, allowing for measuring bias and benchmarking fairness of text classification methods. We report preliminary results on use of the text to predict/categorize users' survey response labels and on the fairness of these models. We also discuss the important implications of our work and resulting testbed for future NLP research on psychometrics and fairness. © 2021 Association for Computational Linguistics
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 11
ER  -

TY  - JOUR
AU  - Morr, C.E.
AU  - Maret, P.
AU  - Muhlenbach, F.
AU  - Dharmalingam, D.
AU  - Tadesse, R.
AU  - Creighton, A.
AU  - Kundi, B.
AU  - Buettgen, A.
AU  - Mgwigwi, T.
AU  - Dinca-Panaitescu, S.
AU  - Dua, E.
AU  - Gorman, R.
TI  - A virtual community for disability advocacy: Development of a searchable artificial intelligence-supported platform
PY  - 2021
T2  - JMIR Formative Research
VL  - 5
IS  - 11
C7  - e33335
DO  - 10.2196/33335
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118943335&doi=10.2196%2f33335&partnerID=40&md5=c4430429148b55aa0af4d3eb88b4800e
AB  - Background: The lack of availability of disability data has been identified as a major challenge hindering continuous disability equity monitoring. It is important to develop a platform that enables searching for disability data to expose systemic discrimination and social exclusion, which increase vulnerability to inequitable social conditions. Objective: Our project aims to create an accessible and multilingual pilot disability website that structures and integrates data about people with disabilities and provides data for national and international disability advocacy communities. The platform will be endowed with a document upload function with hybrid (automated and manual) paragraph tagging, while the querying function will involve an intelligent natural language search in the supported languages. Methods: We have designed and implemented a virtual community platform using Wikibase, Semantic Web, machine learning, and web programming tools to enable disability communities to upload and search for disability documents. The platform data model is based on an ontology we have designed following the United Nations Convention on the Rights of Persons with Disabilities (CRPD). The virtual community facilitates the uploading and sharing of validated information, and supports disability rights advocacy by enabling dissemination of knowledge. Results: Using health informatics and artificial intelligence techniques (namely Semantic Web, machine learning, and natural language processing techniques), we were able to develop a pilot virtual community that supports disability rights advocacy by facilitating uploading, sharing, and accessing disability data. The system consists of a website on top of a Wikibase (a Semantic Web-based datastore). The virtual community accepts 4 types of users: Information producers, information consumers, validators, and administrators. The virtual community enables the uploading of documents, semiautomatic tagging of their paragraphs with meaningful keywords, and validation of the process before uploading the data to the disability Wikibase. Once uploaded, public users (information consumers) can perform a semantic search using an intelligent and multilingual search engine (QAnswer). Further enhancements of the platform are planned. Conclusions: The platform ontology is flexible and can accommodate advocacy reports and disability policy and legislation from specific jurisdictions, which can be accessed in relation to the CRPD articles. The platform ontology can be expanded to fit international contexts. The virtual community supports information upload and search. Semiautomatic tagging and intelligent multilingual semantic search using natural language are enabled using artificial intelligence techniques, namely Semantic Web, machine learning, and natural language processing. © 2021 JMIR Publications Inc..
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - CONF
AU  - Zhangy, X.
AU  - Liy, S.
AU  - Chengy, Z.
AU  - Callcut, R.
AU  - Petzold, L.
TI  - Domain Adaptation for Trauma Mortality Prediction in EHRs with Feature Disparity
PY  - 2021
T2  - Proceedings - 2021 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2021
SP  - 1145
EP  - 1152
DO  - 10.1109/BIBM52615.2021.9669798
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125204448&doi=10.1109%2fBIBM52615.2021.9669798&partnerID=40&md5=87a3a6fc5f933b6fc9321711868025ac
AB  - Trauma mortality prediction from electronic health records (EHRs) with machine learning models has received growing attention in medical fields, but EHRs in different hospitals and sub-medical domain populations are often scarce due to expensive collection processes or privacy issues. Domain Adaptation (DA) has emerged as a promising approach in computer vision and natural language processing to improve model performance in small data regimes by leveraging domain-invariant knowledge learned from a different yet related large source dataset. However, its applicability in trauma mortality prediction is challenging since EHRs collected from different hospital systems encounter feature disparity, i.e. distinct features between the source and target domain data. This paper demonstrates the effectiveness of three DA techniques in trauma mortality prediction, with a private encoding strategy that maps EHRs in both source and target domains with different raw features into the same latent space to alleviate feature disparity issues. Our experimental results on two real-world EHR datasets with various training data scenarios show that DA can improve mortality prediction consistently and significantly with private encoding. Finally, an ablation study manifests the importance of modeling feature disparity in DA, and 2-d t-SNE analysis explains its effectiveness.  © 2021 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Alhaj, F.
AU  - Al-Haj, A.
AU  - Sharieh, A.
AU  - Jabri, R.
TI  - Improving Arabic Cognitive Distortion Classification in Twitter using BERTopic
PY  - 2022
T2  - International Journal of Advanced Computer Science and Applications
VL  - 13
IS  - 1
SP  - 854
EP  - 860
DO  - 10.14569/IJACSA.2022.0130199
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124018308&doi=10.14569%2fIJACSA.2022.0130199&partnerID=40&md5=250f7463c75a9aa294a10862670c8843
AB  - Social media platforms allow users to share thoughts, experiences, and beliefs. These platforms represent a rich resource for natural language processing techniques to make inferences in the context of cognitive psychology. Some inaccurate and biased thinking patterns are defined as cognitive distortions. Detecting these distortions helps users restructure how to perceive thoughts in a healthier way. This paper proposed a machine learning-based approach to improve cognitive distortions’ classification of the Arabic content over Twitter. One of the challenges that face this task is the text shortness, which results in a sparsity of co-occurrence patterns and a lack of context information (semantic features). The proposed approach enriches text representation by defining the latent topics within tweets. Although classification is a supervised learning concept, the enrichment step uses unsupervised learning. The proposed algorithm utilizes a transformer-based topic modeling (BERTopic). It employs two types of document representations and performs averaging and concatenation to produce contextual topic embeddings. A comparative analysis of F1-score, precision, recall, and accuracy is presented. The experimental results demonstrate that our enriched representation outperformed the baseline models by different rates. These encouraging results suggest that using latent topic distribution, obtained from the BERTopic technique, can improve the classifier’s ability to distinguish between different CD categories. © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.
PB  - Science and Information Organization
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 36
ER  -

TY  - JOUR
AU  - Haynes, D.
AU  - Pampari, A.
AU  - Topham, C.
AU  - Schwarzenberger, K.
AU  - Heath, M.
AU  - Zou, J.
AU  - Greiling, T.M.
TI  - Patient Experience Surveys Reveal Gender-Biased Descriptions of Their Care Providers
PY  - 2021
T2  - Journal of Medical Systems
VL  - 45
IS  - 10
C7  - 90
DO  - 10.1007/s10916-021-01766-z
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114229778&doi=10.1007%2fs10916-021-01766-z&partnerID=40&md5=fa196c02160b30c3568e1c3ad4bd4851
AB  - Patient experience surveys (PES) are collected by healthcare systems as a surrogate marker of quality and published unedited online for the purpose of transparency, but these surveys may reflect gender biases directed toward healthcare providers. This retrospective study evaluated PES at a single university hospital between July 2016 and June 2018. Surveys were stratified by overall provider rating and self-identified provider gender. Adjectives from free-text survey comments were extracted using natural language processing techniques and applied to a statistical machine learning model to identify descriptors predictive of provider gender. 109,994 surveys were collected, 17,395 contained free-text comments describing 687 unique providers. The mean overall rating between male (8.84, n = 8558) and female (8.80, n = 8837) providers did not differ (p = 0.149). However, highly-rated male providers were more often described for their agentic qualities using adjectives such as “informative,” “forthright,” “superior,” and “utmost” (OR 1.48, p < 0.01)—whereas highly-rated female providers were more often described by their communal qualities through adjectives such as “empathetic,” “sweet,” “warm,” “attentive,” and “approachable” (OR 2.11, p < 0.0001). PES may contain gender stereotypes, raising questions about their impact on physicians and their validity as a quality metric which must be balanced with the need for unedited transparency. Future prospective studies are needed to further characterize this trend across geographically and racially diverse healthcare providers. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
PB  - Springer
C2  - 34468879
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Shang, C.
AU  - Cui, S.
AU  - Li, T.
AU  - Wang, X.
AU  - Li, Y.
AU  - Jiang, J.
TI  - MATNet: Exploiting Multi-Modal Features for Radiology Report Generation
PY  - 2022
T2  - IEEE Signal Processing Letters
VL  - 29
SP  - 2692
EP  - 2696
DO  - 10.1109/LSP.2022.3229844
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146215632&doi=10.1109%2fLSP.2022.3229844&partnerID=40&md5=a0f08f5bff981b74921650adc5904da6
AB  - Medical imaging is widely used in hospital clinical workflows. Assisting physicians in diagnosis by automatically generating reports from radiological images is an unmet clinical demand and requires urgent attention. However, this task suffers from two significant problems: 1) visual and textual data biases, and 2) the Transformer decoder makes no distinction between visual and non-visual words. We propose a novel multi-task approach combining natural language processing with machine learning techniques to meet this clinical need, i.e., creating fluent and accurate radiology reports. We name our system as Multi-modal Adaptive Transformer (MATNet), which consists of three key modules. First, Multi-Modal Encoder (MME) explores the relationship between radiology images and clinical notes. Second, Disease Classifier (DC) classifies the states of each disease topic and provides state-aware disease embeddings to alleviate visual data bias. Last, Adaptive Decoder (AD) dynamically measures the contribution of source signals and target signals when generating the next word. Based on our evaluations using benchmark IU-XRay and MIMIC-CXR datasets, the proposed MATNet outperformed previous state-of-the-art models on language fluency and clinical accuracy metrics such as BLEU scores.  © 1994-2012 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Lynch, K.E.
AU  - Shipherd, J.C.
AU  - Gatsby, E.
AU  - Viernes, B.
AU  - DuVall, S.L.
AU  - Blosnich, J.R.
TI  - Sexual orientation-related disparities in health conditions that elevate COVID-19 severity
PY  - 2022
T2  - Annals of Epidemiology
VL  - 66
SP  - 5
EP  - 12
DO  - 10.1016/j.annepidem.2021.11.006
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120866150&doi=10.1016%2fj.annepidem.2021.11.006&partnerID=40&md5=60f8bbf93b098c7434c0f6feddeb3621
AB  - Purpose: The Veterans Health Administration (VA) is the largest single integrated healthcare system in the US and is likely the largest healthcare provider for people with minoritized sexual orientations (e.g., gay, lesbian, bisexual). The purpose of this study was to use electronic health record (EHR) data to replicate self-reported survey findings from the general US population and assess whether sexual orientation is associated with diagnosed physical health conditions that may elevate risk of COVID-19 severity among veterans who utilize the VA. Methods: A retrospective analysis of VA EHR data from January 10, 1999–January 07, 2019 analyzed in 2021. Veterans with minoritized sexual orientations were included if they had documentation of a minoritized sexual orientation within clinical notes identified via natural language processing. Veterans without minoritized sexual orientation documentation comprised the comparison group. Adjusted prevalence and prevalence ratios (aPR) were calculated overall and by race/ethnicity while accounting for differences in distributions of sex assigned at birth, age, calendar year of first VA visit, volumes of healthcare utilization, and VA priority group. Results: Data from 108,401 veterans with minoritized sexual orientation and 6,511,698 controls were analyzed. After adjustment, veterans with minoritized sexual orientations had a statistically significant elevated prevalence of 10 of the 11 conditions. Amongst the highest disparities observed were COPD (aPR:1.24 [95% confidence interval:1.23–1.26]), asthma (1.22 [1.20–1.24]), and stroke (1.26 [1.24–1.28]). Conclusions: Findings largely corroborated patterns among the general US population. Further research is needed to determine if these disparities translate to poorer COVID-19 outcomes for individuals with minoritized sexual orientation. © 2021 Elsevier Inc.
PB  - Elsevier Inc.
C2  - 34785397
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 11
ER  -

TY  - JOUR
AU  - Chen, H.
AU  - Ji, M.
TI  - Experimental Comparison of Classification Methods under Class Imbalance
PY  - 2021
T2  - EAI Endorsed Transactions on Scalable Information Systems
VL  - 8
IS  - 33
C7  - e3
DO  - 10.4108/EAI.11-6-2021.170234
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119054057&doi=10.4108%2fEAI.11-6-2021.170234&partnerID=40&md5=c6a2fb98b97e1b18603cb759defd2661
AB  - The class imbalance problem is prevalent in many domains including medical, natural language processing, image recognition, economic and geographic areas etc. We perform a systematic experimental comparison of different imbalance classification algorithms — ranging from sampling, distance metric learning, cost-sensitive learning to ensemble learning approaches — on several datasets from UCI, KEEL and OpenML. The algorithms included DDAE, MWMOTE, SMOTE, RUSBoost, AdaBoost, cost-sensitive decision tree (csDCT), self-paced Ensemble Classifier, MetaCost, CAdaMEC and Iterative Metric Learning (IML). As the substantial bias potentially caused by imbalance classification can be harmful for underrepresented classes which are of critical social and economic values and even lives, the main objective of our study is thus to understand the impact of imbalance ratio and the size of the utilized datasets on the performance of the above-mentioned algorithms. Our experiments show that 1) Sampling methods perform the worst and cannot be used directly for imbalanced classification, since they lack of consideration of neighborhoods based on distance. However, some classifiers can be improved after the balance of class distribution. 2) Cost-sensitive learning models should be utilized when the dataset is less imbalanced, because it is difficult to set an appropriate cost matrix for a specific dataset, which can cause performance fluctuations. 3) IML consistently shows good performance (in terms of F1 and AUCPRC), is resilient to different imbalance ratios but sensitive to the data distribution of the dataset. 4) Ensemble learning techniques generally perform better over other approaches due to their combined intelligence of multiple basic classifiers. 5) In terms of system performance, self-paced Ensemble Classifier performs fairly well with regards to learning time, while IML and DDAE yield the longest learning time; AdaBoost and self-paced Ensemble Classifier are two algorithms require lowest memory usage. We also provide our empirical recommendation for algorithm selection under different requirements and usage scenarios based on our analysis. © 2021. Hui Chen and Mengru Ji, licensed to EAI. This is an open access article distributed under the terms of the Creative Commons Attribution license (http://creativecommons.org/licenses/by/3.0/), which permits unlimited use, distribution and reproduction in any medium so long as the original work is properly cited.
PB  - European Alliance for Innovation
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Badal, V.D.
AU  - Nebeker, C.
AU  - Shinkawa, K.
AU  - Yamada, Y.
AU  - Rentscher, K.E.
AU  - Kim, H.-C.
AU  - Lee, E.E.
TI  - Do Words Matter? Detecting Social Isolation and Loneliness in Older Adults Using Natural Language Processing
PY  - 2021
T2  - Frontiers in Psychiatry
VL  - 12
C7  - 728732
DO  - 10.3389/fpsyt.2021.728732
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120534457&doi=10.3389%2ffpsyt.2021.728732&partnerID=40&md5=89a67829527345e3058a169e1673823b
AB  - Introduction: Social isolation and loneliness (SI/L) are growing problems with serious health implications for older adults, especially in light of the COVID-19 pandemic. We examined transcripts from semi-structured interviews with 97 older adults (mean age 83 years) to identify linguistic features of SI/L. Methods: Natural Language Processing (NLP) methods were used to identify relevant interview segments (responses to specific questions), extract the type and number of social contacts and linguistic features such as sentiment, parts-of-speech, and syntactic complexity. We examined: (1) associations of NLP-derived assessments of social relationships and linguistic features with validated self-report assessments of social support and loneliness; and (2) important linguistic features for detecting individuals with higher level of SI/L by using machine learning (ML) models. Results: NLP-derived assessments of social relationships were associated with self-reported assessments of social support and loneliness, though these associations were stronger in women than in men. Usage of first-person plural pronouns was negatively associated with loneliness in women and positively associated with emotional support in men. ML analysis using leave-one-out methodology showed good performance (F1 = 0.73, AUC = 0.75, specificity = 0.76, and sensitivity = 0.69) of the binary classification models in detecting individuals with higher level of SI/L. Comparable performance were also observed when classifying social and emotional support measures. Using ML models, we identified several linguistic features (including use of first-person plural pronouns, sentiment, sentence complexity, and sentence similarity) that most strongly predicted scores on scales for loneliness and social support. Discussion: Linguistic data can provide unique insights into SI/L among older adults beyond scale-based assessments, though there are consistent gender differences. Future research studies that incorporate diverse linguistic features as well as other behavioral data-streams may be better able to capture the complexity of social functioning in older adults and identification of target subpopulations for future interventions. Given the novelty, use of NLP should include prospective consideration of bias, fairness, accountability, and related ethical and social implications. Copyright © 2021 Badal, Nebeker, Shinkawa, Yamada, Rentscher, Kim and Lee.
PB  - Frontiers Media S.A.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 21
ER  -

TY  - CONF
AU  - Dev, S.
AU  - Sameki, M.
AU  - Dhamala, J.
AU  - Hsieh, C.-J.
TI  - Measures and Best Practices for Responsible AI
PY  - 2021
T2  - Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
SP  - 4118
DO  - 10.1145/3447548.3469458
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114953604&doi=10.1145%2f3447548.3469458&partnerID=40&md5=ea9fe802d9de704f72e9e5165e4934d5
AB  - The use of machine learning (ML) based systems has become ubiquitous including their usage in critical applications like medicine and assistive technologies. Therefore, it is important to determine the trustworthiness of these ML models and tasks. A key component in this determination is the development of task specific datasets, metrics, and best practices which are able to measure the various aspects of responsible model development and deployment including robustness, interpretability and fairness. Further, datasets are also key when training for a given task, be it coreference resolution in language modeling or facial recognition in computer vision. Imbalances and inadequate representation in datasets can have repercussions of an undesirable nature. Some common examples include how coreference resolution systems in NLU are often not all gender inclusive, discrepancies in the measurement of how robust and trustworthy machine predictions are in domains where the selective labels problem is prevalent, and discriminatory determination of pain or care levels of people belonging to different demographics in health science applications. Development of task specific datasets which do better in this regard is also extremely vital. In this workshop, we invite contributions towards different (i) datasets which help enhance task performance and inclusivity, (ii) measures and metrics which help in determining the trustworthiness of a model/dataset, (iii) assessment or remediation tools for fairer, more transparent, robust, and reliable models, and (iv) case studies describing responsible development and deployment of AI systems across fields such as healthcare, financial services, insurance, etc. The datasets, measures, mitigation techniques, and best practices could focus on different areas including (but not restricted to) the following: Fairness and Bias Robustness Reliability and Safety Interpretability Explainability Ethical AI Causal Inference Counterfactual Example Analysis They could also be focussed on the applications in diverse fields such as industry, finance, healthcare and beyond. Text based datasets can be in languages other than English as well.  © 2021 Owner/Author.
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Soni, S.
AU  - Gudala, M.
AU  - Pajouhi, A.
AU  - Roberts, K.
TI  - RadQA: A Question Answering Dataset to Improve Comprehension of Radiology Reports
PY  - 2022
T2  - 2022 Language Resources and Evaluation Conference, LREC 2022
SP  - 6250
EP  - 6259
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144397508&partnerID=40&md5=e665fb222233cf6a9d182c1e11c4532b
AB  - We present a radiology question answering dataset, RadQA, with 3074 questions posed against radiology reports and annotated with their corresponding answer spans (resulting in a total of 6148 question-answer evidence pairs) by physicians. The questions are manually created using the clinical referral section of the reports that take into account the actual information needs of ordering physicians and eliminate bias from seeing the answer context (and, further, organically create unanswerable questions). The answer spans are marked within the Findings and Impressions sections of a report. The dataset aims to satisfy the complex clinical requirements by including complete (yet concise) answer phrases (which are not just entities) that can span multiple lines. We conduct a thorough analysis of the proposed dataset by examining the broad categories of disagreement in annotation (providing insights on the errors made by humans) and the reasoning requirements to answer a question (uncovering the huge dependence on medical knowledge for answering the questions). The advanced transformer language models achieve the best F1 score of 63.55 on the test set, however, the best human performance is 90.31 (with an average of 84.52). This demonstrates the challenging nature of RadQA that leaves ample scope for future method research. © European Language Resources Association (ELRA), licensed under CC-BY-NC-4.0.
PB  - European Language Resources Association (ELRA)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 9
ER  -

TY  - CONF
AU  - Blasi, D.
AU  - Anastasopoulos, A.
AU  - Neubig, G.
TI  - Systematic Inequalities in Language Technology Performance across the World's Languages
PY  - 2022
T2  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
VL  - 1
SP  - 5486
EP  - 5505
DO  - 10.18653/v1/2022.acl-long.376
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135833603&doi=10.18653%2fv1%2f2022.acl-long.376&partnerID=40&md5=771c095babb48ebb5cec7d6b039a0b9a
AB  - Natural language processing (NLP) systems have become a central technology in communication, education, medicine, artificial intelligence, and many other domains of research and development. While the performance of NLP methods has grown enormously over the last decade, this progress has been restricted to a minuscule subset of the world's ˜6,500 languages. We introduce a framework for estimating the global utility of language technologies as revealed in a comprehensive snapshot of recent publications in NLP. Our analyses involve the field at large, but also more in-depth studies on both user-facing technologies (machine translation, language understanding, question answering, text-to-speech synthesis) as well as foundational NLP tasks (dependency parsing, morphological inflection). In the process, we (1) quantify disparities in the current state of NLP research, (2) explore some of its associated societal and academic factors, and (3) produce tailored recommendations for evidence-based policy making aimed at promoting more global and equitable language technologies. © 2022 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 68
ER  -

TY  - CONF
AU  - Nakov, P.
AU  - Da San Martino, G.
TI  - Fake News, Disinformation, Propaganda, and Media Bias
PY  - 2021
T2  - International Conference on Information and Knowledge Management, Proceedings
SP  - 4862
EP  - 4865
DO  - 10.1145/3459637.3482026
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119176133&doi=10.1145%2f3459637.3482026&partnerID=40&md5=64b3fb84f6b676ecb392328691bd318e
AB  - The rise of Internet and social media changed not only how we consume information, but it also democratized the process of content creation and dissemination, thus making it easily available to anybody. Despite the hugely positive impact, this situation has the downside that the public was left unprotected against biased, deceptive, and disinformative content, which could now travel online at breaking-news speed and allegedly influence major events such as political elections, or disturb the efforts of governments and health officials to fight the ongoing COVID-19 pandemic. The research community responded to the issue, proposing a number of inter-connected research directions such as fact-checking, disinformation, misinformation, fake news, propaganda, and media bias detection. Below, we cover the mainstream research, and we also pay attention to less popular, but emerging research directions, such as propaganda detection, check-worthiness estimation, detecting previously fact-checked claims, and multimodality, which are of interest to human fact-checkers and journalists. We further cover relevant topics such as stance detection, source reliability estimation, detection of persuasion techniques in text and memes, and detecting malicious users in social media. Moreover, we discuss large-scale pre-trained language models, and the challenges and opportunities they offer for generating and for defending against neural fake news. Finally, we explore some recent efforts aiming at flattening the curve of the COVID-19 infodemic. © 2021 ACM.
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - CONF
AU  - Sherman, E.
AU  - Harrigian, K.
AU  - Aguirre, C.
AU  - Dredze, M.
TI  - Towards Understanding the Role of Gender in Deploying Social Media-Based Mental Health Surveillance Models
PY  - 2021
T2  - Computational Linguistics and Clinical Psychology: Improving Access, CLPsych 2021 - Proceedings of the 7th Workshop, in conjunction with NAACL 2021
SP  - 217
EP  - 223
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123177403&partnerID=40&md5=b1deea09d3ed67471d980b0af9ca72a6
AB  - Spurred by advances in machine learning and natural language processing, developing social media-based mental health surveillance models has received substantial recent attention. For these models to be maximally useful, it is necessary to understand how they perform on various subgroups, especially those defined in terms of protected characteristics. In this paper we study the relationship between user demographics - focusing on gender - and depression. Considering a population of Reddit users with known genders and depression statuses, we analyze the degree to which depression predictions are subject to biases along gender lines using domaininformed classifiers. We then study our models' parameters to gain qualitative insight into the differences in posting behavior across genders.  ©2021 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Funken, D.
AU  - Götz, F.
AU  - Bültmann, E.
AU  - Hennies, I.
AU  - Gburek-Augustat, J.
AU  - Hempel, J.
AU  - Dressler, F.
AU  - Baumann, U.
AU  - Klemann, C.
TI  - Focal Seizures and Posterior Reversible Encephalopathy Syndrome as Presenting Signs of IgA Vasculitis/Henoch-Schoenlein Purpura—An Educative Case and Systematic Review of the Literature
PY  - 2021
T2  - Frontiers in Neurology
VL  - 12
C7  - 759386
DO  - 10.3389/fneur.2021.759386
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120554711&doi=10.3389%2ffneur.2021.759386&partnerID=40&md5=ef50495743ea2d7cf6f7581f8a19cbef
AB  - Background: IgA vasculitis/Henoch-Schoenlein purpura (IgAV/HSP) is a systemic small vessel vasculitis of unknown pathogenesis predominantly affecting children. While skin, GI tract, joints, and kidneys are frequently affected and considered, central nervous system (CNS) involvement of this disease is underestimated. Methods: We provide a case report and systematically review the literature on IgAV, collecting data on the spectrum of neurological manifestations. Results: We report on a 7-year-old girl with IgAV who presented with diplopia and afebrile focal seizures, which preceded the onset of purpura. Cranial magnetic resonance imaging was consistent with posterior reversible encephalopathy syndrome (PRES), showing typical focal bilateral parietal swelling and cortical and subcortical high signal intensities on T2-fluid attenuated inversion recovery (FLAIR) images predominantly without diffusion restriction. Cerebrospinal fluid analysis and blood tests excluded systemic inflammation or vasculitis. Interestingly, hypertension was not a hallmark of the developing disease in the initial phase of PRES manifestation. Renal disease and other secondary causes for PRES were also excluded. Supportive- and steroid treatment resulted in restitution ad integrum. Reviewing the literature, we identified 28 other cases of IgAV with CNS involvement. Severe CNS involvement includes seizures, cerebral edema, or hemorrhage, as well as PRES. Thirteen patients fulfilled all diagnostic criteria of PRES. The mean age was 11.2 years (median 8.0, range 5-42 years), with no reported bias toward gender or ethnic background. Treatment regimens varied from watchful waiting to oral and intravenously steroids up to plasmapheresis. Three cases showed permanent CNS impairment. Conclusion: Collectively, our data demonstrate that (I) severe CNS involvement such as PRES is an underappreciated feature of IgAV, (II) CNS symptoms may precede other features of IgAV, (III) PRES can occur in IgAV, and differentiation from CNS vasculitis is challenging, (IV) pathogenesis of PRES in the context of IgAV remains elusive, which hampers treatment decisions. We, therefore, conclude that clinical awareness and the collection of structured data are necessary to elucidate the pathophysiological connection of IgAV and PRES. Copyright © 2021 Funken, Götz, Bültmann, Hennies, Gburek-Augustat, Hempel, Dressler, Baumann and Klemann.
PB  - Frontiers Media S.A.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Basu, T.
AU  - Goldsworthy, S.
AU  - Gkoutos, G.V.
TI  - A sentence classification framework to identify geometric errors in radiation therapy from relevant literature
PY  - 2021
T2  - Information (Switzerland)
VL  - 12
IS  - 4
C7  - 139
DO  - 10.3390/info12040139
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103888025&doi=10.3390%2finfo12040139&partnerID=40&md5=2fc063d81cac603f08fb68390b0a3ae4
AB  - The objective of systematic reviews is to address a research question by summarizing relevant studies following a detailed, comprehensive, and transparent plan and search protocol to reduce bias. Systematic reviews are very useful in the biomedical and healthcare domain; however, the data extraction phase of the systematic review process necessitates substantive expertise and is labour-intensive and time-consuming. The aim of this work is to partially automate the process of building systematic radiotherapy treatment literature reviews by summarizing the required data elements of geometric errors of radiotherapy from relevant literature using machine learning and natural language processing (NLP) approaches. A framework is developed in this study that initially builds a training corpus by extracting sentences containing different types of geometric errors of radiotherapy from relevant publications. The publications are retrieved from PubMed following a given set of rules defined by a domain expert. Subsequently, the method develops a training corpus by extracting relevant sentences using a sentence similarity measure. A support vector machine (SVM) classifier is then trained on this training corpus to extract the sentences from new publications which contain relevant geometric errors. To demonstrate the proposed approach, we have used 60 publications containing geometric errors in radiotherapy to automatically extract the sentences stating the mean and standard deviation of different types of errors between planned and executed radiotherapy. The experimental results show that the recall and precision of the proposed framework are, respectively, 97% and 72%. The results clearly show that the framework is able to extract almost all sentences containing required data of geometric errors. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
PB  - MDPI AG
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Andrews, J.
AU  - Chartash, D.
AU  - Hay, S.
TI  - Gender bias in resident evaluations: Natural language processing and competency evaluation
PY  - 2021
T2  - Medical Education
VL  - 55
IS  - 12
SP  - 1383
EP  - 1387
DO  - 10.1111/medu.14593
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111524615&doi=10.1111%2fmedu.14593&partnerID=40&md5=df76146c9b5da89f62a9be9ac5c6ddf4
AB  - Background: Research shows that female trainees experience evaluation penalties for gender non-conforming behaviour during medical training. Studies of medical education evaluations and performance scores do reflect a gender bias, though studies are of varying methodology and results have not been consistent. Objective: We sought to examine the differences in word use, competency themes and length within written evaluations of internal medicine residents at scale, considering the impact of both faculty and resident gender. We hypothesised that female internal medicine residents receive more negative feedback, and different thematic feedback than male residents. Methods: This study utilised a corpus of 3864 individual responses to positive and negative questions over the course of six years (2012-2018) within Yale University School of Medicine's internal medicine residency. Researchers developed a sentiment model to assess the valence of evaluation responses. We then used natural language processing (NLP) to evaluate whether female versus male residents received more positive or negative feedback and if that feedback focussed on different Accreditation Council for Graduate Medical Education (ACGME) core competencies based on their gender. Evaluator-evaluatee gender dyad was analysed to see how it impacted quantity and quality of feedback. Results: We found that female and male residents did not have substantively different numbers of positive or negative comments. While certain competencies were discussed more than others, gender did not seem to influence which competencies were discussed. Neither gender trainee received more written feedback, though female evaluators tended to write longer evaluations. Conclusions: We conclude that when examined at scale, quantitative gender differences are not as prevalent as has been seen in qualitative work. We suggest that further investigation of linguistic phenomena (such as context) is warranted to reconcile this finding with prior work. © 2021 John Wiley & Sons Ltd and The Association for the Study of Medical Education
PB  - John Wiley and Sons Inc
C2  - 34224606
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 12
ER  -

TY  - CONF
AU  - Zhang, B.
AU  - Buendia, R.
AU  - Iannoti, N.
AU  - Ramsden, E.
AU  - O'Regan, P.
AU  - Swift, J.
AU  - Lockwood, S.
AU  - Jackson, D.J.
AU  - Dennis, G.
AU  - Hagger, L.
AU  - Havsol, J.
TI  - Home-based Digital Assessments with Applied Sentiment Emotion AI Capture Improved Quality-of-life in Asthma Patients
PY  - 2021
T2  - Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS
SP  - 4994
EP  - 4997
DO  - 10.1109/EMBC46164.2021.9629985
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122541340&doi=10.1109%2fEMBC46164.2021.9629985&partnerID=40&md5=3035c1b164bcf6a903517d5e583c80ab
AB  - With the rise of digital transformation in the pharmaceutical industry, digital therapeutics are being integrated in drug development clinical trials. In the TWINKLE study, information about asthmatic patients' disease control and quality-of-life (QoL) was measured by daily video recording, in conjunction with daily electronic questionnaires and home-based spirometry. From the video messages, sentiment and emotion AI was applied to detect subtle QoL changes in asthmatic patients after receiving treatments. Sentiment scores, derived from patients' daily messages via natural language processing, correlated strongly with metrics of lung functions and outcomes of electronic questionnaires. However, video-derived emotional analysis exhibited strong interpersonal variations and systematic biases, yet still showed utility in detecting QoL changes after personalized calibration and signal aggregation. Compared to traditional patient-reported outcomes, all three categories of digital measurements were able to detect significantly improved asthma control from patients who responded to treatments. The result provides insights into developing novel digital outcomes through the application of connected digital devices and advanced AI tailored to clinical settings.Clinical relevance - Digital outcomes involving connected digital devices and AI for sentiment/emotion analysis could capture subtle QoL changes reliably and earlier than hospital visits, reducing burden and improving disease management. Integrating digital therapeutics in asthma drug development trials may prove to be feasible and valuable. © 2021 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
C2  - 34892329
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Peden, M.
AU  - Puvanachandra, P.
AU  - Keller, M.-E.
AU  - Rodrigues, E.-M.
AU  - Quistberg, D.A.
AU  - Jagnoor, J.
TI  - How the Covid-19 pandemic has drawn attention to the issue of active mobility and co-benefits in Latin American cities
ST  - Cómo la pandemia de Covid-19 ha llamado la atención sobre la cuestión de la movilidad activa y los beneficios colaterales en las ciudades latinoamericanas
PY  - 2022
T2  - Salud Publica de Mexico
VL  - 64
SP  - 14
EP  - 21
DO  - 10.21149/12786
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132549922&doi=10.21149%2f12786&partnerID=40&md5=9de0338467edbf500787ef7d4f68f81f
AB  - The Covid-19 pandemic has brought to the fore many issues that will impact public health for years to come –one such impact is on the nexus between transportation and health. Promoting safe, active transport is an activity that has many physical and mental health benefits. During lockdowns, many cities in Latin America imposed infrastructural and legislative changes in order to abide with public health and social measures to reduce virus spread. These ranged from additional bike lanes to reduced speed limits or incentives to purchase bicycles. These cities showed reduced motorized transport, improved air quality and increased active transport, all of which have multiple health and equity benefits. As countries “build back better”, promoting active transport offers the most value for investment and improves health and well-being while continuing to offer social distancing. Quantified case studies are needed to have a more comprehensive understanding of the impact of active transport in various contexts © 2022. Salud Publica de Mexico.All Rights Reserved.
PB  - Instituto Nacional de Salud Publica
C2  - 36130399
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 6
ER  -

TY  - CONF
TI  - Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias
PY  - 2022
T2  - Lecture Notes in Electrical Engineering
VL  - 940
SP  - 13
EP  - 45
DO  - 10.1007/978-981-19-4453-6_2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142712246&doi=10.1007%2f978-981-19-4453-6_2&partnerID=40&md5=43e5315c058abad134c8832d54641f63
AB  - The remarkable progress in Natural Language Processing (NLP) brought about by deep learning, particularly with the recent advent of large pre-trained neural language models, is brought into scrutiny as several studies began to discuss and report potential biases in NLP applications. Bias in NLP is found to originate from latent historical biases encoded by humans into textual data which gets perpetuated or even amplified by NLP algorithm. We present a survey to comprehend bias in large pre-trained language models and analyze the stages at which they occur in these models, and various ways in which these biases could be quantified and mitigated. Considering wide applicability of textual affective computing-based downstream tasks in real-world systems such as business, health care, and education, we give a special emphasis on investigating bias in the context of affect (emotion) i.e., Affective Bias, in large pre-trained language models. We present a summary of various bias evaluation corpora that help to aid future research and discuss challenges in the research on bias in pre-trained language models. We believe that our attempt to draw a comprehensive view of bias in pre-trained language models, and especially the exploration of affective bias will be highly beneficial to researchers interested in this evolving field. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
PB  - Springer Science and Business Media Deutschland GmbH
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Straw, I.
AU  - Callison-Burch, C.
TI  - Artificial Intelligence in mental health and the biases of language based models
PY  - 2020
T2  - PLoS ONE
VL  - 15
IS  - 12 December
C7  - e0240376
DO  - 10.1371/journal.pone.0240376
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098289179&doi=10.1371%2fjournal.pone.0240376&partnerID=40&md5=59e8ba42c85ba4d59ffb5ee9a619ffa6
AB  - Background The rapid integration of Artificial Intelligence (AI) into the healthcare field has occurred with little communication between computer scientists and doctors. The impact of AI on health outcomes and inequalities calls for health professionals and data scientists to make a collaborative effort to ensure historic health disparities are not encoded into the future. We present a study that evaluates bias in existing Natural Language Processing (NLP) models used in psychiatry and discuss how these biases may widen health inequalities. Our approach systematically evaluates each stage of model development to explore how biases arise from a clinical, data science and linguistic perspective. Design/Methods A literature review of the uses of NLP in mental health was carried out across multiple disciplinary databases with defined Mesh terms and keywords. Our primary analysis evaluated biases within 'GloVe' and 'Word2Vec' word embeddings. Euclidean distances were measured to assess relationships between psychiatric terms and demographic labels, and vector similarity functions were used to solve analogy questions relating to mental health. Results Our primary analysis of mental health terminology in GloVe and Word2Vec embeddings demonstrated significant biases with respect to religion, race, gender, nationality, sexuality and age. Our literature review returned 52 papers, of which none addressed all the areas of possible bias that we identify in model development. In addition, only one article existed on more than one research database, demonstrating the isolation of research within disciplinary silos and inhibiting cross-disciplinary collaboration or communication. Conclusion Our findings are relevant to professionals who wish to minimize the health inequalities that may arise as a result of AI and data-driven algorithms. We offer primary research identifying biases within these technologies and provide recommendations for avoiding these harms in the future. © 2020 Straw, Callison-Burch. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
PB  - Public Library of Science
C2  - 33332380
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 69
ER  -

TY  - JOUR
AU  - Zhang, C.
AU  - Xu, S.
AU  - Li, Z.
AU  - Hu, S.
TI  - Understanding concerns, sentiments, and disparities among population groups during the COVID-19 pandemic via twitter data mining: Large-scale cross-sectional study
PY  - 2021
T2  - Journal of Medical Internet Research
VL  - 23
IS  - 3
C7  - e26482
DO  - 10.2196/26482
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102607401&doi=10.2196%2f26482&partnerID=40&md5=2aeb27eb14a1b67994ed0cbaa189c6b8
AB  - Background: Since the beginning of the COVID-19 pandemic in late 2019, its far-reaching impacts have been witnessed globally across all aspects of human life, such as health, economy, politics, and education. Such widely penetrating impacts cast significant and profound burdens on all population groups, incurring varied concerns and sentiments among them. Objective: This study aims to identify the concerns, sentiments, and disparities of various population groups during the COVID-19 pandemic through a cross-sectional study conducted via large-scale Twitter data mining infoveillance. Methods: This study consisted of three steps: first, tweets posted during the pandemic were collected and preprocessed on a large scale; second, the key population attributes, concerns, sentiments, and emotions were extracted via a collection of natural language processing procedures; third, multiple analyses were conducted to reveal concerns, sentiments, and disparities among population groups during the pandemic. Overall, this study implemented a quick, effective, and economical approach for analyzing population-level disparities during a public health event. The source code developed in this study was released for free public use at GitHub. Results: A total of 1,015,655 original English tweets posted from August 7 to 12, 2020, were acquired and analyzed to obtain the following results. Organizations were significantly more concerned about COVID-19 (odds ratio [OR] 3.48, 95% CI 3.39-3.58) and expressed more fear and depression emotions than individuals. Females were less concerned about COVID-19 (OR 0.73, 95% CI 0.71-0.75) and expressed less fear and depression emotions than males. Among all age groups (ie, ≤18, 19-29, 30-39, and ≥40 years of age), the attention ORs of COVID-19 fear and depression increased significantly with age. It is worth noting that not all females paid less attention to COVID-19 than males. In the age group of 40 years or older, females were more concerned than males, especially regarding the economic and education topics. In addition, males 40 years or older and 18 years or younger were the least positive. Lastly, in all sentiment analyses, the sentiment polarities regarding political topics were always the lowest among the five topics of concern across all population groups. Conclusions: Through large-scale Twitter data mining, this study revealed that meaningful differences regarding concerns and sentiments about COVID-19-related topics existed among population groups during the study period. Therefore, specialized and varied attention and support are needed for different population groups. In addition, the efficient analysis method implemented by our publicly released code can be utilized to dynamically track the evolution of each population group during the pandemic or any other major event for better informed public health research and interventions. © 2021 Journal of Medical Internet Research. All rights reserved.
PB  - JMIR Publications Inc.
C2  - 33617460
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 18
ER  -

TY  - CONF
AU  - Chandler, C.
AU  - Foltz, P.W.
AU  - Cohen, A.S.
AU  - Holmlund, T.B.
AU  - Elvevåg, B.
TI  - Safeguarding against spurious AI-based predictions: The case of automated verbal memory assessment
PY  - 2021
T2  - Computational Linguistics and Clinical Psychology: Improving Access, CLPsych 2021 - Proceedings of the 7th Workshop, in conjunction with NAACL 2021
SP  - 181
EP  - 191
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123187604&partnerID=40&md5=f474e174dd97d170ed17401db147d973
AB  - A growing amount of psychiatric research incorporates machine learning and natural language processing methods, however findings have yet to be translated into actual clinical decision support systems. Many of these studies are based on relatively small datasets in homogeneous populations, which has the associated risk that the models may not perform adequately on new data in real clinical practice. The nature of serious mental illness is that it is hard to define, hard to capture, and requires frequent monitoring, which leads to imperfect data where attribute and class noise are common. With the goal of an effective AI-mediated clinical decision support system, there must be computational safeguards placed on the models used in order to avoid spurious predictions and thus allow humans to review data in the settings where models are unstable or bound not to generalize. This paper describes two approaches to implementing safeguards: (1) the determination of cases in which models are unstable by means of attribute and class based outlier detection and (2) finding the extent to which models show inductive bias. These safeguards are illustrated in the automated scoring of a story recall task via natural language processing methods. With the integration of human-in-the-loop machine learning in the clinical implementation process, incorporating safeguards such as these into the models will offer patients increased protection from spurious predictions.  ©2021 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - CONF
AU  - Guo, X.
AU  - Sun, Y.
AU  - Vosoughi, S.
TI  - Emotion-based Modeling of Mental Disorders on Social Media
PY  - 2021
T2  - ACM International Conference Proceeding Series
SP  - 8
EP  - 16
DO  - 10.1145/3486622.3493916
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128693009&doi=10.1145%2f3486622.3493916&partnerID=40&md5=ccc09b4019fb8e2c5df6be80b4ac7c8f
AB  - According to the World Health Organization (WHO), one in four people will be affected by mental disorders at some point in their lives. However, in many parts of the world, patients do not actively seek professional diagnosis because of stigma attached to mental illness, ignorance of mental health and its associated symptoms. In this paper, we propose a model for passively detecting mental disorders using conversations on Reddit. Specifically, we focus on a subset of mental disorders that are characterized by distinct emotional patterns (henceforth called emotional disorders): major depressive, anxiety, and bipolar disorders. Through passive (i.e., unprompted) detection, we can encourage patients to seek diagnosis and treatment for mental disorders. Our proposed model is different from other work in this area in that our model is based entirely on the emotional states, and the transition between these states of users on Reddit, whereas prior work is typically based on content-based representations (e.g., n-grams, language model embeddings, etc). We show that content-based representation is affected by domain and topic bias and thus does not generalize, while our model, on the other hand, suppresses topic-specific information and thus generalizes well across different topics and times. We conduct experiments on our model's ability to detect different emotional disorders and on the generalizability of our model. Our experiments show that while our model performs comparably to content-based models, such as BERT, it generalizes much better across time and topic.  © 2021 ACM.
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 13
ER  -

TY  - JOUR
AU  - Weerakody, P.B.
AU  - Wong, K.W.
AU  - Wang, G.
AU  - Ela, W.
TI  - A review of irregular time series data handling with gated recurrent neural networks
PY  - 2021
T2  - Neurocomputing
VL  - 441
SP  - 161
EP  - 178
DO  - 10.1016/j.neucom.2021.02.046
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102354427&doi=10.1016%2fj.neucom.2021.02.046&partnerID=40&md5=734f81f990e7e5e5308ec7133f90a8a3
AB  - Irregular time series data is becoming increasingly prevalent with the growth of multi-sensor systems as well as the continued use of unstructured manual data recording mechanisms. Irregular data and the resulting missing values severely limit the data's ability to be analysed and modelled for classification and forecasting tasks. Often, conventional methods used for handling time series data introduce bias and make strong assumptions on the underlying data generation process, which can lead to poor model predictions. Traditional machine learning and deep learning methods, although at the forefront of data modelling, are at best compromised by irregular time series data sets and fail to model the temporal irregularity of incomplete time series. Gated recurrent neural networks (RNN), such as LSTM and GRU, have had outstanding success in sequential modelling, and have been applied in many application fields, including natural language processing. These models have become an obvious choice for time series modelling and a promising tool for handling irregular time series data. RNNs have a unique ability to be adapted to make effective use of missing value patterns, time intervals and complex temporal dependencies in irregular univariate and multivariate time series data. In this paper, we provide a systematic review of recent studies in which gated recurrent neural networks have been successfully applied to irregular time series data for prediction tasks within several fields, including medical, human activity recognition, traffic monitoring and environmental monitoring. The review highlights the two common approaches for handling irregular time series data: missing value imputation at the data pre-processing stage and modification of algorithms to directly handle missing values in the learning process. Reviewed models are confined to those that can address issues with irregular time series data and does not cover the broader range of models that deal more generally with sequences and regular time series. This paper aims to present the most effective techniques emerging within this branch of research as well as to identify remaining challenges, so that researchers may build upon this platform of work towards further novel techniques for handling irregular time series data. © 2021 Elsevier B.V.
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 255
ER  -

TY  - CONF
AU  - Nakov, P.
AU  - Da San Martino, G.
TI  - Fake News, Disinformation, Propaganda, Media Bias, and Flattening the Curve of the COVID-19 Infodemic
PY  - 2021
T2  - Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
SP  - 4054
EP  - 4055
DO  - 10.1145/3447548.3470790
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114919853&doi=10.1145%2f3447548.3470790&partnerID=40&md5=3e7624f8dad81d7011d6360ba6840ac1
AB  - The rise of social media has democratized content creation and has made it easy for anybody to share and to spread information online. On the positive side, this has given rise to citizen journalism, thus enabling much faster dissemination of information compared to what was possible with newspapers, radio, and TV. On the negative side, stripping traditional media from their gate-keeping role has left the public unprotected against the spread of disinformation, which could now travel at breaking-news speed over the same democratic channel. This situation gave rise to the proliferation of false information, specifically created to affect individual people's beliefs, and ultimately to influence major events such as political elections; it also set the dawn of the Post-Truth Era, where appeal to emotions has become more important than the truth. More recently, with the emergence of the COVID-19 pandemic, a new blending of medical and political misinformation and disinformation has given rise to the first global infodemic. Limiting the impact of these negative developments has become a major focus for journalists, social media companies, and regulatory authorities. We offer an overview of the emerging and inter-connected research areas of fact-checking, misinformation, disinformation, "fake news'', propaganda, and media bias detection, with focus on text and computational approaches. We explore the general fact-checking pipeline and important elements thereof such as check-worthiness estimation, spotting previously fact-checked claims, stance detection, source reliability estimation, detection of persuasion/propaganda techniques in text and memes, and detecting malicious users in social media. We further cover large-scale pre-trained language models, and the challenges and opportunities they offer for generating and for defending against neural fake news. Finally, we explore some recent efforts towards flattening the curve of the COVID-19 infodemic.  © 2021 Owner/Author.
PB  - Association for Computing Machinery
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - JOUR
AU  - Castro, V.M.
AU  - Dligach, D.
AU  - Finan, S.
AU  - Yu, S.
AU  - Can, A.
AU  - Abd-El-Barr, M.
AU  - Gainer, V.
AU  - Shadick, N.A.
AU  - Murphy, S.
AU  - Cai, T.
AU  - Savova, G.
AU  - Weiss, S.T.
AU  - Du, R.
TI  - Large-scale identification of patients with cerebral aneurysms using natural language processing
PY  - 2017
T2  - Neurology
VL  - 88
IS  - 2
SP  - 164
EP  - 168
DO  - 10.1212/WNL.0000000000003490
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009080828&doi=10.1212%2fWNL.0000000000003490&partnerID=40&md5=0ba5db821b9acb2f058108eecaf9da66
AB  - Objective: To use natural language processing (NLP) in conjunction with the electronic medical record (EMR) to accurately identify patients with cerebral aneurysms and their matched controls. Methods: ICD-9 and Current Procedural Terminology codes were used to obtain an initial data mart of potential aneurysm patients from the EMR. NLP was then used to train a classification algorithm with.632 bootstrap cross-validation used for correction of overfitting bias. The classification rule was then applied to the full data mart. Additional validation was performed on 300 patients classified as having aneurysms. Controls were obtained by matching age, sex, race, and healthcare use. Results: We identified 55,675 patients of 4.2 million patients with ICD-9 and Current Procedural Terminology codes consistent with cerebral aneurysms. Of those, 16,823 patients had the term aneurysm occur near relevant anatomic terms. After training, a final algorithm consisting of 8 coded and 14 NLP variables was selected, yielding an overall area under the receiver-operating characteristic curve of 0.95. After the final algorithm was applied, 5,589 patients were classified as having aneurysms, and 54,952 controls were matched to those patients. The positive predictive value based on a validation cohort of 300 patients was 0.86. Conclusions: We harnessed the power of the EMR by applying NLP to obtain a large cohort of patients with intracranial aneurysms and their matched controls. Such algorithms can be generalized to other diseases for epidemiologic and genetic studies. © 2016 American Academy of Neurology.
PB  - Lippincott Williams and Wilkins
C2  - 27927935
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 85
ER  -

TY  - JOUR
AU  - Um, H.
AU  - Tixier, F.
AU  - Bermudez, D.
AU  - Deasy, J.O.
AU  - Young, R.J.
AU  - Veeraraghavan, H.
TI  - Impact of image preprocessing on the scanner dependence of multi-parametric MRI radiomic features and covariate shift in multi-institutional glioblastoma datasets
PY  - 2019
T2  - Physics in Medicine and Biology
VL  - 64
IS  - 16
C7  - 165011
DO  - 10.1088/1361-6560/ab2f44
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071701840&doi=10.1088%2f1361-6560%2fab2f44&partnerID=40&md5=12d4fcdf6f1095474cc96f6b57805109
AB  - Recent advances in radiomics have enhanced the value of medical imaging in various aspects of clinical practice, but a crucial component that remains to be investigated further is the robustness of quantitative features to imaging variations and across multiple institutions. In the case of MRI, signal intensity values vary according to the acquisition parameters used, yet no consensus exists on which preprocessing techniques are favorable in reducing scanner-dependent variability of image-based features. Hence, the purpose of this study was to assess the impact of common image preprocessing methods on the scanner dependence of MRI radiomic features in multi-institutional glioblastoma multiforme (GBM) datasets. Two independent GBM cohorts were analyzed: 50 cases from the TCGA-GBM dataset and 111 cases acquired in our institution, and each case consisted of 3 MRI sequences viz. FLAIR, T1-weighted, and T1-weighted post-contrast. Five image preprocessing techniques were examined: 8-bit global rescaling, 8-bit local rescaling, bias field correction, histogram standardization, and isotropic resampling. A total of 420 features divided into eight categories representing texture, shape, edge, and intensity histogram were extracted. Two distinct imaging parameters were considered: scanner manufacturer and scanner magnetic field strength. Wilcoxon tests identified features robust to the considered acquisition parameters under the selected image preprocessing techniques. A machine learning-based strategy was implemented to measure the covariate shift between the analyzed datasets using features computed using the aforementioned preprocessing methods. Finally, radiomic scores (rad-scores) were constructed by identifying features relevant to patients' overall survival after eliminating those impacted by scanner variability. These were then evaluated for their prognostic significance through Kaplan-Meier and Cox hazards regression analyses. Our results demonstrate that overall, histogram standardization contributes the most in reducing radiomic feature variability as it is the technique to reduce the covariate shift for three feature categories and successfully discriminate patients into groups of different survival risks. © 2019 Institute of Physics and Engineering in Medicine.
PB  - Institute of Physics Publishing
C2  - 31272093
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 85
ER  -

TY  - CONF
AU  - Friedman, S.
AU  - Schmer-Galunder, S.
AU  - Chen, A.
AU  - Goldman, R.
AU  - Ausman, M.
TI  - Gender Gaps Correlate with Gender Bias in Social Media Word Embeddings
PY  - 2020
T2  - Proceedings for the 42nd Annual Meeting of the Cognitive Science Society: Developing a Mind: Learning in Humans, Animals, and Machines, CogSci 2020
SP  - 2587
EP  - 2593
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139471286&partnerID=40&md5=093b4b724f25933aaecde9a7d8c52341
AB  - Gender status, gender roles, and gender values vary widely across cultures. Anthropology has provided qualitative accounts of economic, cultural, and biological factors that impact social groups, and international organizations have gathered indices and surveys to help quantify gender inequalities in states. Concurrently, machine learning research has recently characterized pervasive gender biases in AI language models, rooting from biases in their textual training data. While these machine biases produce sub-optimal inferences, they may help us characterize and predict statistical gender gaps and gender values in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach to (1) construct word embeddings (i.e., vector-based lexical semantics) from a region's social media, (2) quantify gender bias in word embeddings, and (3) correlate biases with survey responses and statistical gender gaps in education, politics, economics, and health. We validate this approach using 2018 Twitter data spanning 143 countries and 51 U.S. territories, 23 international and 7 U.S. gender gap statistics, and seven international survey results from the World Value Survey. Integrating these heterogeneous data across cultures is an important step toward understanding (1) how biases in culture might manifest in machine learning models and (2) how to estimate gender inequality from big data. © 2020 The Author(s). This work is licensed under a Creative Commons Attribution 4.0 International License (CC BY)
PB  - The Cognitive Science Society
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - De Leeuw, C.N.
AU  - Vogelbaum, M.A.
TI  - Supratotal resection in glioma: A systematic review
PY  - 2019
T2  - Neuro-Oncology
VL  - 21
IS  - 2
SP  - 179
EP  - 188
DO  - 10.1093/neuonc/noy166
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062234827&doi=10.1093%2fneuonc%2fnoy166&partnerID=40&md5=9a35be4871d0732a1be67e9fe9210636
AB  - Background Emerging evidence suggests survival benefit from resection beyond all MRI abnormalities present on T1-enhanced and T2-fluid attenuated inversion recovery (FLAIR) modalities in glioma (supratotal resection); however, the quality of evidence is unclear. We addressed this question via systematic review of the literature. Methods EMBASE, MEDLINE, Scopus, and Web of Science databases were queried. Case studies, reviews or editorials, non-English, abstract-only, brain metastases, and descriptive works were excluded. All others were included. Results Three hundred and nine unique references yielded 41 studies for full-text review, with 7 included in the final analysis. Studies were mostly of Oxford Center for Evidence-Based Medicine Level 4 quality. A total of 88 patients underwent supratotal resection in a combined cohort of 492 patients (214 males and 278 females, age 18 to 82 years). Fifty-one supratotal resections were conducted on high-grade gliomas, and 37 on low-grade gliomas. Karnofsky performance status, overall survival, progression-free survival, neurological deficits postoperatively, and anaplastic transformation were the main measured outcomes. No randomized controlled trials were identified. Preliminary low-quality support was found for supratotal resection in increasing overall survival and progression-free survival for both low-grade and high-grade glioma. Conclusion The literature suggests insufficient evidence for carte blanche application of supratotal resection, particularly in lower-grade gliomas where neurological deficits can result in long-term disability. While the preliminary studies discussed here, containing data from only a few centers, have reported increased progression-free and overall survival, these claims require validation in prospective research studies involving larger patient populations with clearly defined appropriate outcome metrics in order to reduce potential bias. © 2018 The Author(s).
PB  - Oxford University Press
C2  - 30321384
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 80
ER  -

TY  - CONF
AU  - Shang, J.
AU  - Ma, T.
AU  - Xiao, C.
AU  - Sun, J.
TI  - Pre-training of graph augmented transformers for medication recommendation
PY  - 2019
T2  - IJCAI International Joint Conference on Artificial Intelligence
VL  - 2019-August
SP  - 5953
EP  - 5959
DO  - 10.24963/ijcai.2019/825
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074900402&doi=10.24963%2fijcai.2019%2f825&partnerID=40&md5=a142e7568371ef790e6574d41003f6da
AB  - Medication recommendation is an important healthcare application. It is commonly formulated as a temporal prediction task. Hence, most existing works only utilize longitudinal electronic health records (EHRs) from a small number of patients with multiple visits ignoring a large number of patients with a single visit (selection bias). Moreover, important hierarchical knowledge such as diagnosis hierarchy is not leveraged in the representation learning process. To address these challenges, we propose G-BERT, a new model to combine the power of Graph Neural Networks (GNNs) and BERT (Bidirectional Encoder Representations from Transformers) for medical code representation and medication recommendation. We use GNNs to represent the internal hierarchical structures of medical codes. Then we integrate the GNN representation into a transformer-based visit encoder and pre-train it on EHR data from patients only with a single visit. The pre-trained visit encoder and representation are then fine-tuned for downstream predictive tasks on longitudinal EHRs from patients with multiple visits. G-BERT is the first to bring the language model pre-training schema into the healthcare domain and it achieved state-of-the-art performance on the medication recommendation task. © 2019 International Joint Conferences on Artificial Intelligence. All rights reserved.
PB  - International Joint Conferences on Artificial Intelligence
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 126
ER  -

TY  - JOUR
AU  - Grubb, T.L.
AU  - Anderson, D.E.
TI  - Assessment of clinical application of pulse oximetry probes in llamas and alpacas
PY  - 2017
T2  - Veterinary Medicine and Science
VL  - 3
IS  - 3
SP  - 169
EP  - 175
DO  - 10.1002/vms3.68
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050928962&doi=10.1002%2fvms3.68&partnerID=40&md5=dba7a4ce74894bccf7807e000c7af685
AB  - The placement and accuracy of pulse oximeter probes can vary markedly among species. For our study, we aimed to assess the accuracy of pulse oximetry and to determine the most clinically useful sites for probe placement in llamas and alpacas. The objectives included an analysis of pulse oximetry probes for accurate assessment of llamas and alpacas and to determine the best placement of the probes to achieve accurate readings. For study 1, saturation of haemoglobin with oxygen was measured in 184 arterial blood gas samples (SaO2) using a co-oximeter and compared to saturation of haemoglobin with oxygen simultaneously measured using a pulse oximeter (SpO2). The bias and precision for the SpO2-SaO2 difference was calculated and plotted on a Bland-Altman plot. For study 2, SpO2 data was collected 624 times from a variety of sites [tongue (T), nasal septum (NS), lip (L), vulva (V), prepuce (P), ear (E), and scrotum (S)] and recorded based upon a percentage of successful readings. Results for study 1 revealed that SpO2 was consistently 0 to −6% points different than SaO2. The bias and precision of the SpO2–SaO2 difference was −2.6 ± 1.7%. Results for study 2 uncovered that 540 recordings were successful readings and were obtained from the tongue and nasal septum with 97% accuracy, the lip 80%, vulva 62%, prepuce 59%, ear and scrotum < 50%. We concluded that pulse oximetry probes provide reliable estimates of arterial haemoglobin oxygen saturation in llamas and alpacas and is most accurately read when placed on the nasal septum or tongue. © 2017 The Authors.
PB  - Wiley-Blackwell
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - CONF
AU  - Han, R.
AU  - Zhou, Y.
AU  - Peng, N.
TI  - Domain knowledge empowered structured neural net for end-to-end event temporal relation extraction
PY  - 2020
T2  - EMNLP 2020 - 2020 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference
SP  - 5717
EP  - 5729
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100512893&partnerID=40&md5=f9242d1dcf5d522cad99e992088fdecd
AB  - Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding. Prior systems leverage deep learning and pre-trained language models to improve the performance of the task. However, these systems often suffer from two shortcomings: 1) when performing maximum a posteriori (MAP) inference based on neural models, previous systems only used structured knowledge that is assumed to be absolutely correct, i.e., hard constraints; 2) biased predictions on dominant temporal relations when training with a limited amount of data. To address these issues, we propose a framework that enhances deep neural network with distributional constraints constructed by probabilistic domain knowledge. We solve the constrained inference problem via Lagrangian Relaxation and apply it to end-to-end event temporal relation extraction tasks. Experimental results show our framework is able to improve the baseline neural network models with strong statistical significance on two widely used datasets in news and clinical domains. © 2020 Association for Computational Linguistics
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 32
ER  -

TY  - JOUR
AU  - Leeson, W.
AU  - Resnick, A.
AU  - Alexander, D.
AU  - Rovers, J.
TI  - Natural Language Processing (NLP) in Qualitative Public Health Research: A Proof of Concept Study
PY  - 2019
T2  - International Journal of Qualitative Methods
VL  - 18
DO  - 10.1177/1609406919887021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075146556&doi=10.1177%2f1609406919887021&partnerID=40&md5=4f11ccbb247bca29a2f3dcc7c54227a2
AB  - Qualitative data-analysis methods provide thick, rich descriptions of subjects’ thoughts, feelings, and lived experiences but may be time-consuming, labor-intensive, or prone to bias. Natural language processing (NLP) is a machine learning technique from computer science that uses algorithms to analyze textual data. NLP allows processing of large amounts of data almost instantaneously. As researchers become conversant with NLP, it is becoming more frequently employed outside of computer science and shows promise as a tool to analyze qualitative data in public health. This is a proof of concept paper to evaluate the potential of NLP to analyze qualitative data. Specifically, we ask if NLP can support conventional qualitative analysis, and if so, what its role is. We compared a qualitative method of open coding with two forms of NLP, Topic Modeling, and Word2Vec to analyze transcripts from interviews conducted in rural Belize querying men about their health needs. All three methods returned a series of terms that captured ideas and concepts in subjects’ responses to interview questions. Open coding returned 5–10 words or short phrases for each question. Topic Modeling returned a series of word-probability pairs that quantified how well a word captured the topic of a response. Word2Vec returned a list of words for each interview question ordered by which words were predicted to best capture the meaning of the passage. For most interview questions, all three methods returned conceptually similar results. NLP may be a useful adjunct to qualitative analysis. NLP may be performed after data have undergone open coding as a check on the accuracy of the codes. Alternatively, researchers can perform NLP prior to open coding and use the results to guide their creation of their codebook. © The Author(s) 2019.
PB  - SAGE Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 55
ER  -

TY  - JOUR
AU  - Wang, E.A.
AU  - Long, J.B.
AU  - McGinnis, K.A.
AU  - Wang, K.H.
AU  - Wildeman, C.J.
AU  - Kim, C.
AU  - Bucklen, K.B.
AU  - Fiellin, D.A.
AU  - Bates, J.
AU  - Brandt, C.
AU  - Justice, A.C.
TI  - Measuring Exposure to Incarceration Using the Electronic Health Record
PY  - 2019
T2  - Medical Care
VL  - 57
SP  - S157
EP  - S163
DO  - 10.1097/MLR.0000000000001049
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065985016&doi=10.1097%2fMLR.0000000000001049&partnerID=40&md5=cdf3dc547fe4df188bd5c8d3bba5bc24
AB  - Background: Electronic health records (EHRs) are a rich source of health information; however social determinants of health, including incarceration, and how they impact health and health care disparities can be hard to extract. Objective: The main objective of this study was to compare sensitivity and specificity of patient self-report with various methods of identifying incarceration exposure using the EHR. Research Design: Validation study using multiple data sources and types. Subjects: Participants of the Veterans Aging Cohort Study (VACS), a national observational cohort based on data from the Veterans Health Administration (VHA) EHR that includes all human immunodeficiency virus-infected patients in care (47,805) and uninfected patients (99,060) matched on region, age, race/ethnicity, and sex. Measures and Data Sources: Self-reported incarceration history compared with: (1) linked VHA EHR data to administrative data from a state Department of Correction (DOC), (2) linked VHA EHR data to administrative data on incarceration from Centers for Medicare and Medicaid Services (CMS), (3) VHA EHR-specific identifier codes indicative of receipt of VHA incarceration reentry services, and (4) natural language processing (NLP) in unstructured text in VHA EHR. Results: Linking the EHR to DOC data: sensitivity 2.5%, specificity 100%; linking the EHR to CMS data: sensitivity 7.9%, specificity 99.3%; VHA EHR-specific identifier for receipt of reentry services: sensitivity 7.3%, specificity 98.9%; and NLP, sensitivity 63.5%, specificity 95.9%. Conclusions: NLP tools hold promise as a feasible and valid method to identify individuals with exposure to incarceration in EHR. Future work should expand this approach using a larger body of documents and refinement of the methods, which may further improve operating characteristics of this method. © Copyright 2019 Wolters Kluwer Health, Inc. All rights reserved.
PB  - Lippincott Williams and Wilkins
C2  - 31095055
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 14
ER  -

TY  - CONF
AU  - Johannßen, D.
AU  - Biemann, C.
TI  - Between the lines: Machine learning for prediction of psychological traits - A survey
PY  - 2018
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 11015 LNCS
SP  - 192
EP  - 211
DO  - 10.1007/978-3-319-99740-7_13
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053178962&doi=10.1007%2f978-3-319-99740-7_13&partnerID=40&md5=4f65311ad2cf705b946ccda091175c2d
AB  - A connection between language and psychology of natural language processing for predicting psychological traits (NLPsych) is apparent and holds great potential for accessing the psyche, understand cognitive processes and detect mental health conditions. However, results of works in this field that we call NLPsych could be further improved and is sparse and fragmented, even though approaches and findings often are alike. This survey collects such research and summarizes approaches, data sources, utilized tools and methods, as well as findings. Approaches of included work can roughly be divided into two main strands: word-list-based inquiries and data-driven research. Some findings show that the change of language can indicate the course of mental health diseases, subsequent academic success can be predicted by the use of function words and dream narratives show highly complex cognitive processes – to name but a few. By surveying results of included work, we draw the ‘bigger picture’ that in order to grasp someone’s psyche, it is more important to research how people express themselves rather than what they say, which surfaces in function words. Furthermore, often research unawarely induce biases that worsen results, thus leading to the conclusion that future research should rather focus on data-driven approaches rather than hand-crafted attempts. © IFIP International Federation for Information Processing 2018.
PB  - Springer Verlag
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 9
ER  -

TY  - JOUR
AU  - Bullen, L.E.
AU  - Evola, M.G.
AU  - Griffith, E.H.
AU  - Seiler, G.S.
AU  - Saker, K.E.
TI  - Validation of ultrasonographic muscle thickness measurements as compared to the gold standard of computed tomography in dogs
PY  - 2017
T2  - PeerJ
VL  - 2017
IS  - 1
C7  - e2926
DO  - 10.7717/peerj.2926
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011035289&doi=10.7717%2fpeerj.2926&partnerID=40&md5=e38fa7c8fa314d4174b4dca38f926471
AB  - Objective. The objective was to quantitatively evaluate the validity of ultrasonographic (US) muscle measurements as compared to the gold standard of computed tomography (CT) in the canine. Design. This was a prospective study. Population. Twenty-five, client-owned dogs scheduled for CT as part of a diagnostic work-up for the management of their primary disease process were included. Materials and Methods. Specific appendicular (cubital flexors and extensors, cox- ofemoral flexors and extensors) and axial (temporalis, supraspinatus, infraspinatus, lum- bar epaxials) muscle groups were selected for quantitative measure based on CT planning and patient position. Prior to CT scan, the skin over the muscle sites was shaved and marked with a permanent marker. Patient body position was determined based on the patient's CT plan; positioning was consistent between CT and US imaging. To ensure identical imaging position for both CT and US measurements, radio-opaque fiducial markers were placed directly over the skin marks once the dog was positioned. Quantitative measurements (cm) for both lean muscle mass (LMM) and subcutaneous adipose (SQA) were recorded. Statistical comparisons between CT and US values were done separately for each site and type. Results. Muscle groups and associated SQA measured by US and CT were not statistically different based on an adjusted p-value using Bonferroni's correction (p < 0.0031). In addition, allLMMand SQA sites had good reliability and agreement (Cronbach's α = 0.8-1.0) between the two metrics, excluding the coxofemoral extensor muscle group (Cronbach's α = 0.73232). Linear regression analysis of muscle measures indicated close agreement (slope range 0.93-1.09) and minimal bias of variation (intercept range 0.05-0.11) between CT versus US modalities, with the exception of the coxofemoral extensor muscle. Similarly, SQA CT and US measures indicated close agreement with the slope range of 0.88-1.02 and minimal bias of variation with an intercept range of 0.021-0.098, excluding the cubital flexor and extensor groups. Additionally, the R2 values for these remaining LMM and SQA sites are reported as > 0.897 for LLM and > 0.8289 for SQA. Conclusions. Ultrasound imaging of selected appendicular and axial muscle groups in dogs can provide comparable assessment of muscle thickness to the current gold standard, CT. In consideration of both statistical reliability to CT and cage-side accessibility, the temporalis, supraspinatus, infraspinatus, and lumbar epaxial LMM sites are considered the most useful targets for US LMM assessment in the canine. Our findings support the potential utility of US as a clinical tool in veterinary medicine to assess LMM status in patients. Additional studies are indicated to develop standardized protocols of its use in a cage-side setting and to elucidate the benefit of this modality, in conjunction with nutritional interventions, to manage body LLM stores in compromised patients. © 2017 Bullen et al.
PB  - PeerJ Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - JOUR
AU  - Feldman, J.
AU  - Thomas-Bachli, A.
AU  - Forsyth, J.
AU  - Patel, Z.H.
AU  - Khan, K.
TI  - Development of a global infectious disease activity database using natural language processing, machine learning, and human expertise
PY  - 2019
T2  - Journal of the American Medical Informatics Association
VL  - 26
IS  - 11
SP  - 1355
EP  - 1359
DO  - 10.1093/jamia/ocz112
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073184126&doi=10.1093%2fjamia%2focz112&partnerID=40&md5=ba9167ee14ce4b2603974ec7d03b9126
AB  - Objective: We assessed whether machine learning can be utilized to allow efficient extraction of infectious disease activity information from online media reports. Materials and Methods: We curated a data set of labeled media reports (n = 8322) indicating which articles contain updates about disease activity. We trained a classifier on this data set. To validate our system, we used a held out test set and compared our articles to the World Health Organization Disease Outbreak News reports. Results: Our classifier achieved a recall and precision of 88.8% and 86.1%, respectively. The overall surveillance system detected 94% of the outbreaks identified by the WHO covered by online media (89%) and did so 43.4 (IQR: 9.5-61) days earlier on average. Discussion: We constructed a global real-time disease activity database surveilling 114 illnesses and syndromes. We must further assess our system for bias, representativeness, granularity, and accuracy. Conclusion: Machine learning, natural language processing, and human expertise can be used to efficiently identify disease activity from digital media reports. © 2019 The Author(s).
PB  - Oxford University Press
C2  - 31361300
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 17
ER  -

TY  - JOUR
AU  - Fernandes, A.C.
AU  - Dutta, R.
AU  - Velupillai, S.
AU  - Sanyal, J.
AU  - Stewart, R.
AU  - Chandran, D.
TI  - Identifying Suicide Ideation and Suicidal Attempts in a Psychiatric Clinical Research Database using Natural Language Processing
PY  - 2018
T2  - Scientific Reports
VL  - 8
IS  - 1
C7  - 7426
DO  - 10.1038/s41598-018-25773-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046880201&doi=10.1038%2fs41598-018-25773-2&partnerID=40&md5=cdc11896534cb8367b9cfaa5a164d3e5
AB  - Research into suicide prevention has been hampered by methodological limitations such as low sample size and recall bias. Recently, Natural Language Processing (NLP) strategies have been used with Electronic Health Records to increase information extraction from free text notes as well as structured fields concerning suicidality and this allows access to much larger cohorts than previously possible. This paper presents two novel NLP approaches-a rule-based approach to classify the presence of suicide ideation and a hybrid machine learning and rule-based approach to identify suicide attempts in a psychiatric clinical database. Good performance of the two classifiers in the evaluation study suggest they can be used to accurately detect mentions of suicide ideation and attempt within free-text documents in this psychiatric database. The novelty of the two approaches lies in the malleability of each classifier if a need to refine performance, or meet alternate classification requirements arises. The algorithms can also be adapted to fit infrastructures of other clinical datasets given sufficient clinical recording practice knowledge, without dependency on medical codes or additional data extraction of known risk factors to predict suicidal behaviour. © 2018 The Author(s).
PB  - Nature Publishing Group
C2  - 29743531
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 112
ER  -

TY  - JOUR
AU  - Perry, L.A.
AU  - Korfiatis, P.
AU  - Agrawal, J.P.
AU  - Erickson, B.J.
TI  - Increased signal intensity within glioblastoma resection cavities on fluid-attenuated inversion recovery imaging to detect early progressive disease in patients receiving radiotherapy with concomitant temozolomide therapy
PY  - 2018
T2  - Neuroradiology
VL  - 60
IS  - 1
SP  - 35
EP  - 42
DO  - 10.1007/s00234-017-1941-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032817814&doi=10.1007%2fs00234-017-1941-9&partnerID=40&md5=ddb19f3983a30931ad64adc097728be8
AB  - Purpose: Our study tested the diagnostic accuracy of increased signal intensity (SI) within FLAIR MR images of resection cavities in differentiating early progressive disease (ePD) from pseudoprogression (PsP) in patients with glioblastoma treated with radiotherapy with concomitant temozolomide therapy. Methods: In this retrospective study approved by our Institutional Review Board, we evaluated the records of 122 consecutive patients with partially or totally resected glioblastoma. Region of interest (ROI) analysis assessed 33 MR examinations from 11 subjects with histologically confirmed ePD and 37 MR examinations from 14 subjects with PsP (5 histologically confirmed, 9 clinically diagnosed). After applying an N4 bias correction algorithm to remove B0 field distortion and to standardize image intensities and then normalizing the intensities based on an ROI of uninvolved white matter from the contralateral hemisphere, the mean intensities of the ROI from within the resection cavities were calculated. Measures of diagnostic performance were calculated from the receiver operating characteristic (ROC) curve using the threshold intensity that maximized differentiation. Subgroup analysis explored differences between the patients with biopsy-confirmed disease. Results: At an optimal threshold intensity of 2.9, the area under the ROC curve (AUROC) for FLAIR to differentiate ePD from PsP was 0.79 (95% confidence interval 0.686–0.873) with a sensitivity of 0.818 and specificity of 0.694. The AUROC increased to 0.86 when only the patients with biopsy-confirmed PsP were considered. Conclusions: Increased SI within the resection cavity of FLAIR images is not a highly specific sign of ePD in glioblastoma patients treated with the Stupp protocol. © 2017, Springer-Verlag GmbH Germany.
PB  - Springer Verlag
C2  - 29103145
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Coiera, E.
AU  - Kocaballi, B.
AU  - Halamaka, J.
AU  - Laranjo, L.
TI  - The digital scribe
PY  - 2018
T2  - npj Digital Medicine
VL  - 1
IS  - 1
C7  - 58
DO  - 10.1038/s41746-018-0066-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074380326&doi=10.1038%2fs41746-018-0066-9&partnerID=40&md5=5cff2c80a53849dfa606c3b8f032d9f1
AB  - Current generation electronic health records suffer a number of problems that make them inefficient and associated with poor clinical satisfaction. Digital scribes or intelligent documentation support systems, take advantage of advances in speech recognition, natural language processing and artificial intelligence, to automate the clinical documentation task currently conducted by humans. Whilst in their infancy, digital scribes are likely to evolve through three broad stages. Human led systems task clinicians with creating documentation, but provide tools to make the task simpler and more effective, for example with dictation support, semantic checking and templates. Mixed-initiative systems are delegated part of the documentation task, converting the conversations in a clinical encounter into summaries suitable for the electronic record. Computer-led systems are delegated full control of documentation and only request human interaction when exceptions are encountered. Intelligent clinical environments permit such augmented clinical encounters to occur in a fully digitised space where the environment becomes the computer. Data from clinical instruments can be automatically transmitted, interpreted using AI and entered directly into the record. Digital scribes raise many issues for clinical practice, including new patient safety risks. Automation bias may see clinicians automatically accept scribe documents without checking. The electronic record also shifts from a human created summary of events to potentially a full audio, video and sensor record of the clinical encounter. Digital scribes promisingly offer a gateway into the clinical workflow for more advanced support for diagnostic, prognostic and therapeutic tasks. © 2018, The Author(s).
PB  - Nature Publishing Group
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 72
ER  -

TY  - JOUR
AU  - Abbasi, S.
AU  - Tajeripour, F.
TI  - Detection of brain tumor in 3D MRI images using local binary patterns and histogram orientation gradient
PY  - 2017
T2  - Neurocomputing
VL  - 219
SP  - 526
EP  - 535
DO  - 10.1016/j.neucom.2016.09.051
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994390926&doi=10.1016%2fj.neucom.2016.09.051&partnerID=40&md5=d6ca3f73ce1035e477b5e64147c7a92a
AB  - Brain tumor pathology is one of the most common mortality issues considered as an essential priority for health care societies. Accurate diagnosis of the type of disorder is crucial to make a plan for remedy that can minimize the deadly results. The main purpose of segmentation and detection is to make distinction between different regions of the brain. Besides accuracy, these techniques should be implemented quickly. In this paper an automatic method for brain tumor detection in 3D images has been proposed. In the first step, the bias field correction and histogram matching are used for pre-processing of the images. In the next step, the region of interest is identified and separated from the background of the Flair image. Local binary pattern in three orthogonal planes (LBP-TOP) and histogram of orientation gradients (HOG-TOP) are used as the learning features. Since 3D images are used in this research we use the idea of in local binary pattern in three orthogonal planes in order to extend histogram orientation gradients for 3D images. The random forest is then used to segment tumorous regions. We evaluate the performance of our algorithm on glioma images from BRATS 2013. Our experimental results and analyses indicate that our proposed framework is superior in detecting brain tumors in comparison with other techniques. © 2016 Elsevier B.V.
PB  - Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 171
ER  -

TY  - JOUR
AU  - Su, Y.
AU  - Xiang, H.
AU  - Xie, H.
AU  - Yu, Y.
AU  - Dong, S.
AU  - Yang, Z.
AU  - Zhao, N.
TI  - Application of BERT to Enable Gene Classification Based on Clinical Evidence
PY  - 2020
T2  - BioMed Research International
VL  - 2020
C7  - 5491963
DO  - 10.1155/2020/5491963
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094220538&doi=10.1155%2f2020%2f5491963&partnerID=40&md5=ca16fd1bec6c8eabe1ea9b2e1480c9cf
AB  - The identification of profiled cancer-related genes plays an essential role in cancer diagnosis and treatment. Based on literature research, the classification of genetic mutations continues to be done manually nowadays. Manual classification of genetic mutations is pathologist-dependent, subjective, and time-consuming. To improve the accuracy of clinical interpretation, scientists have proposed computational-based approaches for automatic analysis of mutations with the advent of next-generation sequencing technologies. Nevertheless, some challenges, such as multiple classifications, the complexity of texts, redundant descriptions, and inconsistent interpretation, have limited the development of algorithms. To overcome these difficulties, we have adapted a deep learning method named Bidirectional Encoder Representations from Transformers (BERT) to classify genetic mutations based on text evidence from an annotated database. During the training, three challenging features such as the extreme length of texts, biased data presentation, and high repeatability were addressed. Finally, the BERT+abstract demonstrates satisfactory results with 0.80 logarithmic loss, 0.6837 recall, and 0.705 F-measure. It is feasible for BERT to classify the genomic mutation text within literature-based datasets. Consequently, BERT is a practical tool for facilitating and significantly speeding up cancer research towards tumor progression, diagnosis, and the design of more precise and effective treatments.  © 2020 Yuhan Su et al.
PB  - Hindawi Limited
C2  - 33083472
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 14
ER  -

TY  - JOUR
AU  - Kreimeyer, K.
AU  - Menschik, D.
AU  - Winiecki, S.
AU  - Paul, W.
AU  - Barash, F.
AU  - Woo, E.J.
AU  - Alimchandani, M.
AU  - Arya, D.
AU  - Zinderman, C.
AU  - Forshee, R.
AU  - Botsis, T.
TI  - Using Probabilistic Record Linkage of Structured and Unstructured Data to Identify Duplicate Cases in Spontaneous Adverse Event Reporting Systems
PY  - 2017
T2  - Drug Safety
VL  - 40
IS  - 7
SP  - 571
EP  - 582
DO  - 10.1007/s40264-017-0523-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015203844&doi=10.1007%2fs40264-017-0523-4&partnerID=40&md5=b05cc96c8c2f4b40d9ebc8d9928cf2e2
AB  - Introduction: Duplicate case reports in spontaneous adverse event reporting systems pose a challenge for medical reviewers to efficiently perform individual and aggregate safety analyses. Duplicate cases can bias data mining by generating spurious signals of disproportional reporting of product-adverse event pairs. Objective: We have developed a probabilistic record linkage algorithm for identifying duplicate cases in the US Vaccine Adverse Event Reporting System (VAERS) and the US Food and Drug Administration Adverse Event Reporting System (FAERS). Methods: In addition to using structured field data, the algorithm incorporates the non-structured narrative text of adverse event reports by examining clinical and temporal information extracted by the Event-based Text-mining of Health Electronic Records system, a natural language processing tool. The final component of the algorithm is a novel duplicate confidence value that is calculated by a rule-based empirical approach that looks for similarities in a number of criteria between two case reports. Results: For VAERS, the algorithm identified 77% of known duplicate pairs with a precision (or positive predictive value) of 95%. For FAERS, it identified 13% of known duplicate pairs with a precision of 100%. The textual information did not improve the algorithm’s automated classification for VAERS or FAERS. The empirical duplicate confidence value increased performance on both VAERS and FAERS, mainly by reducing the occurrence of false-positives. Conclusions: The algorithm was shown to be effective at identifying pre-linked duplicate VAERS reports. The narrative text was not shown to be a key component in the automated detection evaluation; however, it is essential for supporting the semi-automated approach that is likely to be deployed at the Food and Drug Administration, where medical reviewers will perform some manual review of the most highly ranked reports identified by the algorithm. © 2017, Springer International Publishing Switzerland 2017(outside the USA).
PB  - Springer International Publishing
C2  - 28293864
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 26
ER  -

TY  - JOUR
AU  - Storelli, L.
AU  - Rocca, M.A.
AU  - Pantano, P.
AU  - Pagani, E.
AU  - De Stefano, N.
AU  - Tedeschi, G.
AU  - Zaratin, P.
AU  - Filippi, M.
AU  - Valsasina, P.
AU  - Sibilia, M.
AU  - Preziosa, P.
AU  - Gallo, A.
AU  - Bisecco, A.
AU  - Docimo, R.
AU  - Petsas, N.
AU  - Ruggieri, S.
AU  - Tommasin, S.
AU  - Stromillo, M.L.
AU  - Brocci, R.T.
TI  - MRI quality control for the Italian Neuroimaging Network Initiative: moving towards big data in multiple sclerosis
PY  - 2019
T2  - Journal of Neurology
VL  - 266
IS  - 11
SP  - 2848
EP  - 2858
DO  - 10.1007/s00415-019-09509-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071130956&doi=10.1007%2fs00415-019-09509-4&partnerID=40&md5=9d9ec7680a4973bba5c082d0cdbf74bf
AB  - The Italian Neuroimaging Network Initiative (INNI) supports the creation of a repository, where MRI, clinical, and neuropsychological data from multiple sclerosis (MS) patients and healthy controls are collected from Italian Research Centers with internationally recognized expertise in MRI applied to MS. However, multicenter MRI data integration needs standardization and quality control (QC). This study aimed to implement quantitative measures for characterizing the standardization and quality of MRI collected within INNI. MRI scans of 423 MS patients, including 3D T1- and T2-weighted, were obtained from INNI repository (from Centers A, B, C, and D). QC measures were implemented to characterize: (1) head positioning relative to the magnet isocenter; (2) intensity inhomogeneity; (3) relative image contrast between brain tissues; and (4) image artefacts. Centers A and D showed the most accurate subject positioning within the MR scanner (median z-offsets = − 2.6 ± 1.7 cm and − 1.1 ± 2 cm). A low, but significantly different, intensity inhomogeneity on 3D T1-weighted MRI was found between all centers (p < 0.05), except for Centers A and C that showed comparable image bias fields. Center D showed the highest relative contrast between gray and normal appearing white matter (NAWM) on 3D T1-weighed MRI (0.63 ± 0.04), while Center B showed the highest relative contrast between NAWM and MS lesions on FLAIR (0.21 ± 0.06). Image artefacts were mainly due to brain movement (60%) and ghosting (35%). The implemented QC procedure ensured systematic data quality assessment within INNI, thus making available a huge amount of high-quality MRI to better investigate pathophysiological substrates and validate novel MRI biomarkers in MS. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.
PB  - Dr. Dietrich Steinkopff Verlag GmbH and Co. KG
C2  - 31422457
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 16
ER  -

TY  - JOUR
AU  - Schirmer, M.D.
AU  - Dalca, A.V.
AU  - Sridharan, R.
AU  - Giese, A.-K.
AU  - Donahue, K.L.
AU  - Nardin, M.J.
AU  - Mocking, S.J.T.
AU  - McIntosh, E.C.
AU  - Frid, P.
AU  - Wasselius, J.
AU  - Cole, J.W.
AU  - Holmegaard, L.
AU  - Jern, C.
AU  - Jimenez-Conde, J.
AU  - Lemmens, R.
AU  - Lindgren, A.G.
AU  - Meschia, J.F.
AU  - Roquer, J.
AU  - Rundek, T.
AU  - Sacco, R.L.
AU  - Schmidt, R.
AU  - Sharma, P.
AU  - Slowik, A.
AU  - Thijs, V.
AU  - Woo, D.
AU  - Vagal, A.
AU  - Xu, H.
AU  - Kittner, S.J.
AU  - McArdle, P.F.
AU  - Mitchell, B.D.
AU  - Rosand, J.
AU  - Worrall, B.B.
AU  - Wu, O.
AU  - Golland, P.
AU  - Rost, N.S.
TI  - White matter hyperintensity quantification in large-scale clinical acute ischemic stroke cohorts – The MRI-GENIE study
PY  - 2019
T2  - NeuroImage: Clinical
VL  - 23
C7  - 101884
DO  - 10.1016/j.nicl.2019.101884
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067075360&doi=10.1016%2fj.nicl.2019.101884&partnerID=40&md5=a6d7c0ac9207a0a1b453bd0c306e7c7f
AB  - White matter hyperintensity (WMH) burden is a critically important cerebrovascular phenotype linked to prediction of diagnosis and prognosis of diseases, such as acute ischemic stroke (AIS). However, current approaches to its quantification on clinical MRI often rely on time intensive manual delineation of the disease on T2 fluid attenuated inverse recovery (FLAIR), which hinders high-throughput analyses such as genetic discovery. In this work, we present a fully automated pipeline for quantification of WMH in clinical large-scale studies of AIS. The pipeline incorporates automated brain extraction, intensity normalization and WMH segmentation using spatial priors. We first propose a brain extraction algorithm based on a fully convolutional deep learning architecture, specifically designed for clinical FLAIR images. We demonstrate that our method for brain extraction outperforms two commonly used and publicly available methods on clinical quality images in a set of 144 subject scans across 12 acquisition centers, based on dice coefficient (median 0.95; inter-quartile range 0.94–0.95; p < 0.01) and Pearson correlation of total brain volume (r = 0.90). Subsequently, we apply it to the large-scale clinical multi-site MRI-GENIE study (N = 2783) and identify a decrease in total brain volume of −2.4 cc/year. Additionally, we show that the resulting total brain volumes can successfully be used for quality control of image preprocessing. Finally, we obtain WMH volumes by building on an existing automatic WMH segmentation algorithm that delineates and distinguishes between different cerebrovascular pathologies. The learning method mimics expert knowledge of the spatial distribution of the WMH burden using a convolutional auto-encoder. This enables successful computation of WMH volumes of 2533 clinical AIS patients. We utilize these results to demonstrate the increase of WMH burden with age (0.950 cc/year) and show that single site estimates can be biased by the number of subjects recruited. © 2019 The Authors
PB  - Elsevier Inc.
C2  - 31200151
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 47
ER  -

TY  - JOUR
AU  - Peltonen, J.I.
AU  - Mäkelä, T.
AU  - Salli, E.
TI  - MRI quality assurance based on 3D FLAIR brain images
PY  - 2018
T2  - Magnetic Resonance Materials in Physics, Biology and Medicine
VL  - 31
IS  - 6
SP  - 689
EP  - 699
DO  - 10.1007/s10334-018-0699-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052115529&doi=10.1007%2fs10334-018-0699-3&partnerID=40&md5=4add41debc4b0c57bd259eedeb4df0b0
AB  - Objective: Quality assurance (QA) of magnetic resonance imaging (MRI) often relies on imaging phantoms with suitable structures and uniform regions. However, the connection between phantom measurements and actual clinical image quality is ambiguous. Thus, it is desirable to measure objective image quality directly from clinical images. Materials and methods: In this work, four measurements suitable for clinical image QA were presented: image resolution, contrast-to-noise ratio, quality index and bias index. The methods were applied to a large cohort of clinical 3D FLAIR volumes over a test period of 9.5 months. The results were compared with phantom QA. Additionally, the effect of patient movement on the presented measures was studied. Results: A connection between the presented clinical QA methods and scanner performance was observed: the values reacted to MRI equipment breakdowns that occurred during the study period. No apparent correlation with phantom QA results was found. The patient movement was found to have a significant effect on the resolution and contrast-to-noise ratio values. Discussion: QA based on clinical images provides a direct method for following MRI scanner performance. The methods could be used to detect problems, and potentially reduce scanner downtime. Furthermore, with the presented methodologies comparisons could be made between different sequences and imaging settings. In the future, an online QA system could recognize insufficient image quality and suggest an immediate re-scan. © 2018, European Society for Magnetic Resonance in Medicine and Biology (ESMRMB).
PB  - Springer Verlag
C2  - 30120616
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 7
ER  -

TY  - CONF
AU  - Rayz, J.T.
AU  - Rayz, V.L.
AU  - Raskin, V.
TI  - Cognitive imaging: Using knowledge representation for reliable segmentation of MR angiography data
PY  - 2017
T2  - Proceedings of 2017 IEEE 16th International Conference on Cognitive Informatics and Cognitive Computing, ICCI*CC 2017
C7  - 8109727
SP  - 37
EP  - 42
DO  - 10.1109/ICCI-CC.2017.8109727
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040628645&doi=10.1109%2fICCI-CC.2017.8109727&partnerID=40&md5=3a9ea7d59266805ae3ab8695ae99ebec
AB  - The paper, co-authored by a biomedical engineer specializing in brain blood circulation modeling and by two experts in meaning-based natural language processing, suggests a cognitive computing technology for medical imaging analysis that removes image artifacts resulting in visual deviations from reality, such as discontinuous blood vessels or two vessels shown merged when they are not. It is implemented by supplying the pertinent knowledge that humans have to the computer and letting it initiate the corrective post-processing. The existing Ontological Semantic Technology (OST) resource is centered on the ontology that is made to accommodate the domain with a minor adjustment effort. The examples from the ontology demonstrate the disparities between what the image shows and what the human knows. The computer detects them autonomously and can initiate the appropriate post-processing. If and when this cognitive imaging prevails, the post-processed images may replace the current ones as legitimate artifact free MRIs. © 2017 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - CONF
AU  - Liang, P.P.
AU  - Li, I.M.
AU  - Zheng, E.
AU  - Lim, Y.C.
AU  - Salakhutdinov, R.
AU  - Morency, L.-P.
TI  - Towards debiasing sentence representations
PY  - 2020
T2  - Proceedings of the Annual Meeting of the Association for Computational Linguistics
SP  - 5502
EP  - 5515
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099848135&partnerID=40&md5=5398ab5c46dd4dfd6f42fb27d165571a
AB  - As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, SENT-DEBIAS, to reduce these biases. We show that SENT-DEBIAS is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP. © 2020 Association for Computational Linguistics
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 119
ER  -

TY  - JOUR
AU  - Riestenberg, R.A.
AU  - Furman, A.
AU  - Cowen, A.
AU  - Pawlowksi, A.
AU  - Schneider, D.
AU  - Lewis, A.A.
AU  - Kelly, S.
AU  - Taiwo, B.
AU  - Achenbach, C.
AU  - Palella, F.
AU  - Stone, N.J.
AU  - Lloyd-Jones, D.M.
AU  - Feinstein, M.J.
TI  - Differences in statin utilization and lipid lowering by race, ethnicity, and HIV status in a real-world cohort of persons with human immunodeficiency virus and uninfected persons
PY  - 2019
T2  - American Heart Journal
VL  - 209
SP  - 79
EP  - 87
DO  - 10.1016/j.ahj.2018.11.012
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060326764&doi=10.1016%2fj.ahj.2018.11.012&partnerID=40&md5=4c61d5ee87773f8ab5f5325f41cb26a3
AB  - Background: Risks for cardiovascular diseases, including myocardial infarction and stroke, are elevated in people with HIV infection (PWH). However, no trials of statin utilization with clinical cardiovascular disease (CVD) end points have been completed in PWH, and there are sparse real-world data regarding statin use and lipid-lowering effectiveness. We therefore used a unique cohort of PWH and uninfected controls to evaluate (1) differences in statin types used for PWH versus uninfected persons; (2) lipid lowering achieved by statin use for PWH versus uninfected persons; and (3) racial and ethnic disparities in appropriate statin use among PWH and uninfected persons. Methods: We analyzed a cohort of 5,039 PWH and 10,011 uninfected demographically matched controls who received care at a large urban medical center between January 1, 2000, and May 17, 2017. Medication administration records, prescription data, and validated natural language processing algorithms were used to determine statin utilization. Statins were categorized by generic active ingredient name and intensity (high, moderate, or low). Lipid values collected in routine clinical care were available for analysis. The first set of analyses was restricted to PWH and uninfected matched controls taking statins and compared (1) differences in statin type and (2) difference in cholesterol levels after versus before statin initiation by HIV status. For the second set of analyses, we first used prevalent CVD risk factors to determine participants with statin indications and then determined how many of these participants were taking statins. We then compared statin utilization among persons with indications for statins by race/ethnic group for PWH and uninfected matched controls using multivariable-adjusted logistic regression. Results: Among people prescribed statins, PWH were more likely than controls to have ever taken pravastatin (34.8% vs 12.3%, P <.001) or atorvastatin (72.2% vs 65.6%, P =.002) and less likely to have ever taken simvastatin (14.2% vs 39.5%, P <.001). Among PWH with indications for statin utilization, 55.7% of whites, 39.4% of blacks, and 45.8% of Hispanics were prescribed statins (P <.001). These differences in statin prescription by race/ethnicity remained significant after adjustment for demographics (including insurance status), cardiovascular risk factors, antiretroviral therapy use, HIV viremia, and CD4 count. These racial/ethnic disparities in statin utilization were less pronounced among uninfected persons. Conclusions: Among PWH with statin indication(s), blacks and Hispanics were less likely than whites to have been prescribed a statin. These racial/ethnic disparities were less pronounced among uninfected persons. There were significant differences in type of statin used for PWH compared to uninfected matched controls. Future efforts addressing disparities in CVD prevention among PWH are warranted. © 2018 Elsevier Inc.
PB  - Mosby Inc.
C2  - 30685678
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 19
ER  -

TY  - JOUR
AU  - Montano, M.
AU  - Bhasin, S.
AU  - D'Aquila, R.T.
AU  - Erlandson, K.M.
AU  - Evans, W.J.
AU  - Funderburg, N.T.
AU  - Justice, A.
AU  - Ndhlovu, L.C.
AU  - Ojikutu, B.
AU  - Pahor, M.
AU  - Pahwa, S.
AU  - Ryan, A.S.
AU  - Schrack, J.
AU  - Schultz, M.B.
AU  - Sebastiani, P.
AU  - Sinclair, D.A.
AU  - Tripp, J.
AU  - Walker, B.
AU  - Womack, J.A.
AU  - Yung, R.
AU  - Reeves, R.K.
TI  - Harvard HIV and aging workshop: Perspectives and Priorities from Claude D. Pepper Centers and Centers for AIDS Research
PY  - 2019
T2  - AIDS Research and Human Retroviruses
VL  - 35
IS  - 11-12
SP  - 999
EP  - 1012
DO  - 10.1089/aid.2019.0130
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075093442&doi=10.1089%2faid.2019.0130&partnerID=40&md5=67680712d96d64942c816471a3a319b4
AB  - People aging with HIV (PAWH) infection experience greater impairments in physical and cognitive function, in addition to higher rates of peripheral comorbid conditions (e.g., renal failure, diabetes, bone fracture, hypertension, cardiovascular disease, polypharmacy, and multimorbidity). While multifactorial drivers, including HIV infection itself, antiretroviral therapy-related toxicities, disparities in care, and biobehavioral factors, likely contribute, there remains an overarching question as to what are the relevant age-related mechanisms and models that could inform interventions that promote health span and life span in PAWH This workshop was convened to hear from experts on the biology of aging and HIV researchers studying PAWH to focus on advancing investigations at the interface of HIV and Aging. In this study, we summarize the discussions from the Harvard Center for AIDS Research and Boston Claude D. Pepper cosponsored workshop on HIV and Aging, which took place in October 2018. © Copyright 2019, Mary Ann Liebert, Inc.
PB  - Mary Ann Liebert Inc.
C2  - 31456412
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - JOUR
AU  - Luther, S.L.
AU  - Thomason, S.S.
AU  - Sabharwal, S.
AU  - Finch, D.K.
AU  - McCart, J.
AU  - Toyinbo, P.
AU  - Bouayad, L.
AU  - Matheny, M.E.
AU  - Gobbel, G.T.
AU  - Powell-Cope, G.
TI  - Leveraging electronic health care record information to measure pressure ulcer risk in veterans with spinal cord injury: A longitudinal study protocol
PY  - 2017
T2  - JMIR Research Protocols
VL  - 6
IS  - 1
C7  - e3
DO  - 10.2196/resprot.5948
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067694230&doi=10.2196%2fresprot.5948&partnerID=40&md5=d70d358a3144c1ffb5e2d015a9b96001
AB  - Background: Pressure ulcers (PrUs) are a frequent, serious, and costly complication for veterans with spinal cord injury (SCI). The health care team should periodically identify PrU risk, although there is no tool in the literature that has been found to be reliable, valid, and sensitive enough to assess risk in this vulnerable population. Objective: The immediate goal is to develop a risk assessment model that validly estimates the probability of developing a PrU. The long-term goal is to assist veterans with SCI and their providers in preventing PrUs through an automated system of risk assessment integrated into the veteran's electronic health record (EHR). Methods: This 5-year longitudinal, retrospective, cohort study targets 12,344 veterans with SCI who were cared for in the Veterans Health Administration (VHA) in fiscal year (FY) 2009 and had no record of a PrU in the prior 12 months. Potential risk factors identified in the literature were reviewed by an expert panel that prioritized factors and determined if these were found in structured data or unstructured form in narrative clinical notes for FY 2009-2013. These data are from the VHA enterprise Corporate Data Warehouse that is derived from the EHR structured (ie, coded in database/table) or narrative (ie, text in clinical notes) data for FY 2009-2013. Results: This study is ongoing and final results are expected in 2017. Thus far, the expert panel reviewed the initial list of risk factors extracted from the literature; the panel recommended additions and omissions and provided insights about the format in which the documentation of the risk factors might exist in the EHR. This list was then iteratively refined through review and discussed with individual experts in the field. The cohort for the study was then identified, and all structured, unstructured, and semistructured data were extracted. Annotation schemas were developed, samples of documents were extracted, and annotations are ongoing. Operational definitions of structured data elements have been created and steps to create an analytic dataset are underway. Conclusions: To our knowledge, this is the largest cohort employed to identify PrU risk factors in the United States. It also represents the first time natural language processing and statistical text mining will be used to expand the number of variables available for analysis. A major strength of this quantitative study is that all VHA SCI centers were included in the analysis, reducing potential for selection bias and providing increased power for complex statistical analyses. This longitudinal study will eventually result in a risk prediction tool to assess PrU risk that is reliable and valid, and that is sensitive to this vulnerable population. © Stephen L Luther, Susan S Thomason, Sunil Sabharwal, Dezon K Finch, James McCart, Peter Toyinbo, Lina Bouayad, Michael E Matheny, Glenn T Gobbel, Gail Powell-Cope. Originally published in JMIR Research Protocols (http://www.researchprotocols.org), 19.01.2017. This is an open-access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in JMIR Research Protocols, is properly cited. The complete bibliographic information, a link to the original publication on http://www.researchprotocols.org, as well as this copyright and license information must be included.
PB  - JMIR Publications Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 5
ER  -

TY  - JOUR
AU  - Thomas, E.G.
AU  - Jayabalasingham, B.
AU  - Collins, T.
AU  - Geertzen, J.
AU  - Bui, C.
AU  - Dominici, F.
TI  - Gender Disparities in Invited Commentary Authorship in 2459 Medical Journals
PY  - 2019
T2  - JAMA Network Open
VL  - 2
IS  - 10
C7  - e1913682
DO  - 10.1001/jamanetworkopen.2019.13682
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073714841&doi=10.1001%2fjamanetworkopen.2019.13682&partnerID=40&md5=8037ef671b65d2ae501c553a42cc61d9
AB  - Importance: In peer-reviewed medical journals, authoring an invited commentary on an original article is a recognition of expertise. It has been documented that women author fewer invited publications than men do. However, it is unknown whether this disparity is due to gender differences in characteristics that are associated with invitations, such as field of expertise, seniority, and scientific output. Objective: To estimate the odds ratio (OR) of authoring an invited commentary for women compared with men who had similar expertise, seniority, and publication metrics. Design, Setting, and Participants: This matched case-control study included all medical invited commentaries published from January 1, 2013, through December 31, 2017, in English-language medical journals and multidisciplinary journals. Invited commentaries were defined as publications that cite another publication within the same journal volume and issue. Bibliometric data were obtained from Scopus. Cases were defined as corresponding authors of invited commentaries in a given journal during the study period. Controls were matched to cases based on scientific expertise by calculating a similarity index for abstracts published during the same period using natural language processing. Data analyses were conducted from March 13, 2019, through May 3, 2019. Exposure: Corresponding or sole author gender was predicted from author first name and country of origin using genderize.io. Main Outcomes and Measures: The OR for gender was estimated after adjusting for field of expertise, publication output, citation impact, and years active (ie, years since first publication), with an interaction between gender and years active. Results: The final data set included 43235 cases across 2549 journals; there were 34047 unique intraciting commentary authors, among whom 9072 (26.6%) were women. For researchers who had been active for the median of 19 years, the odds of invited commentary authorship were 21% lower for women (OR, 0.79 [95% CI, 0.77-0.81]; P <.001) compared with men who had similar scientific expertise, number of publications, and citation impact. For every decile increase in years active, the OR decreased by a factor of 0.97 (95% CI, 0.96-0.98; P <.001). Conclusions and Relevance: In this case-control study, women had lower odds of authoring invited commentaries than their male peers. This disparity was larger for senior researchers. Journal editors could use natural language processing of published research to widen and diversify the pool of experts considered for commentary invitations. © 2019 Thomas EG et al. JAMA Network Open.
PB  - American Medical Association
C2  - 31642926
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 97
ER  -

TY  - JOUR
AU  - Majkowska, A.
AU  - Mittal, S.
AU  - Steiner, D.F.
AU  - Reicher, J.J.
AU  - McKinney, S.M.
AU  - Duggan, G.E.
AU  - Eswaran, K.
AU  - Chen, P.-H.C.
AU  - Liu, Y.
AU  - Kalidindi, S.R.
AU  - Ding, A.
AU  - Corrado, G.S.
AU  - Tse, D.
AU  - Shetty, S.
TI  - Chest radiograph interpretation with deep learning models: Assessment with radiologist-adjudicated reference standards and population-adjusted evaluation
PY  - 2020
T2  - Radiology
VL  - 294
IS  - 2
SP  - 421
EP  - 431
DO  - 10.1148/radiol.2019191293
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078539595&doi=10.1148%2fradiol.2019191293&partnerID=40&md5=42e682b8088fd19423e3cce605095663
AB  - Background: Deep learning has the potential to augment the use of chest radiography in clinical radiology, but challenges include poor generalizability, spectrum bias, and difficulty comparing across studies. Purpose: To develop and evaluate deep learning models for chest radiograph interpretation by using radiologist-adjudicated reference standards. Materials and Methods: Deep learning models were developed to detect four findings (pneumothorax, opacity, nodule or mass, and fracture) on frontal chest radiographs. This retrospective study used two data sets. Data set 1 (DS1) consisted of 759 611 images from a multicity hospital network and ChestX-ray14 is a publicly available data set with 112 120 images. Natural language processing and expert review of a subset of images provided labels for 657 954 training images. Test sets consisted of 1818 and 1962 images from DS1 and ChestX-ray14, respectively. Reference standards were defined by radiologist-adjudicated image review. Performance was evaluated by area under the receiver operating characteristic curve analysis, sensitivity, specificity, and positive predictive value. Four radiologists reviewed test set images for performance comparison. Inverse probability weighting was applied to DS1 to account for positive radiograph enrichment and estimate population-level performance. Results: In DS1, population-adjusted areas under the receiver operating characteristic curve for pneumothorax, nodule or mass, airspace opacity, and fracture were, respectively, 0.95 (95% confidence interval [CI]: 0.91, 0.99), 0.72 (95% CI: 0.66, 0.77), 0.91 (95% CI: 0.88, 0.93), and 0.86 (95% CI: 0.79, 0.92). With ChestX-ray14, areas under the receiver operating characteristic curve were 0.94 (95% CI: 0.93, 0.96), 0.91 (95% CI: 0.89, 0.93), 0.94 (95% CI: 0.93, 0.95), and 0.81 (95% CI: 0.75, 0.86), respectively. Conclusion: Expert-level models for detecting clinically relevant chest radiograph findings were developed for this study by using adjudicated reference standards and with population-level performance estimation. Radiologist-adjudicated labels for 2412 ChestXray14 validation set images and 1962 test set images are provided. © RSNA, 2019.
PB  - Radiological Society of North America Inc.
C2  - 31793848
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 200
ER  -

TY  - CONF
AU  - Rauf, N.
AU  - Alam, D.Y.
AU  - Jamaluddin, M.
AU  - Samad, B.A.
TI  - Improve Image Quality of Transversal Relaxation Time PROPELLER and FLAIR on Magnetic Resonance Imaging
PY  - 2018
T2  - Journal of Physics: Conference Series
VL  - 979
IS  - 1
C7  - 012079
DO  - 10.1088/1742-6596/979/1/012079
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044434315&doi=10.1088%2f1742-6596%2f979%2f1%2f012079&partnerID=40&md5=796efe28ced44d35eff2a9869dbc23fd
AB  - The Magnetic Resonance Imaging (MRI) is a medical imaging technique that uses the interaction between the magnetic field and the nuclear spins. MRI can be used to show disparity of pathology by transversal relaxation time (T2) weighted images. Some techniques for producing T2-weighted images are Periodically Rotated Overlapping Parallel Lines with Enhanced Reconstruction (PROPELLER) and Fluid Attenuated Inversion Recovery (FLAIR). A comparison of T2 PROPELLER and T2 FLAIR parameters in MRI image has been conducted. And improve Image Quality the image by using RadiAnt DICOM Viewer and ENVI software with method of image segmentation and Region of Interest (ROI). Brain images were randomly selected. The result of research showed that Time Repetition (TR) and Time Echo (TE) values in all types of images were not influenced by age. T2 FLAIR images had longer TR value (9000 ms), meanwhile T2 PROPELLER images had longer TE value (100.75 - 102.1 ms). Furthermore, areas with low and medium signal intensity appeared clearer by using T2 PROPELLER images (average coefficients of variation for low and medium signal intensity were 0.0431 and 0.0705, respectively). As for areas with high signal intensity appeared clearer by using T2 FLAIR images (average coefficient of variation was 0.0637). © Published under licence by IOP Publishing Ltd.
PB  - Institute of Physics Publishing
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 1
ER  -

TY  - JOUR
AU  - Maciejewski, M.
AU  - Lounkine, E.
AU  - Whitebread, S.
AU  - Farmer, P.
AU  - DuMouchel, W.
AU  - Shoichet, B.K.
AU  - Urban, L.
TI  - Reverse translation of adverse event reports paves the way for de-risking preclinical off-targets
PY  - 2017
T2  - eLife
VL  - 6
C7  - e25818
DO  - 10.7554/eLife.25818
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029232706&doi=10.7554%2feLife.25818&partnerID=40&md5=6b971aa8407d3b34dcc8541545f4ac0f
AB  - The Food and Drug Administration Adverse Event Reporting System (FAERS) remains the primary source for post-marketing pharmacovigilance. The system is largely un-curated, unstandardized, and lacks a method for linking drugs to the chemical structures of their active ingredients, increasing noise and artefactual trends. To address these problems, we mapped drugs to their ingredients and used natural language processing to classify and correlate drug events. Our analysis exposed key idiosyncrasies in FAERS, for example reports of thalidomide causing a deadly ADR when used against myeloma, a likely result of the disease itself; multiplications of the same report, unjustifiably increasing its importance; correlation of reported ADRs with public events, regulatory announcements, and with publications. Comparing the pharmacological, pharmacokinetic, and clinical ADR profiles of methylphenidate, aripiprazole, and risperidone, and of kinase drugs targeting the VEGF receptor, demonstrates how underlying molecular mechanisms can emerge from ADR co-analysis. The precautions and methods we describe may enable investigators to avoid confounding chemistry-based associations and reporting biases in FAERS, and illustrate how comparative analysis of ADRs can reveal underlying mechanisms. © Maciejewski et al.
PB  - eLife Sciences Publications Ltd
C2  - 28786378
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 38
ER  -

TY  - CONF
AU  - Makiuchi, M.R.
AU  - Warnita, T.
AU  - Uto, K.
AU  - Shinoda, K.
TI  - Multimodal fusion of BERT-CNN and gated CNN representations for depression detection
PY  - 2019
T2  - AVEC 2019 - Proceedings of the 9th International Audio/Visual Emotion Challenge and Workshop, co-located with MM 2019
SP  - 55
EP  - 63
DO  - 10.1145/3347320.3357694
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074919886&doi=10.1145%2f3347320.3357694&partnerID=40&md5=d781a7433fa07444b19873c5427126c2
AB  - Depression is a common, but serious mental disorder that affects people all over the world. Besides providing an easier way of diagnosing the disorder, a computer-aided automatic depression assessment system is demanded in order to reduce subjective bias in the diagnosis. We propose a multimodal fusion of speech and linguistic representation for depression detection. We train our model to infer the Patient Health Questionnaire (PHQ) score of subjects from AVEC 2019 DDS Challenge database, the E-DAIC corpus. For the speech modality, we use deep spectrum features extracted from a pretrained VGG-16 network and employ a Gated Convolutional Neural Network (GCNN) followed by a LSTM layer. For the textual embeddings, we extract BERT textual features and employ a Convolutional Neural Network (CNN) followed by a LSTM layer. We achieved a CCC score equivalent to 0.497 and 0.608 on the E-DAIC corpus development set using the unimodal speech and linguistic models respectively. We further combine the two modalities using a feature fusion approach in which we apply the last representation of each single modality model to a fully-connected layer in order to estimate the PHQ score. With this multimodal approach, it was possible to achieve the CCC score of 0.696 on the development set and 0.403 on the testing set of the E-DAIC corpus, which shows an absolute improvement of 0.283 points from the challenge baseline. © 2019 Association for Computing Machinery.
PB  - Association for Computing Machinery, Inc
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 102
ER  -

TY  - JOUR
AU  - Kim, C.
AU  - Zhu, V.
AU  - Obeid, J.
AU  - Lenert, L.
TI  - Natural language processing and machine learning algorithm to identify brain MRI reports with acute ischemic stroke
PY  - 2019
T2  - PLoS ONE
VL  - 14
IS  - 2
C7  - e0212778
DO  - 10.1371/journal.pone.0212778
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062326831&doi=10.1371%2fjournal.pone.0212778&partnerID=40&md5=d748100ff4ed2594ea670295412c7744
AB  - Background and purpose This project assessed performance of natural language processing (NLP) and machine learning (ML) algorithms for classification of brain MRI radiology reports into acute ischemic stroke (AIS) and non-AIS phenotypes. Materials and methods All brain MRI reports from a single academic institution over a two year period were randomly divided into 2 groups for ML: Training (70%) and testing (30%). Using "quanteda" NLP package, all text data were parsed into tokens to create the data frequency matrix. Ten-fold cross-validation was applied for bias correction of the training set. Labeling for AIS was performed manually, identifying clinical notes. We applied binary logistic regression, naïve Bayesian classification, single decision tree, and support vector machine for the binary classifiers, and we assessed performance of the algorithms by F1-measure. We also assessed how n-grams or term frequency-inverse document frequency weighting affected the performance of the algorithms. Results Of all 3,204 brain MRI documents, 432 (14.3%) were labeled as AIS. AIS documents were longer in character length than those of non-AIS (median [interquartile range]; 551 [377- 681] vs. 309 [164-396]). Of all ML algorithms, single decision tree had the highest F1-measure (93.2) and accuracy (98.0%). Adding bigrams to the ML model improved F1-mesaure of naïve Bayesian classification, but not in others, and term frequency-inverse document frequency weighting to data frequency matrix did not show any additional performance improvements. Conclusions Supervised ML based NLP algorithms are useful for automatic classification of brain MRI reports for identification of AIS patients. Single decision tree was the best classifier to identify brain MRI reports with AIS. © 2019 Public Library of Science. All Rights Reserved.
PB  - Public Library of Science
C2  - 30818342
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 63
ER  -

TY  - JOUR
AU  - Burgos, G.
AU  - Rivera, F.I.
AU  - Garcia, M.A.
TI  - Contextualizing the relationship between culture and Puerto Rican health: Towards a place-based framework of minority health disparities
PY  - 2017
T2  - Centro Journal
VL  - 29
IS  - 3
SP  - 36
EP  - 73
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041801119&partnerID=40&md5=d3ae29ca3474883a1475036b54b7e096
AB  - In both the culture of poverty literature and the acculturation literature, Puerto Ricans are portrayed in negative terms. The culture of poverty framework attributes Puerto Rican poverty to the mental, behavioral, and moral pathology of Puerto Rican individuals and to Puerto Rican culture. Similarly, outdated acculturation frameworks also trace the poor health of immigrants and racialized minorities, such as Puerto Ricans, to equivalent perceived deficiencies. In this paper, we argue that both the culture of poverty and acculturation frameworks are two pillars of the White Racial Frame (Feagin 2009) that sustains racial inequality in the United States. To build our case, we provide an overview of Puerto Rican physical health disparities and highlight key findings. Then, we analyze this literature using natural language processing (NLP) tools to examine the lexicon of words that scholars use to understand such disparities. Our literature review shows that Puerto Ricans are generally doing worse than other groups across a range of health indicators. Results from the NLP analyses reveal that the lexicon of the culture of poverty and outdated notions of acculturation are rhetorical tools that scholars still use to make sense of these conditions. We conclude by arguing that moving away from a White Racial Frame of Puerto Rican health requires a theoretical model that puts race, place, and culture within a multilevel framework that we call the Racialized Place Inequality Framework. © 2017, Hunter College Center for Puerto Rican Studies. All rights reserved.
PB  - Hunter College Center for Puerto Rican Studies
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 10
ER  -

TY  - CONF
AU  - Šuster, S.
AU  - Tulkens, S.
AU  - Daelemans, W.
TI  - A Short Review of Ethical Challenges in Clinical Natural Language Processing
PY  - 2017
T2  - EACL 2017 - Ethics in Natural Language Processing, Proceedings of the 1st ACL Workshop
SP  - 80
EP  - 87
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108661989&partnerID=40&md5=652fa3b1852c890fdf7414aa2a27b26d
AB  - Clinical NLP has an immense potential in contributing to how clinical practice will be revolutionized by the advent of large scale processing of clinical records. However, this potential has remained largely untapped due to slow progress primarily caused by strict data access policies for researchers. In this paper, we discuss the concern for privacy and the measures it entails. We also suggest sources of less sensitive data. Finally, we draw attention to biases that can compromise the validity of empirical research and lead to socially harmful applications. © 2017 Association for Computational Linguistics.
PB  - Association for Computational Linguistics (ACL)
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 17
ER  -

TY  - CONF
AU  - Arguello-Casteleiro, M.
AU  - Jones, P.H.
AU  - Robertson, S.
AU  - Irvine, R.M.
AU  - Twomey, F.
AU  - Nenadic, G.
TI  - Exploring the Automatisation of Animal Health Surveillance Through Natural Language Processing
PY  - 2019
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
VL  - 11927 LNAI
SP  - 213
EP  - 226
DO  - 10.1007/978-3-030-34885-4_17
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076952653&doi=10.1007%2f978-3-030-34885-4_17&partnerID=40&md5=2b02b1bed3b9e0eedd522c8ab30b0b4b
AB  - The Animal and Plant Health Agency (APHA) conducts post-mortem examinations (PMEs) of farm animal species as part of routine scanning surveillance for new and re-emerging diseases that may pose a threat to animal and public health. This paper investigates whether relevant veterinary medical terms can be automatically identified in the free-text summaries entered by Veterinary Investigation Officers (VIOs) on the PME reports. Two natural language processing tasks were performed: (1) named entity recognition, where terms within the free-text were mapped to concepts in the Unified Medical Language System (UMLS) Metathesaurus; and (2) semantic similarity and relatedness also using UMLS. For this pilot study, we focused on two diagnostic codes: salmonellosis (S. Dublin) and Pneumonia NOS (Not Otherwise Specified). The outputs were manually evaluated by VIOs. The results highlight the potential value of natural language processing to identify key concepts and pertinent veterinary medical terms that can be used for scanning surveillance purposes using large, free-text data. We also discuss issues resulting from the inherent bias of UMLS to human medical terms and its use in animal health monitoring. © 2019, Springer Nature Switzerland AG.
PB  - Springer
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 4
ER  -

TY  - JOUR
AU  - Gamerman, V.
AU  - Cai, T.
AU  - Elsäßer, A.
TI  - Pragmatic randomized clinical trials: best practices and statistical guidance
PY  - 2019
T2  - Health Services and Outcomes Research Methodology
VL  - 19
IS  - 1
SP  - 23
EP  - 35
DO  - 10.1007/s10742-018-0192-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057629421&doi=10.1007%2fs10742-018-0192-5&partnerID=40&md5=fad41c530ed1a25228df48a046caea25
AB  - Randomized clinical trials often serve the purpose of assessing the efficacy and safety of a compound. By combining real-world evidence and randomization, pragmatic randomized clinical trials (PrCTs) can be used to inform treatment effectiveness and healthcare decisions. PrCTs, referring to studies where several pragmatic elements are used (eligibility, endpoints, follow-up, etc.), pose unique challenges (Loudon et al. in BMJ 350:h2147, 2015). From a literature review, we propose a definition of PrCT and discuss strategies to overcome some PrCT challenges. Use of alternative data collection approaches may lead to uncertainties, and absence of blinding could potentially lead to non-random missing data at study endpoints such that randomization is no longer protected by an intent to treat. Therefore, more complex randomization strategies may be needed to minimize bias. Additional data sources could be used to synthesize information and create a more accurate endpoint definition, which may require tools such as natural language processing. The statistician must become familiar with the challenges and strengths of PrCTs, ranging from design to analysis to interpretation, in order to transform data into evidence (Califf in Clin Trials 13:471–477, 2016). © 2018, The Author(s).
PB  - Springer New York LLC
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 75
ER  -

TY  - JOUR
AU  - Korach, Z.T.
AU  - Yerneni, S.
AU  - Einbinder, J.
AU  - Kallenberg, C.
AU  - Zhou, L.
TI  - Facilitating information extraction without annotated data using unsupervised and positive-unlabeled learning
PY  - 2020
T2  - AMIA ... Annual Symposium proceedings. AMIA Symposium
VL  - 2020
SP  - 658
EP  - 667
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105349877&partnerID=40&md5=f8c525be3ee0424fa5cdfa717ba85583
AB  - Information extraction (IE), the distillation of specific information from unstructured data, is a core task in natural language processing. For rare entities (<1% prevalence), collection of positive examples required to train a model may require an infeasibly large sample of mostly negative ones. We combined unsupervised- with biased positive-unlabeled (PU) learning methods to: 1) facilitate positive example collection while maintaining the assumptions needed to 2) learn a binary classifier from the biased positive-unlabeled data alone. We tested the methods on a real-life use case of rare (<0.42%) entity extraction from medical malpractice documents. When tested on a manually reviewed random sample of documents, the PU model achieved an area under the precision-recall curve of0.283 and Fj of 0.410, outperforming fully supervised learning (0.022 and 0.096, respectively). The results demonstrate our method's potential to reduce the manual effort required for extracting rare entities from narrative texts. ©2020 AMIA - All rights reserved.
PB  - NLM (Medline)
C2  - 33936440
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

TY  - CONF
AU  - Odlum, M.
AU  - Cho, H.
AU  - Broadwell, P.
AU  - Davis, N.
AU  - Patrao, M.
AU  - Schauer, D.
AU  - Bales, M.E.
AU  - Alcantara, C.
AU  - Yoon, S.
TI  - Application of topic modeling to tweetsas the foundation for health disparity research for COVID-19
PY  - 2020
T2  - Studies in Health Technology and Informatics
VL  - 272
SP  - 24
EP  - 27
DO  - 10.3233/SHTI200484
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087394600&doi=10.3233%2fSHTI200484&partnerID=40&md5=b74465bedab0d46d70685a572ef600e2
AB  - We randomly extracted publicly available Tweets mentioning COVID-19 related terms (n=2,558,474 Tweets) from Tweet corpora collected daily using an API from Jan 21st to May 3rd, 2020. We applied a clustering algorithm to publicly available Tweets authored by African Americans (n=1,763) to detect topics and sentiment applying natural language processing (NLP). We visualized fifteen topics (four themes) using network diagrams (Newman modularity 0.74). Compared to the COVID-19 related Tweets authored by others, positive sentiments, cohesively encouraging online discussions (e.g., Black strong 27.1%, growing up Blacks 22.8%, support Black business 17.0%, how to build resilience 7.8%), and COVID-19 prevention behaviors (e.g., masks 4.7%, encouraging social distancing 9.4%) were uniquely observed in African American Twitter communities. Application of topic modeling techniques to streaming social media Twitter provides the foundation for research team insights regarding information and future virtual based intervention and social media based health disparity research for COVID-19. © 2020 The authors and IOS Press.
PB  - IOS Press
C2  - 32604591
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 27
ER  -

TY  - JOUR
AU  - Henkhaus, N.
AU  - Bartlett, M.
AU  - Gang, D.
AU  - Grumet, R.
AU  - Jordon-Thaden, I.
AU  - Lorence, A.
AU  - Lyons, E.
AU  - Miller, S.
AU  - Murray, S.
AU  - Nelson, A.
AU  - Specht, C.
AU  - Tyler, B.
AU  - Wentworth, T.
AU  - Ackerly, D.
AU  - Baltensperger, D.
AU  - Benfey, P.
AU  - Birchler, J.
AU  - Chellamma, S.
AU  - Crowder, R.
AU  - Donoghue, M.
AU  - Dundore-Arias, J.P.
AU  - Fletcher, J.
AU  - Fraser, V.
AU  - Gillespie, K.
AU  - Guralnick, L.
AU  - Haswell, E.
AU  - Hunter, M.
AU  - Kaeppler, S.
AU  - Kepinski, S.
AU  - Li, F.-W.
AU  - Mackenzie, S.
AU  - McDade, L.
AU  - Min, Y.
AU  - Nemhauser, J.
AU  - Pearson, B.
AU  - Petracek, P.
AU  - Rogers, K.
AU  - Sakai, A.
AU  - Sickler, D.
AU  - Taylor, C.
AU  - Wayne, L.
AU  - Wendroth, O.
AU  - Zapata, F.
AU  - Stern, D.
TI  - Plant science decadal vision 2020–2030: Reimagining the potential of plants for a healthy and sustainable future
PY  - 2020
T2  - Plant Direct
VL  - 4
IS  - 8
C7  - e00252
DO  - 10.1002/pld3.252
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090015644&doi=10.1002%2fpld3.252&partnerID=40&md5=cddab70c202c738b7b15270b56794149
AB  - Plants, and the biological systems around them, are key to the future health of the planet and its inhabitants. The Plant Science Decadal Vision 2020–2030 frames our ability to perform vital and far-reaching research in plant systems sciences, essential to how we value participants and apply emerging technologies. We outline a comprehensive vision for addressing some of our most pressing global problems through discovery, practical applications, and education. The Decadal Vision was developed by the participants at the Plant Summit 2019, a community event organized by the Plant Science Research Network. The Decadal Vision describes a holistic vision for the next decade of plant science that blends recommendations for research, people, and technology. Going beyond discoveries and applications, we, the plant science community, must implement bold, innovative changes to research cultures and training paradigms in this era of automation, virtualization, and the looming shadow of climate change. Our vision and hopes for the next decade are encapsulated in the phrase reimagining the potential of plants for a healthy and sustainable future. The Decadal Vision recognizes the vital intersection of human and scientific elements and demands an integrated implementation of strategies for research (Goals 1–4), people (Goals 5 and 6), and technology (Goals 7 and 8). This report is intended to help inspire and guide the research community, scientific societies, federal funding agencies, private philanthropies, corporations, educators, entrepreneurs, and early career researchers over the next 10 years. The research encompass experimental and computational approaches to understanding and predicting ecosystem behavior; novel production systems for food, feed, and fiber with greater crop diversity, efficiency, productivity, and resilience that improve ecosystem health; approaches to realize the potential for advances in nutrition, discovery and engineering of plant-based medicines, and "green infrastructure." Launching the Transparent Plant will use experimental and computational approaches to break down the phytobiome into a "parts store" that supports tinkering and supports query, prediction, and rapid-response problem solving. Equity, diversity, and inclusion are indispensable cornerstones of realizing our vision. We make recommendations around funding and systems that support customized professional development. Plant systems are frequently taken for granted therefore we make recommendations to improve plant awareness and community science programs to increase understanding of scientific research. We prioritize emerging technologies, focusing on non-invasive imaging, sensors, and plug-and-play portable lab technologies, coupled with enabling computational advances. Plant systems science will benefit from data management and future advances in automation, machine learning, natural language processing, and artificial intelligence-assisted data integration, pattern identification, and decision making. Implementation of this vision will transform plant systems science and ripple outwards through society and across the globe. Beyond deepening our biological understanding, we envision entirely new applications. We further anticipate a wave of diversification of plant systems practitioners while stimulating community engagement, underpinning increasing entrepreneurship. This surge of engagement and knowledge will help satisfy and stoke people's natural curiosity about the future, and their desire to prepare for it, as they seek fuller information about food, health, climate and ecological systems. © 2020 The Authors. Plant Direct published by American Society of Plant Biologists and the Society for Experimental Biology and John Wiley & Sons Ltd
PB  - John Wiley and Sons Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 36
ER  -

TY  - JOUR
AU  - Chen, J.
AU  - Yu, H.
TI  - Unsupervised ensemble ranking of terms in electronic health record notes based on their importance to patients
PY  - 2017
T2  - Journal of Biomedical Informatics
VL  - 68
SP  - 121
EP  - 131
DO  - 10.1016/j.jbi.2017.02.016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016072780&doi=10.1016%2fj.jbi.2017.02.016&partnerID=40&md5=52baaafbdb2918092b6267b7ade74efd
AB  - Background Allowing patients to access their own electronic health record (EHR) notes through online patient portals has the potential to improve patient-centered care. However, EHR notes contain abundant medical jargon that can be difficult for patients to comprehend. One way to help patients is to reduce information overload and help them focus on medical terms that matter most to them. Targeted education can then be developed to improve patient EHR comprehension and the quality of care. Objective The aim of this work was to develop FIT (Finding Important Terms for patients), an unsupervised natural language processing (NLP) system that ranks medical terms in EHR notes based on their importance to patients. Methods We built FIT on a new unsupervised ensemble ranking model derived from the biased random walk algorithm to combine heterogeneous information resources for ranking candidate terms from each EHR note. Specifically, FIT integrates four single views (rankers) for term importance: patient use of medical concepts, document-level term salience, word co-occurrence based term relatedness, and topic coherence. It also incorporates partial information of term importance as conveyed by terms’ unfamiliarity levels and semantic types. We evaluated FIT on 90 expert-annotated EHR notes and used the four single-view rankers as baselines. In addition, we implemented three benchmark unsupervised ensemble ranking methods as strong baselines. Results FIT achieved 0.885 AUC-ROC for ranking candidate terms from EHR notes to identify important terms. When including term identification, the performance of FIT for identifying important terms from EHR notes was 0.813 AUC-ROC. Both performance scores significantly exceeded the corresponding scores from the four single rankers (P < 0.001). FIT also outperformed the three ensemble rankers for most metrics. Its performance is relatively insensitive to its parameter. Conclusions FIT can automatically identify EHR terms important to patients. It may help develop future interventions to improve quality of care. By using unsupervised learning as well as a robust and flexible framework for information fusion, FIT can be readily applied to other domains and applications. © 2017 Elsevier Inc.
PB  - Academic Press Inc.
C2  - 28267590
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 15
ER  -

TY  - JOUR
AU  - Rojek, A.E.
AU  - Khanna, R.
AU  - Yim, J.W.L.
AU  - Gardner, R.
AU  - Lisker, S.
AU  - Hauer, K.E.
AU  - Lucey, C.
AU  - Sarkar, U.
TI  - Differences in Narrative Language in Evaluations of Medical Students by Gender and Under-represented Minority Status
PY  - 2019
T2  - Journal of General Internal Medicine
VL  - 34
IS  - 5
SP  - 684
EP  - 691
DO  - 10.1007/s11606-019-04889-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064539637&doi=10.1007%2fs11606-019-04889-9&partnerID=40&md5=7bb4c455bcbe51cc7c78f39b4cfcd09a
AB  - Background: In varied educational settings, narrative evaluations have revealed systematic and deleterious differences in language describing women and those underrepresented in their fields. In medicine, limited qualitative studies show differences in narrative language by gender and under-represented minority (URM) status. Objective: To identify and enumerate text descriptors in a database of medical student evaluations using natural language processing, and identify differences by gender and URM status in descriptions. Design: An observational study of core clerkship evaluations of third-year medical students, including data on student gender, URM status, clerkship grade, and specialty. Participants: A total of 87,922 clerkship evaluations from core clinical rotations at two medical schools in different geographic areas. Main Measures: We employed natural language processing to identify differences in the text of evaluations for women compared to men and for URM compared to non-URM students. Key Results: We found that of the ten most common words, such as “energetic” and “dependable,” none differed by gender or URM status. Of the 37 words that differed by gender, 62% represented personal attributes, such as “lovely” appearing more frequently in evaluations of women (p < 0.001), while 19% represented competency-related behaviors, such as “scientific” appearing more frequently in evaluations of men (p < 0.001). Of the 53 words that differed by URM status, 30% represented personal attributes, such as “pleasant” appearing more frequently in evaluations of URM students (p < 0.001), and 28% represented competency-related behaviors, such as “knowledgeable” appearing more frequently in evaluations of non-URM students (p < 0.001). Conclusions: Many words and phrases reflected students’ personal attributes rather than competency-related behaviors, suggesting a gap in implementing competency-based evaluation of students. We observed a significant difference in narrative evaluations associated with gender and URM status, even among students receiving the same grade. This finding raises concern for implicit bias in narrative evaluation, consistent with prior studies, and suggests opportunities for improvement. © 2019, Society of General Internal Medicine.
PB  - Springer New York LLC
C2  - 30993609
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 149
ER  -

TY  - JOUR
AU  - Wilcox, M.A.
AU  - Coppola, D.
AU  - Bailey, N.
AU  - Wilson, A.
AU  - Kamauu, A.W.C.
AU  - Alba, P.R.
AU  - Patterson, O.V.
AU  - Viernes, B.
AU  - Denhalter, D.W.
AU  - Solomon, I.D.
AU  - DuVall, S.L.
TI  - Risperdal® CONSTA® Needle Detachment. Incidence Rates Before and After Kit Redesign: A Retrospective Study using Electronic Health Records and Natural Language Processing in the Department of Veterans Affairs
PY  - 2019
T2  - Neurology and Therapy
VL  - 8
IS  - 1
SP  - 95
EP  - 108
DO  - 10.1007/s40120-019-0130-7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066309470&doi=10.1007%2fs40120-019-0130-7&partnerID=40&md5=67c9eb57b214600964d7ef6f1030955e
AB  - Introduction: Janssen received reports of needle detachments for Risperdal® CONSTA® and, in response, redesigned the kit. Objective: The study objective was to estimate the rate of Risperdal® CONSTA® needle detachments prior to and after the introduction of a redesigned kit. Methods: This retrospective study used record abstraction in the US Department of Veterans Affairs (VA). The 3 phases included: (1) a pilot study for methods evaluation in a sample of 6 hospitals with previously reported detachments; (2) a baseline study to ascertain the baseline detachment rate; and (3) a follow-up study to ascertain the rate for the redesigned kit. Administrative codes and natural language processing with clinical review were used to identify detachments. Results: Pilot: we identified a subset of spontaneously reported detachments and several previously unreported events. In the baseline study (original device), from January through December 2013, 22 needle detachments were identified among 47,934 administrations of the drug in a census of administrations in the VA; an incidence of 0.0459%. In the follow-up study (redesigned device), from December 2015 through December 2016, there were 14 reported detachments in 41,819 injections, 0.0335%. This represents a reduction of 27% from the baseline. Conclusion: This approach enabled us to identify needle detachments we would not have otherwise found (“solicited”). However, it likely resulted in incomplete outcome ascertainment. While this may have resulted in lower overall rates, it did not bias the comparison of the baseline and follow-up studies. The results showed that the redesigned Risperdal® CONSTA® kit reduced the incidence of needle detachment events in the VA. Funding: Janssen Pharmaceuticals, Inc. © 2019, The Author(s).
PB  - Springer Healthcare
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 3
ER  -

TY  - JOUR
AU  - Della Mea, V.
AU  - Popescu, M.H.
AU  - Roitero, K.
TI  - Underlying cause of death identification from death certificates using reverse coding to text and a NLP based deep learning approach
PY  - 2020
T2  - Informatics in Medicine Unlocked
VL  - 21
C7  - 100456
DO  - 10.1016/j.imu.2020.100456
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093662751&doi=10.1016%2fj.imu.2020.100456&partnerID=40&md5=6844ae9844bf84bdbf4f586ab3f6fcce
AB  - The identification of the underlying cause of death is a matter of primary importance and one of the most challenging issues in the setting of healthcare policy making. The World Health Organisation provides guidelines for death certificates coding using the ICD-10 classification. Guidelines can be manually applied, but there exist some coding support systems that implement them to simplify the coding work. Nevertheless, there is disparity among countries with respect to the level and the quality of death certificates registration. In this work we propose an effective supervised model based on Natural Language Processing algorithms to the aim of correctly classifying the underlying cause of death from death certificates. In our study we compared tabular representations of the death certificate, including the hierarchical path of each condition in the classification, with a novel representation consisting in translating back to their standard title the conditions expressed as ICD-10 codes. Our experimental evaluation, after training on 10.5 million certificates, reached a 99.03% accuracy, which currently outperforms state-of-the-art systems. For its practical applicability, we studied performance by classification chapter and found that accuracy is low only for chapters including very rare death causes. Finally, to show the robustness of our model, we leverage the model confidence to help identifying death certificates for which a manual coding is needed. © 2020 The Author(s)
PB  - Elsevier Ltd
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 14
ER  -

TY  - JOUR
AU  - Chui, C.K.
AU  - Lin, S.-B.
AU  - Zhou, D.-X.
TI  - Construction of Neural Networks for Realization of Localized Deep Learning
PY  - 2018
T2  - Frontiers in Applied Mathematics and Statistics
VL  - 4
C7  - 14
DO  - 10.3389/fams.2018.00014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059325169&doi=10.3389%2ffams.2018.00014&partnerID=40&md5=246d857e3e2a36d3355aac0917df1e92
AB  - The subject of deep learning has recently attracted users of machine learning from various disciplines, including: medical diagnosis and bioinformatics, financial market analysis and online advertisement, speech and handwriting recognition, computer vision and natural language processing, time series forecasting, and search engines. However, theoretical development of deep learning is still at its infancy. The objective of this paper is to introduce a deep neural network (also called deep-net) approach to localized manifold learning, with each hidden layer endowed with a specific learning task. For the purpose of illustrations, we only focus on deep-nets with three hidden layers, with the first layer for dimensionality reduction, the second layer for bias reduction, and the third layer for variance reduction. A feedback component is also designed to deal with outliers. The main theoretical result in this paper is the order (Formula presented.) of approximation of the regression function with regularity s, in terms of the number m of sample points, where the (unknown) manifold dimension d replaces the dimension D of the sampling (Euclidean) space for shallow nets. © Copyright © 2018 Chui, Lin and Zhou.
PB  - Frontiers Media S.A.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 28
ER  -

TY  - JOUR
AU  - Wang, J.
AU  - Hu, C.
AU  - Xu, H.
AU  - Leng, Y.
AU  - Zhang, L.
AU  - Zhao, Y.
TI  - A novel multi-atlas and multi-channel (MAMC) approach for multiple sclerosis lesion segmentation in brain MRI
PY  - 2019
T2  - Signal, Image and Video Processing
VL  - 13
IS  - 5
SP  - 1019
EP  - 1027
DO  - 10.1007/s11760-019-01440-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062015518&doi=10.1007%2fs11760-019-01440-5&partnerID=40&md5=6962a18517afce57eba880d544f7606d
AB  - This paper presents a novel approach for automatic segmentation of MS lesion including both of number and volume. The novelty includes the combination of the multiplicative intrinsic component optimization algorithm (Li et al. in Magn Reson Imaging 32:913–923, 2014) in bias field correction and normal tissue segmentation simultaneously, and the development of a multi-atlas and multi-channel (MAMC) segmentation approach. The first research focus is the classification of brain tissue into white matter, cerebrospinal fluid and gray matter in T1-w image and FLAIR image. The second research focus is the segmentation of MS lesion in white matter region using atlas. In label fusion, the coefficient as a specific weight is assigned to target label image based on the correlation function between atlases. This novel MAMC approach is evaluated by 20 training cases obtained from Medical Image Computing and Computer Aided Intervention Society 2008 MS Lesions Segmentation Challenge. The numerical results are presented in terms of accuracy, specificity and absolute volume difference. A comparison of MAMC approach and other conventional approaches is presented in terms of the true positive rate and the positive predictive value. Furthermore, the total lesion volume is calculated and compared with expert delineation. It can be seen that the MAMC approach is able to acquire a larger mean value of the Dice similarity coefficient than the other conventional approaches do. Therefore, this novel approach is an added value for the clinical evaluation of MS patients. © 2019, Springer-Verlag London Ltd., part of Springer Nature.
PB  - Springer London
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - JOUR
AU  - Hu, W.-H.
AU  - Liu, L.-N.
AU  - Zhao, B.-T.
AU  - Wang, X.
AU  - Zhang, C.
AU  - Shao, X.-Q.
AU  - Zhang, K.
AU  - Ma, Y.-S.
AU  - Ai, L.
AU  - Li, J.-J.
AU  - Zhang, J.-G.
TI  - Use of an automated quantitative analysis of hippocampal volume, signal, and glucose metabolism to detect hippocampal sclerosis
PY  - 2018
T2  - Frontiers in Neurology
VL  - 9
IS  - OCT
C7  - 820
DO  - 10.3389/fneur.2018.00820
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055118665&doi=10.3389%2ffneur.2018.00820&partnerID=40&md5=4962ba613a29faea70bacc88e957d8ee
AB  - Purpose: Magnetic resonance imaging (MRI) and positron emission tomography (PET) with 18F-fluorodeoxyglucose (18FDG) are valuable tools for evaluating hippocampal sclerosis (HS); however, bias may arise during visual analyses. The aim of this study was to evaluate and compare MRI and PET post-processing techniques, automated quantitative hippocampal volume (Q-volume), and fluid-attenuated inversion-recovery (FLAIR) signal (Q-FLAIR) and glucose metabolism (Q-PET) analyses in patients with HS. Methods: We collected MRI and 18FDG-PET images from 54 patients with HS and 22 healthy controls and independently performed conventional visual analyses (CVA) of PET (CVA-PET) and MRI (CVAMRI) images. During the subsequent quantitative analyses, the hippocampus was segmented from the 3D T1 image, and the mean volumetric, FLAIR intensity and standardized uptake value ratio (SUVR) values of the left and right hippocampus were assessed in each subject. Threshold confidence levels calculated from the mean volumetric, FLAIR intensity and SUVR values of the controls were used to identify healthy subjects or subjects with HS. The performance of the three methods was assessed using receiver operating characteristic (ROC) curves, and the detection rates of CVA-MRI, CVA-PET, Qvolume, Q-FLAIR, and Q-PET were statistically compared. Results: The areas under the curves (AUCs) for the Q-volume, QFLAIR, and Q-PET ROC analyses were 0.88, 0.41, and 0.98, which suggested a diagnostic method with moderate, poor, and high accuracy, respectively. Although Q-PET had the highest detection rate among the two CVA methods and three quantitative methods, the difference between Q-volume and Q-PET did not reach statistical significance. Regarding the HS subtypes, CVA-MRI, CVA-PET, Q-volume, and QPET had similar detection rates for type 1 HS, and Q-PET was the most sensitive method for detecting types 2 and 3 HS. Conclusions: In MRI or 18FDG-PET images that have been visually assessed by experts, the quantification of hippocampal volume or glucose uptake can increase the detection of HS and appear to be additional valuable diagnostic tools for evaluating patients with epilepsy who are suspected of having HS. © 2018 Frontiers Media S.A. All Rights Reserved.
PB  - Frontiers Media S.A.
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 8
ER  -

TY  - CONF
AU  - Zhang, H.
AU  - Lu, A.X.
AU  - Abdalla, M.
AU  - McDermott, M.
AU  - Ghassemi, M.
TI  - Hurtful words
PY  - 2020
T2  - ACM CHIL 2020 - Proceedings of the 2020 ACM Conference on Health, Inference, and Learning
SP  - 110
EP  - 120
DO  - 10.1145/3368555.3384448
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082755230&doi=10.1145%2f3368555.3384448&partnerID=40&md5=9ba0603f594f15e1582b51fca5dae8a7
AB  - In this work, we examine the extent to which embeddings may encode marginalized populations differently, and how this may lead to a perpetuation of biases and worsened performance on clinical tasks. We pretrain deep embedding models (BERT) on medical notes from the MIMIC-III hospital dataset, and quantify potential disparities using two approaches. First, we identify dangerous latent relationships that are captured by the contextual word embeddings using a fill-in-the-blank method with text from real clinical notes and a log probability bias score quantification. Second, we evaluate performance gaps across different definitions of fairness on over 50 downstream clinical prediction tasks that include detection of acute and chronic conditions. We find that classifiers trained from BERT representations exhibit statistically significant differences in performance, often favoring the majority group with regards to gender, language, ethnicity, and insurance status. Finally, we explore shortcomings of using adversarial debiasing to obfuscate subgroup information in contextual word embeddings, and recommend best practices for such deep embedding models in clinical settings. © 2020 ACM.
PB  - Association for Computing Machinery, Inc
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 107
ER  -

TY  - JOUR
AU  - Gao, Y.
AU  - Liu, Y.
AU  - Wang, Y.
AU  - Shi, Z.
AU  - Yu, J.
TI  - A Universal Intensity Standardization Method Based on a Many-to-One Weak-Paired Cycle Generative Adversarial Network for Magnetic Resonance Images
PY  - 2019
T2  - IEEE Transactions on Medical Imaging
VL  - 38
IS  - 9
SP  - 2059
EP  - 2069
DO  - 10.1109/TMI.2019.2894692
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071736603&doi=10.1109%2fTMI.2019.2894692&partnerID=40&md5=ceaf7542145faa6b6814a182550f480f
AB  - In magnetic resonance imaging (MRI), different imaging settings lead to various intensity distributions for a specific imaging object, which brings huge diversity to data-driven medical applications. To standardize the intensity distribution of magnetic resonance (MR) images from multiple centers and multiple machines using one model, a cycle generative adversarial network (CycleGAN)based framework is proposed. It utilizes a unified forward generative adversarial network (GAN) path and multiple independent backward GAN paths to transform images in different groups into a single reference one. To preserve image details and prevent resolution loss, two jump connections are applied in the CycleGAN generators. A weak-pair strategy is designed to fully utilize the prior knowledge of the organ structure and promote the performance of the GANs. The experiments were conducted on a T2-FLAIR image database with 8192 slices from 489 patients. The database was obtained from four hospitals and five MRI scanners and was divided into nine groups with different imaging parameters. Compared with the representative algorithms, the peak signal-to-noise ratio, the histogram correlation, and the structural similarity were increased by 3.7%, 5.1%, and 0.1% on average, respectively; the gradient magnitude similarity deviation, the mean square error, and the average disparity were reduced by 19.0%, 15.7%, and 9.9% on average, respectively. Experiments also showed the robustness of the proposed model with a different training set configuration and effectiveness of the proposed framework over the original CycleGAN. Therefore, the MR images with different imaging settings could be efficiently standardized by the proposed method, which would benefit various data-driven applications. © 2019 IEEE.
PB  - Institute of Electrical and Electronics Engineers Inc.
C2  - 30676951
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 43
ER  -

TY  - JOUR
AU  - Thompson, H.M.
AU  - Coleman, J.A.
AU  - Kent, P.M.
TI  - LGBT Medical Education: First-Year Medical Students’ Self-Assessed Knowledge and Comfort with Transgender and LGB Populations
PY  - 2018
T2  - Medical Science Educator
VL  - 28
IS  - 4
SP  - 693
EP  - 697
DO  - 10.1007/s40670-018-0614-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061923829&doi=10.1007%2fs40670-018-0614-6&partnerID=40&md5=2c782bac4be4d4343c8573cf9814a86b
AB  - Background: Medical education lags with respect to care for lesbian, gay, bisexual, and transgender populations, all of which experience disparities around access to care and health outcomes. Objective: (1) To evaluate M1 students’ perceived preparedness to care for transgender patients compared to LGB populations, (2) to identify knowledge and skills gaps, and (3) to recommend curricular developments. Methods: An online survey was administered to M1 students (N = 137) and assessed knowledge of and comfort levels with special populations. Responses were compared regarding transgender and nonbinary populations to those regarding LGB ones. Analyses consisted of t tests of response means and natural language processing of a free-text field querying for knowledge and skills needed to work with these subpopulations. Results: With a 100% response rate, students expressed significantly lower levels of knowledge and comfort regarding transgender and nonbinary populations compared to LGB ones. Natural language processing of the sentiment of the free-text field revealed greater magnitude for transgender populations (83.7 vs. 62.1) and equal average sentiment scores (0.2). Content also revealed a greater emphasis on direct interaction with trans and nonbinary persons in order to develop gender-affirming language and skills whereas LGB content focused on acquiring knowledge of LGB health issues. Conclusions: M1 students feel less knowledgeable about and comfort with transgender persons compared to LGB ones; they want more exposure to and interaction with trans-patient experiences in order to develop gender-affirming language and clinical practices that address the range of healthcare needs. © 2018, International Association of Medical Science Educators.
PB  - Springer
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 12
ER  -

TY  - JOUR
AU  - Kauttonen, J.
AU  - Hannukainen, J.
AU  - Tikka, P.
AU  - Suomala, J.
TI  - Predictive modeling for trustworthiness and other subjective text properties in online nutrition and health communication
PY  - 2020
T2  - PLoS ONE
VL  - 15
IS  - 8 August
C7  - e0237144
DO  - 10.1371/journal.pone.0237144
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089171577&doi=10.1371%2fjournal.pone.0237144&partnerID=40&md5=80401d56bde042c26e05c21d210689e2
AB  - While the internet has democratized and accelerated content creation and sharing, it has also made people more vulnerable to manipulation and misinformation. Also, the received information can be distorted by psychological biases. This is problematic especially in health-related communications which can greatly affect the quality of life of individuals. We assembled and analyzed 364 texts related to nutrition and health from Finnish online sources, such as news, columns and blogs, and asked non-experts to subjectively evaluate the texts. Texts were rated for their trustworthiness, sentiment, logic, information, clarity, and neutrality properties. We then estimated individual biases and consensus ratings that were used in training regression models. Firstly, we found that trustworthiness was significantly correlated to the information, neutrality and logic of the texts. Secondly, individual ratings for information and logic were significantly biased by the age and diet of the raters. Our best regression models explained up to 70% of the total variance of consensus ratings based on the low-level properties of texts, such as semantic embeddings, presence of keyterms and part-of-speech tags, references, quotes and paragraphs. With a novel combination of crowdsourcing, behavioral analysis, natural language processing and predictive modeling, our study contributes to the automated identification of reliable and high-quality online information. While critical evaluation of truthfulness cannot be surrendered to the machine only, our findings provide new insights into automated evaluation of subjective text properties and analysis of morphologically-rich languages in regards to trustworthiness. © 2020 Kauttonen et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
PB  - Public Library of Science
C2  - 32760095
M3  - Article
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 2
ER  -

TY  - CONF
AU  - Fang, H.
AU  - Huang, H.
AU  - Li, G.
AU  - Li, Y.
TI  - Learning from mistakes: Constructing and mining misdiagnosis database to reduce cognitive error
PY  - 2018
T2  - International Conference on Information Systems 2018, ICIS 2018
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062556659&partnerID=40&md5=022386226bcf1e41f5176d6feaa46e70
AB  - Diagnostic error is now a leading problem globally with the increase of mortality rate, financial cost and social burden. Existing IT artifacts mainly focus on tackling diagnostic errors caused by system flaws but fail to address users' cognitive biases. In view of it, we systematically design a two-phase framework to proactively enrich medical professionals' knowledge on diagnostic errors and to manage cognitive biases. Specifically, we first employ natural language processing techniques including BI-LSTM-CRF to extract misdiagnosis relations from medical literature, which are then stored in the misdiagnosis database. Secondly, we conduct in-depth knowledge discovery, including rule-based reasoning, network analysis and exploratory analysis to uncover the hidden knowledge on diagnostic errors from the constructed database. The on-going experiment verifies the effectiveness of BI-LSTM-CRF on performing the misdiagnosis relation extraction task. Our proposed framework serves as a promising infrastructure in both Healthcare IS and medical research for improving patient safety. © International Conference on Information Systems 2018, ICIS 2018.All rights reserved.
PB  - Association for Information Systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 31 March 2025; Cited By: 0
ER  -

