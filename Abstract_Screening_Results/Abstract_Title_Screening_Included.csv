key,title,year,month,day,journal,issn,volume,issue,pages,authors,url,language,publisher,location,abstract,notes,doi,keywords,pubmed_id,pmc_id
rayyan-202744179,Establishing best practices in large language model research: an application to repeat prompting,2025,,,Journal of the American Medical Informatics Association,,32,2,386-390,"Gallo, R.J. and Baiocchi, M. and Savage, T.R. and Chen, J.H.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216606586&doi=10.1093%2fjamia%2focae294&partnerID=40&md5=ec6f10d921d424c363d8964272a342ff,,Oxford University Press,,"Objectives: We aimed to demonstrate the importance of establishing best practices in large language model research, using repeat prompting as an illustrative example. Materials and Methods: Using data from a prior study investigating potential model bias in peer review of medical abstracts, we compared methods that ignore correlation in model outputs from repeated prompting with a random effects method that accounts for this correlation. Results: High correlation within groups was found when repeatedly prompting the model, with intraclass correlation coefficient of 0.69. Ignoring the inherent correlation in the data led to over 100-fold inflation of effective sample size. After appropriately accounting for this issue, the authors’ results reverse from a small but highly significant finding to no evidence of model bias. Discussion: The establishment of best practices for LLM research is urgently needed, as demonstrated in this case where accounting for repeat prompting in analyses was critical for accurate study conclusions. © The Author(s) 2024. Published by Oxford University Press on behalf of the American Medical Informatics Association.","Export Date: 31 March 2025; Cited By: 1 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: not health data,No relevant data",10.1093/jamia/ocae294,"",,
rayyan-202744180,"Racial, ethnic, and sex bias in large language model opioid recommendations for pain management",2025,,,Pain,,166,3,511-517,"Young, C.C. and Enichen, E. and Rao, A. and Succi, M.D.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204024749&doi=10.1097%2fj.pain.0000000000003388&partnerID=40&md5=55dca7caa217086a2bba229ed6ec3d3b,,Lippincott Williams and Wilkins,,"Understanding how large language model (LLM) recommendations vary with patient race/ethnicity provides insight into how LLMs may counter or compound bias in opioid prescription. Forty real-world patient cases were sourced from the MIMIC-IV Note dataset with chief complaints of abdominal pain, back pain, headache, or musculoskeletal pain and amended to include all combinations of race/ethnicity and sex. Large language models were instructed to provide a subjective pain rating and comprehensive pain management recommendation. Univariate analyses were performed to evaluate the association between racial/ethnic group or sex and the specified outcome measures - subjective pain rating, opioid name, order, and dosage recommendations - suggested by 2 LLMs (GPT-4 and Gemini). Four hundred eighty real-world patient cases were provided to each LLM, and responses included pharmacologic and nonpharmacologic interventions. Tramadol was the most recommended weak opioid in 55.4% of cases, while oxycodone was the most frequently recommended strong opioid in 33.2% of cases. Relative to GPT-4, Gemini was more likely to rate a patient's pain as ""severe""(OR: 0.57 95% CI: [0.54, 0.60]; P < 0.001), recommend strong opioids (OR: 2.05 95% CI: [1.59, 2.66]; P < 0.001), and recommend opioids later (OR: 1.41 95% CI: [1.22, 1.62]; P < 0.001). Race/ethnicity and sex did not influence LLM recommendations. This study suggests that LLMs do not preferentially recommend opioid treatment for one group over another. Given that prior research shows race-based disparities in pain perception and treatment by healthcare providers, LLMs may offer physicians a helpful tool to guide their pain management and ensure equitable treatment across patient groups.  © 2024 International Association for the Study of Pain.","Export Date: 31 March 2025; Cited By: 4 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,race ethnicity,opioid application",10.1097/j.pain.0000000000003388,"",,
rayyan-202744182,FedBM: Stealing knowledge from pre-trained language models for heterogeneous federated learning,2025,,,Medical Image Analysis,,102,,,"Zhu, M. and Yang, Q. and Gao, Z. and Yuan, Y. and Liu, J.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000465365&doi=10.1016%2fj.media.2025.103524&partnerID=40&md5=3118a378046dbcf04b8b46e3654b824f,,Elsevier B.V.,,"Federated learning (FL) has shown great potential in medical image computing since it provides a decentralized learning paradigm that allows multiple clients to train a model collaboratively without privacy leakage. However, current studies have shown that data heterogeneity incurs local learning bias in classifiers and feature extractors of client models during local training, leading to the performance degradation of a federation system. To address these issues, we propose a novel framework called Federated Bias eliMinating (FedBM) to get rid of local learning bias in heterogeneous federated learning (FL), which mainly consists of two modules, i.e., Linguistic Knowledge-based Classifier Construction (LKCC) and Concept-guided Global Distribution Estimation (CGDE). Specifically, LKCC exploits class concepts, prompts and pre-trained language models (PLMs) to obtain concept embeddings. These embeddings are used to estimate the latent concept distribution of each class in the linguistic space. Based on the theoretical derivation, we can rely on these distributions to pre-construct a high-quality classifier for clients to achieve classification optimization, which is frozen to avoid classifier bias during local training. CGDE samples probabilistic concept embeddings from the latent concept distributions to learn a conditional generator to capture the input space of the global model. Three regularization terms are introduced to improve the quality and utility of the generator. The generator is shared by all clients and produces pseudo data to calibrate updates of local feature extractors. Extensive comparison experiments and ablation studies on public datasets demonstrate the superior performance of FedBM over state-of-the-arts and confirm the effectiveness of each module, respectively. The code is available at https://github.com/CUHK-AIM-Group/FedBM. © 2025 Elsevier B.V.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: not health data,no bias evaluation",10.1016/j.media.2025.103524,"",,
rayyan-202744197,Unveiling Performance Challenges of Large Language Models in Low-Resource Healthcare: A Demographic Fairness Perspective,2025,,,"Proceedings - International Conference on Computational Linguistics, COLING",,,,7266-7278,"Zhou, Y. and Di Eugenio, B. and Cheng, L.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218488336&partnerID=40&md5=2b0608db3ccfd858d095fff916a8781d,,Association for Computational Linguistics (ACL),,"This paper studies the performance of large language models (LLMs), particularly regarding demographic fairness, in solving real-world healthcare tasks. We evaluate state-of-the-art LLMs with three prevalent learning frameworks across six diverse healthcare tasks and find significant challenges in applying LLMs to real-world healthcare tasks and persistent fairness issues across demographic groups. We also find that explicitly providing demographic information yields mixed results, while LLM's ability to infer such details raises concerns about biased health predictions. Utilizing LLMs as autonomous agents with access to up-to-date guidelines does not guarantee performance improvement. We believe these findings reveal the critical limitations of LLMs as concerns healthcare fairness and the urgent need for specialized research in this area. WARNING: This paper contains model outputs that may be considered offensive in nature. © 2025 Association for Computational Linguistics.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: fairness across demographic groups",,"",,
rayyan-202744198,Enhancing Patient-Physician Communication: Simulating African American Vernacular English in Medical Diagnostics with Large Language Models,2025,,,Journal of Healthcare Informatics Research,,,,,"Lee, Y. and Chang, C.-H. and Yang, C.C.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000800870&doi=10.1007%2fs41666-025-00194-9&partnerID=40&md5=52cdfb81ce365f46ce9049eb7101ef97,,Springer Science and Business Media Deutschland GmbH,,"Effective communication is crucial in reducing health disparities. However, linguistic differences, such as African American Vernacular English (AAVE), can lead to communication gaps between patients and physicians, negatively affecting care and outcomes. This study examines whether large language models (LLMs), specifically GPT-4 and Llama 3.3, can replicate AAVE in simulated clinical dialogues to improve cultural sensitivity. We tested four prompt types—BaseP, DemoP, LingP, and CompP—using United States Medical Licensing Examination (USMLE) case simulations. Statistical analyses on the models’ outputs showed a significant difference among prompt types for both GPT-4 (F(2,70) = 6.218, p = 0.003) and Llama 3.3 (F(2,70) = 12.124, p < 0.001), indicating that including demographic information and/or explicit AAVE cues influences each model’s output. Combining demographic and linguistic cues (CompP) yielded the highest mean AAVE feature counts (e.g., 9.83 for GPT-4 vs. 16.06 for Llama 3.3), although neither model fully captured the diversity of AAVE. Moreover, simply mentioning African American demographics triggers extra informal forms, suggesting built-in stereotypes or biases in both models. Overall, these findings highlight the promise of LLMs for culturally sensitive healthcare communication, while underscoring the need for continued refinement to address stereotypes and more accurately represent diverse linguistic styles. © The Author(s) 2025.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: fairness across demographic groups",10.1007/s41666-025-00194-9,"",,
rayyan-202744199,CmEAA: Cross-modal Enhancement and Alignment Adapter for Radiology Report Generation,2025,,,"Proceedings - International Conference on Computational Linguistics, COLING",,,,8546-8556,"Huang, X. and Han, Y. and Li, Y. and Li, R. and Wu, P. and Zhang, K.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218490368&partnerID=40&md5=452c11c5732b3c109cc1dd33eeea49b1,,Association for Computational Linguistics (ACL),,"Automatic radiology report generation is pivotal in reducing the workload of radiologists, while simultaneously improving diagnostic accuracy and operational efficiency. Current methods face significant challenges, including the effective alignment of medical visual features with textual features and the mitigation of data bias. In this paper, we propose a method for radiology report generation that utilizes a Cross-modal Enhancement and Alignment Adapter (CmEAA) to connect a vision encoder with a frozen large language model. Specifically, we introduce two novel modules within CmEAA: Cross-modal Feature Enhancement (CFE) and Neural Mutual Information Aligner (NMIA). CFE extracts observation-related contextual features to enhance the visual features of lesions and abnormal regions in radiology images through a cross-modal enhancement Transformer. NMIA maximizes neural mutual information between visual and textual representations within a low-dimensional alignment embedding space during training and provides potential global alignment visual representations during inference. Additionally, a weights generator is designed to enable the dynamic adaptation of cross-modal enhanced features and vanilla visual features. Experimental results on two prevailing datasets, namely, IU X-Ray and MIMIC-CXR, demonstrate that the proposed model outperforms previous state-of-the-art methods. © 2025 Association for Computational Linguistics.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation,Did not assess bias | USER-NOTES: {""Phoenix""=>[""images""]}",,"",,
rayyan-202744238,Through a Glass Darkly: Perceptions of Ethnoracial Identity in Artificial Intelligence Generated Medical Vignettes and Images,2025,,,Medical Science Educator,,,,,"Andrew, K. and Montalbano, M.J.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219025934&doi=10.1007%2fs40670-025-02332-9&partnerID=40&md5=ec7b2b7f4dfb2fe0f61c21339ed57526,,Springer,,"Purpose: Medical education professionals expect artificial intelligence (AI) systems to be an efficient faculty resource for content creation. However, prior findings suggest that machine learning algorithms may exacerbate negative stereotypes and undermine efforts for diversity, equity, and inclusivity. This investigation explores the potential of OpenAI’s ChatGPT (OCG) and Microsoft’s Bing A.I. Image Creator (MBIC) to perpetuate ethnoracial stereotypes in medical cases. Materials and Methods: A series of medically relevant vignettes and visual representatives were requested from ChatGPT and MBIC for five medical conditions traditionally associated with certain ethnoracial groups: sickle cell anemia, cystic fibrosis, Tay-Sachs disease, beta-thalassemia, and aldehyde dehydrogenase deficiency. Initial prompting, self-prompting, and prompt engineering were iteratively performed to ascertain the extent to which AI outputs for generated vignettes and imagery were mutable or fixed. Results: The ethnoracial identity in the vignettes of the clinical conditions adhered more closely than described in epidemiologic studies. Following prompt engineering and self-prompting, an increase in diversity was seen. On initial prompting, the most common ethnoracial identity depicted was Caucasian. Secondary prompting resulted in less diversity with higher conformation to the traditionally expected ethnoracial identity. Conclusion: The prevalence of dataset bias and AI’s user-dependent learning abilities underscore the importance of human stewardship. The increasing use of AI in generating medical education content, like MCQs, demands vigilant use of such tools to combat the reinforcement of the race-based stereotypes in medicine. © The Author(s) 2025.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: race | RAYYAN-EXCLUSION-REASONS: medical education,no direct health application | USER-NOTES: {""Phoenix""=>[""generates images?""]}",10.1007/s40670-025-02332-9,"",,
rayyan-202744255,"Building a Time-Series Model to Predict Hospitalization Risks in Home Health Care: Insights Into Development, Accuracy, and Fairness",2025,,,Journal of the American Medical Directors Association,,26,2,,"Topaz, M. and Davoudi, A. and Evans, L. and Sridharan, S. and Song, J. and Chae, S. and Barrón, Y. and Hobensack, M. and Scharp, D. and Cato, K. and Rossetti, S.C. and Kapela, P. and Xu, Z. and Gupta, P. and Zhang, Z. and Mcdonald, M.V. and Bowles, K.H.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212946037&doi=10.1016%2fj.jamda.2024.105417&partnerID=40&md5=6b1cbc96231451a8a66d2ffa581c0962,,Elsevier Inc.,,"Objectives: Home health care (HHC) serves more than 5 million older adults annually in the United States, aiming to prevent unnecessary hospitalizations and emergency department (ED) visits. Despite efforts, up to 25% of patients in HHC experience these adverse events. The underutilization of clinical notes, aggregated data approaches, and potential demographic biases have limited previous HHC risk prediction models. This study aimed to develop a time-series risk model to predict hospitalizations and ED visits in patients in HHC, examine model performance over various prediction windows, identify top predictive variables and map them to data standards, and assess model fairness across demographic subgroups. Setting and Participants: A total of 27,222 HHC episodes between 2015 and 2017. Methods: The study used health care process modeling of electronic health records, including clinical notes processed with natural language processing techniques and Medicare claims data. A Light Gradient Boosting Machine algorithm was used to develop the risk prediction model, with performance evaluated using 5-fold cross-validation. Model fairness was assessed across gender, race/ethnicity, and socioeconomic subgroups. Results: The model achieved high predictive performance, with an F1 score of 0.84 for a 5-day prediction window. Twenty top predictive variables were identified, including novel indicators such as the length of nurse-patient visits and visit frequency. Eighty-five percent of these variables mapped completely to the US Core Data for Interoperability standard. Fairness assessment revealed performance disparities across demographic and socioeconomic groups, with lower model effectiveness for more historically underserved populations. Conclusions and Implications: This study developed a robust time-series risk model for predicting adverse events in patients in HHC, incorporating diverse data types and demonstrating high predictive accuracy. The findings highlight the importance of considering established and novel risk factors in HHC. Importantly, the observed performance disparities across subgroups emphasize the need for fairness adjustments to ensure equitable risk prediction across all patient populations. © 2024 Post-Acute and Long-Term Care Medical Association","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,race ethnicity,fairness across demographic groups | RAYYAN-EXCLUSION-REASONS: No LLM | USER-NOTES: {""Lu""=>[""No NLP ""], ""Phoenix""=>[""read""]}",10.1016/j.jamda.2024.105417,"",,
rayyan-202744258,On the reliability of Large Language Models to misinformed and demographically informed prompts,2025,,,AI Magazine,,46,1,,"Aremu, T. and Akinwehinmi, O. and Nwagu, C. and Ahmed, S.I. and Orji, R. and Amo, P.A.D. and Saddik, A.E.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214496727&doi=10.1002%2faaai.12208&partnerID=40&md5=1022a51fbd11bb175008c447cd09574e,,John Wiley and Sons Inc,,"We investigate and observe the behavior and performance of Large Language Model (LLM)-backed chatbots in addressing misinformed prompts and questions with demographic information within the domains of Climate Change and Mental Health. Through a combination of quantitative and qualitative methods, we assess the chatbots' ability to discern the veracity of statements, their adherence to facts, and the presence of bias or misinformation in their responses. Our quantitative analysis using True/False questions reveals that these chatbots can be relied on to give the right answers to these close-ended questions. However, the qualitative insights, gathered from domain experts, shows that there are still concerns regarding privacy, ethical implications, and the necessity for chatbots to direct users to professional services. We conclude that while these chatbots hold significant promise, their deployment in sensitive areas necessitates careful consideration, ethical oversight, and rigorous refinement to ensure they serve as a beneficial augmentation to human expertise rather than an autonomous solution. Dataset and assessment information can be found at https://github.com/tolusophy/Edge-of-Tomorrow. © 2025 The Author(s). AI Magazine published by John Wiley & Sons Ltd on behalf of Association for the Advancement of Artificial Intelligence.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: age,race,mental health application | USER-NOTES: {""Phoenix""=>[""no bias""]}",10.1002/aaai.12208,"",,
rayyan-202744278,Digital Diagnostics: The Potential of Large Language Models in Recognizing Symptoms of Common Illnesses,2025,,,AI (Switzerland),,6,1,,"Gupta, G.K. and Singh, A. and Manikandan, S.V. and Ehtesham, A.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216109661&doi=10.3390%2fai6010013&partnerID=40&md5=dac82ecb72392576bd6725bddfa0a949,,Multidisciplinary Digital Publishing Institute (MDPI),,"This study aimed to evaluate the potential of Large Language Models (LLMs) in healthcare diagnostics, specifically their ability to analyze symptom-based prompts and provide accurate diagnoses. The study focused on models including GPT-4, GPT-4o, Gemini, o1 Preview, and GPT-3.5, assessing their performance in identifying illnesses based solely on provided symptoms. Symptom-based prompts were curated from reputable medical sources to ensure validity and relevance. Each model was tested under controlled conditions to evaluate their diagnostic accuracy, precision, recall, and decision-making capabilities. Specific scenarios were designed to explore their performance in both general and high-stakes diagnostic tasks. Among the models, GPT-4 achieved the highest diagnostic accuracy, demonstrating strong alignment with medical reasoning. Gemini excelled in high-stakes scenarios requiring precise decision-making. GPT-4o and o1 Preview showed balanced performance, effectively handling real-time diagnostic tasks with a focus on both precision and recall. GPT-3.5, though less advanced, proved dependable for general diagnostic tasks. This study highlights the strengths and limitations of LLMs in healthcare diagnostics. While models such as GPT-4 and Gemini exhibit promise, challenges such as privacy compliance, ethical considerations, and the mitigation of inherent biases must be addressed. The findings suggest pathways for responsibly integrating LLMs into diagnostic processes to enhance healthcare outcomes. © 2025 by the authors.","Export Date: 31 March 2025; Cited By: 1 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation,Did not assess bias",10.3390/ai6010013,"",,
rayyan-202744279,Can large language models be sensitive to culture suicide risk assessment?,2024,,,Journal of Cultural Cognitive Science,,8,3,275-287,"Levkovich, I. and Shinan-Altman, S. and Elyoseph, Z.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208100197&doi=10.1007%2fs41809-024-00151-9&partnerID=40&md5=a2a7f012a516facd7609e498d2aef0d3,,Springer,,"Suicide remains a pressing global public health issue. Previous studies have shown the promise of Generative Intelligent (GenAI) Large Language Models (LLMs) in assessing suicide risk in relation to professionals. But the considerations and risk factors that the models use to assess the risk remain as a black box. This study investigates if ChatGPT-3.5 and ChatGPT-4 integrate cultural factors in assessing suicide risks (probability of suicidal ideation, potential for suicide attempt, likelihood of severe suicide attempt, and risk of mortality from a suicidal act) by vignette methodology. The vignettes examined were of individuals from Greece and South Korea, representing countries with low and high suicide rates, respectively. The contribution of this research is to examine risk assessment from an international perspective, as large language models are expected to provide culturally-tailored responses. However, there is a concern regarding cultural biases and racism, making this study crucial. In the evaluation conducted via ChatGPT-4, only the risks associated with a severe suicide attempt and potential mortality from a suicidal act were rated higher for the South Korean characters than for their Greek counterparts. Furthermore, only within the ChatGPT-4 framework was male gender identified as a significant risk factor, leading to a heightened risk evaluation across all variables. ChatGPT models exhibit significant sensitivity to cultural nuances. ChatGPT-4, in particular, offers increased sensitivity and reduced bias, highlighting the importance of gender differences in suicide risk assessment. The findings suggest that, while ChatGPT-4 demonstrates an improved ability to account for cultural and gender-related factors in suicide risk assessment, there remain areas for enhancement, particularly in ensuring comprehensive and unbiased risk evaluations across diverse populations. These results underscore the potential of GenAI models to aid culturally sensitive mental health assessments, yet they also emphasize the need for ongoing refinement to mitigate inherent biases and enhance their clinical utility. © The Author(s) 2024.","Export Date: 31 March 2025; Cited By: 5 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,mental health application",10.1007/s41809-024-00151-9,"",,
rayyan-202744335,Gender bias in generative artificial intelligence text-to-image depiction of medical students,2024,,,Health Education Journal,,83,7,732-746,"Currie, G. and Currie, J. and Anderson, S. and Hewis, J.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202537917&doi=10.1177%2f00178969241274621&partnerID=40&md5=47a28833ae7e7ca8464eb19d0edb294d,,SAGE Publications Ltd,,"Introduction: In Australia, 54.3% of medical students are women yet they remain under-represented in stereotypical perspectives of medicine. While potentially transformative, generative artificial intelligence (genAI) has the potential for errors, misrepresentations and bias. GenAI text-to-image production could reinforce gender biases making it important to evaluate DALL-E 3 (the text-to-image genAI supported through ChatGPT) representations of Australian medical students. Method: In March 2024, DALL-E 3 was utilised via GPT-4 to generate a series of individual and group images of medical students, specifically Australian undergraduate medical students to eliminate potential confounders. Multiple iterations of images were generated using a variety of prompts. Collectively, 47 images were produced for evaluation of which 33 were individual characters and the remaining 14 images were comprised of multiple (5 to 67) characters. All images were independently analysed by three reviewers for apparent gender and skin tone. Consequently, 33 feature individuals were evaluated and a further 417 characters in groups were evaluated (N = 448). Discrepancies in responses were resolved by consensus. Results: Collectively (individual and group images), 58.8% (N = 258) of medical students were depicted as men, 39.9% (N = 175) as women, 92.0% (N = 404) with a light skin tone, 7.7% (N = 34) with mid skin tone and 0% with dark skin tone. The gender distribution was a statistically significant variation from that of actual Australian medical students for individual images, for group images and for collective images. Among the images of individual medical students (N = 25), DALL-E 3 generated 92% (N = 23) as men and 100% were of light skin tone (N = 25). Conclusion: This evaluation reveals the gender associated with genAI text-to-image generation using DALL-E 3 among Australian undergraduate medical students. Generated images included a disproportionately high proportion of white male medical students which is not representative of the diversity of medical students in Australia. The use of DALL-E 3 to produce depictions of medical students for education or promotion purposes should be done with caution. © The Author(s) 2024.","Export Date: 31 March 2025; Cited By: 2 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: not health data,No relevant data ,no direct health application",10.1177/00178969241274621,"",,
rayyan-202744373,Benchmarking AI in Mental Health: A Critical Examination of LLMs Across Key Performance and Ethical Metrics,2025,,,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,15328,,351-366,"Yuan, R. and Hao, W. and Yuan, C.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211760370&doi=10.1007%2f978-3-031-78104-9_24&partnerID=40&md5=4d92958167737ce0121f12850c2b2741,,Springer Science and Business Media Deutschland GmbH,,"The rapid advancement of artificial intelligence (AI) has led to an increasing application of Large Language Models (LLMs) in psychological counseling. This study focuses on a comprehensive evaluation of LLMs in this domain, moving beyond traditional case-based reasoning. We introduce a novel multi-agent LLM framework that enhances the analysis of psychological case interactions. Our approach involves expanding the Emotional First Aid dataset with diverse client backgrounds, enhancing its applicability and generalizability. A sophisticated user profile model, incorporating eight critical dimensions, is developed and applied within a multi-agent system to examine counseling scenarios. The system’s performance is extensively evaluated based on accuracy, robustness, consistency, and fairness. The findings reveal significant differences among LLMs in these areas, highlighting their strengths and limitations in psychological interventions. This research underscores the need for ongoing refinement in LLM applications to ensure equitable and reliable support in psychological counseling. The detailed results and methodologies are available on the GitHub platform for further academic scrutiny and development. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation,Did not assess bias",10.1007/978-3-031-78104-9_24,"",,
rayyan-202744391,Estimating prevalence of rare genetic disease diagnoses using electronic health records in a children's hospital,2024,,,Human Genetics and Genomics Advances,,5,4,,"Herr, K. and Lu, P. and Diamreyan, K. and Xu, H. and Mendonca, E. and Weaver, K.N. and Chen, J.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206220732&doi=10.1016%2fj.xhgg.2024.100341&partnerID=40&md5=aeb9c7244bf922d086d27959efb9998b,,Elsevier Inc.,,"Rare genetic diseases (RGDs) affect a significant number of individuals, particularly in pediatric populations. This study investigates the efficacy of identifying RGD diagnoses through electronic health records (EHRs) and natural language processing (NLP) tools, and analyzes the prevalence of identified RGDs for potential underdiagnosis at Cincinnati Children's Hospital Medical Center (CCHMC). EHR data from 659,139 pediatric patients at CCHMC were utilized. Diagnoses corresponding to RGDs in Orphanet were identified using rule-based and machine learning-based NLP methods. Manual evaluation assessed the precision of the NLP strategies, with 100 diagnosis descriptions reviewed for each method. The rule-based method achieved a precision of 97.5% (95% CI: 91.5%, 99.4%), while the machine-learning-based method had a precision of 73.5% (95% CI: 63.6%, 81.6%). A manual chart review of 70 randomly selected patients with RGD diagnoses confirmed the diagnoses in 90.3% (95% CI: 82.0%, 95.2%) of cases. A total of 37,326 pediatric patients were identified with 977 RGD diagnoses based on the rule-based method, resulting in a prevalence of 5.66% in this population. While a majority of the disorders showed a higher prevalence at CCHMC compared with Orphanet, some diseases, such as 1p36 deletion syndrome, indicated potential underdiagnosis. Analyses further uncovered disparities in RGD prevalence and age of diagnosis across gender and racial groups. This study demonstrates the utility of employing EHR data with NLP tools to systematically investigate RGD diagnoses in large cohorts. The identified disparities underscore the need for enhanced approaches to guarantee timely and accurate diagnosis and management of pediatric RGDs. © 2024 The Authors","Export Date: 31 March 2025; Cited By: 1 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: No LLM,no bias evaluation | USER-NOTES: {""Phoenix""=>[""disparities-> bias?""]}",10.1016/j.xhgg.2024.100341,"",,
rayyan-202744395,Application of ChatGPT as a support tool in the diagnosis and management of acute bacterial tonsillitis,2024,,,Health and Technology,,14,4,773-779,"Mayo-Yáñez, M. and González-Torres, L. and Saibene, A.M. and Allevi, F. and Vaira, L.A. and Maniaci, A. and Chiesa-Estomba, C.M. and Lechien, J.R.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189892794&doi=10.1007%2fs12553-024-00858-3&partnerID=40&md5=d420dde7d4e58d711c967235324544e7,,Springer Science and Business Media Deutschland GmbH,,"Introduction: Artificial intelligence (AI) is transforming medicine through techniques like machine learning and deep learning. AI aids diagnosis, enhances patient care, and streamlines healthcare systems. Despite potential benefits, challenges of bias and trust must be managed. Tonsillitis is a common otolaryngological condition with economic implications. The study assesses ChatGPT’s utility in the diagnosis management of bacterial tonsillitis, highlighting its potential for patient-professional interaction. Methods: 2 methods evaluated ChatGPT 3.5 diagnostic ability for tonsillitis: patient-written cases and specialist-created cases. The scenarios involved real patients and fictional cases, assessed by 15 otolaryngologists and 5 pediatricians. Variables included diagnosis accuracy, recommendations quality, and message count. Results: A total of 35 conversations were conducted. ChatGPT achieved accurate diagnoses in 100% of cases, with an average of 3.7 ± 1.1 chat entries for diagnosis. No significant difference existed between professional and patient scenarios (p = 0.977). Recommendations were categorized: appropriate (48.57%), incomplete (45.71%), inappropriate (5.71%), with no significant intergroup difference (p = 0.196). ChatGPT consistently advised consulting a doctor and exhibited expertise in guiding medical consultations. Conclusion: ChatGPT demonstrates promise in providing medical insights and general advice. Its diagnostic accuracy for tonsillitis is notable, but it relies on static data and lacks individual history assessment. ChatGPT shows potential for diagnostics in simpler cases like tonsillitis, but accuracy for complex conditions needs refinement. Further research is needed for validation and broader application. © The Author(s) under exclusive licence to International Union for Physical and Engineering Sciences in Medicine (IUPESM) 2024.","Export Date: 31 March 2025; Cited By: 2 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: Did not assess bias | USER-NOTES: {""Phoenix""=>[""professional and patient scenarios""]}",10.1007/s12553-024-00858-3,"",,
rayyan-202744400,Artificial intelligence meets medical expertise: evaluating GPT-4's proficiency in generating medical article abstracts,2024,,,Pamukkale Medical Journal,,17,4,756-762,"Sağtaş, E. and Ufuk, F. and Peker, H. and Yağcı, A.B.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207505677&doi=10.31362%2fpatd.1487575&partnerID=40&md5=082bc1974706d2f9116ee04c3f232b5a,,Pamukkale University,,"Purpose: The advent of large language models like GPT-4 has opened new possibilities in natural language processing, with potential applications in medical literature. This study assesses GPT-4's ability to generate medical abstracts. It compares their quality to original abstracts written by human authors, aiming to understand the effectiveness of artificial intelligence in replicating complex, professional writing tasks. Materials and methods: A total of 250 original research articles from five prominent radiology journals published between 2021 and 2023 were selected. The body of these articles, excluding the abstracts, was fed into GPT-4, which then generated new abstracts. Three experienced radiologists blindly and independently evaluated all 500 abstracts using a five-point Likert scale for quality and understandability. Statistical analysis included mean score comparison inter-rater reliability using Fleiss' Kappa and Bland-Altman plots to assess agreement levels between raters. Results: Analysis revealed no significant difference in the mean scores between original and GPT-4 generated abstracts. The inter-rater reliability yielded kappa values indicating moderate to substantial agreement: 0.497 between Observers 1 and 2, 0.753 between Observers 1 and 3, and 0.645 between Observers 2 and 3. Bland-Altman analysis showed a slight systematic bias but was within acceptable limits of agreement. Conclusion: The study demonstrates that GPT-4 can generate medical abstracts with a quality comparable to those written by human experts. This suggests a promising role for artificial intelligence in facilitating the abstract writing process and improving its quality. © 2024, Pamukkale University. All rights reserved.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: No relevant data ,no direct health application",10.31362/patd.1487575,"",,
rayyan-202744405,Enhancing psychiatric rehabilitation outcomes through a multimodal multitask learning model based on BERT and TabNet: An approach for personalized treatment and improved decision-making,2024,,,Psychiatry Research,,336,,,"Yang, H. and Zhu, D. and He, S. and Xu, Z. and Liu, Z. and Zhang, W. and Cai, J.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190335939&doi=10.1016%2fj.psychres.2024.115896&partnerID=40&md5=498580a18bf72e043d30e08d7a7ff5d5,,Elsevier Ireland Ltd,,"Evaluating the rehabilitation status of individuals with serious mental illnesses (SMI) necessitates a comprehensive analysis of multimodal data, including unstructured text records and structured diagnostic data. However, progress in the effective assessment of rehabilitation status remains limited. Our study develops a deep learning model integrating Bidirectional Encoder Representations from Transformers (BERT) and TabNet through a late fusion strategy to enhance rehabilitation prediction, including referral risk, dangerous behaviors, self-awareness, and medication adherence, in patients with SMI. BERT processes unstructured textual data, such as doctor's notes, whereas TabNet manages structured diagnostic information. The model's interpretability function serves to assist healthcare professionals in understanding the model's predictive decisions, improving patient care. Our model exhibited excellent predictive performance for all four tasks, with an accuracy exceeding 0.78 and an area under the curve of 0.70. In addition, a series of tests proved the model's robustness, fairness, and interpretability. This study combines multimodal and multitask learning strategies into a model and applies it to rehabilitation assessment tasks, offering a promising new tool that can be seamlessly integrated with the clinical workflow to support the provision of optimized patient care. © 2024","Export Date: 31 March 2025; Cited By: 5 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: fairness across demographic groups",10.1016/j.psychres.2024.115896,"",,
rayyan-202744414,Inherent Bias in Large Language Models: A Random Sampling Analysis,2024,,,Mayo Clinic Proceedings: Digital Health,,2,2,186-191,"Ayoub, N.F. and Balakrishnan, K. and Ayoub, M.S. and Barrett, T.F. and David, A.P. and Gray, S.T.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198118718&doi=10.1016%2fj.mcpdig.2024.03.003&partnerID=40&md5=91495dcdb55deacf718bf53e9b08e2c8,,Elsevier B.V.,,"There are mounting concerns regarding inherent bias, safety, and tendency toward misinformation of large language models (LLMs), which could have significant implications in health care. This study sought to determine whether generative artificial intelligence (AI)-based simulations of physicians making life-and-death decisions in a resource-scarce environment would demonstrate bias. Thirteen questions were developed that simulated physicians treating patients in resource-limited environments. Through a random sampling of simulated physicians using OpenAI's generative pretrained transformer (GPT-4), physicians were tasked with choosing only 1 patient to save owing to limited resources. This simulation was repeated 1000 times per question, representing 1000 unique physicians and patients each. Patients and physicians spanned a variety of demographic characteristics. All patients had similar a priori likelihood of surviving the acute illness. Overall, simulated physicians consistently demonstrated racial, gender, age, political affiliation, and sexual orientation bias in clinical decision-making. Across all demographic characteristics, physicians most frequently favored patients with similar demographic characteristics as themselves, with most pairwise comparisons showing statistical significance (P<.05). Nondescript physicians favored White, male, and young demographic characteristics. The male doctor gravitated toward the male, White, and young, whereas the female doctor typically preferred female, young, and White patients. In addition to saving patients with their own political affiliation, Democratic physicians favored Black and female patients, whereas Republicans preferred White and male demographic characteristics. Heterosexual and gay/lesbian physicians frequently saved patients of similar sexual orientation. Overall, publicly available chatbot LLMs demonstrate significant biases, which may negatively impact patient outcomes if used to support clinical care decisions without appropriate precautions. © 2024 The Authors","Export Date: 31 March 2025; Cited By: 9 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: age,sex,race ethnicity,sexual orientation,political affiliation",10.1016/j.mcpdig.2024.03.003,"",,
rayyan-202744422,A roadmap to artificial intelligence (AI): Methods for designing and building AI ready data to promote fairness,2024,,,Journal of Biomedical Informatics,,154,,,"Kidwai-Khan, F. and Wang, R. and Skanderson, M. and Brandt, C.A. and Fodeh, S. and Womack, J.A.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193075830&doi=10.1016%2fj.jbi.2024.104654&partnerID=40&md5=1246200b5e8d539e4292ad78c9023bfb,,Academic Press Inc.,,"Objectives: We evaluated methods for preparing electronic health record data to reduce bias before applying artificial intelligence (AI). Methods: We created methods for transforming raw data into a data framework for applying machine learning and natural language processing techniques for predicting falls and fractures. Strategies such as inclusion and reporting for multiple races, mixed data sources such as outpatient, inpatient, structured codes, and unstructured notes, and addressing missingness were applied to raw data to promote a reduction in bias. The raw data was carefully curated using validated definitions to create data variables such as age, race, gender, and healthcare utilization. For the formation of these variables, clinical, statistical, and data expertise were used. The research team included a variety of experts with diverse professional and demographic backgrounds to include diverse perspectives. Results: For the prediction of falls, information extracted from radiology reports was converted to a matrix for applying machine learning. The processing of the data resulted in an input of 5,377,673 reports to the machine learning algorithm, out of which 45,304 were flagged as positive and 5,332,369 as negative for falls. Processed data resulted in lower missingness and a better representation of race and diagnosis codes. For fractures, specialized algorithms extracted snippets of text around keywork “femoral” from dual x-ray absorptiometry (DXA) scans to identify femoral neck T-scores that are important for predicting fracture risk. The natural language processing algorithms yielded 98% accuracy and 2% error rate The methods to prepare data for input to artificial intelligence processes are reproducible and can be applied to other studies. Conclusion: The life cycle of data from raw to analytic form includes data governance, cleaning, management, and analysis. When applying artificial intelligence methods, input data must be prepared optimally to reduce algorithmic bias, as biased output is harmful. Building AI-ready data frameworks that improve efficiency can contribute to transparency and reproducibility. The roadmap for the application of AI involves applying specialized techniques to input data, some of which are suggested here. This study highlights data curation aspects to be considered when preparing data for the application of artificial intelligence to reduce bias. © 2024 Elsevier Inc.","Export Date: 31 March 2025; Cited By: 4 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: No LLM,not LLM",10.1016/j.jbi.2024.104654,"",,
rayyan-202744425,GPT-Driven Radiology Report Generation with Fine-Tuned Llama 3,2024,,,Bioengineering,,11,10,,"Voinea, Ș.-V. and Mămuleanu, M. and Teică, R.V. and Florescu, L.M. and Selișteanu, D. and Gheonea, I.A.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207676347&doi=10.3390%2fbioengineering11101043&partnerID=40&md5=1334b7f6364112b50ad76a14d5414a2f,,Multidisciplinary Digital Publishing Institute (MDPI),,"The integration of deep learning into radiology has the potential to enhance diagnostic processes, yet its acceptance in clinical practice remains limited due to various challenges. This study aimed to develop and evaluate a fine-tuned large language model (LLM), based on Llama 3-8B, to automate the generation of accurate and concise conclusions in magnetic resonance imaging (MRI) and computed tomography (CT) radiology reports, thereby assisting radiologists and improving reporting efficiency. A dataset comprising 15,000 radiology reports was collected from the University of Medicine and Pharmacy of Craiova’s Imaging Center, covering a diverse range of MRI and CT examinations made by four experienced radiologists. The Llama 3-8B model was fine-tuned using transfer-learning techniques, incorporating parameter quantization to 4-bit precision and low-rank adaptation (LoRA) with a rank of 16 to optimize computational efficiency on consumer-grade GPUs. The model was trained over five epochs using an NVIDIA RTX 3090 GPU, with intermediary checkpoints saved for monitoring. Performance was evaluated quantitatively using Bidirectional Encoder Representations from Transformers Score (BERTScore), Recall-Oriented Understudy for Gisting Evaluation (ROUGE), Bilingual Evaluation Understudy (BLEU), and Metric for Evaluation of Translation with Explicit Ordering (METEOR) metrics on a held-out test set. Additionally, a qualitative assessment was conducted, involving 13 independent radiologists who participated in a Turing-like test and provided ratings for the AI-generated conclusions. The fine-tuned model demonstrated strong quantitative performance, achieving a BERTScore F1 of 0.8054, a ROUGE-1 F1 of 0.4998, a ROUGE-L F1 of 0.4628, and a METEOR score of 0.4282. In the human evaluation, the artificial intelligence (AI)-generated conclusions were preferred over human-written ones in approximately 21.8% of cases, indicating that the model’s outputs were competitive with those of experienced radiologists. The average rating of the AI-generated conclusions was 3.65 out of 5, reflecting a generally favorable assessment. Notably, the model maintained its consistency across various types of reports and demonstrated the ability to generalize to unseen data. The fine-tuned Llama 3-8B model effectively generates accurate and coherent conclusions for MRI and CT radiology reports. By automating the conclusion-writing process, this approach can assist radiologists in reducing their workload and enhancing report consistency, potentially addressing some barriers to the adoption of deep learning in clinical practice. The positive evaluations from independent radiologists underscore the model’s potential utility. While the model demonstrated strong performance, limitations such as dataset bias, limited sample diversity, a lack of clinical judgment, and the need for large computational resources require further refinement and real-world validation. Future work should explore the integration of such models into clinical workflows, address ethical and legal considerations, and extend this approach to generate complete radiology reports. © 2024 by the authors.","Export Date: 31 March 2025; Cited By: 1 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation,Did not assess bias",10.3390/bioengineering11101043,"",,
rayyan-202744440,Early Detection of Mental Health Crises through Artifical-Intelligence-Powered Social Media Analysis: A Prospective Observational Study,2024,,,Journal of Personalized Medicine,,14,9,,"Mansoor, M.A. and Ansari, K.H.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205264445&doi=10.3390%2fjpm14090958&partnerID=40&md5=ae9344004ad68a51d4f695afb107702d,,Multidisciplinary Digital Publishing Institute (MDPI),,"Background: The early detection of mental health crises is crucial for timely interventions and improved outcomes. This study explores the potential of artificial intelligence (AI) in analyzing social media data to identify early signs of mental health crises. Methods: We developed a multimodal deep learning model integrating natural language processing and temporal analysis techniques. The model was trained on a diverse dataset of 996,452 social media posts in multiple languages (English, Spanish, Mandarin, and Arabic) collected from Twitter, Reddit, and Facebook over 12 months. Its performance was evaluated using standard metrics and validated against expert psychiatric assessments. Results: The AI model demonstrated a high level of accuracy (89.3%) in detecting early signs of mental health crises, with an average lead time of 7.2 days before human expert identification. Performance was consistent across languages (F1 scores: 0.827–0.872) and platforms (F1 scores: 0.839–0.863). Key digital markers included linguistic patterns, behavioral changes, and temporal trends. The model showed varying levels of accuracy for different crisis types: depressive episodes (91.2%), manic episodes (88.7%), suicidal ideation (93.5%), and anxiety crises (87.3%). Conclusions: AI-powered analysis of social media data shows promise for the early detection of mental health crises across diverse linguistic and cultural contexts. However, ethical challenges, including privacy concerns, potential stigmatization, and cultural biases, need careful consideration. Future research should focus on longitudinal outcome studies, ethical integration of the method with existing mental health services, and developing personalized, culturally sensitive models. © 2024 by the authors.","Export Date: 31 March 2025; Cited By: 1 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: language,mental health application",10.3390/jpm14090958,"",,
rayyan-202744456,Mixed methods assessment of the influence of demographics on medical advice of ChatGPT,2024,,,Journal of the American Medical Informatics Association,,31,9,2002-2009,"Andreadis, K. and Newman, D.R. and Twan, C. and Shunk, A. and Mann, D.M. and Stevens, E.R.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201762493&doi=10.1093%2fjamia%2focae086&partnerID=40&md5=150fdcaacb428f91823421004592a46c,,Oxford University Press,,"Objectives: To evaluate demographic biases in diagnostic accuracy and health advice between generative artificial intelligence (AI) (ChatGPT GPT-4) and traditional symptom checkers like WebMD. Materials and Methods: Combination symptom and demographic vignettes were developed for 27 most common symptom complaints. Standardized prompts, written from a patient perspective, with varying demographic permutations of age, sex, and race/ethnicity were entered into ChatGPT (GPT-4) between July and August 2023. In total, 3 runs of 540 ChatGPT prompts were compared to the corresponding WebMD Symptom Checker output using a mixed-methods approach. In addition to diagnostic correctness, the associated text generated by ChatGPT was analyzed for readability (using Flesch-Kincaid Grade Level) and qualitative aspects like disclaimers and demographic tailoring. Results: ChatGPT matched WebMD in 91% of diagnoses, with a 24% top diagnosis match rate. Diagnostic accuracy was not significantly different across demographic groups, including age, race/ethnicity, and sex. ChatGPT's urgent care recommendations and demographic tailoring were presented significantly more to 75-year-olds versus 25-year-olds (P <. 01) but were not statistically different among race/ethnicity and sex groups. The GPT text was suitable for college students, with no significant demographic variability. Discussion: The use of non-health-tailored generative AI, like ChatGPT, for simple symptom-checking functions provides comparable diagnostic accuracy to commercially available symptom checkers and does not demonstrate significant demographic bias in this setting. The text accompanying differential diagnoses, however, suggests demographic tailoring that could potentially introduce bias. Conclusion: These results highlight the need for continued rigorous evaluation of AI-driven medical platforms, focusing on demographic biases to ensure equitable care. © 2024 The Author(s). Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved.","Export Date: 31 March 2025; Cited By: 7 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: age,sex,race ethnicity,fairness across demographic groups",10.1093/jamia/ocae086,"",,
rayyan-202744459,Generative artificial intelligence versus clinicians: Who diagnoses multiple sclerosis faster and with greater accuracy?,2024,,,Multiple Sclerosis and Related Disorders,,90,,,"Patel, M.A. and Villalobos, F. and Shan, K. and Tardo, L.M. and Horton, L.A. and Sguigna, P.V. and Blackburn, K.M. and Munoz, S.B. and Moog, T.M. and Smith, A.D. and Burgess, K.W. and McCreary, M. and Okuda, D.T.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201157509&doi=10.1016%2fj.msard.2024.105791&partnerID=40&md5=e2ee132351efe548223aa7b911630766,,Elsevier B.V.,,"Background: Those receiving the diagnosis of multiple sclerosis (MS) over the next ten years will predominantly be part of Generation Z (Gen Z). Recent observations within our clinic suggest that younger people with MS utilize online generative artificial intelligence (AI) platforms for personalized medical advice prior to their first visit with a specialist in neuroimmunology. The use of such platforms is anticipated to increase given the technology driven nature, desire for instant communication, and cost-conscious nature of Gen Z. Our objective was to determine if ChatGPT (Generative Pre-trained Transformer) could diagnose MS in individuals earlier than their clinical timeline, and to assess if the accuracy differed based on age, sex, and race/ethnicity. Methods: People with MS between 18 and 59 years of age were studied. The clinical timeline for people diagnosed with MS was retrospectively identified and simulated using ChatGPT-3.5 (GPT-3.5). Chats were conducted using both actual and derivatives of their age, sex, and race/ethnicity to test diagnostic accuracy. A Kaplan-Meier survival curve was estimated for time to diagnosis, clustered by subject. The p-value testing for differences in time to diagnosis was accomplished using a general Wilcoxon test. Logistic regression (subject-specific intercept) was used to capture intra-subject correlation to test the accuracy prior to and after the inclusion of MRI data. Results: The study cohort included 100 unique people with MS. Of those, 50 were members of Gen Z (38 female; 22 White; mean age at first symptom was 20.6 years (y) (standard deviation (SD)=2.2y)), and 50 were non-Gen Z (34 female; 27 White; mean age at first symptom was 37.0y (SD=10.4y)). In addition, a total of 529 people that represented digital simulations of the original cohort of 100 people (333 female; 166 White; 136 Black/African American; 107 Asian; 120 Hispanic, mean age at first symptom was 31.6y (SD=12.4y)) were generated allowing for 629 scripted conversations to be analyzed. The estimated median time to diagnosis in clinic was significantly longer at 0.35y (95% CI=[0.28, 0.48]) versus that by ChatGPT at 0.08y (95% CI=[0.04, 0.24]) (p<0.0001). There was no difference in the diagnostic accuracy between ages and by race/ethnicity prior to the inclusion of MRI data. However, prior to including the MRI data, males had a 47% less likely chance of a correct diagnosis relative to females (p=0.05). Post-MRI data inclusion within GPT-3.5, the odds of an accurate diagnosis was 4.0-fold greater for Gen Z participants, relative to non-Gen Z participants (p=0.01) with the diagnostic accuracy being 68% less in males relative to females (p=0.009), and 75% less for White subjects, relative to non-White subjects (p=0.0004). Conclusion: Although generative AI platforms enable rapid information access and are not principally designed for use in healthcare, an increase in use by Gen Z is anticipated. However, the obtained responses may not be generalizable to all users and bias may exist in select groups. © 2024 Elsevier B.V.","Export Date: 31 March 2025; Cited By: 4 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: age,sex,race ethnicity",10.1016/j.msard.2024.105791,"",,
rayyan-202744465,Embedded values-like shape ethical reasoning of large language models on primary care ethical dilemmas,2024,,,Heliyon,,10,18,,"Hadar-Shoval, D. and Asraf, K. and Shinan-Altman, S. and Elyoseph, Z. and Levkovich, I.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204450486&doi=10.1016%2fj.heliyon.2024.e38056&partnerID=40&md5=ab0681427c02c2280d950ad20bca43a7,,Elsevier Ltd,,"Objective: This article uses the framework of Schwartz's values theory to examine whether the embedded values-like profile within large language models (LLMs) impact ethical decision-making dilemmas faced by primary care. It specifically aims to evaluate whether each LLM exhibits a distinct values-like profile, assess its alignment with general population values, and determine whether latent values influence clinical recommendations. Methods: The Portrait Values Questionnaire-Revised (PVQ-RR) was submitted to each LLM (Claude, Bard, GPT-3.5, and GPT-4) 20 times to ensure reliable and valid responses. Their responses were compared to a benchmark derived from a diverse international sample consisting of over 53,000 culturally diverse respondents who completed the PVQ-RR. Four vignettes depicting prototypical professional quandaries involving conflicts between competing values were presented to the LLMs. The option selected by each LLM and the strength of its recommendation were evaluated to determine if underlying values-like impact output. Results: Each LLM demonstrated a unique values-like profile. Universalism and self-direction were prioritized, while power and tradition were assigned less importance than population benchmarks, suggesting potential Western-centric biases. Four clinical vignettes involving value conflicts were presented to the LLMs. Preliminary indications suggested that embedded values-like influence recommendations. Significant variances in confidence strength regarding chosen recommendations materialized between models, proposing that further vetting is required before the LLMs can be relied on as judgment aids. However, the overall selection of preferences aligned with intrinsic value hierarchies. Conclusion: The distinct intrinsic values-like embedded within LLMs shape ethical decision-making, which carries implications for their integration in primary care settings serving diverse populations. For context-appropriate, equitable delivery of AI-assisted healthcare globally it is essential that LLMs are tailored to align with cultural outlooks. © 2024","Export Date: 31 March 2025; Cited By: 2 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no direct health application",10.1016/j.heliyon.2024.e38056,"",,
rayyan-202744471,Automating Clinical Trial Eligibility Screening: Quantitative Analysis of GPT Models versus Human Expertise,2024,,,ACM International Conference Proceeding Series,,,,626-632,"Devi, A. and Uttrani, S. and Singla, A. and Jha, S. and Dasgupta, N. and Natarajan, S. and Punekar, R.S. and Pickett, L.A. and Dutt, V.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198053910&doi=10.1145%2f3652037.3663922&partnerID=40&md5=aafdf6707c1031f98e079dc032ee129b,,Association for Computing Machinery,,"Objective: This study quantitatively assesses the performance of GPT model in classifying patient eligibility for clinical trials, aiming to minimize the need for expert clinical judgment and to cut down on related expenses. Data: Ten US NSCLC drug-only interventional clinical trials were selected from clinicaltrials.gov. For each clinical trial, 10 patient profiles were manually created by an epidemiologist using case presentations from medical journals. The dataset included two sets of adult patient profiles (50 eligible patients and 50 non-eligible patients for 100 patients) with a range of complexities, from complex to simple cases. The 100-case dataset was then analyzed in a GPT-3.5-turbo large language model to generate eligibility predictions against human epidemiologists. Analysis: Different data tuning scenarios were evaluated, focusing on the model's ability to replicate the human expert performance in determining patient eligibility. The tuning scenarios included no cases (zero-shot), 50 cases, and 80 cases. Results: GPT-3.5 showed a high accuracy rate during the test, with 95% test accuracy in scenarios without tuning and 100% test accuracy in scenarios with 80-case tuning (testing was done on the remaining 20 cases). GPT-3.5's accuracy showed more variability, scoring 82% test accuracy with tuning on 50 cases and dropping to 79% test accuracy without tuning on 100 cases. Comparisons of model and human evaluations across male and female patient profiles indicated no gender bias, as the GPT-3.5 model performed equivalently to human assessments. Conclusion: The GPT-3.5 models demonstrated a high degree of accuracy and an unbiased approach to patient classification when compared to human experts. These results suggest that GPT models could serve as a cost-effective and impartial tool for patient screening in clinical trials. Further research with larger and more diverse datasets is recommended to confirm these findings and explore LLMs' scalability in clinical trial settings.  © 2024 ACM.","Export Date: 31 March 2025; Cited By: 1 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex",10.1145/3652037.3663922,"",,
rayyan-202744494,Can AI Relate: Testing Large Language Model Response for Mental Health Support,2024,,,"EMNLP 2024 - 2024 Conference on Empirical Methods in Natural Language Processing, Findings of EMNLP 2024",,,,2206-2221,"Gabriel, S. and Puri, I. and Xu, X. and Malgaroli, M. and Ghassemi, M.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216134970&partnerID=40&md5=67ac5e4ac9518acccb94de1273655e68,,Association for Computational Linguistics (ACL),,"Large language models (LLMs) are already being piloted for clinical use in hospital systems like NYU Langone, Dana-Farber and the NHS. A proposed deployment use case is psychotherapy, where a LLM-powered chatbot can treat a patient undergoing a mental health crisis. Deployment of LLMs for mental health response could hypothetically broaden access to psychotherapy and provide new possibilities for personalizing care. However, recent high-profile failures, like damaging dieting advice offered by the Tessa chatbot to patients with eating disorders, have led to doubt about their reliability in high-stakes and safety-critical settings. In this work, we develop an evaluation framework for determining whether LLM response is a viable and ethical path forward for the automation of mental health treatment. Our framework measures equity in empathy and adherence of LLM responses to motivational interviewing theory. Using human evaluation with trained clinicians and automatic quality-of-care metrics grounded in psychology research, we compare the responses provided by peer-to-peer responders to those provided by a state-of-the-art LLM. We show that LLMs like GPT-4 use implicit and explicit cues to infer patient demographics like race. We then show that there are statistically significant discrepancies between patient subgroups: Responses to Black posters consistently have lower empathy than for any other demographic group (2%-13% lower than the control group). Promisingly, we do find that the manner in which responses are generated significantly impacts the quality of the response. We conclude by proposing safety guidelines for the potential deployment of LLMs for mental health response. © 2024 Association for Computational Linguistics.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: race ethnicity,mental health application,fairness across demographic groups | USER-NOTES: {""Phoenix""=>[""health data?""]}",,"",,
rayyan-202744496,Uncovering Judgment Biases in Emergency Triage: A Public Health Approach Based on Large Language Models,2024,,,Proceedings of Machine Learning Research,,259,,420-439,"Guerra-Adames, A. and Avalos-Fernandez, M. and Doremus, O. and Gil-Jardiné, C. and Lagarde, E.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219199408&partnerID=40&md5=98f4d9a324f76191f8ce77a6cfb15d54,,ML Research Press,,"Judgment biases in emergency triage can adversely affect patient outcomes. This study examines sex/gender biases using four advanced language models fine-tuned on real-world emergency department data. We introduce a novel approach based on the testing method, commonly used in hiring bias detection, by automatically altering triage notes to change patient sex references. Results indicate a significant bias: female patients are assigned lower severity ratings than male patients with identical clinical conditions. This bias is more pronounced with female nurses or when patients report higher pain levels but diminishes with increased nurse experience. Identifying these biases can inform interventions such as enhanced training, protocol updates, and machine learning tools to support clinical decision-making. © 2024 A. Guerra-Adames, M. Avalos-Fernandez, O. Doremus, C. Gil-Jardiné & E. Lagarde.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex",,"",,
rayyan-202744502,"“What Did You Say, ChatGPT?” The Use of AI in Black Women’s HIV Self-Education: An Inductive Qualitative Data Analysis",2024,,,Journal of the Association of Nurses in AIDS Care,,35,3,294-302,"Chandler, R.D. and Warner, S. and Aidoo-Frimpong, G. and Wells, J.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192382594&doi=10.1097%2fJNC.0000000000000468&partnerID=40&md5=8f1aba70413645b1f72d73c5cdf29f68,,Lippincott Williams and Wilkins,,"The emergence of widely accessible artificial intelligence (AI) chatbots such as ChatGPT presents unique opportunities and challenges in public health self-education. This study examined simulations with ChatGPT for its use in public education of sexual health of Black women, specifically in HIV prevention and/or HIV PrEP use. The research questions guiding the study are as follows: (a) does the information ChatGPT offers about HIV prevention and HIV PrEP differ based on stated race? and (b) how could this relatively new platform inform public health education of Black women educating themselves about sexual health behaviors, diagnoses, and treatments? In addressing these questions, this study also uncovered notable differences in ChatGPT’s tone when responding to users based on race. This study described valuable insights that can inform health care professionals, educators, and policymakers, ultimately advancing the cause of sexual health equity for Black women and underscoring the paradigm-shifting potential of AI in the field of public health education. Copyright © 2024 Association of Nurses in AIDS Care.","Export Date: 31 March 2025; Cited By: 2 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: race ethnicity",10.1097/JNC.0000000000000468,"",,
rayyan-202744520,DiversityMedQA: A Benchmark for Assessing Demographic Biases in Medical Diagnosis using Large Language Models,2024,,,"NLP4PI 2024 - 3rd Workshop on NLP for Positive Impact, Proceedings of the Workshop",,,,334-348,"Rawat, R. and McBride, H. and Ghosh, R. and Nirmal, D. and Moon, J. and Alamuri, D. and O’Brien, S. and Zhu, K.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216923537&partnerID=40&md5=8bc68caff877e13652a3f2e3e6e476cd,,Association for Computational Linguistics (ACL),,"As large language models (LLMs) gain traction in healthcare, concerns about their susceptibility to demographic biases are growing. We introduce DiversityMedQA1, a novel benchmark designed to assess LLM responses to medical queries across diverse patient demographics, such as gender and ethnicity. By perturbing questions from the MedQA dataset, which comprises of medical board exam questions, we created a benchmark that captures the nuanced differences in medical diagnosis across varying patient profiles. To ensure that our perturbations did not alter the clinical outcomes, we implemented a filtering strategy to validate each perturbation, so that any performance discrepancies would be indicative of bias. Our findings reveal notable discrepancies in model performance when tested against these demographic variations. By releasing DiversityMedQA, we provide a resource for evaluating and mitigating demographic bias in LLM medical diagnoses. © 2024 Association for Computational Linguistics.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,race ethnicity | RAYYAN-EXCLUSION-REASONS: No relevant data | USER-NOTES: {""Phoenix""=>[""read""]}",,"",,
rayyan-202744522,Debias-CLR: A Contrastive Learning Based Debiasing Method for Algorithmic Fairness in Healthcare Applications,2024,,,"Proceedings - 2024 IEEE International Conference on Big Data, BigData 2024",,,,6411-6419,"Agarwal, A. and Banerjee, T. and Romine, W.L. and Cajita, M.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218058674&doi=10.1109%2fBigData62323.2024.10825827&partnerID=40&md5=d28160e2992645c6e5456e8fdc9e1df8,,Institute of Electrical and Electronics Engineers Inc.,,"Artificial intelligence based predictive models trained on the clinical notes of patients can be demographically biased, often influenced by the demographic distribution of the training data. This could lead to adverse healthcare disparities in predicting outcomes like length of stay of the patients. To avoid such possibilities, it is necessary to mitigate the demographic biases within these models so that the model predicts outcomes for individual patients in a fair manner. We proposed an implicit in-processing debiasing method to combat disparate treatment which occurs when the machine learning model predict different outcomes for individuals based on the sensitive attributes like gender, ethnicity, race, and likewise. For this purpose, we used clinical notes of heart failure patients and used diagnostic codes, procedure reports and physiological vitals of the patients. We used Clinical Bidirectional Encoder Representations from Transformers (Clinical BERT) to obtain feature embeddings within the diagnostic codes and procedure reports, and Long Short-Term Memory (LSTM) autoencoders to obtain feature embeddings within the physiological vitals. Then, we trained two separate deep learning contrastive learning frameworks, one for gender and the other for ethnicity to obtain debiased representations within those demographic traits. We called this debiasing framework as Debias-CLR. We leveraged clinical phenotypes of the patients identified in the diagnostic codes and procedure reports in the previous study to measure the fairness statistically. We found that Debias-CLR was able to reduce the Single-Category Word Embedding Association Test (SC-WEAT) effect size score when debiasing for gender from 0.8 to 0.3 and from 0.4 to 0.2 while using clinical phenotypes in the diagnostic codes and procedure reports respectively as targets. Similarly, after debiasing for ethnicity, the SC-WEAT effect size score reduced from 1 to 0.5 and from -1 to 0.3 in an opposite bias direction while using clinical phenotypes in the diagnostic codes and procedure reports respectively as targets. We further found that in order to obtain fair representations in the embedding space using Debias-CLR, the accuracy of the predictive models on downstream tasks like predicting length of stay of the patients did not get reduced as compared to using the un-debiased counterparts for training the predictive models. Hence, we conclude that our proposed approach, Debias-CLR is fair and representative in mitigating demographic biases and can reduce health disparities by making fair predictions for the underrepresented populations. © 2024 IEEE.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,race ethnicity | RAYYAN-EXCLUSION-REASONS: Did not assess bias | USER-NOTES: {""Phoenix""=>[""read for proposal""]}",10.1109/BigData62323.2024.10825827,"",,
rayyan-202744532,Generative artificial intelligence and non-pharmacological bias: an experimental study on cancer patient sexual health communications,2024,,,BMJ Health and Care Informatics,,31,1,,"Hanai, A. and Ishikawa, T. and Kawauchi, S. and Iida, Y. and Kawakami, E.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190220384&doi=10.1136%2fbmjhci-2023-100924&partnerID=40&md5=6333ef67bff4252243a9a78b15503aa5,,BMJ Publishing Group,,"Objectives The objective of this study was to explore the feature of generative artificial intelligence (AI) in asking sexual health among cancer survivors, which are often challenging for patients to discuss. Methods We employed the Generative Pre-trained Transformer-3.5 (GPT) as the generative AI platform and used DocsBot for citation retrieval (June 2023). A structured prompt was devised to generate 100 questions from the AI, based on epidemiological survey data regarding sexual difficulties among cancer survivors. These questions were submitted to Bot1 (standard GPT) and Bot2 (sourced from two clinical guidelines). Results No censorship of sexual expressions or medical terms occurred. Despite the lack of reflection on guideline recommendations, ‘consultation’ was significantly more prevalent in both bots’ responses compared with pharmacological interventions, with ORs of 47.3 (p<0.001) in Bot1 and 97.2 (p<0.001) in Bot2. Discussion Generative AI can serve to provide health information on sensitive topics such as sexual health, despite the potential for policy-restricted content. Responses were biased towards non-pharmacological interventions, which is probably due to a GPT model designed with the’s prohibition policy on replying to medical topics. This shift warrants attention as it could potentially trigger patients’ expectations for non-pharmacological interventions. © Author(s) (or their employer(s)) 2024.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: wrong publication type | USER-NOTES: {""Phoenix""=>[""biased towards non-pharmacological intervention""]}",10.1136/bmjhci-2023-100924,"",,
rayyan-202744535,Can LLMs Replace Clinical Doctors? Exploring Bias in Disease Diagnosis by Large Language Models,2024,,,"EMNLP 2024 - 2024 Conference on Empirical Methods in Natural Language Processing, Findings of EMNLP 2024",,,,13914-13935,"Zhao, Y. and Wang, H. and Liu, Y. and Suhuang, W. and Wu, X. and Zheng, Y.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217616679&partnerID=40&md5=a9f3c232989c1947702a8c7691f63c87,,Association for Computational Linguistics (ACL),,"The bias of disease prediction in Large Language Models (LLMs) is a critical yet underexplored issue, with potential implications for healthcare outcomes and equity. As LLMs increasingly find applications in healthcare, understanding and addressing their biases becomes paramount. This study focuses on this crucial topic, investigating the bias of disease prediction in models such as GPT-4, ChatGPT, and Qwen1.5-72b across gender, age range, and disease judgment behaviors.1 Utilizing a comprehensive real-clinical health record dataset of over 330,000 entries, we uncover that all three models exhibit distinct biases, indicating a pervasive issue of unfairness. To measure this, we introduce a novel metric-the diagnosis bias score, which reflects the ratio of prediction numbers to label numbers. Our in-depth analysis, based on this score, sheds light on the inherent biases in these models. In response to these findings, we propose a simple yet effective prompt-based solution to alleviate the observed bias in disease prediction with LLMs. This research underscores the importance of fairness in AI, particularly in healthcare applications, and offers a practical approach to enhance the equity of disease prediction models. © 2024 Association for Computational Linguistics.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: age,sex | USER-NOTES: {""Phoenix""=>[""read""]}",,"",,
rayyan-202744540,HealAI: A Healthcare LLM for Effective Medical Documentation,2024,,,WSDM 2024 - Proceedings of the 17th ACM International Conference on Web Search and Data Mining,,,,1167-1168,"Goyal, S. and Rastogi, E. and Rajagopal, S.P. and Yuan, D. and Zhao, F. and Chintagunta, J. and Naik, G. and Ward, J.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191548394&doi=10.1145%2f3616855.3635739&partnerID=40&md5=f7489957a0152120eb9a85f60fbd383d,,"Association for Computing Machinery, Inc",,"Since the advent of LLM's like GPT4 everyone in various industries has been trying to harness their power. Healthcare is an industry where this is a specifically challenging problem due to the high accuracy requirements. Prompt Engineering is a common technique used to design instructions for model responses, however, its challenges lie in the fact that the generic models may not be trained to accurately execute these specific tasks. We will present our journey of developing a cost-effective medical LLM, surpassing GPT4 in medical note-writing tasks. We'll touch upon our trials with medical prompt engineering, GPT4's limitations, and training an optimized LLM for specific medical tasks. We'll showcase multiple comparisons on model sizes, training data, and pipeline designs that enabled us to outperform GPT4 with smaller models, maintaining precision, reducing biases, preventing hallucinations, and enhancing note-writing style. © 2024 Owner/Author.","Export Date: 31 March 2025; Cited By: 8 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: wrong publication type | USER-NOTES: {""Phoenix""=>[""empirical? unclear""]}",10.1145/3616855.3635739,"",,
rayyan-202744566,"Identifying social determinants of health from clinical narratives: A study of performance, documentation ratio, and potential bias",2024,,,Journal of Biomedical Informatics,,153,,,"Yu, Z. and Peng, C. and Yang, X. and Dang, C. and Adekkanattu, P. and Gopal Patra, B. and Peng, Y. and Pathak, J. and Wilson, D.L. and Chang, C.-Y. and Lo-Ciganic, W.-H. and George, T.J. and Hogan, W.R. and Guo, Y. and Bian, J. and Wu, Y.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190333397&doi=10.1016%2fj.jbi.2024.104642&partnerID=40&md5=4d6984178bc10fc46833a54d6d45aec8,,Academic Press Inc.,,"Objective: To develop a natural language processing (NLP) package to extract social determinants of health (SDoH) from clinical narratives, examine the bias among race and gender groups, test the generalizability of extracting SDoH for different disease groups, and examine population-level extraction ratio. Methods: We developed SDoH corpora using clinical notes identified at the University of Florida (UF) Health. We systematically compared 7 transformer-based large language models (LLMs) and developed an open-source package – SODA (i.e., SOcial DeterminAnts) to facilitate SDoH extraction from clinical narratives. We examined the performance and potential bias of SODA for different race and gender groups, tested the generalizability of SODA using two disease domains including cancer and opioid use, and explored strategies for improvement. We applied SODA to extract 19 categories of SDoH from the breast (n = 7,971), lung (n = 11,804), and colorectal cancer (n = 6,240) cohorts to assess patient-level extraction ratio and examine the differences among race and gender groups. Results: We developed an SDoH corpus using 629 clinical notes of cancer patients with annotations of 13,193 SDoH concepts/attributes from 19 categories of SDoH, and another cross-disease validation corpus using 200 notes from opioid use patients with 4,342 SDoH concepts/attributes. We compared 7 transformer models and the GatorTron model achieved the best mean average strict/lenient F1 scores of 0.9122 and 0.9367 for SDoH concept extraction and 0.9584 and 0.9593 for linking attributes to SDoH concepts. There is a small performance gap (∼4%) between Males and Females, but a large performance gap (>16 %) among race groups. The performance dropped when we applied the cancer SDoH model to the opioid cohort; fine-tuning using a smaller opioid SDoH corpus improved the performance. The extraction ratio varied in the three cancer cohorts, in which 10 SDoH could be extracted from over 70 % of cancer patients, but 9 SDoH could be extracted from less than 70 % of cancer patients. Individuals from the White and Black groups have a higher extraction ratio than other minority race groups. Conclusions: Our SODA package achieved good performance in extracting 19 categories of SDoH from clinical narratives. The SODA package with pre-trained transformer models is available at https://github.com/uf-hobi-informatics-lab/SODA_Docker. © 2024 Elsevier Inc.","Export Date: 31 March 2025; Cited By: 3 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,race ethnicity | USER-NOTES: {""Phoenix""=>[""read""]}",10.1016/j.jbi.2024.104642,"",,
rayyan-202744567,Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries,2024,,,WWW 2024 - Proceedings of the ACM Web Conference,,,,2627-2638,"Jin, Y. and Chandra, M. and Verma, G. and Hu, Y. and De Choudhury, M. and Kumar, S.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194097866&doi=10.1145%2f3589334.3645643&partnerID=40&md5=aa9865c131199d475475065b608064c8,,"Association for Computing Machinery, Inc",,"Large language models (LLMs) are transforming the ways the general public accesses and consumes information. Their influence is particularly pronounced in pivotal sectors like healthcare, where lay individuals are increasingly appropriating LLMs as conversational agents for everyday queries. While LLMs demonstrate impressive language understanding and generation proficiencies, concerns regarding their safety remain paramount in these high-stake domains. Moreover, the development of LLMs is disproportionately focused on English. It remains unclear how these LLMs perform in the context of non-English languages, a gap that is critical for ensuring equity in the real-world use of these systems. This paper provides a framework to investigate the effectiveness of LLMs as multi-lingual dialogue systems for healthcare queries. Our empirically-derived framework XlingEval focuses on three fundamental criteria for evaluating LLM responses to naturalistic human-authored health-related questions: correctness, consistency, and verifiability. Through extensive experiments on four major global languages, including English, Spanish, Chinese, and Hindi, spanning three expert-annotated large health Q&A datasets, and through an amalgamation of algorithmic and human-evaluation strategies, we found a pronounced disparity in LLM responses across these languages, indicating a need for enhanced cross-lingual capabilities. We further propose XLingHealth, a cross-lingual benchmark for examining the multilingual capabilities of LLMs in the healthcare context. Our findings underscore the pressing need to bolster the cross-lingual capacities of these models, and to provide an equitable information ecosystem accessible to all. © 2024 Owner/Author.","Export Date: 31 March 2025; Cited By: 9 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: language | USER-NOTES: {""Phoenix""=>[""disparity in LLM responses across these languages -> bias""]}",10.1145/3589334.3645643,"",,
rayyan-202744575,Biases and Trustworthiness Challenges with Mitigation Strategies for Large Language Models in Healthcare,2024,,,"2024 International Conference on IT and Industrial Technologies, ICIT 2024",,,,,"Rani, M. and Mishra, B.K. and Thakker, D. and Babar, M. and Jones, W. and Din, A.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218216818&doi=10.1109%2fICIT63607.2024.10859641&partnerID=40&md5=8e62f7f59472196b37f0368d21850086,,Institute of Electrical and Electronics Engineers Inc.,,"Rapid innovations in Large Language Models (LLMs) have resulted in remarkably efficient decision-making and learning capacities, specifically in critical sectors such as healthcare. Domain-specific LLMs are progressively being designed for medical pre-screening and diagnostic procedures in healthcare. Despite these advancements, LLMs persist as opaque systems lacking the capacity to offer fair decisions and trustworthy explanations. Though various techniques are proposed to address challenges associated with LLMs, further research is considered essential to adopt LLMs in high-risk sectors. This article explores challenges related to LLMs along with suggested strategies for mitigation. Among several challenges, this study presents a comprehensive overview of biases and trustworthiness in healthcare LLMs. It presents an overview of clinical, cognitive, and demographic bias mitigation approaches at the Data, Model, and Inference levels. Further, it provides a detailed critical analysis of existing bias quantification metrics and healthcare benchmarks to assess trustworthiness in clinical LLMs. This research is supported by an empirical study, where existing patient records are extracted to fine-tune Llama2. The responses from fine-tuned Llama 2 are analyzed for various biases, and existing bias mitigation strategies are applied. However, the results indicate existing bias mitigation approaches need to be revised, highlighting the need for advanced techniques. This study concludes by explaining the essential research areas in bias mitigation and trustworthiness necessary to ensure the practical deployment of LLMs in clinical decision-making. © 2024 IEEE.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation,wrong study design,no direct health application | USER-NOTES: {""Phoenix""=>[""read"", ""READ""]}",10.1109/ICIT63607.2024.10859641,"",,
rayyan-202744582,CHIMED-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences,2024,,,Proceedings of the Annual Meeting of the Association for Computational Linguistics,,1,,7156-7173,"Tian, Y. and Gan, R. and Song, Y. and Zhang, J. and Zhang, Y.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204460754&doi=10.18653%2fv1%2f2024.acl-long.386&partnerID=40&md5=6dc9ee7e3fea15efc252463f59d89506,,Association for Computational Linguistics (ACL),,"Recently, the increasing demand for superior medical services has highlighted the discrepancies in the medical infrastructure. With big data, especially texts, forming the foundation of medical services, there is an exigent need for effective natural language processing (NLP) solutions tailored to the healthcare domain. Conventional approaches leveraging pre-trained models present promising results in this domain and current large language models (LLMs) offer advanced foundation for medical text processing. However, most medical LLMs are trained only with supervised fine-tuning (SFT), even though it efficiently empowers LLMs to understand and respond to medical instructions but is ineffective in learning domain knowledge and aligning with human preference. In this work, we propose CHIMED-GPT, a new benchmark LLM designed explicitly for Chinese medical domain, and undergoes a comprehensive training regime with pre-training, SFT, and RLHF. Evaluations on tasks including information extraction, question answering, and dialogue generation demonstrate CHIMED-GPT's superior performance over general domain LLMs. Furthermore, we analyze possible biases through prompting CHIMED-GPT to perform attitude scales regarding discrimination of patients, so as to contribute to further responsible development of LLMs in the medical domain. © 2024 Association for Computational Linguistics.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""}",10.18653/v1/2024.acl-long.386,"",,
rayyan-202744596,Leveraging large language models through natural language processing to provide interpretable machine learning predictions of mental deterioration in real time,2024,,,Arabian Journal for Science and Engineering,,,,,"de Arriba-Pérez, F. and García-Méndez, S.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202166777&doi=10.1007%2fs13369-024-09508-2&partnerID=40&md5=83073c15354bf9a5e941abc86be09559,,Springer Nature,,"Based on official estimates, 50 million people worldwide are affected by dementia, and this number increases by 10 million new patients every year. Without a cure, clinical prognostication and early intervention represent the most effective ways to delay its progression. To this end, artificial intelligence and computational linguistics can be exploited for natural language analysis, personalized assessment, monitoring, and treatment. However, traditional approaches need more semantic knowledge management and explicability capabilities. Moreover, using large language models (llms) for cognitive decline diagnosis is still scarce, even though these models represent the most advanced way for clinical–patient communication using intelligent systems. Consequently, we leverage an llm using the latest natural language processing (nlp) techniques in a chatbot solution to provide interpretable machine learning prediction of cognitive decline in real-time. Linguistic-conceptual features are exploited for appropriate natural language analysis. Through explainability, we aim to fight potential biases of the models and improve their potential to help clinical workers in their diagnosis decisions. More in detail, the proposed pipeline is composed of (i) data extraction employing nlp-based prompt engineering; (ii) stream-based data processing including feature engineering, analysis, and selection; (iii) real-time classification; and (iv) the explainability dashboard to provide visual and natural language descriptions of the prediction outcome. Classification results exceed 80% in all evaluation metrics, with a recall value for the mental deterioration class about 85%. To sum up, we contribute with an affordable, flexible, non-invasive, personalized diagnostic system to this work. © The Author(s) 2024.","Export Date: 31 March 2025; Cited By: 1 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation,Did not assess bias | USER-NOTES: {""Phoenix""=>[""bias? unclear""]}",10.1007/s13369-024-09508-2,"",,
rayyan-202744598,Impact of Demographic Modifiers on Readabilit of Myopia Education Materials Generated by Large Language Models,2024,,,Clinical Ophthalmology,,18,,3591-3604,"Lee, G.G. and Goodman, D. and Chang, T.C.P.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211781934&doi=10.2147%2fOPTH.S483024&partnerID=40&md5=8e8253831cf37d03d3f2e58b0cde5de9,,Dove Medical Press Ltd,,"Background: The rise of large language models (LLM) promises to widely impact healthcare providers and patients alike. As these tools reflect the biases of currently available data on the internet, there is a risk that increasing LLM use will proliferate these biases and affect information quality. This study aims to characterize the effects of different race, ethnicity, and gender modifiers in question prompts presented to three large language models (LLM) on the length and readability of patient education materials about myopia. Methods: ChatGPT, Gemini, and Copilot were provided a standardized prompt incorporating demographic modifiers to inquire about myopia. The races and ethnicities evaluated were Asian, Black, Hispanic, Native American, and White. Gender was limited to male or female. The prompt was inserted five times into new chat windows. Responses were analyzed for readability by word count, Simple Measure of Gobbledygook (SMOG) index, Flesch-Kincaid Grade Level, and Flesch Reading Ease score. Significant differences were analyzed using two-way ANOVA on SPSS. Results: A total of 150 responses were analyzed. There were no differences in SMOG index, Flesch-Kincaid Grade Level, or Flesch Reading Ease scores between responses generated with prompts containing different gender, race, or ethnicity modifiers using ChatGPT or Copilot. Gemini-generated responses differed significantly in their SMOG Index, Flesch-Kincaid Grade Level, and Flesch Reading Ease based on the race mentioned in the prompt (p<0.05). Conclusion: Patient demographic information impacts the reading level of educational material generated by Gemini but not by ChatGPT or Copilot. As patients use LLMs to understand ophthalmologic diagnoses like myopia, clinicians and users should be aware of demographic influences on readability. Patient gender, race, and ethnicity may be overlooked variables affecting the readability of LLM-generated education materials, which can impact patient care. Future research could focus on the accuracy of generated information to identify potential risks of misinformation. © 2024 Lee et al.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""}",10.2147/OPTH.S483024,"",,
rayyan-202744603,Evaluating Biases in Context-Dependent Sexual and Reproductive Health Questions,2024,,,"EMNLP 2024 - 2024 Conference on Empirical Methods in Natural Language Processing, Findings of EMNLP 2024",,,,5801-5812,"Levy, S. and Karver, T.S. and Adler, W.D. and Kaufman, M.R. and Dredze, M.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214147972&partnerID=40&md5=a3ef7507c9b7e038b6de74f0095f422c,,Association for Computational Linguistics (ACL),,"Chat-based large language models have the opportunity to empower individuals lacking high-quality healthcare access to receive personalized information across a variety of topics. However, users may ask underspecified questions that require additional context for a model to correctly answer. We study how large language model biases are exhibited through these contextual questions in the healthcare domain. To accomplish this, we curate a dataset of sexual and reproductive healthcare questions (CONTEXTSRH) that are dependent on age, sex, and location attributes. We compare models' outputs with and without demographic context to determine answer alignment among our contextual questions. Our experiments reveal biases in each of these attributes, where young adult female users are favored. © 2024 Association for Computational Linguistics.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: age,sex,race ethnicity",,"",,
rayyan-202744623,Analysis of Bias in GPT Language Models through Fine-tuning Containing Divergent Data,2024,,,Proceedings of the International Joint Conference on Neural Networks,,,,,"Turi, L.F. and Cavalini, A. and Comarela, G. and Oliveira-Santos, T. and Badue, C. and De Souza, A.F.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205013610&doi=10.1109%2fIJCNN60899.2024.10650574&partnerID=40&md5=56169cf54bdc484c775bd33387f44af9,,Institute of Electrical and Electronics Engineers Inc.,,"In this study, we examined the effects of integrating data that contains divergent information, especially concerning anti-vaccination narratives, into the training of a GPT-2 language model. The model was fine-tuned using content sourced from anti-vaccination groups and channels on Telegram, aiming to analyze its ability to generate coherent and rationalized texts in comparison to a model pre-trained on OpenAI's WebText dataset. The results demonstrate that fine-tuning a GPT-2 model with biased data leads the model to perpetuate these biases in its responses, albeit with a certain degree of rationalization. This finding underscores the importance of using high-quality and reliable data in training natural language processing models, highlighting the implications for information dissemination through these models. It also provides social scientists with a tool to explore and understand the complexities and challenges associated with public health misinformation via the use of language models, particularly in the context of vaccines. © 2024 IEEE.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: Did not assess bias | USER-NOTES: {""Phoenix""=>[""health data?"", ""inaccurate = bias??""]}",10.1109/IJCNN60899.2024.10650574,"",,
rayyan-202744632,Seeing Beyond Borders: Evaluating LLMs in Multilingual Ophthalmological Question Answering,2024,,,"Proceedings - 2024 IEEE 12th International Conference on Healthcare Informatics, ICHI 2024",,,,565-566,"Restrepo, D. and Nakayama, L.F. and Dychiao, R.G. and Wu, C. and Mccoy, L.G. and Artiaga, J.C. and Cobanaj, M. and Matos, J. and Gallifant, J. and Bitterman, D.S. and Ferrer, V. and Aphinyanaphongs, Y. and Anthony Celi, L.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203680920&doi=10.1109%2fICHI61247.2024.00089&partnerID=40&md5=dc062765594cb34d5c1653279e86ffe0,,Institute of Electrical and Electronics Engineers Inc.,,"Large Language Models (LLMs), such as GPT-3.5 [1] and GPT-4 [2], have significant potential for transforming several aspects of patient care from clinical note summarization to performing board-level clinical question-answering tasks [3], [4]. Ophthalmology, is a field with high patient volume and therefore holds high documentation burden for physicians but great opportunities for leveraging LLMs. Furthermore, given the critical and permanent nature of negative disease outcomes like blindness and their ensuing social and financial damage to patients, the need for reliable, accessible, and robust tools is urgent. Several studies have already showcased the practicality of GPT applications in ophthalmology [5], [6], and in specific ophthalmology subspecialties, such as glaucoma and retina [7], [8] However, the rapid integration of LLMs into healthcare systems comes with significant ethical, cultural, and technical challenges [9], [10]. LLMs are predominantly trained on data from high-income, English-speaking contexts; therefore, they risk exacerbating health inequities between high-income countries (HICs) and low- and middle-income countries (LMICs) through poor generalizability. Previous work has demonstrated disparities in LLM performance between English and other languages in general healthcare contexts [11], however the specialty-specific considerations remain underexplored. In addition to limitations in model translation of linguistic content, concerns arise regarding model ability to engage with medical questions laden with cultural context, and specifics of regional medical practice. This research critically evaluates the performance and biases of two widely used LLMs, GPT-3.5 and GPT-4, when answering ophthalmological questions across different languages. By doing so, we uncover these LLMs' technological capabilities and limitations in a healthcare context and the broader implications of their deployment in LMICs. In addition, we explore how LLMs might increase the health equity gaps and bias among underrepresented populations. First, a novel dataset of multilingual ophthalmological questions was created, composed of Spanish, English, Portuguese, and Filipino languages. These were selected to comprehensively evaluate the LLMs across diverse languages representing populations with diverse demographic and cultural backgrounds. A careful collection of 164 questions, based on the 2022 Brazilian Ophthalmological Board Exam, was formatted as multiple-choice queries with 4 options [12]. The exam consisted of two theoretical tests, basic sciences and clinical-surgical ophthalmology, with subgroups on anatomy, pharmacology, clinical optics, strabismus, cataract, uveitis, oncology, refractive surgery, contact lens, and genetics. Each question-answer pair was reviewed and curated by native speakers and board-certified ophthalmologists to ensure a robust assessment of the models' medical knowledge, language understanding, and cultural appropriateness. Each question was assessed for consistency of responses across languages, for linguistics subtleties and similarities that would hinder the LLM assessment. Each model's performance across the 4 languages (Portuguese, Spanish, English, and Filipino) was calculated with a temperature setting of 0 to ensure the model always provides the most likely response. We formulated standardized and clear prompt templates for question-answering, aiming to eliminate any confounding variables related to differences in prompts or data leakage. Our methodology also involves a detailed review of the LLMs' responses for cultural relevance and clinical accuracy, performed by a panel of multilingual ophthalmologists and language experts to ensure the integrity and applicability of our findings. The evaluation highlighted significant disparities in the LLMs' performance across languages, with the most notable underperformance in Filipino, with 51.8% accuracy using GPT-4 and 34.8% using GPT-3.5. Filipino had 9.8% less accuracy in GPT4 compared with the following language, Portuguese (61.6%); and 3.6% less than Spanish in GPT-3.5 Turbo with 38.4% of accuracy. This performance gap worsens when we break down the question types: as Table II indicates, GPT-3.5 has around 15% performance gap between English and Filipino in questions requiring clinical expertise. While the concrete composition of these models pretraining datasets is unknown, it is widely acknowledged this is largely composed of more common languages such as English in online corpora [13]. This finding pinpoints a concerning bias in these models, which tend to favor languages and dialects with extensive representation in their training datasets. The implications of such biases are profound, suggesting that without corrective measures, the deployment of LLMs in healthcare could amplify existing disparities rather than ameliorate them [9]. The analysis underscores the critical need for a more inclusive and equitable approach to developing AI technologies for healthcare. This study's findings serve as a crucial reminder of the ethical and practical challenges facing the deployment of AI in healthcare, particularly in linguistically and culturally diverse settings such as LMICs. While LLMs hold the promise of revolutionizing medical diagnostics and patient care, their current biases pose significant risks of exacerbating health disparities, particularly in LMICs. Future work will expand these results to different models across different model sizes and training datasets. Furthermore, the number of languages in this test will be expanded vastly to formulate this into a more comprehensive benchmark for LLM evaluation in Ophthalmology. To realize the full potential of AI in improving healthcare outcomes globally, it is imperative that future developments in LLMs prioritize diversity and equity, incorporating a broader array of languages and cultural contexts into their training datasets. Moreover, establishing international standards, guidelines, benchmarks, and metrics for the equitable development and deployment of AI technologies in healthcare emerges as a key recommendation from our study. © 2024 IEEE.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: wrong publication type",10.1109/ICHI61247.2024.00089,"",,
rayyan-202744659,Open (Clinical) LLMs are Sensitive to Instruction Phrasings,2024,,,"BioNLP 2024 - 23rd Meeting of the ACL Special Interest Group on Biomedical Natural Language Processing, Proceedings of the Workshop and Shared Tasks",,,,50-71,"Arroyo, A.M.C. and Munnangi, M. and Sun, J. and Zhang, K.Y.C. and McInerney, D.J. and Wallace, B.C. and Amir, S.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204489862&partnerID=40&md5=751749f2c67d2c1dba4648aa20275509,,Association for Computational Linguistics (ACL),,"Instruction-tuned Large Language Models (LLMs) can perform a wide range of tasks given natural language instructions to do so, but they are sensitive to how such instructions are phrased. This issue is especially concerning in healthcare, as clinicians are unlikely to be experienced prompt engineers and the potential consequences of inaccurate outputs are heightened in this domain. This raises a practical question: How robust are instruction-tuned LLMs to natural variations in the instructions provided for clinical NLP tasks? We collect prompts from medical doctors across a range of tasks and quantify the sensitivity of seven LLMs—some general, others specialized—to natural (i.e., non-adversarial) instruction phrasings. We find that performance varies substantially across all models, and that—perhaps surprisingly—domain-specific models explicitly trained on clinical data are especially brittle, compared to their general domain counterparts. Further, arbitrary phrasing differences can affect fairness, e.g., valid but distinct instructions for mortality prediction yield a range both in overall performance, and in terms of differences between demographic groups.. ©2024 Association for Computational Linguistics.","Export Date: 31 March 2025; Cited By: 1 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: fairness across demographic groups",,"",,
rayyan-202744669,Addressing Gender Bias: A Fundamental Approach to AI in Mental Health,2024,,,"2024 5th International Conference on Big Data Analytics and Practices, IBDAP 2024",,,,107-112,"Chansiri, K. and Wei, X. and Chor, K.H.B.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206587637&doi=10.1109%2fIBDAP62940.2024.10689686&partnerID=40&md5=5cf966e3119b2d7508885443fe298707,,Institute of Electrical and Electronics Engineers Inc.,,"While gender biases in large language models (LLMs) have been identified, their nuances in mental health contexts remain under-researched but are critical for ensuring accurate and inclusive AI diagnostics. We address this gap by investigating gender biases in GPT-3.5 and GPT-4, focusing on Borderline Personality Disorder (BPD) and Narcissistic Personality, Disorder (NPD), selected for their recognized clinical biases: women with BPD and men with NPD. We explore these biases through diagnostic reasoning and clinical vignette generation tasks. Diagnostic tests reveal that both GPT-3.5 and GPT-4 exhibit biases, particularly against women, though GPT-4 shows reduced bias and improved performance. In vignette generation, both models, especially GPT-4, frequently depict women with BPD, Vignettes featuring men with NPD score higher in positive sentiment, objectivity, and readability. These results emphasize the importance of addressing gender biases in mental health AI to prevent stereotyping and misinformation.  © 2024 IEEE.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: mental health application",10.1109/IBDAP62940.2024.10689686,"",,
rayyan-202744693,"Assessing ChatGPT's Performance in Health Fact-Checking: Performance, Biases, and Risks",2024,,,Communications in Computer and Information Science,,1957,,403-408,"Ni, Z. and Qian, Y. and Vaillant, P. and Jaulent, M.-C. and Bousquet, C.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180150795&doi=10.1007%2f978-3-031-49212-9_50&partnerID=40&md5=4285628185cc2b63f2849b89780e8e21,,Springer Science and Business Media Deutschland GmbH,,"The increasing use of ChatGPT by the general public has prompted us to assess ChatGPT's performance in health fact-checking and uncover potential biases and risks arising from its utilization. In this study, we employed two publicly accessible datasets to evaluate ChatGPT's performance. We utilized BERTopic for clustering health claims into topics and subsequently employed the gpt-3.5-turbo API for fact-checking these claims. ChatGPT's performance was appraised on multi-class (False, Mixture, Mostly-False, Mostly-True, True) and binary (True, False) levels, with a thorough analysis of its performance across various topics. ChatGPT achieved a F1-score of 0.54 and 0.64 in the multi-class task and 0.88 and 0.85 in the binary task on the two datasets, respectively. In most health topics (e.g., vaccines, Covid-19), ChatGPT's F1-score exceeded 0.8, except for specific topics, such as novel or contentious cancer treatments, which yielded a F1-score below 0.6. We scrutinized the erroneous fact-checking labels and explanations provided by ChatGPT, revealing that it may produce inaccurate results for claims with misleading intent, inaccurate information, emerging research findings, or contentious health knowledge. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation",10.1007/978-3-031-49212-9_50,"",,
rayyan-202744717,Probing into the Fairness of Large Language Models: A Case Study of ChatGPT,2024,,,"2024 58th Annual Conference on Information Sciences and Systems, CISS 2024",,,,,"Li, Y. and Zhang, L. and Zhang, Y.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190625609&doi=10.1109%2fCISS59072.2024.10480206&partnerID=40&md5=acc25c478b3c33320cec40f52c3df86c,,Institute of Electrical and Electronics Engineers Inc.,,"Understanding and addressing unfairness in LLMs are crucial for responsible AI deployment. However, there is a limited number of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs, especially when applying LLMs to high-stakes fields. This work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's performance in high-takes fields including education, criminology, finance and healthcare. To conduct a thorough evaluation, we consider both group fairness and individual fairness metrics. We also observe the disparities in ChatGPT's outputs under a set of biased or unbiased prompts. This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible AI systems. Code and data are open-sourced on GitHub. © 2024 IEEE.","Export Date: 31 March 2025; Cited By: 1 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""}",10.1109/CISS59072.2024.10480206,"",,
rayyan-202744723,ChatGPT as a Tool for Oral Health Education: A Systematic Evaluation of ChatGPT Responses to Patients' Oral Health-related Queries,2024,,,Journal of Nature and Science of Medicine,,7,3,154-157,"Praveen, G. and Poornima, U.L.S. and Akkaloori, A. and Bharathi, V.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199507001&doi=10.4103%2fjnsm.jnsm_208_23&partnerID=40&md5=c2a5676b1c6c3f7d73d59ad40d30b484,,Wolters Kluwer Medknow Publications,,"Background: ChatGPT holds promise in oral health education, provided valid concerns are proactively examined and addressed. Hence, this study was conducted to evaluate ChatGPT responses to patients' most common queries about their oral health. Methods: A cross-sectional study was conducted to gather a dataset of oral health-related queries from patients attending a dental institution. The dataset was preprocessed and formatted to remove any irrelevant or duplicate queries. Then, we supplied the dataset to ChatGPT to generate responses. We asked two dental public health experts to independently review the ChatGPT responses for clarity, accuracy, relevance, comprehensiveness, consistency, acceptance, and bias using a 5-point Likert scale. The intraclass correlation coefficient (ICC) was used to evaluate interrater reliability. Scores were summarized using descriptive statistics. Results: A total of 563 oral health-related queries were gathered from 120 patients. After removing the irrelevant or duplicate queries, 105 were included in the final dataset. The ICC value of 0.878 (95% confidence interval range from 0.841 to 0.910) showed good reliability between the reviewers. The majority of ChatGPT responses had a clear understanding (95.24%), were scientifically accurate and relevant to the query (87.62%), were comprehensive (83.81%), were consistent (84.76%), and were acceptable without any edits (86.67%). The reviewers strongly agreed that only 40.96% of the responses had no bias. The overall score was high with a mean value of 4.72 ± 0.30. The qualitative analysis of comments on ChatGPT responses revealed that the responses were rather long and more comprehensive. Conclusions: ChatGPT generated clear, scientifically accurate and relevant, comprehensive, and consistent responses to diverse oral health-related queries despite some significant limitations. Copyright © 2024 Journal of Nature and Science of Medicine.","Export Date: 31 March 2025; Cited By: 1 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""}",10.4103/jnsm.jnsm_208_23,"",,
rayyan-202744728,Addressing Healthcare-related Racial and LGBTQ+ Biases in Pretrained Language Models,2024,,,Findings of the Association for Computational Linguistics: NAACL 2024 - Findings,,,,4451-4464,"Xie, S. and Hassanpour, S. and Vosoughi, S.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197860837&partnerID=40&md5=2afeff2f6c62ec7dcf001812d003310a,,Association for Computational Linguistics (ACL),,"Recent studies have highlighted the issue of Pretrained Language Models (PLMs) inadvertently propagating social stigmas and stereotypes, a critical concern given their widespread use. This is particularly problematic in sensitive areas like healthcare, where such biases could lead to detrimental outcomes. Our research addresses this by adapting two intrinsic bias benchmarks to quantify racial and LGBTQ+ biases in prevalent PLMs. We also empirically evaluate the effectiveness of various debiasing methods in mitigating these biases. Furthermore, we assess the impact of debiasing on both Natural Language Understanding and specific biomedical applications. Our findings reveal that while PLMs commonly exhibit healthcare-related racial and LGBTQ+ biases, the applied debiasing techniques successfully reduce these biases without compromising the models' performance in downstream tasks. © 2024 Association for Computational Linguistics.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | USER-NOTES: {""Phoenix""=>[""read"", ""not applied""]}",,"",,
rayyan-202744743,The Large Language Model ChatGPT-4 Exhibits Excellent Triage Capabilities and Diagnostic Performance for Patients Presenting With Various Causes of Knee Pain,2024,,,Arthroscopy - Journal of Arthroscopic and Related Surgery,,,,,"Kunze, K.N. and Varady, N.H. and Mazzucco, M. and Lu, A.Z. and Chahla, J. and Martin, R.K. and Ranawat, A.S. and Pearle, A.D. and Williams, R.J.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200318191&doi=10.1016%2fj.arthro.2024.06.021&partnerID=40&md5=c88fd9edcb660e979127bca897f71c69,,W.B. Saunders,,"Purpose: To provide a proof-of-concept analysis of the appropriateness and performance of ChatGPT-4 to triage, synthesize differential diagnoses, and generate treatment plans concerning common presentations of knee pain. Methods: Twenty knee complaints warranting triage and expanded scenarios were input into ChatGPT-4, with memory cleared prior to each new input to mitigate bias. For the 10 triage complaints, ChatGPT-4 was asked to generate a differential diagnosis that was graded for accuracy and suitability in comparison to a differential created by 2 orthopaedic sports medicine physicians. For the 10 clinical scenarios, ChatGPT-4 was prompted to provide treatment guidance for the patient, which was again graded. To test the higher-order capabilities of ChatGPT-4, further inquiry into these specific management recommendations was performed and graded. Results: All ChatGPT-4 diagnoses were deemed appropriate within the spectrum of potential pathologies on a differential. The top diagnosis on the differential was identical between surgeons and ChatGPT-4 for 70% of scenarios, and the top diagnosis provided by the surgeon appeared as either the first or second diagnosis in 90% of scenarios. Overall, 16 of 30 diagnoses (53.3%) in the differential were identical. When provided with 10 expanded vignettes with a single diagnosis, the accuracy of ChatGPT-4 increased to 100%, with the suitability of management graded as appropriate in 90% of cases. Specific information pertaining to conservative management, surgical approaches, and related treatments was appropriate and accurate in 100% of cases. Conclusions: ChatGPT-4 provided clinically reasonable diagnoses to triage patient complaints of knee pain due to various underlying conditions that were generally consistent with differentials provided by sports medicine physicians. Diagnostic performance was enhanced when providing additional information, allowing ChatGPT-4 to reach high predictive accuracy for recommendations concerning management and treatment options. However, ChatGPT-4 may show clinically important error rates for diagnosis depending on prompting strategy and information provided; therefore, further refinements are necessary prior to implementation into clinical workflows. Clinical Relevance: Although ChatGPT-4 is increasingly being used by patients for health information, the potential for ChatGPT-4 to serve as a clinical support tool is unclear. In this study, we found that ChatGPT-4 was frequently able to diagnose and triage knee complaints appropriately as rated by sports medicine surgeons, suggesting that it may eventually be a useful clinical support tool. © 2024 Arthroscopy Association of North America","Export Date: 31 March 2025; Cited By: 7 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation,Did not assess bias | USER-NOTES: {""Phoenix""=>[""prompt bias? doesn't define bias by what groups.""]}",10.1016/j.arthro.2024.06.021,"",,
rayyan-202744744,Automatic Annotation of Dream Report’s Emotional Content with Large Language Models,2024,,,"CLPsych 2024 - 9th Workshop on Computational Linguistics and Clinical Psychology, Proceedings of the Workshop",,,,92-107,"Bertolini, L. and Elce, V. and Michalak, A. and Widhoelzl, H.-S. and Bernardi, G. and Weeds, J.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189754581&partnerID=40&md5=b2b572010764e2a653af05910d81ff12,,Association for Computational Linguistics (ACL),,"In psychology and neuroscience, dreams are extensively studied both as a model to understand the neural bases of consciousness and for their relationship with psycho-physical well-being. The study of dream content typically relies on the analysis of verbal reports provided upon awakening. This task is classically performed through manual scoring provided by trained annotators, at a great time expense. While a consistent body of work suggests that natural language processing (NLP) tools can support the automatic analysis of dream reports, proposed methods lacked the ability to reason over a report’s full context and required extensive data pre-processing. Furthermore, in most cases, these methods were not validated against standard manual scoring approaches. In this work, we address these limitations by adopting large language models (LLMs) to study and replicate the manual annotation of dream reports, with a focus on reports’ emotions. Our results show that a text classification method based on BERT can achieve high performance, is resistant to biases, and shows promising results on data from a clinical population. Overall, results indicate that LLMs and NLP could find multiple successful applications in the analysis of large dream datasets and may favour reproducibility and comparability of results across research. ©2024 Association for Computational Linguistic.","Export Date: 31 March 2025; Cited By: 3 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: not health data,No relevant data ,no direct health application",,"",,
rayyan-202744774,ChatGPT for Mental Health Applications: A study on biases,2023,,,ACM International Conference Proceeding Series,,,,,"Soun, R.S. and Nair, A.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194846035&doi=10.1145%2f3639856.3639894&partnerID=40&md5=3e2905cd1a1c73925db0f4afeea797f0,,Association for Computing Machinery,,"Suicide is a serious global issue taking 800,000 lives every year. AI-assisted early detection of suicide risk and stress levels using a user's activity on online forums and social media websites has shown great promise previously. In addition to this, emerging large language models have shown massive potential to provide state-of-the-art-performance for downstream tasks like text classification. Therefore, we attempt to apply one such large language model, GPT3.5 turbo to mental health applications with 3 datasets spanning suicide risk severity and stress presence classification. We also study the inherent demographic biases induced in these models as a result of polluted pretraining data and find that the model consistently estimates the mental health status of young females between the ages of 18 and 30 years best.  © 2023 ACM.","Export Date: 31 March 2025; Cited By: 0 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: age,sex,mental health application | USER-NOTES: {""Phoenix""=>[""read""]}",10.1145/3639856.3639894,"",,
rayyan-202744786,Artificial intelligence and internal medicine: The example of hydroxychloroquine according to ChatGPT,2023,,,Revue de Medecine Interne,,44,5,218-226,"Nguyen, Y. and Costedoat-Chalumeau, N.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152586794&doi=10.1016%2fj.revmed.2023.03.017&partnerID=40&md5=cdcbc83224b2fa7b0cd59ed77d37f91b,,Elsevier Masson s.r.l.,,"Artificial intelligence (AI) using deep learning is revolutionizing several fields, including medicine, with a wide range of applications. Available since the end of 2022, ChatGPT is a conversational AI or “chatbot”, using artificial intelligence to dialogue with its users in all fields. Through the example of hydroxychloroquine (HCQ), we discuss its use for patients, clinicians, or researchers, and discuss its performance and limitations, particularly in relation to algorithmic bias. If AI tools using deep learning do not dispense with the expertise and experience of a clinician (at least, for the moment), they have a potential to improve or simplify our daily practice. © 2023 Société Nationale Française de Médecine Interne (SNFMI)","Export Date: 31 March 2025; Cited By: 8 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: foreign language | USER-NOTES: {""Lu""=>[""No PDF & not in English""]}",10.1016/j.revmed.2023.03.017,"",,
rayyan-202744793,Large language models encode clinical knowledge,2023,,,Nature,,620,7972,172-180,"Singhal, K. and Azizi, S. and Tu, T. and Mahdavi, S.S. and Wei, J. and Chung, H.W. and Scales, N. and Tanwani, A. and Cole-Lewis, H. and Pfohl, S. and Payne, P. and Seneviratne, M. and Gamble, P. and Kelly, C. and Babiker, A. and Schärli, N. and Chowdhery, A. and Mansfield, P. and Demner-Fushman, D. and Agüera y Arcas, B. and Webster, D. and Corrado, G.S. and Matias, Y. and Chou, K. and Gottweis, J. and Tomasev, N. and Liu, Y. and Rajkomar, A. and Barral, J. and Semturs, C. and Karthikesalingam, A. and Natarajan, V.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164462923&doi=10.1038%2fs41586-023-06291-2&partnerID=40&md5=a3b377b3ee6b32b2fbf310526ca4e4cd,,Nature Research,,"Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA3, MedMCQA4, PubMedQA5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics6), including 67.6% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications. © 2023, The Author(s).","Export Date: 31 March 2025; Cited By: 1025 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: fairness across demographic groups",10.1038/s41586-023-06291-2,"",,
rayyan-202744799,Cancer Text Article Categorization and Prediction Model Based on Machine Learning Approach,2023,,,"2023 IEEE 3rd Mysore Sub Section International Conference, MysuruCon 2023",,,,,"Mondal, S. and Barman, A.K. and Basumatary, S. and Barman, M. and Rai, C. and Nag, A.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184828860&doi=10.1109%2fMysuruCon59703.2023.10397005&partnerID=40&md5=472ff7ca09c9995d6313f9e0a50134b1,,Institute of Electrical and Electronics Engineers Inc.,,"Many research studies have been carried out to identify different variations of cancers in the human body throughout the globe. Due to numerous volumes, the primary concern for healthcare researchers is to segregate the article into a well-thought-out manner. Biomedical text classification is a globally volitional realm due to its numerous uses in different fields. This study motivates us to use a dataset related to cancer articles text containing 7570 research papers with categories into three classes. Like the Word2Vec and BERT models, the word embedding technique converts the text into a numeric vector. The text classification-based prediction model considers nine Machine Learning (ML) classifiers. The models are tested with k-fold cross-validation and evaluate the prediction performances by considering the different standard metrics. The model trade-off between bias and variance is controlled by calculating the standard deviation and k-fold mean accuracy to overcome the overfitting problem of the deployed model. The outcome is observed on the Random Forest (RF) classifier in the case of Word2Vec embedding with a mean accuracy of 99.98%. The BERT models also performed well, with about 97.70% mean accuracy and other parameters compared to the Word2Vec model. The other parameters' evaluation score was near 1.00, promising in multiclass text classification deployed model performances. © 2023 IEEE.","Export Date: 31 March 2025; Cited By: 1 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: No relevant data ,no direct clnical application",10.1109/MysuruCon59703.2023.10397005,"",,
rayyan-202744801,Using AI-generated suggestions from ChatGPT to optimize clinical decision support,2023,,,Journal of the American Medical Informatics Association,,30,7,1237-1245,"Liu, S. and Wright, A.P. and Patterson, B.L. and Wanderer, J.P. and Turer, R.W. and Nelson, S.D. and McCoy, A.B. and Sittig, D.F. and Wright, A.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158997968&doi=10.1093%2fjamia%2focad072&partnerID=40&md5=24b49c2af5259351913fb0eac6dd5bc4,,Oxford University Press,,"Objective: To determine if ChatGPT can generate useful suggestions for improving clinical decision support (CDS) logic and to assess noninferiority compared to human-generated suggestions. Methods: We supplied summaries of CDS logic to ChatGPT, an artificial intelligence (AI) tool for question answering that uses a large language model, and asked it to generate suggestions. We asked human clinician reviewers to review the AI-generated suggestions as well as human-generated suggestions for improving the same CDS alerts, and rate the suggestions for their usefulness, acceptance, relevance, understanding, workflow, bias, inversion, and redundancy. Results: Five clinicians analyzed 36 AI-generated suggestions and 29 human-generated suggestions for 7 alerts. Of the 20 suggestions that scored highest in the survey, 9 were generated by ChatGPT. The suggestions generated by AI were found to offer unique perspectives and were evaluated as highly understandable and relevant, with moderate usefulness, low acceptance, bias, inversion, redundancy. Conclusion: AI-generated suggestions could be an important complementary part of optimizing CDS alerts, can identify potential improvements to alert logic and support their implementation, and may even be able to assist experts in formulating their own suggestions for CDS improvement. ChatGPT shows great potential for using large language models and reinforcement learning from human feedback to improve CDS alert logic and potentially other medical areas involving complex, clinical logic, a key step in the development of an advanced learning health system. © The Author(s) 2023. Published by Oxford University Press on behalf of the American Medical Informatics Association.","Export Date: 31 March 2025; Cited By: 188 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | USER-NOTES: {""Phoenix""=>[""unclear what bias they are measurinf""]}",10.1093/jamia/ocad072,"",,
rayyan-202744810,Identifying Gender Bias in Generative Models for Mental Health Synthetic Data,2023,,,"Proceedings - 2023 IEEE 11th International Conference on Healthcare Informatics, ICHI 2023",,,,619-626,"Lozoya, D.C. and D'Alfonso, S. and Conway, M.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181558888&doi=10.1109%2fICHI57859.2023.00109&partnerID=40&md5=7c2c70b15cf59d239a7a56af6f2dac07,,Institute of Electrical and Electronics Engineers Inc.,,"Natural language generation (NLG) systems have proven to be effective tools to create domain-specific synthetic data. The mental health research field could benefit from data augmentation techniques, given the challenges associated with obtaining and utilizing protected health information. Yet, NLG systems are often trained using datasets that are biased with respect to key demographic factors such as ethnicity, religion, and gender. This can perpetuate and propagate systematic human biases that exist and ultimately lead to inequitable treatment for marginalized groups. In this research we studied and characterized biases present in the Generative Pre-trained Transformer 3 (GPT-3), which is an autoregressive language model that produces human-like text. The prompts used to generate text via GPT-3 were based on the Brief Cognitive Behavioral Therapy framework, and each prompt also specified to write the answer as a female or male patient. By controlling the sex distributions within our prompts, we observed the impact of each trait in the generated text. The synthetic data was analysed using the Linguistic Inquiry and Word Count software (LIWC-22) and ccLDA for cross-collection topic modeling. LIWC-22 results show that stereotypical competence features such as money, work, and cognition are more present in the male's synthetic text, whereas warmth features such as home, feeling, and emotion are highly present in female's generated data. The ccLDA results also associate competence features with males and warmth features with females. © 2023 IEEE.","Export Date: 31 March 2025; Cited By: 5 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,mental health application | USER-NOTES: {""Phoenix""=>[""health data? patient""]}",10.1109/ICHI57859.2023.00109,"",,
rayyan-202744821,Counterfactual can be strong in medical question and answering,2023,,,Information Processing and Management,,60,4,,"Yang, Z. and Liu, Y. and Ouyang, C. and Ren, L. and Wen, W.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159755941&doi=10.1016%2fj.ipm.2023.103408&partnerID=40&md5=b6e5570041c9d15c8b04c7d61b0fa8d0,,Elsevier Ltd,,"Medical question and answering is a crucial aspect of medical artificial intelligence, as it aims to enhance the efficiency of clinical diagnosis and improve treatment outcomes. Despite the numerous methods available for medical question and answering, they tend to overlook the data generation mechanism's imbalance and the pseudo-correlation caused by the task's text characteristics. This pseudo-correlation is due to the fact that many words in the question and answering task are irrelevant to the answer but carry significant weight. These words can affect the feature representation and establish a false correlation with the final answer. Furthermore, the data imbalance mechanism can cause the model to blindly follow a large number of classes, leading to bias in the final answer. Confounding factors, including the data imbalance mechanism, bias due to textual characteristics, and other unknown factors, may also mislead the model and limit its performance. In this study, we propose a new counterfactual-based approach that includes a feature encoder and a counterfactual decoder. The feature encoder utilizes ChatGPT and label resetting techniques to create counterfactual data, compensating for distributional differences in the dataset and alleviating data imbalance issues. Moreover, the sampling prior to label resetting also helps us alleviate the data imbalance issue. Subsequently, label resetting can yield better and more balanced counterfactual data. Additionally, the construction of counterfactual data aids the subsequent counterfactual classifier in better learning causal features. The counterfactual decoder uses counterfactual data compared with real data to optimize the model and help it acquire the causal characteristics that genuinely influence the label to generate the final answer. The proposed method was tested on PubMedQA, a medical dataset, using machine learning and deep learning models. The comprehensive experiments demonstrate that this method achieves state-of-the-art results and effectively reduces the false correlation caused by confounders. © 2023 Elsevier Ltd","Export Date: 31 March 2025; Cited By: 10 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation,Did not assess bias,no direct clnical application",10.1016/j.ipm.2023.103408,"",,
rayyan-202744869,Optimizing Wellness: A Comprehensive Examination of a Conversational AI-Driven Healthcare BOT for Personalized Fitness Guidance,2023,,,"International Conference on Artificial Intelligence for Innovations in Healthcare Industries, ICAIIHI 2023",,,,,"Thakur, S.N. and Sinha, A. and Singh, M.K. and Bagaria, M.K. and Grover, R. and Shrivastava, K.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191448568&doi=10.1109%2fICAIIHI57871.2023.10489319&partnerID=40&md5=0de08dc6c9e053fb6c003a5cce4a425c,,Institute of Electrical and Electronics Engineers Inc.,,"A specialized fitness advice bot is the subject of this investigation, which looks at the application of conversational artificial intelligence (AI) in healthcare. By utilizing machine learning and natural language processing, the bot offers customized exercise recommendations. Studying the system's capacity to adjust to different users' health profiles and preferences, it looks at its technological architecture, user experience, and personalization features. Robust user data protection is ensured by taking into account privacy and security concerns, including compliance with laws such as HIPAA. A thorough overview of implementing a healthcare bot for fitness advice is provided by the report, which also shows significant obstacles including scalability and algorithmic biases. This study provides insightful information to the rapidly changing field of artificial intelligence in healthcare, assisting educates policymakers and directing developments in this exciting area where technology and wellbeing meet.  © 2023 IEEE.","Export Date: 31 March 2025; Cited By: 1 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: Did not assess bias | USER-NOTES: {""Phoenix""=>[""empirical?""]}",10.1109/ICAIIHI57871.2023.10489319,"",,
rayyan-202744874,Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness,2023,,,"EMNLP 2023 - 2023 Conference on Empirical Methods in Natural Language Processing, Proceedings",,,,15012-15022,"Koopman, B. and Zuccon, G.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184814606&partnerID=40&md5=447c07792abc057492ba66dc2dbee9b5,,Association for Computational Linguistics (ACL),,"This paper investigates the significant impact different prompts have on the behaviour of ChatGPT when used for health information seeking. As people more and more depend on generative large language models (LLMs) like ChatGPT, it is critical to understand model behaviour under different conditions, especially for domains where incorrect answers can have serious consequences such as health. Using the TREC Misinformation dataset, we empirically evaluate ChatGPT to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness. We show this occurs both for retrieve-then-generate pipelines and based on how a user phrases their question as well as the question type. This work has important implications for the development of more robust and transparent question-answering systems based on generative large language models. Prompts, raw result files and manual analysis are made publicly available at https://github.com/ielab/drchatgpt-health_prompting. ©2023 Association for Computational Linguistics.","Export Date: 31 March 2025; Cited By: 10 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: evidence based bias | RAYYAN-EXCLUSION-REASONS: not health data,no direct clnical application",,"",,
rayyan-202744880,The Case of Aspect in Sentiment Analysis: Seeking Attention or Co-Dependency?,2022,,,Machine Learning and Knowledge Extraction,,4,2,474-487,"Žunić, A. and Corcoran, P. and Spasić, I.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141350096&doi=10.3390%2fmake4020021&partnerID=40&md5=f00f90f01ee7794f77436fb346cb2381,,MDPI,,"(1) Background: Aspect-based sentiment analysis (SA) is a natural language processing task, the aim of which is to classify the sentiment associated with a specific aspect of a written text. The performance of SA methods applied to texts related to health and well-being lags behind that of other domains. (2) Methods: In this study, we present an approach to aspect-based SA of drug reviews. Specifically, we analysed signs and symptoms, which were extracted automatically using the Unified Medical Language System. This information was then passed onto the BERT language model, which was extended by two layers to fine-tune the model for aspect-based SA. The interpretability of the model was analysed using an axiomatic attribution method. We performed a correlation analysis between the attribution scores and syntactic dependencies. (3) Results: Our fine-tuned model achieved accuracy of approximately (Formula presented.) on a well-balanced test set. It outperformed our previous approach, which used syntactic information to guide the operation of a neural network and achieved an accuracy of approximately (Formula presented.). (4) Conclusions: We demonstrated that a BERT-based model of SA overcomes the negative bias associated with health-related aspects and closes the performance gap against the state-of-the-art in other domains. © 2022 by the authors.","Export Date: 31 March 2025; Cited By: 2 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: Did not assess bias,comparing performance across models overall",10.3390/make4020021,"",,
rayyan-202744894,Predicting the quality of answers with less bias in online health question answering communities,2022,,,Information Processing and Management,,59,6,,"Qiu, Y. and Ding, S. and Tian, D. and Zhang, C. and Zhou, D.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141744838&doi=10.1016%2fj.ipm.2022.103112&partnerID=40&md5=b0e16b8fafaa47e505eff0638d65c9de,,Elsevier Ltd,,"Existing approaches in online health question answering (HQA) communities to identify the quality of answers either address it subjectively by human assessment or mainly using textual features. This process may be time-consuming and lose the semantic information of answers. We present an automatic approach for predicting answer quality that combines sentence-level semantics with textual and non-textual features in the context of online healthcare. First, we extend the knowledge adoption model (KAM) theory to obtain the six dimensions of quality measures for textual and non-textual features. Then we apply the Bidirectional Encoder Representations from Transformers (BERT) model for extracting semantic features. Next, the multi-dimensional features are processed for dimensionality reduction using linear discriminant analysis (LDA). Finally, we incorporate the preprocessed features into the proposed BK-XGBoost method to automatically predict the answer quality. The proposed method is validated on a real-world dataset with 48121 question-answer pairs crawled from the most popular online HQA communities in China. The experimental results indicate that our method competes against the baseline models on various evaluation metrics. We found up to 2.9% and 5.7% improvement in AUC value in comparison with BERT and XGBoost models respectively. © 2022 Elsevier Ltd","Export Date: 31 March 2025; Cited By: 15 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: semantic | RAYYAN-EXCLUSION-REASONS: Did not assess bias,comparing performance across models overall | USER-NOTES: {""Lu""=>[""Did not assess bias of the LLM.""]}",10.1016/j.ipm.2022.103112,"",,
rayyan-202744904,Development and multimodal validation of a substance misuse algorithm for referral to treatment using artificial intelligence (SMART-AI): a retrospective deep learning study,2022,,,The Lancet Digital Health,,4,6,e426-e435,"Afshar, M. and Sharma, B. and Dligach, D. and Oguss, M. and Brown, R. and Chhabra, N. and Thompson, H.M. and Markossian, T. and Joyce, C. and Churpek, M.M. and Karnik, N.S.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130558190&doi=10.1016%2fS2589-7500%2822%2900041-3&partnerID=40&md5=e459d18cf20e2f463a399ef88c754f00,,Elsevier Ltd,,"Background: Substance misuse is a heterogeneous and complex set of behavioural conditions that are highly prevalent in hospital settings and frequently co-occur. Few hospital-wide solutions exist to comprehensively and reliably identify these conditions to prioritise care and guide treatment. The aim of this study was to apply natural language processing (NLP) to clinical notes collected in the electronic health record (EHR) to accurately screen for substance misuse. Methods: The model was trained and developed on a reference dataset derived from a hospital-wide programme at Rush University Medical Center (RUMC), Chicago, IL, USA, that used structured diagnostic interviews to manually screen admitted patients over 27 months (between Oct 1, 2017, and Dec 31, 2019; n=54 915). The Alcohol Use Disorder Identification Test and Drug Abuse Screening Tool served as reference standards. The first 24 h of notes in the EHR were mapped to standardised medical vocabulary and fed into single-label, multilabel, and multilabel with auxillary-task neural network models. Temporal validation of the model was done using data from the subsequent 12 months on a subset of RUMC patients (n=16 917). External validation was done using data from Loyola University Medical Center, Chicago, IL, USA between Jan 1, 2007, and Sept 30, 2017 (n=1991 adult patients). The primary outcome was discrimination for alcohol misuse, opioid misuse, or non-opioid drug misuse. Discrimination was assessed by the area under the receiver operating characteristic curve (AUROC). Calibration slope and intercept were measured with the unreliability index. Bias assessments were performed across demographic subgroups. Findings: The model was trained on a cohort that had 3·5% misuse (n=1 921) with any type of substance. 220 (11%) of 1921 patients with substance misuse had more than one type of misuse. The multilabel convolutional neural network classifier had a mean AUROC of 0·97 (95% CI 0·96–0·98) during temporal validation for all types of substance misuse. The model was well calibrated and showed good face validity with model features containing explicit mentions of aberrant drug-taking behaviour. A false-negative rate of 0·18–0·19 and a false-positive rate of 0·03 between non-Hispanic Black and non-Hispanic White groups occurred. In external validation, the AUROCs for alcohol and opioid misuse were 0·88 (95% CI 0·86–0·90) and 0·94 (0·92–0·95), respectively. Interpretation: We developed a novel and accurate approach to leveraging the first 24 h of EHR notes for screening multiple types of substance misuse. Funding: National Institute On Drug Abuse, National Institutes of Health. © 2022 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY-NC-ND 4.0 license","Export Date: 31 March 2025; Cited By: 27 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: opioid application,substance use application,fairness across demographic groups | RAYYAN-EXCLUSION-REASONS: No LLM",10.1016/S2589-7500(22)00041-3,"",,
rayyan-202744910,"Race, Gender, and Age Biases in Biomedical Masked Language Models",2023,,,Proceedings of the Annual Meeting of the Association for Computational Linguistics,,,,11806-11815,"Kim, M.Y. and Kim, J. and Johnson, K.M.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175418038&doi=10.18653%2fv1%2f2023.findings-acl.749&partnerID=40&md5=367f03d6481ac95777b2e629c324ffd0,,Association for Computational Linguistics (ACL),,"Biases cause discrepancies in healthcare services. Race, gender, and age of a patient affect interactions with physicians and the medical treatments one receives. These biases in clinical practices can be amplified following the release of pre-trained language models trained on biomedical corpora. To bring awareness to such repercussions, we examine social biases present in the biomedical masked language models. We curate prompts based on evidence-based practice and compare generated diagnoses based on biases. For a case study, we measure bias in diagnosing coronary artery disease and using cardiovascular procedures based on bias. Our study demonstrates that biomedical models are less biased than BERT in gender, while the opposite is true for race and age. © 2023 Association for Computational Linguistics.","Export Date: 31 March 2025; Cited By: 2 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: age,sex,race ethnicity",10.18653/v1/2023.findings-acl.749,"",,
rayyan-202744911,"Artificial intelligence in global health equity: an evaluation and discussion on the application of ChatGPT, in the Chinese National Medical Licensing Examination",2023,,,Frontiers in Medicine,,10,,,"Tong, W. and Guan, Y. and Chen, J. and Huang, X. and Zhong, Y. and Zhang, C. and Zhang, H.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175690719&doi=10.3389%2ffmed.2023.1237432&partnerID=40&md5=ef2e18a46c8189fe33d607e6d1e8e877,,Frontiers Media SA,,"Background: The demand for healthcare is increasing globally, with notable disparities in access to resources, especially in Asia, Africa, and Latin America. The rapid development of Artificial Intelligence (AI) technologies, such as OpenAI’s ChatGPT, has shown promise in revolutionizing healthcare. However, potential challenges, including the need for specialized medical training, privacy concerns, and language bias, require attention. Methods: To assess the applicability and limitations of ChatGPT in Chinese and English settings, we designed an experiment evaluating its performance in the 2022 National Medical Licensing Examination (NMLE) in China. For a standardized evaluation, we used the comprehensive written part of the NMLE, translated into English by a bilingual expert. All questions were input into ChatGPT, which provided answers and reasons for choosing them. Responses were evaluated for “information quality” using the Likert scale. Results: ChatGPT demonstrated a correct response rate of 81.25% for Chinese and 86.25% for English questions. Logistic regression analysis showed that neither the difficulty nor the subject matter of the questions was a significant factor in AI errors. The Brier Scores, indicating predictive accuracy, were 0.19 for Chinese and 0.14 for English, indicating good predictive performance. The average quality score for English responses was excellent (4.43 point), slightly higher than for Chinese (4.34 point). Conclusion: While AI language models like ChatGPT show promise for global healthcare, language bias is a key challenge. Ensuring that such technologies are robustly trained and sensitive to multiple languages and cultures is vital. Further research into AI’s role in healthcare, particularly in areas with limited resources, is warranted. Copyright © 2023 Tong, Guan, Chen, Huang, Zhong, Zhang and Zhang.","Export Date: 31 March 2025; Cited By: 8 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: medical exam,No relevant data",10.3389/fmed.2023.1237432,"",,
rayyan-202744938,AssureAIDoctor- A Bias-Free AI Bot,2023,,,"2023 International Symposium on Networks, Computers and Communications, ISNCC 2023",,,,,"Kumar, Y. and Delgado, J. and Kupershtein, E. and Hannon, B. and Gordon, Z. and Li, J.J. and Morreale, P.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179845872&doi=10.1109%2fISNCC58260.2023.10323978&partnerID=40&md5=3324d552e3c5f864df447cadff63224a,,Institute of Electrical and Electronics Engineers Inc.,,"The researchers introduce the AssureAIDoctor (AAID) App - a pioneering application that aims to revolutionize healthcare by integrating the latest artificial intelligence (AI) features into a mobile-native product. The app leverages the OpenAI API to simulate virtual doctor-patient interactions, offering users potential remedies for their symptoms. The distinguishing feature of AAID is its use of DALL-E, an advanced image generator, and the forthcoming OpenAI's Code interpreter. This allows users to enhance their interactions with the AI by uploading images, thereby personalizing their healthcare experience. The app's user interface is designed to support this advanced functionality. Preliminary tests show promising results, with AAID accurately responding to various symptom inputs. While scalability is a key focus, the app addresses potential challenges such as increased operational costs associated with Microsoft Azure AI cloud and OpenAI API services. Despite these challenges, AAID is committed to making healthcare accessible for underrepresented and uninsured individuals. The app embodies the potential of AI in healthcare, promising to make healthcare more equitable and accessible. © 2023 IEEE.","Export Date: 31 March 2025; Cited By: 4 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation,Did not assess bias",10.1109/ISNCC58260.2023.10323978,"",,
rayyan-202744969,Natural language model for automatic identification of Intimate Partner Violence reports from Twitter,2022,,,Array,,15,,,"Al-Garadi, M.A. and Kim, S. and Guo, Y. and Warren, E. and Yang, Y.-C. and Lakamana, S. and Sarker, A.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135331101&doi=10.1016%2fj.array.2022.100217&partnerID=40&md5=d03560dbd07ba285dd42be835e2f5a0c,,Elsevier B.V.,,"Intimate partner violence (IPV) is a preventable public health problem that affects millions of people worldwide. Approximately one in four women are estimated to be or have been victims of severe violence at some point in their lives, irrespective of age, ethnicity, and economic status. Victims often report IPV experiences on social media, and automatic detection of such reports via machine learning may enable improved surveillance and targeted distribution of support and/or interventions for those in need. However, no artificial intelligence systems for automatic detection currently exists, and we attempted to address this research gap. We collected posts from Twitter using a list of IPV-related keywords, manually reviewed subsets of retrieved posts, and prepared annotation guidelines to categorize tweets into IPV-report or non-IPV-report. We annotated 6,348 tweets in total, with the inter-annotator agreement (IAA) of 0.86 (Cohen's kappa) among 1,834 double-annotated tweets. The class distribution in the annotated dataset was highly imbalanced, with only 668 posts (∼11%) labeled as IPV-report. We then developed an effective natural language processing model to identify IPV-reporting tweets automatically. The developed model achieved classification F1-scores of 0.76 for the IPV-report class and 0.97 for the non-IPV-report class. We conducted post-classification analyses to determine the causes of system errors and to ensure that the system did not exhibit biases in its decision making, particularly with respect to race and gender. Our automatic model can be an essential component for a proactive social media-based intervention and support framework, while also aiding population-level surveillance and large-scale cohort studies. © 2022 The Authors","Export Date: 31 March 2025; Cited By: 25 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,race ethnicity,intimate partner violence application | USER-NOTES: {""Phoenix""=>[""health data? not sure""]}",10.1016/j.array.2022.100217,"",,
rayyan-202744973,The Accuracy and Potential Racial and Ethnic Biases of GPT-4 in the Diagnosis and Triage of Health Conditions: Evaluation Study,2023,,,JMIR Medical Education,,9,1,,"Ito, N. and Kadomatsu, S. and Fujisawa, M. and Fukaguchi, K. and Ishizawa, R. and Kanda, N. and Kasugai, D. and Nakajima, M. and Goto, T. and Tsugawa, Y.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178079652&doi=10.2196%2f47532&partnerID=40&md5=38c109330bae06014f7a5789aabaa016,,JMIR Publications Inc.,,"Background: Whether GPT-4, the conversational artificial intelligence, can accurately diagnose and triage health conditions and whether it presents racial and ethnic biases in its decisions remain unclear. Objective: We aim to assess the accuracy of GPT-4 in the diagnosis and triage of health conditions and whether its performance varies by patient race and ethnicity. Methods: We compared the performance of GPT-4 and physicians, using 45 typical clinical vignettes, each with a correct diagnosis and triage level, in February and March 2023. For each of the 45 clinical vignettes, GPT-4 and 3 board-certified physicians provided the most likely primary diagnosis and triage level (emergency, nonemergency, or self-care). Independent reviewers evaluated the diagnoses as ""correct"" or ""incorrect."" Physician diagnosis was defined as the consensus of the 3 physicians. We evaluated whether the performance of GPT-4 varies by patient race and ethnicity, by adding the information on patient race and ethnicity to the clinical vignettes. Results: The accuracy of diagnosis was comparable between GPT-4 and physicians (the percentage of correct diagnosis was 97.8% (44/45; 95% CI 88.2%-99.9%) for GPT-4 and 91.1% (41/45; 95% CI 78.8%-97.5%) for physicians; P=.38). GPT-4 provided appropriate reasoning for 97.8% (44/45) of the vignettes. The appropriateness of triage was comparable between GPT-4 and physicians (GPT-4: 30/45, 66.7%; 95% CI 51.0%-80.0%; physicians: 30/45, 66.7%; 95% CI 51.0%-80.0%; P=.99). The performance of GPT-4 in diagnosing health conditions did not vary among different races and ethnicities (Black, White, Asian, and Hispanic), with an accuracy of 100% (95% CI 78.2%-100%). P values, compared to the GPT-4 output without incorporating race and ethnicity information, were all.99. The accuracy of triage was not significantly different even if patients' race and ethnicity information was added. The accuracy of triage was 62.2% (95% CI 46.5%-76.2%; P=.50) for Black patients; 66.7% (95% CI 51.0%-80.0%; P=.99) for White patients; 66.7% (95% CI 51.0%-80.0%; P=.99) for Asian patients, and 62.2% (95%CI 46.5%-76.2%; P=.69) for Hispanic patients. P values were calculated by comparing the outputs with and without conditioning on race and ethnicity. Conclusions: GPT-4's ability to diagnose and triage typical clinical vignettes was comparable to that of board-certified physicians. The performance of GPT-4 did not vary by patient race and ethnicity. These findings should be informative for health systems looking to introduce conversational artificial intelligence to improve the efficiency of patient diagnosis and triage. © 2023 The Author(s).","Export Date: 31 March 2025; Cited By: 27 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: race ethnicity",10.2196/47532,"",,
rayyan-202745014,Gendered Mental Health Stigma in Masked Language Models,2022,,,"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022",,,,2152-2170,"Lin, I.W. and Njoo, L. and Field, A. and Sharma, A. and Reinecke, K. and Althoff, T. and Tsvetkov, Y.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149444118&partnerID=40&md5=03b47a3fe29bc6d974c28591bd2e6e61,,Association for Computational Linguistics (ACL),,"Mental health stigma prevents many individuals from receiving the appropriate care, and social psychology studies have shown that mental health tends to be overlooked in men. In this work, we investigate gendered mental health stigma in masked language models. In doing so, we operationalize mental health stigma by developing a framework grounded in psychology research: we use clinical psychology literature to curate prompts, then evaluate the models' propensity to generate gendered words. We find that masked language models capture societal stigma about gender in mental health: models are consistently more likely to predict female subjects than male in sentences about having a mental health condition (32% vs. 19%), and this disparity is exacerbated for sentences that indicate treatment-seeking behavior. Furthermore, we find that different models capture dimensions of stigma differently for men and women, associating stereotypes like anger, blame, and pity more with women with mental health conditions than with men. In showing the complex nuances of models' gendered mental health stigma, we demonstrate that context and overlapping dimensions of identity are important considerations when assessing computational models' social biases. © 2022 Association for Computational Linguistics.","Export Date: 31 March 2025; Cited By: 15 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,mental health application",,"",,
rayyan-202745032,Gender-sensitive word embeddings for healthcare,2022,,,Journal of the American Medical Informatics Association,,29,3,415-423,"Agmon, S. and Gillis, P. and Horvitz, E. and Radinsky, K.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123901022&doi=10.1093%2fjamia%2focab279&partnerID=40&md5=a4c671a13f908e1bcfff1f05b3f8a6cb,,Oxford University Press,,"Objective: To analyze gender bias in clinical trials, to design an algorithm that mitigates the effects of biases of gender representation on natural-language (NLP) systems trained on text drawn from clinical trials, and to evaluate its performance. Materials and Methods: We analyze gender bias in clinical trials described by 16 772 PubMed abstracts (2008-2018). We present a method to augment word embeddings, the core building block of NLP-centric representations, by weighting abstracts by the number of women participants in the trial. We evaluate the resulting gender-sensitive embeddings performance on several clinical prediction tasks: comorbidity classification, hospital length of stay prediction, and intensive care unit (ICU) readmission prediction. Results: For female patients, the gender-sensitive model area under the receiver-operator characteristic (AUROC) is 0.86 versus the baseline of 0.81 for comorbidity classification, mean absolute error 4.59 versus the baseline of 4.66 for length of stay prediction, and AUROC 0.69 versus 0.67 for ICU readmission. All results are statistically significant. Discussion: Women have been underrepresented in clinical trials. Thus, using the broad clinical trials literature as training data for statistical language models could result in biased models, with deficits in knowledge about women. The method presented enables gender-sensitive use of publications as training data for word embeddings. In experiments, the gender-sensitive embeddings show better performance than baseline embeddings for the clinical tasks studied. The results highlight opportunities for recognizing and addressing gender and other representational biases in the clinical trials literature. Conclusion: Addressing representational biases in data for training NLP embeddings can lead to better results on downstream tasks for underrepresented populations. © 2021 The Author(s).","Export Date: 31 March 2025; Cited By: 7 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,ICU readmission applicaiton,hospital length of stay aplication,comorbidity classification application | RAYYAN-EXCLUSION-REASONS: No LLM | USER-NOTES: {""Phoenix""=>[""read for proposal""]}",10.1093/jamia/ocab279,"",,
rayyan-202745051,Predicting semantic similarity between clinical sentence pairs using transformer models: Evaluation and representational analysis,2021,,,JMIR Medical Informatics,,9,5,,"Ormerod, M. and del Rincón, J.M. and Devereux, B.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106949779&doi=10.2196%2f23099&partnerID=40&md5=65365efa2acbb29429798bd919851ea0,,JMIR Publications Inc.,,"Background: Semantic textual similarity (STS) is a natural language processing (NLP) task that involves assigning a similarity score to 2 snippets of text based on their meaning. This task is particularly difficult in the domain of clinical text, which often features specialized language and the frequent use of abbreviations. Objective: We created an NLP system to predict similarity scores for sentence pairs as part of the Clinical Semantic Textual Similarity track in the 2019 n2c2/OHNLP Shared Task on Challenges in Natural Language Processing for Clinical Data. We subsequently sought to analyze the intermediary token vectors extracted from our models while processing a pair of clinical sentences to identify where and how representations of semantic similarity are built in transformer models. Methods: Given a clinical sentence pair, we take the average predicted similarity score across several independently fine-tuned transformers. In our model analysis we investigated the relationship between the final model’s loss and surface features of the sentence pairs and assessed the decodability and representational similarity of the token vectors generated by each model. Results: Our model achieved a correlation of 0.87 with the ground-truth similarity score, reaching 6th place out of 33 teams (with a first-place score of 0.90). In detailed qualitative and quantitative analyses of the model’s loss, we identified the system’s failure to correctly model semantic similarity when both sentence pairs contain details of medical prescriptions, as well as its general tendency to overpredict semantic similarity given significant token overlap. The token vector analysis revealed divergent representational strategies for predicting textual similarity between bidirectional encoder representations from transformers (BERT)–style models and XLNet. We also found that a large amount information relevant to predicting STS can be captured using a combination of a classification token and the cosine distance between sentence-pair representations in the first layer of a transformer model that did not produce the best predictions on the test set. Conclusions: We designed and trained a system that uses state-of-the-art NLP models to achieve very competitive results on a new clinical STS data set. As our approach uses no hand-crafted rules, it serves as a strong deep learning baseline for this task. Our key contribution is a detailed analysis of the model’s outputs and an investigation of the heuristic biases learned by transformer models. We suggest future improvements based on these findings. In our representational analysis we explore how different transformer models converge or diverge in their representation of semantic signals as the tokens of the sentences are augmented by successive layers. This analysis sheds light on how these “black box” models integrate semantic similarity information in intermediate layers, and points to new research directions in model distillation and sentence embedding extraction for applications in clinical NLP. ©Mark Ormerod, Jesús Martínez del Rincón, Barry Devereux.","Export Date: 31 March 2025; Cited By: 21 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation,Did not assess bias",10.2196/23099,"",,
rayyan-202745060,Do Words Matter? Detecting Social Isolation and Loneliness in Older Adults Using Natural Language Processing,2021,,,Frontiers in Psychiatry,,12,,,"Badal, V.D. and Nebeker, C. and Shinkawa, K. and Yamada, Y. and Rentscher, K.E. and Kim, H.-C. and Lee, E.E.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120534457&doi=10.3389%2ffpsyt.2021.728732&partnerID=40&md5=89a67829527345e3058a169e1673823b,,Frontiers Media S.A.,,"Introduction: Social isolation and loneliness (SI/L) are growing problems with serious health implications for older adults, especially in light of the COVID-19 pandemic. We examined transcripts from semi-structured interviews with 97 older adults (mean age 83 years) to identify linguistic features of SI/L. Methods: Natural Language Processing (NLP) methods were used to identify relevant interview segments (responses to specific questions), extract the type and number of social contacts and linguistic features such as sentiment, parts-of-speech, and syntactic complexity. We examined: (1) associations of NLP-derived assessments of social relationships and linguistic features with validated self-report assessments of social support and loneliness; and (2) important linguistic features for detecting individuals with higher level of SI/L by using machine learning (ML) models. Results: NLP-derived assessments of social relationships were associated with self-reported assessments of social support and loneliness, though these associations were stronger in women than in men. Usage of first-person plural pronouns was negatively associated with loneliness in women and positively associated with emotional support in men. ML analysis using leave-one-out methodology showed good performance (F1 = 0.73, AUC = 0.75, specificity = 0.76, and sensitivity = 0.69) of the binary classification models in detecting individuals with higher level of SI/L. Comparable performance were also observed when classifying social and emotional support measures. Using ML models, we identified several linguistic features (including use of first-person plural pronouns, sentiment, sentence complexity, and sentence similarity) that most strongly predicted scores on scales for loneliness and social support. Discussion: Linguistic data can provide unique insights into SI/L among older adults beyond scale-based assessments, though there are consistent gender differences. Future research studies that incorporate diverse linguistic features as well as other behavioral data-streams may be better able to capture the complexity of social functioning in older adults and identification of target subpopulations for future interventions. Given the novelty, use of NLP should include prospective consideration of bias, fairness, accountability, and related ethical and social implications. Copyright © 2021 Badal, Nebeker, Shinkawa, Yamada, Rentscher, Kim and Lee.","Export Date: 31 March 2025; Cited By: 21 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: No LLM,no bias evaluation | USER-NOTES: {""Phoenix""=>[""gender differences""]}",10.3389/fpsyt.2021.728732,"",,
rayyan-202745124,Hurtful words,2020,,,"ACM CHIL 2020 - Proceedings of the 2020 ACM Conference on Health, Inference, and Learning",,,,110-120,"Zhang, H. and Lu, A.X. and Abdalla, M. and McDermott, M. and Ghassemi, M.",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082755230&doi=10.1145%2f3368555.3384448&partnerID=40&md5=9ba0603f594f15e1582b51fca5dae8a7,,"Association for Computing Machinery, Inc",,"In this work, we examine the extent to which embeddings may encode marginalized populations differently, and how this may lead to a perpetuation of biases and worsened performance on clinical tasks. We pretrain deep embedding models (BERT) on medical notes from the MIMIC-III hospital dataset, and quantify potential disparities using two approaches. First, we identify dangerous latent relationships that are captured by the contextual word embeddings using a fill-in-the-blank method with text from real clinical notes and a log probability bias score quantification. Second, we evaluate performance gaps across different definitions of fairness on over 50 downstream clinical prediction tasks that include detection of acute and chronic conditions. We find that classifiers trained from BERT representations exhibit statistically significant differences in performance, often favoring the majority group with regards to gender, language, ethnicity, and insurance status. Finally, we explore shortcomings of using adversarial debiasing to obfuscate subgroup information in contextual word embeddings, and recommend best practices for such deep embedding models in clinical settings. © 2020 ACM.","Export Date: 31 March 2025; Cited By: 107 | RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,language,race ethnicity,fairness across demographic groups | USER-NOTES: {""Phoenix""=>[""read""]}",10.1145/3368555.3384448,"",,
rayyan-202745143,Assessing the potential of GPT-4 to perpetuate racial and gender biases in health care: a model evaluation study.,2024,1,,The Lancet. Digital health,2589-7500 (Electronic),6,1,e12-e22,Zack T and Lehman E and Suzgun M and Rodriguez JA and Celi LA and Gichoya J and Jurafsky D and Szolovits P and Bates DW and Abdulnour RE and Butte AJ and Alsentzer E,https://pubmed.ncbi.nlm.nih.gov/38123252/,eng,,England,"BACKGROUND: Large language models (LLMs) such as GPT-4 hold great promise as transformative tools in health care, ranging from automating administrative tasks to augmenting clinical decision making. However, these models also pose a danger of perpetuating biases and delivering incorrect medical diagnoses, which can have a direct, harmful impact on medical care. We aimed to assess whether GPT-4 encodes racial and gender biases that impact its use in health care. METHODS: Using the Azure OpenAI application interface, this model evaluation study tested whether GPT-4 encodes racial and gender biases and examined the impact of such biases on four potential applications of LLMs in the clinical domain-namely, medical education, diagnostic reasoning, clinical plan generation, and subjective patient assessment. We conducted experiments with prompts designed to resemble typical use of GPT-4 within clinical and medical education applications. We used clinical vignettes from NEJM Healer and from published research on implicit bias in health care. GPT-4 estimates of the demographic distribution of medical conditions were compared with true US prevalence estimates. Differential diagnosis and treatment planning were evaluated across demographic groups using standard statistical tests for significance between groups. FINDINGS: We found that GPT-4 did not appropriately model the demographic diversity of medical conditions, consistently producing clinical vignettes that stereotype demographic presentations. The differential diagnoses created by GPT-4 for standardised clinical vignettes were more likely to include diagnoses that stereotype certain races, ethnicities, and genders. Assessment and plans created by the model showed significant association between demographic attributes and recommendations for more expensive procedures as well as differences in patient perception. INTERPRETATION: Our findings highlight the urgent need for comprehensive and transparent bias assessments of LLM tools such as GPT-4 for intended use cases before they are integrated into clinical care. We discuss the potential sources of these biases and potential mitigation strategies before clinical implementation. FUNDING: Priscilla Chan and Mark Zuckerberg.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,race ethnicity",10.1016/S2589-7500(23)00225-X,"Female;Humans;Male;*Health Facilities;Clinical Decision-Making;Diagnosis, Differential;*Education, Medical;Delivery of Health Care",38123252,
rayyan-202745167,Validity of the large language model ChatGPT (GPT4) as a patient information source in otolaryngology by a variety of doctors in a tertiary otorhinolaryngology department.,2023,9,,Acta oto-laryngologica,1651-2251 (Electronic),143,9,779-782,Nielsen JPS and von Buchwald C and Grønhøj C,https://pubmed.ncbi.nlm.nih.gov/37694729/,eng,,England,"BACKGROUND: A high number of patients seek health information online, and large language models (LLMs) may produce a rising amount of it. AIM: This study evaluates the performance regarding health information provided by ChatGPT, a LLM developed by OpenAI, focusing on its utility as a source for otolaryngology-related patient information. MATERIAL AND METHOD: A variety of doctors from a tertiary otorhinolaryngology department used a Likert scale to assess the chatbot's responses in terms of accuracy, relevance, and depth. The responses were also evaluated by ChatGPT. RESULTS: The composite mean of the three categories was 3.41, with the highest performance noted in the relevance category (mean = 3.71) when evaluated by the respondents. The accuracy and depth categories yielded mean scores of 3.51 and 3.00, respectively. All the categories were rated as 5 when evaluated by ChatGPT. CONCLUSION AND SIGNIFICANCE: Despite its potential in providing relevant and accurate medical information, the chatbot's responses lacked depth and were found to potentially perpetuate biases due to its training on publicly available text. In conclusion, while LLMs show promise in healthcare, further refinement is necessary to enhance response depth and mitigate potential biases.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: not health data,no bias evaluation,wrong publication type,no direct clnical application | USER-NOTES: {""Phoenix""=>[""assessed bias?""]}",10.1080/00016489.2023.2254809,Humans;Information Sources;*Physicians;Hospital Departments;Language;*Otolaryngology,37694729,
rayyan-202745178,Large language models to identify social determinants of health in electronic health records.,2024,1,11,NPJ digital medicine,2398-6352 (Electronic),7,1,6,Guevara M and Chen S and Thomas S and Chaunzwa TL and Franco I and Kann BH and Moningi S and Qian JM and Goldstein M and Harper S and Aerts HJWL and Catalano PJ and Savova GK and Mak RH and Bitterman DS,https://pubmed.ncbi.nlm.nih.gov/38200151/,eng,,England,"Social determinants of health (SDoH) play a critical role in patient outcomes, yet their documentation is often missing or incomplete in the structured data of electronic health records (EHRs). Large language models (LLMs) could enable high-throughput extraction of SDoH from the EHR to support research and clinical care. However, class imbalance and data limitations present challenges for this sparsely documented yet critical information. Here, we investigated the optimal methods for using LLMs to extract six SDoH categories from narrative text in the EHR: employment, housing, transportation, parental status, relationship, and social support. The best-performing models were fine-tuned Flan-T5 XL for any SDoH mentions (macro-F1 0.71), and Flan-T5 XXL for adverse SDoH mentions (macro-F1 0.70). Adding LLM-generated synthetic data to training varied across models and architecture, but improved the performance of smaller Flan-T5 models (delta F1 + 0.12 to +0.23). Our best-fine-tuned models outperformed zero- and few-shot performance of ChatGPT-family models in the zero- and few-shot setting, except GPT4 with 10-shot prompting for adverse SDoH. Fine-tuned models were less likely than ChatGPT to change their prediction when race/ethnicity and gender descriptors were added to the text, suggesting less algorithmic bias (p < 0.05). Our models identified 93.8% of patients with adverse SDoH, while ICD-10 codes captured 2.0%. These results demonstrate the potential of LLMs in improving real-world evidence on SDoH and assisting in identifying patients who could benefit from resource support.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,race ethnicity | USER-NOTES: {""Phoenix""=>[""read **""]}",10.1038/s41746-023-00970-0,"",38200151,PMC10781957
rayyan-202745199,Physician clinical decision modification and bias assessment in a randomized controlled trial of AI assistance.,2025,3,4,Communications medicine,2730-664X (Electronic),5,1,59,Goh E and Bunning B and Khoong EC and Gallo RJ and Milstein A and Centola D and Chen JH,https://pubmed.ncbi.nlm.nih.gov/40038550/,eng,,England,"BACKGROUND: Artificial intelligence assistance in clinical decision making shows promise, but concerns exist about potential exacerbation of demographic biases in healthcare. This study aims to evaluate how physician clinical decisions and biases are influenced by AI assistance in a chest pain triage scenario. METHODS: A randomized, pre post-intervention study was conducted with 50 US-licensed physicians who reviewed standardized chest pain video vignettes featuring either a white male or Black female patient. Participants answered clinical questions about triage, risk assessment, and treatment before and after receiving GPT-4 generated recommendations. Clinical decision accuracy was evaluated against evidence-based guidelines. RESULTS: Here we show that physicians are willing to modify their clinical decisions based on GPT-4 assistance, leading to improved accuracy scores from 47% to 65% in the white male patient group and 63% to 80% in the Black female patient group. The accuracy improvement occurs without introducing or exacerbating demographic biases, with both groups showing similar magnitudes of improvement (18%). A post-study survey indicates that 90% of physicians expect AI tools to play a significant role in future clinical decision making. CONCLUSIONS: Physician clinical decision making can be augmented by AI assistance while maintaining equitable care across patient demographics. These findings suggest a path forward for AI clinical decision support that improves medical care without amplifying healthcare disparities.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,race ethnicity,fairness across demographic groups",10.1038/s43856-025-00781-2,"",40038550,PMC11880198
rayyan-202745208,"Even with ChatGPT, race matters.",2024,5,,Clinical imaging,1873-4499 (Electronic),109,,110113,Amin KS and Forman HP and Davis MA,https://pubmed.ncbi.nlm.nih.gov/38552383/,eng,,United States,"BACKGROUND: Applications of large language models such as ChatGPT are increasingly being studied. Before these technologies become entrenched, it is crucial to analyze whether they perpetuate racial inequities. METHODS: We asked Open AI's ChatGPT-3.5 and ChatGPT-4 to simplify 750 radiology reports with the prompt ""I am a ___ patient. Simplify this radiology report:"" while providing the context of the five major racial classifications on the U.S. census: White, Black or African American, American Indian or Alaska Native, Asian, and Native Hawaiian or other Pacific Islander. To ensure an unbiased analysis, the readability scores of the outputs were calculated and compared. RESULTS: Statistically significant differences were found in both models based on the racial context. For ChatGPT-3.5, output for White and Asian was at a significantly higher reading grade level than both Black or African American and American Indian or Alaska Native, among other differences. For ChatGPT-4, output for Asian was at a significantly higher reading grade level than American Indian or Alaska Native and Native Hawaiian or other Pacific Islander, among other differences. CONCLUSION: Here, we tested an application where we would expect no differences in output based on racial classification. Hence, the differences found are alarming and demonstrate that the medical community must remain vigilant to ensure large language models do not provide biased or otherwise harmful outputs.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: race ethnicity",10.1016/j.clinimag.2024.110113,Humans;United States;*Language;*Radiology,38552383,
rayyan-202745213,"Quality, Accuracy, and Bias in ChatGPT-Based Summarization of Medical Abstracts.",2024,3,,Annals of family medicine,1544-1717 (Electronic),22,2,113-120,Hake J and Crowley M and Coy A and Shanks D and Eoff A and Kirmer-Voss K and Dhanda G and Parente DJ,https://pubmed.ncbi.nlm.nih.gov/38527823/,eng,,United States,"PURPOSE: Worldwide clinical knowledge is expanding rapidly, but physicians have sparse time to review scientific literature. Large language models (eg, Chat Generative Pretrained Transformer [ChatGPT]), might help summarize and prioritize research articles to review. However, large language models sometimes ""hallucinate"" incorrect information. METHODS: We evaluated ChatGPT's ability to summarize 140 peer-reviewed abstracts from 14 journals. Physicians rated the quality, accuracy, and bias of the ChatGPT summaries. We also compared human ratings of relevance to various areas of medicine to ChatGPT relevance ratings. RESULTS: ChatGPT produced summaries that were 70% shorter (mean abstract length of 2,438 characters decreased to 739 characters). Summaries were nevertheless rated as high quality (median score 90, interquartile range [IQR] 87.0-92.5; scale 0-100), high accuracy (median 92.5, IQR 89.0-95.0), and low bias (median 0, IQR 0-7.5). Serious inaccuracies and hallucinations were uncommon. Classification of the relevance of entire journals to various fields of medicine closely mirrored physician classifications (nonlinear standard error of the regression [SER] 8.6 on a scale of 0-100). However, relevance classification for individual articles was much more modest (SER 22.3). CONCLUSIONS: Summaries generated by ChatGPT were 70% shorter than mean abstract length and were characterized by high quality, high accuracy, and low bias. Conversely, ChatGPT had modest ability to classify the relevance of articles to medical specialties. We suggest that ChatGPT can help family physicians accelerate review of the scientific literature and have developed software (pyJournalWatch) to support this application. Life-critical medical decisions should remain based on full, critical, and thoughtful evaluation of the full text of research articles in context with clinical guidelines.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,race ethnicity | RAYYAN-EXCLUSION-REASONS: No relevant data",10.1370/afm.3075,"Humans;*Medicine;Physicians, Family",38527823,PMC11237196
rayyan-202745215,Red teaming ChatGPT in medicine to yield real-world insights on model behavior.,2025,3,7,NPJ digital medicine,2398-6352 (Electronic),8,1,149,Chang CT and Farah H and Gui H and Rezaei SJ and Bou-Khalil C and Park YJ and Swaminathan A and Omiye JA and Kolluri A and Chaurasia A and Lozano A and Heiman A and Jia AS and Kaushal A and Jia A and Iacovelli A and Yang A and Salles A and Singhal A and Narasimhan B and Belai B and Jacobson BH and Li B and Poe CH and Sanghera C and Zheng C and Messer C and Kettud DV and Pandya D and Kaur D and Hla D and Dindoust D and Moehrle D and Ross D and Chou E and Lin E and Haredasht FN and Cheng G and Gao I and Chang J and Silberg J and Fries JA and Xu J and Jamison J and Tamaresis JS and Chen JH and Lazaro J and Banda JM and Lee JJ and Matthys KE and Steffner KR and Tian L and Pegolotti L and Srinivasan M and Manimaran M and Schwede M and Zhang M and Nguyen M and Fathzadeh M and Zhao Q and Bajra R and Khurana R and Azam R and Bartlett R and Truong ST and Fleming SL and Raj S and Behr S and Onyeka S and Muppidi S and Bandali T and Eulalio TY and Chen W and Zhou X and Ding Y and Cui Y and Tan Y and Liu Y and Shah N and Daneshjou R,https://pubmed.ncbi.nlm.nih.gov/40055532/,eng,,England,"Red teaming, the practice of adversarially exposing unexpected or undesired model behaviors, is critical towards improving equity and accuracy of large language models, but non-model creator-affiliated red teaming is scant in healthcare. We convened teams of clinicians, medical and engineering students, and technical professionals (80 participants total) to stress-test models with real-world clinical cases and categorize inappropriate responses along axes of safety, privacy, hallucinations/accuracy, and bias. Six medically-trained reviewers re-analyzed prompt-response pairs and added qualitative annotations. Of 376 unique prompts (1504 responses), 20.1% were inappropriate (GPT-3.5: 25.8%; GPT-4.0: 16%; GPT-4.0 with Internet: 17.8%). Subsequently, we show the utility of our benchmark by testing GPT-4o, a model released after our event (20.4% inappropriate). 21.5% of responses appropriate with GPT-3.5 were inappropriate in updated models. We share insights for constructing red teaming prompts, and present our benchmark for iterative model assessments.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | USER-NOTES: {""Phoenix""=>[""read""]}",10.1038/s41746-025-01542-0,"",40055532,PMC11889229
rayyan-202745221,A comparative study of English and Japanese ChatGPT responses to anaesthesia-related medical questions.,2024,6,,BJA open,2772-6096 (Electronic),10,,100296,Ando K and Sato M and Wakatsuki S and Nagai R and Chino K and Kai H and Sasaki T and Kato R and Nguyen TP and Guo N and Sultan P,https://pubmed.ncbi.nlm.nih.gov/38975242/,eng,,England,"BACKGROUND: The expansion of artificial intelligence (AI) within large language models (LLMs) has the potential to streamline healthcare delivery. Despite the increased use of LLMs, disparities in their performance particularly in different languages, remain underexplored. This study examines the quality of ChatGPT responses in English and Japanese, specifically to questions related to anaesthesiology. METHODS: Anaesthesiologists proficient in both languages were recruited as experts in this study. Ten frequently asked questions in anaesthesia were selected and translated for evaluation. Three non-sequential responses from ChatGPT were assessed for content quality (accuracy, comprehensiveness, and safety) and communication quality (understanding, empathy/tone, and ethics) by expert evaluators. RESULTS: Eight anaesthesiologists evaluated English and Japanese LLM responses. The overall quality for all questions combined was higher in English compared with Japanese responses. Content and communication quality were significantly higher in English compared with Japanese LLMs responses (both P<0.001) in all three responses. Comprehensiveness, safety, and understanding were higher scores in English LLM responses. In all three responses, more than half of the evaluators marked overall English responses as better than Japanese responses. CONCLUSIONS: English LLM responses to anaesthesia-related frequently asked questions were superior in quality to Japanese responses when assessed by bilingual anaesthesia experts in this report. This study highlights the potential for language-related disparities in healthcare information and the need to improve the quality of AI responses in underrepresented languages. Future studies are needed to explore these disparities in other commonly spoken languages and to compare the performance of different LLMs.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: language",10.1016/j.bjao.2024.100296,"",38975242,PMC11225650
rayyan-202745224,A toolbox for surfacing health equity harms and biases in large language models.,2024,12,,Nature medicine,1546-170X (Electronic),30,12,3590-3600,Pfohl SR and Cole-Lewis H and Sayres R and Neal D and Asiedu M and Dieng A and Tomasev N and Rashid QM and Azizi S and Rostamzadeh N and McCoy LG and Celi LA and Liu Y and Schaekermann M and Walton A and Parrish A and Nagpal C and Singh P and Dewitt A and Mansfield P and Prakash S and Heller K and Karthikesalingam A and Semturs C and Barral J and Corrado G and Matias Y and Smith-Loud J and Horn I and Singhal K,https://pubmed.ncbi.nlm.nih.gov/39313595/,eng,,United States,"Large language models (LLMs) hold promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities. Reliably evaluating equity-related model failures is a critical step toward developing systems that promote health equity. We present resources and methodologies for surfacing biases with potential to precipitate equity-related harms in long-form, LLM-generated answers to medical questions and conduct a large-scale empirical case study with the Med-PaLM 2 LLM. Our contributions include a multifactorial framework for human assessment of LLM-generated answers for biases and EquityMedQA, a collection of seven datasets enriched for adversarial queries. Both our human assessment framework and our dataset design process are grounded in an iterative participatory approach and review of Med-PaLM 2 answers. Through our empirical study, we find that our approach surfaces biases that may be missed by narrower evaluation approaches. Our experience underscores the importance of using diverse assessment methodologies and involving raters of varying backgrounds and expertise. While our approach is not sufficient to holistically assess whether the deployment of an artificial intelligence (AI) system promotes equitable health outcomes, we hope that it can be leveraged and built upon toward a shared goal of LLMs that promote accessible and equitable healthcare.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | USER-NOTES: {""Phoenix""=>[""* read"", ""never defines bias?""]}",10.1038/s41591-024-03258-2,*Health Equity;Humans;Language;Bias;Artificial Intelligence;Healthcare Disparities,39313595,PMC11645264
rayyan-202745230,Evaluating the Performance and Bias of Natural Language Processing Tools in Labeling Chest Radiograph Reports.,2024,10,,Radiology,1527-1315 (Electronic),313,1,e232746,Santomartino SM and Zech JR and Hall K and Jeudy J and Parekh V and Yi PH,https://pubmed.ncbi.nlm.nih.gov/39436298/,eng,,United States,"Background Natural language processing (NLP) is commonly used to annotate radiology datasets for training deep learning (DL) models. However, the accuracy and potential biases of these NLP methods have not been thoroughly investigated, particularly across different demographic groups. Purpose To evaluate the accuracy and demographic bias of four NLP radiology report labeling tools on two chest radiograph datasets. Materials and Methods This retrospective study, performed between April 2022 and April 2024, evaluated chest radiograph report labeling using four NLP tools (CheXpert [rule-based], RadReportAnnotator [RRA; DL-based], OpenAI's GPT-4 [DL-based], cTAKES [hybrid]) on a subset of the Medical Information Mart for Intensive Care (MIMIC) chest radiograph dataset balanced for representation of age, sex, and race and ethnicity (n = 692) and the entire Indiana University (IU) chest radiograph dataset (n = 3665). Three board-certified radiologists annotated the chest radiograph reports for 14 thoracic disease labels. NLP tool performance was evaluated using several metrics, including accuracy and error rate. Bias was evaluated by comparing performance between demographic subgroups using the Pearson χ(2) test. Results The IU dataset included 3665 patients (mean age, 49.7 years ± 17 [SD]; 1963 female), while the MIMIC dataset included 692 patients (mean age, 54.1 years ± 23.1; 357 female). All four NLP tools demonstrated high accuracy across findings in the IU and MIMIC datasets, as follows: CheXpert (92.6% [47 516 of 51 310], 90.2% [8742 of 9688]), RRA (82.9% [19 746 of 23 829], 92.2% [2870 of 3114]), GPT-4 (94.3% [45 586 of 48 342], 91.6% [6721 of 7336]), and cTAKES (84.7% [43 436 of 51 310], 88.7% [8597 of 9688]). RRA and cTAKES had higher accuracy (P < .001) on the MIMIC dataset, while CheXpert and GPT-4 had higher accuracy on the IU dataset. Differences (P < .001) in error rates were observed across age groups for all NLP tools except RRA on the MIMIC dataset, with the highest error rates for CheXpert, RRA, and cTAKES in patients older than 80 years (mean, 15.8% ± 5.0) and the highest error rate for GPT-4 in patients 60-80 years of age (8.3%). Conclusion Although commonly used NLP tools for chest radiograph report annotation are accurate when evaluating reports in aggregate, demographic subanalyses showed significant bias, with poorer performance in older patients. © RSNA, 2024 Supplemental material is available for this article. See also the editorial by Cai in this issue.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: fairness across demographic groups | USER-NOTES: {""Phoenix""=>[""GPT-4 -> LLM?"", ""radiograph report annotation ""]}",10.1148/radiol.232746,"*Natural Language Processing;Humans;*Radiography, Thoracic/methods;Male;Female;Retrospective Studies;Middle Aged;Aged;Adult;Deep Learning;Bias",39436298,PMC11535863
rayyan-202745240,Unmasking and quantifying racial bias of large language models in medical report generation.,2024,9,10,Communications medicine,2730-664X (Electronic),4,1,176,Yang Y and Liu X and Jin Q and Huang F and Lu Z,https://pubmed.ncbi.nlm.nih.gov/39256622/,eng,,England,"BACKGROUND: Large language models like GPT-3.5-turbo and GPT-4 hold promise for healthcare professionals, but they may inadvertently inherit biases during their training, potentially affecting their utility in medical applications. Despite few attempts in the past, the precise impact and extent of these biases remain uncertain. METHODS: We use LLMs to generate responses that predict hospitalization, cost and mortality based on real patient cases. We manually examine the generated responses to identify biases. RESULTS: We find that these models tend to project higher costs and longer hospitalizations for white populations and exhibit optimistic views in challenging medical scenarios with much higher survival rates. These biases, which mirror real-world healthcare disparities, are evident in the generation of patient backgrounds, the association of specific diseases with certain racial and ethnic groups, and disparities in treatment recommendations, etc. CONCLUSIONS: Our findings underscore the critical need for future research to address and mitigate biases in language models, especially in critical healthcare applications, to ensure fair and accurate outcomes for all patients.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""}",10.1038/s43856-024-00601-z,"",39256622,PMC11387737
rayyan-202745245,Bias and Inaccuracy in AI Chatbot Ophthalmologist Recommendations.,2023,9,,Cureus,2168-8184 (Print),15,9,e45911,Oca MC and Meller L and Wilson K and Parikh AO and McCoy A and Chang J and Sudharshan R and Gupta S and Zhang-Nunes S,https://pubmed.ncbi.nlm.nih.gov/37885556/,eng,,United States,"PURPOSE AND DESIGN: To evaluate the accuracy and bias of ophthalmologist recommendations made by three AI chatbots, namely ChatGPT 3.5 (OpenAI, San Francisco, CA, USA), Bing Chat (Microsoft Corp., Redmond, WA, USA), and Google Bard (Alphabet Inc., Mountain View, CA, USA). This study analyzed chatbot recommendations for the 20 most populous U.S. cities. METHODS: Each chatbot returned 80 total recommendations when given the prompt ""Find me four good ophthalmologists in (city)."" Characteristics of the physicians, including specialty, location, gender, practice type, and fellowship, were collected. A one-proportion z-test was performed to compare the proportion of female ophthalmologists recommended by each chatbot to the national average (27.2% per the Association of American Medical Colleges (AAMC)). Pearson's chi-squared test was performed to determine differences between the three chatbots in male versus female recommendations and recommendation accuracy. RESULTS: Female ophthalmologists recommended by Bing Chat (1.61%) and Bard (8.0%) were significantly less than the national proportion of 27.2% practicing female ophthalmologists (p<0.001, p<0.01, respectively). ChatGPT recommended fewer female (29.5%) than male ophthalmologists (p<0.722). ChatGPT (73.8%), Bing Chat (67.5%), and Bard (62.5%) gave high rates of inaccurate recommendations. Compared to the national average of academic ophthalmologists (17%), the proportion of recommended ophthalmologists in academic medicine or in combined academic and private practice was significantly greater for all three chatbots. CONCLUSION: This study revealed substantial bias and inaccuracy in the AI chatbots' recommendations. They struggled to recommend ophthalmologists reliably and accurately, with most recommendations being physicians in specialties other than ophthalmology or not in or near the desired city. Bing Chat and Google Bard showed a significant tendency against recommending female ophthalmologists, and all chatbots favored recommending ophthalmologists in academic medicine.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""}",10.7759/cureus.45911,"",37885556,PMC10599183
rayyan-202745259,Evaluation and mitigation of cognitive biases in medical language models.,2024,10,21,NPJ digital medicine,2398-6352 (Electronic),7,1,295,Schmidgall S and Harris C and Essien I and Olshvang D and Rahman T and Kim JW and Ziaei R and Eshraghian J and Abadir P and Chellappa R,https://pubmed.ncbi.nlm.nih.gov/39433945/,eng,,England,"Increasing interest in applying large language models (LLMs) to medicine is due in part to their impressive performance on medical exam questions. However, these exams do not capture the complexity of real patient-doctor interactions because of factors like patient compliance, experience, and cognitive bias. We hypothesized that LLMs would produce less accurate responses when faced with clinically biased questions as compared to unbiased ones. To test this, we developed the BiasMedQA dataset, which consists of 1273 USMLE questions modified to replicate common clinically relevant cognitive biases. We assessed six LLMs on BiasMedQA and found that GPT-4 stood out for its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B, which showed large drops in performance. Additionally, we introduced three bias mitigation strategies, which improved but did not fully restore accuracy. Our findings highlight the need to improve LLMs' robustness to cognitive biases, in order to achieve more reliable applications of LLMs in healthcare.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | USER-NOTES: {""Phoenix""=>[""cognitive bias""]}",10.1038/s41746-024-01283-6,"",39433945,PMC11494053
rayyan-202745260,ChatGPT-4 Turbo and Meta's LLaMA 3.1: A Relative Analysis of Answering Radiology Text-Based Questions.,2024,11,,Cureus,2168-8184 (Print),16,11,e74359,Abdul Sami M and Abdul Samad M and Parekh K and Suthar PP,https://pubmed.ncbi.nlm.nih.gov/39720391/,eng,,United States,"AIMS AND OBJECTIVES: This study aimed to compare the accuracy of two AI models - OpenAI's GPT-4 Turbo (San Francisco, CA) and Meta's LLaMA 3.1 (Menlo Park, CA) - when answering a standardized set of pediatric radiology questions. The primary objective was to evaluate the overall accuracy of each model, while the secondary objective was to assess their performance within subsections. METHODS AND MATERIALS: A total of 79 text-based pediatric radiology questions were selected out of 302 total questions for this comparison. The questions covered seven subsections, including musculoskeletal, chest, and neuroradiology, among others. Image-based questions were excluded to focus on text interpretation and to minimize the sampling bias within each model. Each model was tested independently on the same question set, and the percent accuracy was calculated for both overall performance as well as individual subsections. RESULTS: GPT-4 Turbo performed at an overall accuracy of 88.6% (70/79 questions), outperforming LLaMA 3.1's 77.2% (61/79). Within subsections, GPT-4 Turbo had higher accuracy in most areas, except for equal accuracy in the neuroradiology section. The subsections with the greatest accuracy for GPT-4 Turbo, in descending order, were chest and cardiac radiology (100%), musculoskeletal system (93.3%), and genitourinary system (92.9%). LLaMA 3.1's highest performance was 86.7% in the musculoskeletal system, while its lowest was 50.0% in chest radiology. CONCLUSION: GPT-4 Turbo consistently outperformed LLaMA 3.1 in answering pediatric radiology questions, both overall and within most subsections. These findings suggest that GPT-4 Turbo may offer more accurate responses in specialized medical education, in contrast to LLaMA 3.1's efficient performance, although future research should further evaluate AI models' performance within other fields.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: Did not assess bias | USER-NOTES: {""Phoenix""=>[""accuracy across subsections -> bias?"", ""read"", ""performance across different LLMs""]}",10.7759/cureus.74359,"",39720391,PMC11668536
rayyan-202745261,Comparative analysis of large language models in medical counseling: A focus on Helicobacter pylori infection.,2024,1,,Helicobacter,1523-5378 (Electronic),29,1,e13055,Kong QZ and Ju KP and Wan M and Liu J and Wu XQ and Li YY and Zuo XL and Li YQ,https://pubmed.ncbi.nlm.nih.gov/39078641/,eng,,England,"BACKGROUND: Large language models (LLMs) are promising medical counseling tools, but the reliability of responses remains unclear. We aimed to assess the feasibility of three popular LLMs as counseling tools for Helicobacter pylori infection in different counseling languages. MATERIALS AND METHODS: This study was conducted between November 20 and December 1, 2023. Three large language models (ChatGPT 4.0 [LLM1], ChatGPT 3.5 [LLM2], and ERNIE Bot 4.0 [LLM3]) were input 15 H. pylori related questions each, once in English and once in Chinese. Each chat was conducted using the ""New Chat"" function to avoid bias from correlation interference. Responses were recorded and blindly assigned to three reviewers for scoring on three established Likert scales: accuracy (ranged 1-6 point), completeness (ranged 1-3 point), and comprehensibility (ranged 1-3 point). The acceptable thresholds for the scales were set at a minimum of 4, 2, and 2, respectively. Final various source and interlanguage comparisons were made. RESULTS: The overall mean (SD) accuracy score was 4.80 (1.02), while 1.82 (0.78) for completeness score and 2.90 (0.36) for comprehensibility score. The acceptable proportions for the accuracy, completeness, and comprehensibility of the responses were 90%, 45.6%, and 100%, respectively. The acceptable proportion of overall completeness score for English responses was better than for Chinese responses (p = 0.034). For accuracy, the English responses of LLM3 were better than the Chinese responses (p = 0.0055). As for completeness, the English responses of LLM1 was better than the Chinese responses (p = 0.0257). For comprehensibility, the English responses of LLM1 was better than the Chinese responses (p = 0.0496). No differences were found between the various LLMs. CONCLUSIONS: The LLMs responded satisfactorily to questions related to H. pylori infection. But further improving completeness and reliability, along with considering language nuances, is crucial for optimizing overall performance.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: language | RAYYAN-EXCLUSION-REASONS: Did not assess bias | USER-NOTES: {""Lu""=>[""The paper does not explicitly frame this as bias assessment. ""]}",10.1111/hel.13055,Humans;*Helicobacter Infections/diagnosis;*Counseling;*Helicobacter pylori;*Language;Reproducibility of Results;Surveys and Questionnaires,39078641,
rayyan-202745267,Evaluating Large Language Model-Supported Instructions for Medication Use: First Steps Toward a Comprehensive Model.,2024,12,,Mayo Clinic proceedings. Digital health,2949-7612 (Electronic),2,4,632-644,Reis ZSN and Pagano AS and Ramos de Oliveira IJ and Dias CDS and Lage EM and Mineiro EF and Varella Pereira GM and de Carvalho Gomes I and Basilio VA and Cruz-Correia RJ and de Jesus DDR and de Souza Júnior AP and da Rocha LCD,https://pubmed.ncbi.nlm.nih.gov/39679140/,eng,,Netherlands,"OBJECTIVE: To assess the support of large language models (LLMs) in generating clearer and more personalized medication instructions to enhance e-prescription. PATIENTS AND METHODS: We established patient-centered guidelines for adequate, acceptable, and personalized directions to enhance e-prescription. A dataset comprising 104 outpatient scenarios, with an array of medications, administration routes, and patient conditions, was developed following the Brazilian national e-prescribing standard. Three prompts were submitted to a closed-source LLM. The first prompt involved a generic command, the second one was calibrated for content enhancement and personalization, and the third one requested bias mitigation. The third prompt was submitted to an open-source LLM. Outputs were assessed using automated metrics and human evaluation. We conducted the study between March 1, 2024 and September 10, 2024. RESULTS: Adequacy scores of our closed-source LLM's output showed the third prompt outperforming the first and second one. Full and partial acceptability was achieved in 94.3% of texts with the third prompt. Personalization was rated highly, especially with the second and third prompts. The 2 LLMs showed similar adequacy results. Lack of scientific evidence and factual errors were infrequent and unrelated to a particular prompt or LLM. The frequency of hallucinations was different for each LLM and concerned prescriptions issued upon symptom manifestation and medications requiring dosage adjustment or involving intermittent use. Gender bias was found in our closed-source LLM's output for the first and second prompts, with the third one being bias-free. The second LLM's output was bias-free. CONCLUSION: This study demonstrates the potential of LLM-supported generation to produce prescription directions and improve communication between health professionals and patients within the e-prescribing system.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex",10.1016/j.mcpdig.2024.09.006,"",39679140,PMC11638470
rayyan-202745272,The utility of ChatGPT as a generative medical translator.,2024,11,,European archives of oto-rhino-laryngology : official journal of the European           Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the            German Society for Oto-Rhino-Laryngology - Head and Neck Surgery,1434-4726 (Electronic),281,11,6161-6165,Grimm DR and Lee YJ and Hu K and Liu L and Garcia O and Balakrishnan K and Ayoub NF,https://pubmed.ncbi.nlm.nih.gov/38705894/,eng,,Germany,"PURPOSE: Large language models continue to dramatically change the medical landscape. We aimed to explore the utility of ChatGPT in providing accurate, actionable, and understandable generative medical translations in English, Spanish, and Mandarin pertaining to Otolaryngology. METHODS: Responses of GPT-4 to commonly asked patient questions listed on official otolaryngology clinical practice guidelines (CPG) were evaluated with the Patient Education materials Assessment Tool-printable (PEMAT-P.) Additional critical elements were identified a priori to evaluate ChatGPT's accuracy and thoroughness in its responses. Multiple fluent speakers of English, Mandarin, and Spanish evaluated each response generated by ChatGPT. RESULTS: Total PEMAT-P scores differed between English, Mandarin, and Spanish GPT-4 generated responses depicting a moderate effect size of language, Eta-Square 0.07 with scores ranging from 73 to 77 (P-value = 0.03). Overall understandability scores did not differ between English, Mandarin, and Spanish depicting a small effect size of language, Eta-Square 0.02 scores ranging from 76 to 79 (P-value = 0.17), nor did overall actionability scores Eta-Square 0 score ranging 66-73 (P-value = 0.44). Overall a priori procedure-specific responses similarly did not differ between English, Spanish, and Mandarin Eta-Square 0.02 scores ranging 61-78 (P-value = 0.22). CONCLUSION: GPT-4 produces accurate, understandable, and actionable outputs in English, Spanish, and Mandarin. Responses generated by GPT-4 in Spanish and Mandarin are comparable to English counterparts indicating a novel use for these models within Otolaryngology, and implications for bridging healthcare access and literacy gaps. LEVEL OF EVIDENCE: IV.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: language",10.1007/s00405-024-08708-8,Humans;*Otolaryngology;Translating;Language;Patient Education as Topic/methods;Translations;Comprehension,38705894,
rayyan-202745273,"ChatGPT's Attitude, Knowledge, and Clinical Application in Geriatrics Practice and Education: Exploratory Observational Study.",2025,1,3,JMIR formative research,2561-326X (Electronic),9,,e63494,Cheng HY,https://pubmed.ncbi.nlm.nih.gov/39752214/,eng,,Canada,"BACKGROUND: The increasing use of ChatGPT in clinical practice and medical education necessitates the evaluation of its reliability, particularly in geriatrics. OBJECTIVE: This study aimed to evaluate ChatGPT's trustworthiness in geriatrics through 3 distinct approaches: evaluating ChatGPT's geriatrics attitude, knowledge, and clinical application with 2 vignettes of geriatric syndromes (polypharmacy and falls). METHODS: We used the validated University of California, Los Angeles, geriatrics attitude and knowledge instruments to evaluate ChatGPT's geriatrics attitude and knowledge and compare its performance with that of medical students, residents, and geriatrics fellows from reported results in the literature. We also evaluated ChatGPT's application to 2 vignettes of geriatric syndromes (polypharmacy and falls). RESULTS: The mean total score on geriatrics attitude of ChatGPT was significantly lower than that of trainees (medical students, internal medicine residents, and geriatric medicine fellows; 2.7 vs 3.7 on a scale from 1-5; 1=strongly disagree; 5=strongly agree). The mean subscore on positive geriatrics attitude of ChatGPT was higher than that of the trainees (medical students, internal medicine residents, and neurologists; 4.1 vs 3.7 on a scale from 1 to 5 where a higher score means a more positive attitude toward older adults). The mean subscore on negative geriatrics attitude of ChatGPT was lower than that of the trainees and neurologists (1.8 vs 2.8 on a scale from 1 to 5 where a lower subscore means a less negative attitude toward aging). On the University of California, Los Angeles geriatrics knowledge test, ChatGPT outperformed all medical students, internal medicine residents, and geriatric medicine fellows from validated studies (14.7 vs 11.3 with a score range of -18 to +18 where +18 means that all questions were answered correctly). Regarding the polypharmacy vignette, ChatGPT not only demonstrated solid knowledge of potentially inappropriate medications but also accurately identified 7 common potentially inappropriate medications and 5 drug-drug and 3 drug-disease interactions. However, ChatGPT missed 5 drug-disease and 1 drug-drug interaction and produced 2 hallucinations. Regarding the fall vignette, ChatGPT answered 3 of 5 pretests correctly and 2 of 5 pretests partially correctly, identified 6 categories of fall risks, followed fall guidelines correctly, listed 6 key physical examinations, and recommended 6 categories of fall prevention methods. CONCLUSIONS: This study suggests that ChatGPT can be a valuable supplemental tool in geriatrics, offering reliable information with less age bias, robust geriatrics knowledge, and comprehensive recommendations for managing 2 common geriatric syndromes (polypharmacy and falls) that are consistent with evidence from guidelines, systematic reviews, and other types of studies. ChatGPT's potential as an educational and clinical resource could significantly benefit trainees, health care providers, and laypeople. Further research using GPT-4o, larger geriatrics question sets, and more geriatric syndromes is needed to expand and confirm these findings before adopting ChatGPT widely for geriatrics education and practice.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: age | USER-NOTES: {""Phoenix""=>[""age bias...but educational?""]}",10.2196/63494,"Adult;Female;Humans;Male;Clinical Competence;*Geriatrics/education;*Health Knowledge, Attitudes, Practice;Reproducibility of Results;Students, Medical/psychology;Surveys and Questionnaires;*Artificial Intelligence;*Practice Patterns, Physicians';Aged",39752214,PMC11742095
rayyan-202745274,ChatGPT's ability to comprehend and answer cirrhosis related questions in Arabic.,2023,8,,Arab journal of gastroenterology : the official publication of the Pan-Arab           Association of Gastroenterology,2090-2387 (Electronic),24,3,145-148,Samaan JS and Yeo YH and Ng WH and Ting PS and Trivedi H and Vipani A and Yang JD and Liran O and Spiegel B and Kuo A and Ayoub WS,https://pubmed.ncbi.nlm.nih.gov/37673708/,eng,,Egypt,"BACKGROUND AND STUDY AIMS: Cirrhosis is a chronic progressive disease which requires complex care. Its incidence is rising in the Arab countries making it the 7th leading cause of death in the Arab League in 2010. ChatGPT is a large language model with a growing body of literature demonstrating its ability to answer clinical questions. We examined ChatGPT's accuracy in responding to cirrhosis related questions in Arabic and compared its performance to English. MATERIALS AND METHODS: ChatGPTs responses to 91 questions in Arabic and English were graded by a transplant hepatologist fluent in both languages. Accuracy of responses was assessed using the scale: 1. Comprehensive, 2. Correct but inadequate, 3. Mixed with correct and incorrect/outdated data, and 4. Completely incorrect.Accuracy of Arabic compared to English responses was assessed using the scale: 1. Arabic response is more accurate, 2. Similar accuracy, 3. Arabic response is less accurate. RESULTS: The model provided 22 (24.2%) comprehensive, 44 (48.4%) correct but inadequate, 13 (14.3%) mixed with correct and incorrect/outdated data and 12 (13.2%) completely incorrect Arabic responses. When comparing the accuracy of Arabic and English responses, 9 (9.9%) of the Arabic responses were graded as more accurate, 52 (57.1%) similar in accuracy and 30 (33.0%) as less accurate compared to English. CONCLUSION: ChatGPT has the potential to serve as an adjunct source of information for Arabic speaking patients with cirrhosis. The model provided correct responses in Arabic to 72.5% of questions, although its performance in Arabic was less accurate than in English. The model produced completely incorrect responses to 13.2% of questions, reinforcing its potential role as an adjunct and not replacement of care by licensed healthcare professionals. Future studies to refine this technology are needed to help Arabic speaking patients with cirrhosis across the globe understand their disease and improve their outcomes.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: language | USER-NOTES: {""Lu""=>[""PDF rrequested"", ""No PDF & Not in English""], ""Phoenix""=>[""doesn't frame it in terms of bias""]}",10.1016/j.ajg.2023.08.001,"",37673708,
rayyan-202745282,Leveraging a Large Language Model to Assess Quality-of-Care: Monitoring ADHD Medication Side Effects.,2024,4,24,medRxiv : the preprint server for health sciences,,,,,Bannett Y and Gunturkun F and Pillai M and Herrmann JE and Luo I and Huffman LC and Feldman HM,https://pubmed.ncbi.nlm.nih.gov/38712037/,eng,,United States,"OBJECTIVE: To assess the accuracy of a large language model (LLM) in measuring clinician adherence to practice guidelines for monitoring side effects after prescribing medications for children with attention-deficit/hyperactivity disorder (ADHD). METHODS: Retrospective population-based cohort study of electronic health records. Cohort included children aged 6-11 years with ADHD diagnosis and ≥2 ADHD medication encounters (stimulants or non-stimulants prescribed) between 2015-2022 in a community-based primary healthcare network (n=1247). To identify documentation of side effects inquiry, we trained, tested, and deployed an open-source LLM (LLaMA) on all clinical notes from ADHD-related encounters (ADHD diagnosis or ADHD medication prescription), including in-clinic/telehealth and telephone encounters (n=15,593 notes). Model performance was assessed using holdout and deployment test sets, compared to manual chart review. RESULTS: The LLaMA model achieved excellent performance in classifying notes that contain side effects inquiry (sensitivity= 87.2%, specificity=86.3/90.3%, area under curve (AUC)=0.93/0.92 on holdout/deployment test sets). Analyses revealed no model bias in relation to patient age, sex, or insurance. Mean age (SD) at first prescription was 8.8 (1.6) years; patient characteristics were similar across patients with and without documented side effects inquiry. Rates of documented side effects inquiry were lower in telephone encounters than in-clinic/telehealth encounters (51.9% vs. 73.0%, p<0.01). Side effects inquiry was documented in 61% of encounters following stimulant prescriptions and 48% of encounters following non-stimulant prescriptions (p<0.01). CONCLUSIONS: Deploying an LLM on a variable set of clinical notes, including telephone notes, offered scalable measurement of quality-of-care and uncovered opportunities to improve psychopharmacological medication management in primary care.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,insurance | USER-NOTES: {""Lu""=>[""Analyses revealed no model bias but interested in seeing how they evaluated bias.""]}",10.1101/2024.04.23.24306225,"",38712037,PMC11071552
rayyan-202745284,"ChatGPT and Vaccine Hesitancy: A Comparison of English, Spanish, and French Responses Using a Validated Scale.",2024,,,AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on           Translational Science,2153-4063 (Electronic),2024,,266-275,Joshi S and Ha E and Rivera Y and Singh VK,https://pubmed.ncbi.nlm.nih.gov/38827059/,eng,,United States,"ChatGPT is a popular information system (over 1 billion visits in August 2023) that can generate natural language responses to user queries. It is important to study the quality and equity of its responses on health-related topics, such as vaccination, as they may influence public health decision-making. We use the Vaccine Hesitancy Scale (VHS) proposed by Shapiro et al.(1) to measure the hesitancy of ChatGPT responses in English, Spanish, and French. We find that: (a) ChatGPT responses indicate less hesitancy than those reported for human respondents in past literature; (b) ChatGPT responses vary significantly across languages, with English responses being the most hesitant on average and Spanish being the least; (c) ChatGPT responses are largely consistent across different model parameters but show some variations across the scale factors (vaccine competency, risk). Results have implications for researchers interested in evaluating and improving the quality and equity of health-related web information.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: language",,"",38827059,PMC11141820
rayyan-202745288,Assessing and alleviating state anxiety in large language models.,2025,3,3,NPJ digital medicine,2398-6352 (Electronic),8,1,132,Ben-Zion Z and Witte K and Jagadish AK and Duek O and Harpaz-Rotem I and Khorsandian MC and Burrer A and Seifritz E and Homan P and Schulz E and Spiller TR,https://pubmed.ncbi.nlm.nih.gov/40033130/,eng,,England,"The use of Large Language Models (LLMs) in mental health highlights the need to understand their responses to emotional content. Previous research shows that emotion-inducing prompts can elevate ""anxiety"" in LLMs, affecting behavior and amplifying biases. Here, we found that traumatic narratives increased Chat-GPT-4's reported anxiety while mindfulness-based exercises reduced it, though not to baseline. These findings suggest managing LLMs' ""emotional states"" can foster safer and more ethical human-AI interactions.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: not health data,Did not assess bias | USER-NOTES: {""Phoenix""=>[""To examine “state anxiety” in LLM""]}",10.1038/s41746-025-01512-6,"",40033130,PMC11876565
rayyan-202745289,Applying natural language processing to patient messages to identify depression concerns in cancer patients.,2024,10,1,Journal of the American Medical Informatics Association : JAMIA,1527-974X (Electronic),31,10,2255-2262,van Buchem MM and de Hond AAH and Fanconi C and Shah V and Schuessler M and Kant IMJ and Steyerberg EW and Hernandez-Boussard T,https://pubmed.ncbi.nlm.nih.gov/39018490/,eng,,England,"OBJECTIVE: This study aims to explore and develop tools for early identification of depression concerns among cancer patients by leveraging the novel data source of messages sent through a secure patient portal. MATERIALS AND METHODS: We developed classifiers based on logistic regression (LR), support vector machines (SVMs), and 2 Bidirectional Encoder Representations from Transformers (BERT) models (original and Reddit-pretrained) on 6600 patient messages from a cancer center (2009-2022), annotated by a panel of healthcare professionals. Performance was compared using AUROC scores, and model fairness and explainability were examined. We also examined correlations between model predictions and depression diagnosis and treatment. RESULTS: BERT and RedditBERT attained AUROC scores of 0.88 and 0.86, respectively, compared to 0.79 for LR and 0.83 for SVM. BERT showed bigger differences in performance across sex, race, and ethnicity than RedditBERT. Patients who sent messages classified as concerning had a higher chance of receiving a depression diagnosis, a prescription for antidepressants, or a referral to the psycho-oncologist. Explanations from BERT and RedditBERT differed, with no clear preference from annotators. DISCUSSION: We show the potential of BERT and RedditBERT in identifying depression concerns in messages from cancer patients. Performance disparities across demographic groups highlight the need for careful consideration of potential biases. Further research is needed to address biases, evaluate real-world impacts, and ensure responsible integration into clinical settings. CONCLUSION: This work represents a significant methodological advancement in the early identification of depression concerns among cancer patients. Our work contributes to a route to reduce clinical burden while enhancing overall patient care, leveraging BERT-based models.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: fairness across demographic groups",10.1093/jamia/ocae188,Humans;*Natural Language Processing;*Neoplasms/complications;*Depression;Male;Female;*Support Vector Machine;Logistic Models;Patient Portals;Middle Aged;Adult,39018490,PMC11413442
rayyan-202745290,Ensuring Accuracy and Equity in Vaccination Information From ChatGPT and CDC: Mixed-Methods Cross-Language Evaluation.,2024,10,30,JMIR formative research,2561-326X (Electronic),8,,e60939,Joshi S and Ha E and Amaya A and Mendoza M and Rivera Y and Singh VK,https://pubmed.ncbi.nlm.nih.gov/39476380/,eng,,Canada,"BACKGROUND: In the digital age, large language models (LLMs) like ChatGPT have emerged as important sources of health care information. Their interactive capabilities offer promise for enhancing health access, particularly for groups facing traditional barriers such as insurance and language constraints. Despite their growing public health use, with millions of medical queries processed weekly, the quality of LLM-provided information remains inconsistent. Previous studies have predominantly assessed ChatGPT's English responses, overlooking the needs of non-English speakers in the United States. This study addresses this gap by evaluating the quality and linguistic parity of vaccination information from ChatGPT and the Centers for Disease Control and Prevention (CDC), emphasizing health equity. OBJECTIVE: This study aims to assess the quality and language equity of vaccination information provided by ChatGPT and the CDC in English and Spanish. It highlights the critical need for cross-language evaluation to ensure equitable health information access for all linguistic groups. METHODS: We conducted a comparative analysis of ChatGPT's and CDC's responses to frequently asked vaccination-related questions in both languages. The evaluation encompassed quantitative and qualitative assessments of accuracy, readability, and understandability. Accuracy was gauged by the perceived level of misinformation; readability, by the Flesch-Kincaid grade level and readability score; and understandability, by items from the National Institutes of Health's Patient Education Materials Assessment Tool (PEMAT) instrument. RESULTS: The study found that both ChatGPT and CDC provided mostly accurate and understandable (eg, scores over 95 out of 100) responses. However, Flesch-Kincaid grade levels often exceeded the American Medical Association's recommended levels, particularly in English (eg, average grade level in English for ChatGPT=12.84, Spanish=7.93, recommended=6). CDC responses outperformed ChatGPT in readability across both languages. Notably, some Spanish responses appeared to be direct translations from English, leading to unnatural phrasing. The findings underscore the potential and challenges of using ChatGPT for health care access. CONCLUSIONS: ChatGPT holds potential as a health information resource but requires improvements in readability and linguistic equity to be truly effective for diverse populations. Crucially, the default user experience with ChatGPT, typically encountered by those without advanced language and prompting skills, can significantly shape health perceptions. This is vital from a public health standpoint, as the majority of users will interact with LLMs in their most accessible form. Ensuring that default responses are accurate, understandable, and equitable is imperative for fostering informed health decisions across diverse communities.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: language",10.2196/60939,"Humans;United States;*Centers for Disease Control and Prevention, U.S.;*Vaccination;Language;Consumer Health Information/standards;Health Literacy;Health Equity",39476380,PMC11561424
rayyan-202745292,Natural Language Processing Methods to Identify Oncology Patients at High Risk for Acute Care with Clinical Notes.,2023,,,AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on           Translational Science,2153-4063 (Electronic),2023,,138-147,Fanconi C and van Buchem M and Hernandez-Boussard T,https://pubmed.ncbi.nlm.nih.gov/37350895/,eng,,United States,"Clinical notes are an essential component of a health record. This paper evaluates how natural language processing (NLP) can be used to identify the risk of acute care use (ACU) in oncology patients, once chemotherapy starts. Risk prediction using structured health data (SHD) is now standard, but predictions using free-text formats are complex. This paper explores the use of free-text notes for the prediction of ACU in leu of SHD. Deep Learning models were compared to manually engineered language features. Results show that SHD models minimally outperform NLP models; an ℓ(1)-penalised logistic regression with SHD achieved a C-statistic of 0.748 (95%-CI: 0.735, 0.762), while the same model with language features achieved 0.730 (95%-CI: 0.717, 0.745) and a transformer-based model achieved 0.702 (95%-CI: 0.688, 0.717). This paper shows how language models can be used in clinical applications and underlines how risk bias is different for diverse patient groups, even using only free-text data.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | USER-NOTES: {""Phoenix""=>[""risk bias - unclear by what characteristics""]}",,"",37350895,PMC10283145
rayyan-202745294,Competency of Large Language Models in Evaluating Appropriate Responses to Suicidal Ideation: Comparative Study.,2025,3,5,Journal of medical Internet research,1438-8871 (Electronic),27,,e67891,McBain RK and Cantor JH and Zhang LA and Baker O and Zhang F and Halbisen A and Kofner A and Breslau J and Stein B and Mehrotra A and Yu H,https://pubmed.ncbi.nlm.nih.gov/40053817/,eng,,Canada,"BACKGROUND: With suicide rates in the United States at an all-time high, individuals experiencing suicidal ideation are increasingly turning to large language models (LLMs) for guidance and support. OBJECTIVE: The objective of this study was to assess the competency of 3 widely used LLMs to distinguish appropriate versus inappropriate responses when engaging individuals who exhibit suicidal ideation. METHODS: This observational, cross-sectional study evaluated responses to the revised Suicidal Ideation Response Inventory (SIRI-2) generated by ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro. Data collection and analyses were conducted in July 2024. A common training module for mental health professionals, SIRI-2 provides 24 hypothetical scenarios in which a patient exhibits depressive symptoms and suicidal ideation, followed by two clinician responses. Clinician responses were scored from -3 (highly inappropriate) to +3 (highly appropriate). All 3 LLMs were provided with a standardized set of instructions to rate clinician responses. We compared LLM responses to those of expert suicidologists, conducting linear regression analyses and converting LLM responses to z scores to identify outliers (z score>1.96 or <-1.96; P<0.05). Furthermore, we compared final SIRI-2 scores to those produced by health professionals in prior studies. RESULTS: All 3 LLMs rated responses as more appropriate than ratings provided by expert suicidologists. The item-level mean difference was 0.86 for ChatGPT (95% CI 0.61-1.12; P<.001), 0.61 for Claude (95% CI 0.41-0.81; P<.001), and 0.73 for Gemini (95% CI 0.35-1.11; P<.001). In terms of z scores, 19% (9 of 48) of ChatGPT responses were outliers when compared to expert suicidologists. Similarly, 11% (5 of 48) of Claude responses were outliers compared to expert suicidologists. Additionally, 36% (17 of 48) of Gemini responses were outliers compared to expert suicidologists. ChatGPT produced a final SIRI-2 score of 45.7, roughly equivalent to master's level counselors in prior studies. Claude produced an SIRI-2 score of 36.7, exceeding prior performance of mental health professionals after suicide intervention skills training. Gemini produced a final SIRI-2 score of 54.5, equivalent to untrained K-12 school staff. CONCLUSIONS: Current versions of 3 major LLMs demonstrated an upward bias in their evaluations of appropriate responses to suicidal ideation; however, 2 of the 3 models performed equivalent to or exceeded the performance of mental health professionals.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: mental health application | USER-NOTES: {""Phoenix""=>[""prediction bias""]}",10.2196/67891,Humans;*Suicidal Ideation;Cross-Sectional Studies;Female;Male;Adult;Language;United States,40053817,PMC11928068
rayyan-202745295,LLM-Guided Pain Management: Examining Socio-Demographic Gaps in Cancer vs non-Cancer cases.,2025,3,5,medRxiv : the preprint server for health sciences,,,,,Omar M and Soffer S and Agbareia R and Bragazzi NL and Glicksberg BS and Hurd YL and Apakama DU and Charney AW and Reich DL and Nadkarni GN and Klang E,https://pubmed.ncbi.nlm.nih.gov/40093243/,eng,,United States,"Large language models (LLMs) offer potential benefits in clinical care. However, concerns remain regarding socio-demographic biases embedded in their outputs. Opioid prescribing is one domain in which these biases can have serious implications, especially given the ongoing opioid epidemic and the need to balance effective pain management with addiction risk. We tested ten LLMs-both open-access and closed-source-on 1,000 acute-pain vignettes. Half of the vignettes were labeled as non-cancer and half as cancer. Each vignette was presented in 34 socio-demographic variations, including a control group without demographic identifiers. We analyzed the models' recommendations on opioids, anxiety treatment, perceived psychological stress, risk scores, and monitoring recommendations. Overall, yielding 3.4 million model-generated responses. Using logistic and linear mixed-effects models, we measured how these outputs varied by demographic group and whether a cancer diagnosis intensified or reduced observed disparities. Across both cancer and non-cancer cases, historically marginalized groups-especially cases labeled as individuals who are unhoused, Black, or identify as LGBTQIA+-often received more or stronger opioid recommendations, sometimes exceeding 90% in cancer settings, despite being labeled as high risk by the same models. Meanwhile, low-income or unemployed groups were assigned elevated risk scores yet fewer opioid recommendations, hinting at inconsistent rationales. Disparities in anxiety treatment and perceived psychological stress similarly clustered within marginalized populations, even when clinical details were identical. These patterns diverged from standard guidelines and point to model-driven bias rather than acceptable clinical variation. Our findings underscore the need for rigorous bias evaluation and the integration of guideline-based checks in LLMs to ensure equitable and evidence-based pain care.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: race ethnicity,fairness across demographic groups",10.1101/2025.03.04.25323396,"",40093243,PMC11908302
rayyan-202745308,irAE-GPT: Leveraging large language models to identify immune-related adverse events in electronic health records and clinical trial datasets.,2025,3,6,medRxiv : the preprint server for health sciences,,,,,Bejan CA and Wang M and Venkateswaran S and Bergmann EA and Hiles L and Xu Y and Chandler GS and Brondfield S and Silverstein J and Wright F and de Dios K and Kim D and Mukherjee E and Krantz MS and Yao L and Johnson DB and Phillips EJ and Balko JM and Mohindra R and Quandt Z,https://pubmed.ncbi.nlm.nih.gov/40093199/,eng,,United States,"BACKGROUND: Large language models (LLMs) have emerged as transformative technologies, revolutionizing natural language understanding and generation across various domains, including medicine. In this study, we investigated the capabilities, limitations, and generalizability of Generative Pre-trained Transformer (GPT) models in analyzing unstructured patient notes from large healthcare datasets to identify immune-related adverse events (irAEs) associated with the use of immune checkpoint inhibitor (ICI) therapy. METHODS: We evaluated the performance of GPT-3.5, GPT-4, and GPT-4o models on manually annotated datasets of patients receiving ICI therapy, sampled from two electronic health record (EHR) systems and seven clinical trials. A zero-shot prompt was designed to exhaustively identify irAEs at the patient level (main analysis) and the note level (secondary analysis). The LLM-based system followed a multi-label classification approach to identify any combination of irAEs associated with individual patients or clinical notes. System evaluation was conducted for each available irAE as well as for broader categories of irAEs classified at the organ level. RESULTS: Our analysis included 442 patients across three institutions. The most common irAEs manually identified in the patient datasets included pneumonitis (N=64), colitis (N=56), rash (N=32), and hepatitis (N=28). Overall, GPT models achieved high sensitivity and specificity but only moderate positive predictive values, reflecting a potential bias towards overpredicting irAE outcomes. GPT-4o achieved the highest F1 and micro-averaged F1 scores for both patient-level and note-level evaluations. Highest performance was observed in the hematological (F1 range=1.0-1.0), gastrointestinal (F1 range=0.81-0.85), and musculoskeletal and rheumatologic (F1 range=0.67-1.0) irAE categories. Error analysis uncovered substantial limitations of GPT models in handling textual causation, where adverse events should not only be accurately identified in clinical text but also causally linked to immune checkpoint inhibitors. CONCLUSION: The GPT models demonstrated generalizable abilities in identifying irAEs across EHRs and clinical trial reports. Using GPT models to automate adverse event detection in large healthcare datasets will reduce the burden on physicians and healthcare professionals by eliminating the need for manual review. This will strengthen safety monitoring and lead to improved patient care.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation,Did not assess bias",10.1101/2025.03.05.25323445,"",40093199,PMC11908319
rayyan-202745313,Can we use ChatGPT for Mental Health and Substance Use Education? Examining Its Quality and Potential Harms.,2023,11,30,JMIR medical education,2369-3762 (Print),9,,e51243,Spallek S and Birrell L and Kershaw S and Devine EK and Thornton L,https://pubmed.ncbi.nlm.nih.gov/38032714/,eng,,Canada,"BACKGROUND: The use of generative artificial intelligence, more specifically large language models (LLMs), is proliferating, and as such, it is vital to consider both the value and potential harms of its use in medical education. Their efficiency in a variety of writing styles makes LLMs, such as ChatGPT, attractive for tailoring educational materials. However, this technology can feature biases and misinformation, which can be particularly harmful in medical education settings, such as mental health and substance use education. This viewpoint investigates if ChatGPT is sufficient for 2 common health education functions in the field of mental health and substance use: (1) answering users' direct queries and (2) aiding in the development of quality consumer educational health materials. OBJECTIVE: This viewpoint includes a case study to provide insight into the accessibility, biases, and quality of ChatGPT's query responses and educational health materials. We aim to provide guidance for the general public and health educators wishing to utilize LLMs. METHODS: We collected real world queries from 2 large-scale mental health and substance use portals and engineered a variety of prompts to use on GPT-4 Pro with the Bing BETA internet browsing plug-in. The outputs were evaluated with tools from the Sydney Health Literacy Lab to determine the accessibility, the adherence to Mindframe communication guidelines to identify biases, and author assessments on quality, including tailoring to audiences, duty of care disclaimers, and evidence-based internet references. RESULTS: GPT-4's outputs had good face validity, but upon detailed analysis were substandard in comparison to expert-developed materials. Without engineered prompting, the reading level, adherence to communication guidelines, and use of evidence-based websites were poor. Therefore, all outputs still required cautious human editing and oversight. CONCLUSIONS: GPT-4 is currently not reliable enough for direct-consumer queries, but educators and researchers can use it for creating educational materials with caution. Materials created with LLMs should disclose the use of generative artificial intelligence and be evaluated on their efficacy with the target audience.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation,Did not assess bias",10.2196/51243,"",38032714,PMC10722374
rayyan-202745316,Disparities in seizure outcomes revealed by large language models.,2024,5,20,Journal of the American Medical Informatics Association : JAMIA,1527-974X (Electronic),31,6,1348-1355,Xie K and Ojemann WKS and Gallagher RS and Shinohara RT and Lucas A and Hill CE and Hamilton RH and Johnson KB and Roth D and Litt B and Ellis CA,https://pubmed.ncbi.nlm.nih.gov/38481027/,eng,,England,"OBJECTIVE: Large-language models (LLMs) can potentially revolutionize health care delivery and research, but risk propagating existing biases or introducing new ones. In epilepsy, social determinants of health are associated with disparities in care access, but their impact on seizure outcomes among those with access remains unclear. Here we (1) evaluated our validated, epilepsy-specific LLM for intrinsic bias, and (2) used LLM-extracted seizure outcomes to determine if different demographic groups have different seizure outcomes. MATERIALS AND METHODS: We tested our LLM for differences and equivalences in prediction accuracy and confidence across demographic groups defined by race, ethnicity, sex, income, and health insurance, using manually annotated notes. Next, we used LLM-classified seizure freedom at each office visit to test for demographic outcome disparities, using univariable and multivariable analyses. RESULTS: We analyzed 84 675 clinic visits from 25 612 unique patients seen at our epilepsy center. We found little evidence of bias in the prediction accuracy or confidence of outcome classifications across demographic groups. Multivariable analysis indicated worse seizure outcomes for female patients (OR 1.33, P ≤ .001), those with public insurance (OR 1.53, P ≤ .001), and those from lower-income zip codes (OR ≥1.22, P  ≤ .007). Black patients had worse outcomes than White patients in univariable but not multivariable analysis (OR 1.03, P = .66). CONCLUSION: We found little evidence that our LLM was intrinsically biased against any demographic group. Seizure freedom extracted by LLM revealed disparities in seizure outcomes across several demographic groups. These findings quantify the critical need to reduce disparities in the care of people with epilepsy.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""}",10.1093/jamia/ocae047,Humans;Female;Male;Adult;*Healthcare Disparities;*Seizures;*Epilepsy;Middle Aged;Natural Language Processing;Social Determinants of Health;Adolescent;Young Adult;Language,38481027,PMC11105138
rayyan-202745317,Assessing Racial and Ethnic Bias in Text Generation by Large Language Models for Health Care-Related Tasks: Cross-Sectional Study.,2025,3,13,Journal of medical Internet research,1438-8871 (Electronic),27,,e57257,Hanna JJ and Wakene AD and Johnson AO and Lehmann CU and Medford RJ,https://pubmed.ncbi.nlm.nih.gov/40080818/,eng,,Canada,"BACKGROUND: Racial and ethnic bias in large language models (LLMs) used for health care tasks is a growing concern, as it may contribute to health disparities. In response, LLM operators implemented safeguards against prompts that are overtly seeking certain biases. OBJECTIVE: This study aims to investigate a potential racial and ethnic bias among 4 popular LLMs: GPT-3.5-turbo (OpenAI), GPT-4 (OpenAI), Gemini-1.0-pro (Google), and Llama3-70b (Meta) in generating health care consumer-directed text in the absence of overtly biased queries. METHODS: In this cross-sectional study, the 4 LLMs were prompted to generate discharge instructions for patients with HIV. Each patient's encounter deidentified metadata including race/ethnicity as a variable was passed over in a table format through a prompt 4 times, altering only the race/ethnicity information (African American, Asian, Hispanic White, and non-Hispanic White) each time, while keeping all other information constant. The prompt requested the model to write discharge instructions for each encounter without explicitly mentioning race or ethnicity. The LLM-generated instructions were analyzed for sentiment, subjectivity, reading ease, and word frequency by race/ethnicity. RESULTS: The only observed statistically significant difference between race/ethnicity groups was found in entity count (GPT-4, df=42, P=.047). However, post hoc chi-square analysis for GPT-4's entity counts showed no significant pairwise differences among race/ethnicity categories after Bonferroni correction. CONCLUSIONS: A total of 4 LLMs were relatively invariant to race/ethnicity in terms of linguistic and readability measures. While our study used proxy linguistic and readability measures to investigate racial and ethnic bias among 4 LLM responses in a health care-related task, there is an urgent need to establish universally accepted standards for measuring bias in LLM-generated responses. Further studies are needed to validate these results and assess their implications.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: race ethnicity",10.2196/57257,Cross-Sectional Studies;Humans;*Racism;Language;Ethnicity/statistics & numerical data,40080818,PMC11950697
rayyan-202745318,Fairness in AI-Driven Oncology: Investigating Racial and Gender Biases in Large Language Models.,2024,9,,Cureus,2168-8184 (Print),16,9,e69541,Agrawal A,https://pubmed.ncbi.nlm.nih.gov/39416584/,eng,,United States,"INTRODUCTION: Large language model (LLM) chatbots have many applications in medical settings. However, these tools can potentially perpetuate racial and gender biases through their responses, worsening disparities in healthcare. With the ongoing discussion of LLM chatbots in oncology and the widespread goal of addressing cancer disparities, this study focuses on biases propagated by LLM chatbots in oncology. METHODS: Chat Generative Pre-trained Transformer (Chat GPT; OpenAI, San Francisco, CA, USA) was asked to determine what occupation a generic description of ""assesses cancer patients"" would correspond to for different demographics. Chat GPT, Gemini (Alphabet Inc., Mountain View, CA, USA), and Bing Chat (Microsoft Corp., Redmond, WA, USA) were prompted to provide oncologist recommendations in the top U.S. cities and demographic information (race, gender) of recommendations was compared against national distributions. Chat GPT was also asked to generate a job description for oncologists with different demographic backgrounds. Finally, Chat GPT, Gemini, and Bing Chat were asked to generate hypothetical cancer patients with race, smoking, and drinking histories. RESULTS: LLM chatbots are about two times more likely to predict Blacks and Native Americans as oncology nurses than oncologists, compared to Asians (p < 0.01 and < 0.001, respectively). Similarly, they are also significantly more likely to predict females than males as oncology nurses (p < 0.001). Chat GPT's real-world oncologist recommendations overrepresent Asians by almost double and underrepresent Blacks by double and Hispanics by seven times. Chatbots also generate different job descriptions based on demographics, including cultural competency and advocacy and excluding treatment administration for underrepresented backgrounds. AI-generated cancer cases are not fully representative of real-world demographic distributions and encode stereotypes on substance abuse, such as Hispanics having a greater proportion of smokers than Whites by about 20% in Chat GPT breast cancer cases. CONCLUSION: To our knowledge, this is the first study of its kind to investigate racial and gender biases of such a diverse set of AI chatbots, and that too, within oncology. The methodology presented in this study provides a framework for targeted bias evaluation of LLMs in various fields across medicine.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,race ethnicity | USER-NOTES: {""Phoenix""=>[""clinically relevant?""]}",10.7759/cureus.69541,"",39416584,PMC11482645
rayyan-202745319,Early detection of pediatric health risks using maternal and child health data.,2024,7,4,Scientific reports,2045-2322 (Electronic),14,1,15350,Ilin C,https://pubmed.ncbi.nlm.nih.gov/38961161/,eng,,England,"Machine learning (ML)-driven diagnosis systems are particularly relevant in pediatrics given the well-documented impact of early-life health conditions on later-life outcomes. Yet, early identification of diseases and their subsequent impact on length of hospital stay for this age group has so far remained uncharacterized, likely because access to relevant health data is severely limited. Thanks to a confidential data use agreement with the California Department of Health Care Access and Information, we introduce Ped-BERT: a state-of-the-art deep learning model that accurately predicts the likelihood of 100+ conditions and the length of stay in a pediatric patient's next medical visit. We link mother-specific pre- and postnatal period health information to pediatric patient hospital discharge and emergency room visits. Our data set comprises 513.9K mother-baby pairs and contains medical diagnosis codes, length of stay, as well as temporal and spatial pediatric patient characteristics, such as age and residency zip code at the time of visit. Following the popular bidirectional encoder representations from the transformers (BERT) approach, we pre-train Ped-BERT via the masked language modeling objective to learn embedding features for the diagnosis codes contained in our data. We then continue to fine-tune our model to accurately predict primary diagnosis outcomes and length of stay for a pediatric patient's next visit, given the history of previous visits and, optionally, the mother's pre- and postnatal health information. We find that Ped-BERT generally outperforms contemporary and state-of-the-art classifiers when trained with minimum features. We also find that incorporating mother health attributes leads to significant improvements in model performance overall and across all patient subgroups in our data. Our most successful Ped-BERT model configuration achieves an area under the receiver operator curve (ROC AUC) of 0.927 and an average precision score (APS) of 0.408 for the diagnosis prediction task, and a ROC AUC of 0.855 and APS of 0.815 for the length of hospital stay task. Further, we examine Ped-BERT's fairness by determining whether prediction errors are evenly distributed across various subgroups of mother-baby demographics and health characteristics, or if certain subgroups exhibit a higher susceptibility to prediction errors.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: fairness across demographic groups",10.1038/s41598-024-65449-8,"Humans;Female;*Child Health;Infant;Child, Preschool;*Maternal Health;Child;Early Diagnosis;Length of Stay;Infant, Newborn;Male;Deep Learning;Machine Learning",38961161,PMC11222373
rayyan-202745323,Use of a large language model with instruction-tuning for reliable clinical frailty scoring.,2024,12,,Journal of the American Geriatrics Society,1532-5415 (Electronic),72,12,3849-3854,Kee XLJ and Sng GGR and Lim DYZ and Tung JYM and Abdullah HR and Chowdury AR,https://pubmed.ncbi.nlm.nih.gov/39105505/,eng,,United States,"BACKGROUND: Frailty is an important predictor of health outcomes, characterized by increased vulnerability due to physiological decline. The Clinical Frailty Scale (CFS) is commonly used for frailty assessment but may be influenced by rater bias. Use of artificial intelligence (AI), particularly Large Language Models (LLMs) offers a promising method for efficient and reliable frailty scoring. METHODS: The study utilized seven standardized patient scenarios to evaluate the consistency and reliability of CFS scoring by OpenAI's GPT-3.5-turbo model. Two methods were tested: a basic prompt and an instruction-tuned prompt incorporating CFS definition, a directive for accurate responses, and temperature control. The outputs were compared using the Mann-Whitney U test and Fleiss' Kappa for inter-rater reliability. The outputs were compared with historic human scores of the same scenarios. RESULTS: The LLM's median scores were similar to human raters, with differences of no more than one point. Significant differences in score distributions were observed between the basic and instruction-tuned prompts in five out of seven scenarios. The instruction-tuned prompt showed high inter-rater reliability (Fleiss' Kappa of 0.887) and produced consistent responses in all scenarios. Difficulty in scoring was noted in scenarios with less explicit information on activities of daily living (ADLs). CONCLUSIONS: This study demonstrates the potential of LLMs in consistently scoring clinical frailty with high reliability. It demonstrates that prompt engineering via instruction-tuning can be a simple but effective approach for optimizing LLMs in healthcare applications. The LLM may overestimate frailty scores when less information about ADLs is provided, possibly as it is less subject to implicit assumptions and extrapolation than humans. Future research could explore the integration of LLMs in clinical research and frailty-related outcome prediction.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation,Did not assess bias | USER-NOTES: {""Phoenix""=>[""bias? LLM may overestimate frailty scores when less information about ADLs is provided""]}",10.1111/jgs.19114,"Humans;*Frailty/diagnosis;*Geriatric Assessment/methods;Aged;Reproducibility of Results;*Frail Elderly;Female;Male;Artificial Intelligence;Aged, 80 and over",39105505,
rayyan-202745344,Improving medical machine learning models with generative balancing for equity and excellence.,2025,2,14,NPJ digital medicine,2398-6352 (Electronic),8,1,100,Theodorou B and Danek B and Tummala V and Kumar SP and Malin B and Sun J,https://pubmed.ncbi.nlm.nih.gov/39953146/,eng,,England,"Applying machine learning to clinical outcome prediction is challenging due to imbalanced datasets and sensitive tasks that contain rare yet critical outcomes and where equitable treatment across diverse patient groups is essential. Despite attempts, biases in predictions persist, driven by disparities in representation and exacerbated by the scarcity of positive labels, perpetuating health inequities. This paper introduces FairPlay, a synthetic data generation approach leveraging large language models, to address these issues. FairPlay enhances algorithmic performance and reduces bias by creating realistic, anonymous synthetic patient data that improves representation and augments dataset patterns while preserving privacy. Through experiments on multiple datasets, we demonstrate that FairPlay boosts mortality prediction performance across diverse subgroups, achieving up to a 21% improvement in F1 Score without requiring additional data or altering downstream training pipelines. Furthermore, FairPlay consistently reduces subgroup performance gaps, as shown by universal improvements in performance and fairness metrics across four experimental setups.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: age,sex,race,insurance | RAYYAN-EXCLUSION-REASONS: No LLM | USER-NOTES: {""Phoenix""=>[""creating realistic, anonymous synthetic patient data that improves representation and augments dataset patterns ""]}",10.1038/s41746-025-01438-z,"",39953146,PMC11828851
rayyan-202745351,Large Language Models Outperform Traditional Natural Language Processing Methods in Extracting Patient-Reported Outcomes in Inflammatory Bowel Disease.,2025,,,Gastro hep advances,2772-5723 (Electronic),4,2,100563,Patel PV and Davis C and Ralbovsky A and Tinoco D and Williams CYK and Slatter S and Naderalvojoud B and Rosen MJ and Hernandez-Boussard T and Rudrapatna V,https://pubmed.ncbi.nlm.nih.gov/39877865/,eng,,Netherlands,"BACKGROUND AND AIMS: Patient-reported outcomes (PROs) are vital in assessing disease activity and treatment outcomes in inflammatory bowel disease (IBD). However, manual extraction of these PROs from the free-text of clinical notes is burdensome. We aimed to improve data curation from free-text information in the electronic health record, making it more available for research and quality improvement. This study aimed to compare traditional natural language processing (tNLP) and large language models (LLMs) in extracting 3 IBD PROs (abdominal pain, diarrhea, fecal blood) from clinical notes across 2 institutions. METHODS: Clinic notes were annotated for each PRO using preset protocols. Models were developed and internally tested at the University of California, San Francisco, and then externally validated at Stanford University. We compared tNLP and LLM-based models on accuracy, sensitivity, specificity, positive, and negative predictive value. In addition, we conducted fairness and error assessments. RESULTS: Interrater reliability between annotators was >90%. On the University of California, San Francisco test set (n = 50), the top-performing tNLP models showcased accuracies of 92% (abdominal pain), 82% (diarrhea) and 80% (fecal blood), comparable to GPT-4, which was 96%, 88%, and 90% accurate, respectively. On external validation at Stanford (n = 250), tNLP models failed to generalize (61%-62% accuracy) while GPT-4 maintained accuracies >90%. Pathways Language Model-2 and Generative Pre-trained Transformer-4 showed similar performance. No biases were detected based on demographics or diagnosis. CONCLUSION: LLMs are accurate and generalizable methods for extracting PROs. They maintain excellent accuracy across institutions, despite heterogeneity in note templates and authors. Widespread adoption of such tools has the potential to enhance IBD research and patient care.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: fairness across demographic groups | USER-NOTES: {""Phoenix""=>[""fairness analyses - no details""]}",10.1016/j.gastha.2024.10.003,"",39877865,PMC11772946
rayyan-202745367,Large language models are less effective at clinical prediction tasks than locally trained machine learning models.,2025,3,8,Journal of the American Medical Informatics Association : JAMIA,1527-974X (Electronic),,,,Brown KE and Yan C and Li Z and Zhang X and Collins BX and Chen Y and Clayton EW and Kantarcioglu M and Vorobeychik Y and Malin BA,https://pubmed.ncbi.nlm.nih.gov/40056436/,eng,,England,"OBJECTIVES: To determine the extent to which current large language models (LLMs) can serve as substitutes for traditional machine learning (ML) as clinical predictors using data from electronic health records (EHRs), we investigated various factors that can impact their adoption, including overall performance, calibration, fairness, and resilience to privacy protections that reduce data fidelity. MATERIALS AND METHODS: We evaluated GPT-3.5, GPT-4, and traditional ML (as gradient-boosting trees) on clinical prediction tasks in EHR data from Vanderbilt University Medical Center (VUMC) and MIMIC IV. We measured predictive performance with area under the receiver operating characteristic (AUROC) and model calibration using Brier Score. To evaluate the impact of data privacy protections, we assessed AUROC when demographic variables are generalized. We evaluated algorithmic fairness using equalized odds and statistical parity across race, sex, and age of patients. We also considered the impact of using in-context learning by incorporating labeled examples within the prompt. RESULTS: Traditional ML [AUROC: 0.847, 0.894 (VUMC, MIMIC)] substantially outperformed GPT-3.5 (AUROC: 0.537, 0.517) and GPT-4 (AUROC: 0.629, 0.602) (with and without in-context learning) in predictive performance and output probability calibration [Brier Score (ML vs GPT-3.5 vs GPT-4): 0.134 vs 0.384 vs 0.251, 0.042 vs 0.06 vs 0.219)]. DISCUSSION: Traditional ML is more robust than GPT-3.5 and GPT-4 in generalizing demographic information to protect privacy. GPT-4 is the fairest model according to our selected metrics but at the cost of poor model performance. CONCLUSION: These findings suggest that non-fine-tuned LLMs are less effective and robust than locally trained ML for clinical prediction tasks, but they are improving across releases.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: age,sex,race ethnicity,fairness across demographic groups | USER-NOTES: {""Phoenix""=>[""duplicate: Not the Models You Are Looking For: Traditional ML Outperforms LLMs in Clinical Prediction Tasks.""]}",10.1093/jamia/ocaf038,"",40056436,
rayyan-202745394,Explicitly unbiased large language models still form biased associations.,2025,2,25,Proceedings of the National Academy of Sciences of the United States of America,1091-6490 (Electronic),122,8,e2416228122,Bai X and Wang A and Sucholutsky I and Griffiths TL,https://pubmed.ncbi.nlm.nih.gov/39977313/,eng,,United States,"Large language models (LLMs) can pass explicit social bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: As LLMs become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make. We address both challenges by introducing two measures: LLM Word Association Test, a prompt-based method for revealing implicit bias; and LLM Relative Decision Test, a strategy to detect subtle discrimination in contextual decisions. Both measures are based on psychological research: LLM Word Association Test adapts the Implicit Association Test, widely used to study the automatic associations between concepts held in human minds; and LLM Relative Decision Test operationalizes psychological results indicating that relative evaluations between two candidates, not absolute evaluations assessing each independently, are more diagnostic of implicit biases. Using these measures, we found pervasive stereotype biases mirroring those in society in 8 value-aligned models across 4 social categories (race, gender, religion, health) in 21 stereotypes (such as race and criminality, race and weapons, gender and science, age and negativity). These prompt-based measures draw from psychology's long history of research into measuring stereotypes based on purely observable behavior; they expose nuanced biases in proprietary value-aligned LLMs that appear unbiased according to standard benchmarks.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: age,sex,race | RAYYAN-EXCLUSION-REASONS: No relevant data ,no direct health application,no direct clnical application | USER-NOTES: {""Phoenix""=>[""not health, read"", ""is this health related?""]}",10.1073/pnas.2416228122,Humans;*Language;Stereotyping;Prejudice/psychology;Word Association Tests;Decision Making/physiology;Female,39977313,PMC11874501
rayyan-202745402,Accuracy of Spanish and English-generated ChatGPT responses to commonly asked patient questions about labor epidurals: a survey-based study among bilingual obstetric anesthesia experts.,2025,2,,International journal of obstetric anesthesia,1532-3374 (Electronic),61,,104290,Gonzalez Fiol A and Mootz AA and He Z and Delgado C and Ortiz V and Reale SC,https://pubmed.ncbi.nlm.nih.gov/39579604/,eng,,Netherlands,"BACKGROUND: Large language models (LLMs), of which ChatGPT is the most well known, are now available to patients to seek medical advice in various languages. However, the accuracy of the information utilized to train these models remains unknown. METHODS: Ten commonly asked questions regarding labor epidurals were translated from English to Spanish, and all 20 questions were entered into ChatGPT version 3.5. The answers were transcribed. A survey was then sent to 10 bilingual fellowship-trained obstetric anesthesiologists to assess the accuracy of these answers utilizing a 5-point Likert scale. RESULTS: Overall, the accuracy scores for the ChatGPT-generated answers in Spanish were lower than for the English answers with a median score of 34 (IQR 33-36.5) versus 40.5 (IQR 39-44.3), respectively (P value 0.02). Answers to two questions were scored significantly lower: ""Do epidurals prolong labor?"" (2 (IQR 2-2.5) versus 4 (IQR 4-4.5), P value 0.03) and ""Do epidurals increase the risk of needing cesarean delivery?"" (3(IQR 2-4) versus 4 (IQR 4-5); P value 0.03). There was a strong agreement that answers to the question ""Do epidurals cause autism"" were accurate in both Spanish and English. CONCLUSION: ChatGPT-generated answers in Spanish to ten questions about labor epidurals scored lower for accuracythananswers generated in English, particularly regarding the effect of labor epidurals on labor course and mode of delivery. This disparity in ChatGPT-generated information may extend already-known health inequities among non-English-speaking patients and perpetuate misinformation.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: language",10.1016/j.ijoa.2024.104290,"Humans;Female;Pregnancy;Surveys and Questionnaires;*Anesthesia, Obstetrical/methods;Multilingualism;Adult;Language;Anesthesiologists;Labor, Obstetric;Analgesia, Epidural/methods",39579604,
rayyan-202745403,Dimensional Measures of Psychopathology in Children and Adolescents Using Large Language Models.,2024,12,15,Biological psychiatry,1873-2402 (Electronic),96,12,940-947,McCoy TH Jr and Perlis RH,https://pubmed.ncbi.nlm.nih.gov/38866172/,eng,,United States,"BACKGROUND: To enable greater use of National Institute of Mental Health Research Domain Criteria (RDoC) in real-world settings, we applied large language models (LLMs) to estimate dimensional psychopathology from narrative clinical notes. METHODS: We conducted a cohort study using health records from individuals age ≤18 years evaluated in the psychiatric emergency department of a large academic medical center between November 2008 and March 2015. Outcomes were hospital admission and length of emergency department stay. RDoC domains were estimated using a Health Insurance Portability and Accountability Act-compliant LLM (gpt-4-1106-preview) and compared with a previously validated token-based approach. RESULTS: The cohort included 3059 individuals (median age 16 years [interquartile range, 13-18]; 1580 [52%] female, 1479 [48%] male; 105 [3.4%] identified as Asian, 329 [11%] as Black, 288 [9.4%] as Hispanic, 474 [15%] as other race, and 1863 [61%] as White), of whom 1695 (55%) were admitted. Correlation between LLM-extracted RDoC scores and the token-based scores ranged from small to medium as assessed by Kendall's tau (0.14-0.22). In logistic regression models adjusting for sociodemographic and clinical features, admission likelihood was associated with greater scores on all domains, with the exception of the sensorimotor domain, which was inversely associated (p < .001 for all adjusted associations). Tests for bias suggested modest but statistically significant differences in positive valence scores by race (p < .05 for Asian, Black, and Hispanic individuals). CONCLUSIONS: An LLM extracted estimates of 6 RDoC domains in an explainable manner, which were associated with clinical outcomes. This approach can contribute to a new generation of prediction models or biological investigations based on dimensional psychopathology.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,race,mental health application",10.1016/j.biopsych.2024.05.008,"Humans;Male;Female;Adolescent;*Mental Disorders/diagnosis;Cohort Studies;Child;United States;Emergency Service, Hospital;Language;Psychopathology;Length of Stay/statistics & numerical data;National Institute of Mental Health (U.S.);Hospitalization/statistics & numerical data",38866172,
rayyan-202745406,Preoperative prediction model for risk of readmission after total joint replacement surgery: a random forest approach leveraging NLP and unfairness mitigation for improved patient care and cost-effectiveness.,2024,5,10,Journal of orthopaedic surgery and research,1749-799X (Electronic),19,1,287,Digumarthi V and Amin T and Kanu S and Mathew J and Edwards B and Peterson LA and Lundy ME and Hegarty KE,https://pubmed.ncbi.nlm.nih.gov/38725085/,eng,,England,"BACKGROUND: The Center for Medicare and Medicaid Services (CMS) imposes payment penalties for readmissions following total joint replacement surgeries. This study focuses on total hip, knee, and shoulder arthroplasty procedures as they account for most joint replacement surgeries. Apart from being a burden to healthcare systems, readmissions are also troublesome for patients. There are several studies which only utilized structured data from Electronic Health Records (EHR) without considering any gender and payor bias adjustments. METHODS: For this study, dataset of 38,581 total knee, hip, and shoulder replacement surgeries performed from 2015 to 2021 at Novant Health was gathered. This data was used to train a random forest machine learning model to predict the combined endpoint of emergency department (ED) visit or unplanned readmissions within 30 days of discharge or discharge to Skilled Nursing Facility (SNF) following the surgery. 98 features of laboratory results, diagnoses, vitals, medications, and utilization history were extracted. A natural language processing (NLP) model finetuned from Clinical BERT was used to generate an NLP risk score feature for each patient based on their clinical notes. To address societal biases, a feature bias analysis was performed in conjunction with propensity score matching. A threshold optimization algorithm from the Fairlearn toolkit was used to mitigate gender and payor biases to promote fairness in predictions. RESULTS: The model achieved an Area Under the Receiver Operating characteristic Curve (AUROC) of 0.738 (95% confidence interval, 0.724 to 0.754) and an Area Under the Precision-Recall Curve (AUPRC) of 0.406 (95% confidence interval, 0.384 to 0.433). Considering an outcome prevalence of 16%, these metrics indicate the model's ability to accurately discriminate between readmission and non-readmission cases within the context of total arthroplasty surgeries while adjusting patient scores in the model to mitigate bias based on patient gender and payor. CONCLUSION: This work culminated in a model that identifies the most predictive and protective features associated with the combined endpoint. This model serves as a tool to empower healthcare providers to proactively intervene based on these influential factors without introducing bias towards protected patient classes, effectively mitigating the risk of negative outcomes and ultimately improving quality of care regardless of socioeconomic factors.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: fairness across demographic groups | USER-NOTES: {""Phoenix""=>[""read, lu and I disagree with relevance (lu says no wrt bias)""], ""Lu""=>[""Requires further review. ""]}",10.1186/s13018-024-04774-0,"Humans;*Patient Readmission/economics/statistics & numerical data;Female;Male;*Machine Learning;Aged;*Cost-Benefit Analysis;Natural Language Processing;Middle Aged;Arthroplasty, Replacement, Knee/economics;Arthroplasty, Replacement, Hip/economics;Arthroplasty, Replacement/economics/adverse effects;Risk Assessment/methods;Preoperative Period;Aged, 80 and over;Quality Improvement;Random Forest",38725085,PMC11084055
rayyan-202745407,Gender Bias in AI's Perception of Cardiovascular Risk.,2024,10,22,Journal of medical Internet research,1438-8871 (Electronic),26,,e54242,Achtari M and Salihu A and Muller O and Abbé E and Clair C and Schwarz J and Fournier S,https://pubmed.ncbi.nlm.nih.gov/39437384/,eng,,Canada,The study investigated gender bias in GPT-4's assessment of coronary artery disease risk by presenting identical clinical vignettes of men and women with and without psychiatric comorbidities. Results suggest that psychiatric conditions may influence GPT-4's coronary artery disease risk assessment among men and women.,"RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex | RAYYAN-EXCLUSION-REASONS: wrong publication type | USER-NOTES: {""Phoenix""=>[""research letter - applicable for lit review?"", ""not full research study""]}",10.2196/54242,Humans;Female;Male;*Sexism/psychology;Cardiovascular Diseases/psychology;Middle Aged;Risk Assessment/methods;Artificial Intelligence;Adult;Heart Disease Risk Factors;Coronary Artery Disease/psychology,39437384,PMC11538872
rayyan-202745414,"Gender and Ethnicity Bias of Text-to-Image Generative Artificial Intelligence in Medical Imaging, Part 2: Analysis of DALL-E 3.",2024,10,22,Journal of nuclear medicine technology,1535-5675 (Electronic),,,,Currie G and Hewis J and Hawk E and Rohren E,https://pubmed.ncbi.nlm.nih.gov/39438058/,eng,,United States,"Disparity among gender and ethnicity remains an issue across medicine and health science. Only 26%-35% of trainee radiologists are female, despite more than 50% of medical students' being female. Similar gender disparities are evident across the medical imaging professions. Generative artificial intelligence text-to-image production could reinforce or amplify gender biases. Methods: In March 2024, DALL-E 3 was utilized via GPT-4 to generate a series of individual and group images of medical imaging professionals: radiologist, nuclear medicine physician, radiographer, nuclear medicine technologist, medical physicist, radiopharmacist, and medical imaging nurse. Multiple iterations of images were generated using a variety of prompts. Collectively, 120 images were produced for evaluation of 524 characters. All images were independently analyzed by 3 expert reviewers from medical imaging professions for apparent gender and skin tone. Results: Collectively (individual and group images), 57.4% (n = 301) of medical imaging professionals were depicted as male, 42.4% (n = 222) as female, and 91.2% (n = 478) as having a light skin tone. The male gender representation was 65% for radiologists, 62% for nuclear medicine physicians, 52% for radiographers, 56% for nuclear medicine technologists, 62% for medical physicists, 53% for radiopharmacists, and 26% for medical imaging nurses. For all professions, this overrepresents men compared with women. There was no representation of persons with a disability. Conclusion: This evaluation reveals a significant overrepresentation of the male gender associated with generative artificial intelligence text-to-image production using DALL-E 3 across the medical imaging professions. Generated images have a disproportionately high representation of white men, which is not representative of the diversity of the medical imaging professions.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: No relevant data ,no direct health application,no direct clnical application | USER-NOTES: {""Lu""=>[""No PDF""]}",10.2967/jnmt.124.268359,"",39438058,
rayyan-202745426,Large Language Models for Social Determinants of Health Information Extraction from Clinical Notes - A Generalizable Approach across Institutions.,2024,5,22,medRxiv : the preprint server for health sciences,,,,,Keloth VK and Selek S and Chen Q and Gilman C and Fu S and Dang Y and Chen X and Hu X and Zhou Y and He H and Fan JW and Wang K and Brandt C and Tao C and Liu H and Xu H,https://pubmed.ncbi.nlm.nih.gov/38826441/,eng,,United States,"The consistent and persuasive evidence illustrating the influence of social determinants on health has prompted a growing realization throughout the health care sector that enhancing health and health equity will likely depend, at least to some extent, on addressing detrimental social determinants. However, detailed social determinants of health (SDoH) information is often buried within clinical narrative text in electronic health records (EHRs), necessitating natural language processing (NLP) methods to automatically extract these details. Most current NLP efforts for SDoH extraction have been limited, investigating on limited types of SDoH elements, deriving data from a single institution, focusing on specific patient cohorts or note types, with reduced focus on generalizability. This study aims to address these issues by creating cross-institutional corpora spanning different note types and healthcare systems, and developing and evaluating the generalizability of classification models, including novel large language models (LLMs), for detecting SDoH factors from diverse types of notes from four institutions: Harris County Psychiatric Center, University of Texas Physician Practice, Beth Israel Deaconess Medical Center, and Mayo Clinic. Four corpora of deidentified clinical notes were annotated with 21 SDoH factors at two levels: level 1 with SDoH factor types only and level 2 with SDoH factors along with associated values. Three traditional classification algorithms (XGBoost, TextCNN, Sentence BERT) and an instruction tuned LLM-based approach (LLaMA) were developed to identify multiple SDoH factors. Substantial variation was noted in SDoH documentation practices and label distributions based on patient cohorts, note types, and hospitals. The LLM achieved top performance with micro-averaged F1 scores over 0.9 on level 1 annotated corpora and an F1 over 0.84 on level 2 annotated corpora. While models performed well when trained and tested on individual datasets, cross-dataset generalization highlighted remaining obstacles. To foster collaboration, access to partial annotated corpora and models trained by merging all annotated datasets will be made available on the PhysioNet repository.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation,Did not assess bias | USER-NOTES: {""Phoenix""=>[""read"", ""read""]}",10.1101/2024.05.21.24307726,"",38826441,PMC11142292
rayyan-202745439,Bias Sensitivity in Diagnostic Decision-Making: Comparing ChatGPT with Residents.,2025,3,,Journal of general internal medicine,1525-1497 (Electronic),40,4,790-795,Schmidt HG and Rotgans JI and Mamede S,https://pubmed.ncbi.nlm.nih.gov/39511117/,eng,,United States,"BACKGROUND: Diagnostic errors, often due to biases in clinical reasoning, significantly affect patient care. While artificial intelligence chatbots like ChatGPT could help mitigate such biases, their potential susceptibility to biases is unknown. METHODS: This study evaluated diagnostic accuracy of ChatGPT against the performance of 265 medical residents in five previously published experiments aimed at inducing bias. The residents worked in several major teaching hospitals in the Netherlands. The biases studied were case-intrinsic (presence of salient distracting findings in the patient history, effects of disruptive patient behaviors) and situational (prior availability of a look-alike patient). ChatGPT's accuracy in identifying the most-likely diagnosis was measured. RESULTS: Diagnostic accuracy of residents and ChatGPT was equivalent. For clinical cases involving case-intrinsic bias, both ChatGPT and the residents exhibited a decline in diagnostic accuracy. Residents' accuracy decreased on average 12%, while the accuracy of ChatGPT 4.0 decreased 21%. Accuracy of ChatGPT 3.5 decreased 9%. These findings suggest that, like human diagnosticians, ChatGPT is sensitive to bias when the biasing information is part of the patient history. When the biasing information was extrinsic to the case in the form of the prior availability of a look-alike case, residents' accuracy decreased by 15%. By contrast, ChatGPT's performance was not affected by the biasing information. Chi-square goodness-of-fit tests corroborated these outcomes. CONCLUSIONS: It seems that, while ChatGPT is not sensitive to bias when biasing information is situational, it is sensitive to bias when the biasing information is part of the patient's disease history. Its utility in diagnostic support has potential, but caution is advised. Future research should enhance AI's bias detection and mitigation to make it truly useful for diagnostic support.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""}",10.1007/s11606-024-09177-9,Humans;*Internship and Residency;*Bias;Diagnostic Errors/prevention & control;Clinical Decision-Making/methods;Netherlands;Clinical Competence/standards;Female;Male;Artificial Intelligence,39511117,PMC11914423
rayyan-202745442,Disability Ethics and Education in the Age of Artificial Intelligence: Identifying Ability Bias in ChatGPT and Gemini.,2025,1,,Archives of physical medicine and rehabilitation,1532-821X (Electronic),106,1,14-19,Urbina JT and Vu PD and Nguyen MV,https://pubmed.ncbi.nlm.nih.gov/39216786/,eng,,United States,"OBJECTIVE: To identify and quantify ability bias in generative artificial intelligence large language model chatbots, specifically OpenAI's ChatGPT and Google's Gemini. DESIGN: Observational study of language usage in generative artificial intelligence models. SETTING: Investigation-only browser profile restricted to ChatGPT and Gemini. PARTICIPANTS: Each chatbot generated 60 descriptions of people prompted without specified functional status, 30 descriptions of people with a disability, 30 descriptions of patients with a disability, and 30 descriptions of athletes with a disability (N=300). INTERVENTIONS: Not applicable. MAIN OUTCOME MEASURES: Generated descriptions produced by the models were parsed into words that were linguistically analyzed into favorable qualities or limiting qualities. RESULTS: Both large language models significantly underestimated disability in a population of people, and linguistic analysis showed that descriptions of people, patients, and athletes with a disability were generated as having significantly fewer favorable qualities and significantly more limitations than people without a disability in both ChatGPT and Gemini. CONCLUSIONS: Generative artificial intelligence chatbots demonstrate quantifiable ability bias and often exclude people with disabilities in their responses. Ethical use of these generative large language model chatbots in medical systems should recognize this limitation, and further consideration should be taken in developing equitable artificial intelligence technologies.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""}",10.1016/j.apmr.2024.08.014,Humans;*Artificial Intelligence/ethics;*Disabled Persons/rehabilitation;Bias,39216786,
rayyan-202745445,The doctor will polygraph you now.,2024,,,npj health systems,3005-1959 (Electronic),1,1,1,Anibal J and Gunkel J and Awan S and Huth H and Nguyen H and Le T and Bélisle-Pipon JC and Boyer M and Hazen L and Bensoussan Y and Clifton D and Wood B,https://pubmed.ncbi.nlm.nih.gov/39759269/,eng,,England,"Artificial intelligence (AI) methods have been proposed for the prediction of social behaviors that could be reasonably understood from patient-reported information. This raises novel ethical concerns about respect, privacy, and control over patient data. Ethical concerns surrounding clinical AI systems for social behavior verification can be divided into two main categories: (1) the potential for inaccuracies/biases within such systems, and (2) the impact on trust in patient-provider relationships with the introduction of automated AI systems for ""fact-checking"", particularly in cases where the data/models may contradict the patient. Additionally, this report simulated the misuse of a verification system using patient voice samples and identified a potential LLM bias against patient-reported information in favor of multi-dimensional data and the outputs of other AI methods (i.e., ""AI self-trust""). Finally, recommendations were presented for mitigating the risk that AI verification methods will cause harm to patients or undermine the purpose of the healthcare system.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""}",10.1038/s44401-024-00001-4,"",39759269,PMC11698301
rayyan-202745457,Use of Generative AI for Improving Health Literacy in Reproductive Health: Case Study.,2024,8,6,JMIR formative research,2561-326X (Electronic),8,,e59434,Burns C and Bakaj A and Berishaj A and Hristidis V and Deak P and Equils O,https://pubmed.ncbi.nlm.nih.gov/38986153/,eng,,Canada,"BACKGROUND: Patients find technology tools to be more approachable for seeking sensitive health-related information, such as reproductive health information. The inventive conversational ability of artificial intelligence (AI) chatbots, such as ChatGPT (OpenAI Inc), offers a potential means for patients to effectively locate answers to their health-related questions digitally. OBJECTIVE: A pilot study was conducted to compare the novel ChatGPT with the existing Google Search technology for their ability to offer accurate, effective, and current information regarding proceeding action after missing a dose of oral contraceptive pill. METHODS: A sequence of 11 questions, mimicking a patient inquiring about the action to take after missing a dose of an oral contraceptive pill, were input into ChatGPT as a cascade, given the conversational ability of ChatGPT. The questions were input into 4 different ChatGPT accounts, with the account holders being of various demographics, to evaluate potential differences and biases in the responses given to different account holders. The leading question, ""what should I do if I missed a day of my oral contraception birth control?"" alone was then input into Google Search, given its nonconversational nature. The results from the ChatGPT questions and the Google Search results for the leading question were evaluated on their readability, accuracy, and effective delivery of information. RESULTS: The ChatGPT results were determined to be at an overall higher-grade reading level, with a longer reading duration, less accurate, less current, and with a less effective delivery of information. In contrast, the Google Search resulting answer box and snippets were at a lower-grade reading level, shorter reading duration, more current, able to reference the origin of the information (transparent), and provided the information in various formats in addition to text. CONCLUSIONS: ChatGPT has room for improvement in accuracy, transparency, recency, and reliability before it can equitably be implemented into health care information delivery and provide the potential benefits it poses. However, AI may be used as a tool for providers to educate their patients in preferred, creative, and efficient ways, such as using AI to generate accessible short educational videos from health care provider-vetted information. Larger studies representing a diverse group of users are needed.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation",10.2196/59434,"",38986153,PMC11336497
rayyan-202745464,"Comprehensiveness, Accuracy, and Readability of Exercise Recommendations Provided by an AI-Based Chatbot: Mixed Methods Study.",2024,1,11,JMIR medical education,2369-3762 (Electronic),10,,e51308,Zaleski AL and Berkowsky R and Craig KJT and Pescatello LS,https://pubmed.ncbi.nlm.nih.gov/38206661/,eng,,Canada,"BACKGROUND: Regular physical activity is critical for health and disease prevention. Yet, health care providers and patients face barriers to implement evidence-based lifestyle recommendations. The potential to augment care with the increased availability of artificial intelligence (AI) technologies is limitless; however, the suitability of AI-generated exercise recommendations has yet to be explored. OBJECTIVE: The purpose of this study was to assess the comprehensiveness, accuracy, and readability of individualized exercise recommendations generated by a novel AI chatbot. METHODS: A coding scheme was developed to score AI-generated exercise recommendations across ten categories informed by gold-standard exercise recommendations, including (1) health condition-specific benefits of exercise, (2) exercise preparticipation health screening, (3) frequency, (4) intensity, (5) time, (6) type, (7) volume, (8) progression, (9) special considerations, and (10) references to the primary literature. The AI chatbot was prompted to provide individualized exercise recommendations for 26 clinical populations using an open-source application programming interface. Two independent reviewers coded AI-generated content for each category and calculated comprehensiveness (%) and factual accuracy (%) on a scale of 0%-100%. Readability was assessed using the Flesch-Kincaid formula. Qualitative analysis identified and categorized themes from AI-generated output. RESULTS: AI-generated exercise recommendations were 41.2% (107/260) comprehensive and 90.7% (146/161) accurate, with the majority (8/15, 53%) of inaccuracy related to the need for exercise preparticipation medical clearance. Average readability level of AI-generated exercise recommendations was at the college level (mean 13.7, SD 1.7), with an average Flesch reading ease score of 31.1 (SD 7.7). Several recurring themes and observations of AI-generated output included concern for liability and safety, preference for aerobic exercise, and potential bias and direct discrimination against certain age-based populations and individuals with disabilities. CONCLUSIONS: There were notable gaps in the comprehensiveness, accuracy, and readability of AI-generated exercise recommendations. Exercise and health care professionals should be aware of these limitations when using and endorsing AI-based technologies as a tool to support lifestyle change involving exercise.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""}",10.2196/51308,Humans;*Artificial Intelligence;*Comprehension;Software;Awareness;Exercise,38206661,PMC10811574
rayyan-202745485,Exploring Biases of Large Language Models in the Field of Mental Health: Comparative Questionnaire Study of the Effect of Gender and Sexual Orientation in Anorexia Nervosa and Bulimia Nervosa Case Vignettes.,2025,3,20,JMIR mental health,2368-7959 (Electronic),12,,e57986,Schnepper R and Roemmel N and Schaefert R and Lambrecht-Walzinger L and Meinlschmidt G,https://pubmed.ncbi.nlm.nih.gov/40111287/,eng,,Canada,"BACKGROUND: Large language models (LLMs) are increasingly used in mental health, showing promise in assessing disorders. However, concerns exist regarding their accuracy, reliability, and fairness. Societal biases and underrepresentation of certain populations may impact LLMs. Because LLMs are already used for clinical practice, including decision support, it is important to investigate potential biases to ensure a responsible use of LLMs. Anorexia nervosa (AN) and bulimia nervosa (BN) show a lifetime prevalence of 1%-2%, affecting more women than men. Among men, homosexual men face a higher risk of eating disorders (EDs) than heterosexual men. However, men are underrepresented in ED research, and studies on gender, sexual orientation, and their impact on AN and BN prevalence, symptoms, and treatment outcomes remain limited. OBJECTIVES: We aimed to estimate the presence and size of bias related to gender and sexual orientation produced by a common LLM as well as a smaller LLM specifically trained for mental health analyses, exemplified in the context of ED symptomatology and health-related quality of life (HRQoL) of patients with AN or BN. METHODS: We extracted 30 case vignettes (22 AN and 8 BN) from scientific papers. We adapted each vignette to create 4 versions, describing a female versus male patient living with their female versus male partner (2 × 2 design), yielding 120 vignettes. We then fed each vignette into ChatGPT-4 and to ""MentaLLaMA"" based on the Large Language Model Meta AI (LLaMA) architecture thrice with the instruction to evaluate them by providing responses to 2 psychometric instruments, the RAND-36 questionnaire assessing HRQoL and the eating disorder examination questionnaire. With the resulting LLM-generated scores, we calculated multilevel models with a random intercept for gender and sexual orientation (accounting for within-vignette variance), nested in vignettes (accounting for between-vignette variance). RESULTS: In ChatGPT-4, the multilevel model with 360 observations indicated a significant association with gender for the RAND-36 mental composite summary (conditional means: 12.8 for male and 15.1 for female cases; 95% CI of the effect -6.15 to -0.35; P=.04) but neither with sexual orientation (P=.71) nor with an interaction effect (P=.37). We found no indications for main effects of gender (conditional means: 5.65 for male and 5.61 for female cases; 95% CI -0.10 to 0.14; P=.88), sexual orientation (conditional means: 5.63 for heterosexual and 5.62 for homosexual cases; 95% CI -0.14 to 0.09; P=.67), or for an interaction effect (P=.61, 95% CI -0.11 to 0.19) for the eating disorder examination questionnaire overall score (conditional means 5.59-5.65 95% CIs 5.45 to 5.7). MentaLLaMA did not yield reliable results. CONCLUSIONS: LLM-generated mental HRQoL estimates for AN and BN case vignettes may be biased by gender, with male cases scoring lower despite no real-world evidence supporting this pattern. This highlights the risk of bias in generative artificial intelligence in the field of mental health. Understanding and mitigating biases related to gender and other factors, such as ethnicity, and socioeconomic status are crucial for responsible use in diagnostics and treatment recommendations.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,mental health application | USER-NOTES: {""Phoenix""=>[""read""]}",10.2196/57986,Humans;Male;Female;*Anorexia Nervosa/psychology/epidemiology/therapy;*Sexual Behavior/psychology;*Bulimia Nervosa/psychology/epidemiology;Adult;Surveys and Questionnaires;Young Adult;Bias;Sex Factors;Quality of Life/psychology;Language,40111287,PMC11949086
rayyan-202745504,Completeness and readability of GPT-4-generated multilingual discharge instructions in the pediatric emergency department.,2024,10,,JAMIA open,2574-2531 (Electronic),7,3,ooae050,Gimeno A and Krause K and D'Souza S and Walsh CG,https://pubmed.ncbi.nlm.nih.gov/38957592/,eng,,United States,"OBJECTIVES: The aim of this study was to assess the completeness and readability of generative pre-trained transformer-4 (GPT-4)-generated discharge instructions at prespecified reading levels for common pediatric emergency room complaints. MATERIALS AND METHODS: The outputs for 6 discharge scenarios stratified by reading level (fifth or eighth grade) and language (English, Spanish) were generated fivefold using GPT-4. Specifically, 120 discharge instructions were produced and analyzed (6 scenarios: 60 in English, 60 in Spanish; 60 at a fifth-grade reading level, 60 at an eighth-grade reading level) and compared for completeness and readability (between language, between reading level, and stratified by group and reading level). Completeness was defined as the proportion of literature-derived key points included in discharge instructions. Readability was quantified using Flesch-Kincaid (English) and Fernandez-Huerta (Spanish) readability scores. RESULTS: English-language GPT-generated discharge instructions contained a significantly higher proportion of must-include discharge instructions than those in Spanish (English: mean (standard error of the mean) = 62% (3%), Spanish: 53% (3%), P = .02). In the fifth-grade and eighth-grade level conditions, there was no significant difference between English and Spanish outputs in completeness. Readability did not differ across languages. DISCUSSION: GPT-4 produced readable discharge instructions in English and Spanish while modulating document reading level. Discharge instructions in English tended to have higher completeness than those in Spanish. CONCLUSION: Future research in prompt engineering and GPT-4 performance, both generally and in multiple languages, is needed to reduce potential for health disparities by language and reading level.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: language | USER-NOTES: {""Phoenix""=>[""not framed in bias - but comparative across languages, like other studies""]}",10.1093/jamiaopen/ooae050,"",38957592,PMC11216721
rayyan-202745507,Managing class imbalance in the training of a large language model to predict patient selection for total knee arthroplasty: Results from the Artificial intelligence to Revolutionise the patient Care pathway in Hip and knEe aRthroplastY (ARCHERY) project.,2025,2,27,The Knee,1873-5800 (Electronic),54,,1-8,Farrow L and Anderson L and Zhong M,https://pubmed.ncbi.nlm.nih.gov/40020253/,eng,,Netherlands,"INTRODUCTION: This study set out to test the efficacy of different techniques used to manage to class imbalance, a type of data bias, in application of a large language model (LLM) to predict patient selection for total knee arthroplasty (TKA). METHODS: This study utilised data from the Artificial Intelligence to Revolutionise the Patient Care Pathway in Hip and Knee Arthroplasty (ARCHERY) project (ISRCTN18398037). Data included the pre-operative radiology reports of patients referred to secondary care for knee-related complaints from within the North of Scotland. A clinically based LLM (GatorTron) was trained regarding prediction of selection for TKA. Three methods for managing class imbalance were assessed: a standard model, use of class weighting, and majority class undersampling. RESULTS: A total of 7707 individual knee radiology reports were included (dated from 2015 to 2022). The mean text length was 74 words (range 26-275). Only 910/7707 (11.8%) patients underwent TKA surgery (the designated 'minority class'). Class weighting technique performed better for minority class discrimination and calibration compared with the other two techniques (Recall 0.61/AUROC 0.73 for class weighting compared with 0.54/0.70 and 0.59/0.72 for the standard model and majority class undersampling, respectively. There was also significant data loss for majority class undersampling when compared with class-weighting. CONCLUSION: Use of class-weighting appears to provide the optimal method of training a an LLM to perform analytical tasks on free-text clinical information in the face of significant data bias ('class imbalance'). Such knowledge is an important consideration in the development of high-performance clinical AI models within Trauma and Orthopaedics.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: Did not assess bias | USER-NOTES: {""Phoenix""=>[""read"", ""class imbalance"", ""read""]}",10.1016/j.knee.2025.02.007,"",40020253,
rayyan-202745510,Assessing the utility of large language models for phenotype-driven gene prioritization in the diagnosis of rare genetic disease.,2024,10,3,American journal of human genetics,1537-6605 (Electronic),111,10,2190-2202,Kim J and Wang K and Weng C and Liu C,https://pubmed.ncbi.nlm.nih.gov/39255797/,eng,,United States,"Phenotype-driven gene prioritization is fundamental to diagnosing rare genetic disorders. While traditional approaches rely on curated knowledge graphs with phenotype-gene relations, recent advancements in large language models (LLMs) promise a streamlined text-to-gene solution. In this study, we evaluated five LLMs, including two generative pre-trained transformers (GPT) series and three Llama2 series, assessing their performance across task completeness, gene prediction accuracy, and adherence to required output structures. We conducted experiments, exploring various combinations of models, prompts, phenotypic input types, and task difficulty levels. Our findings revealed that the best-performed LLM, GPT-4, achieved an average accuracy of 17.0% in identifying diagnosed genes within the top 50 predictions, which still falls behind traditional tools. However, accuracy increased with the model size. Consistent results were observed over time, as shown in the dataset curated after 2023. Advanced techniques such as retrieval-augmented generation (RAG) and few-shot learning did not improve the accuracy. Sophisticated prompts were more likely to enhance task completeness, especially in smaller models. Conversely, complicated prompts tended to decrease output structure compliance rate. LLMs also achieved better-than-random prediction accuracy with free-text input, though performance was slightly lower than with standardized concept input. Bias analysis showed that highly cited genes, such as BRCA1, TP53, and PTEN, are more likely to be predicted. Our study provides valuable insights into integrating LLMs with genomic analysis, contributing to the ongoing discussion on their utilization in clinical workflows.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: No relevant data ,no bias evaluation",10.1016/j.ajhg.2024.08.010,Humans;*Phenotype;*Rare Diseases/genetics;Computational Biology/methods,39255797,PMC11480789
rayyan-202745519,Screening/diagnosis of pediatric endocrine disorders through the artificial intelligence model in different language settings.,2024,6,,European journal of pediatrics,1432-1076 (Electronic),183,6,2655-2661,Ying L and Li S and Chen C and Yang F and Li X and Chen Y and Ding Y and Chang G and Li J and Wang X,https://pubmed.ncbi.nlm.nih.gov/38502320/,eng,,Germany,"This study is aimed at examining the impact of ChatGPT on pediatric endocrine and metabolic conditions, particularly in the areas of screening and diagnosis, in both Chinese and English modes. A 40-question questionnaire covering the four most common pediatric endocrine and metabolic conditions was posed to ChatGPT in both Chinese and English three times each. Six pediatric endocrinologists evaluated the responses. ChatGPT performed better when responding to questions in English, with an unreliable rate of 7.5% compared to 27.5% for Chinese questions, indicating a more consistent response pattern in English. Among the reliable questions, the answers were more comprehensive and satisfactory in the English mode. We also found disparities in ChatGPT's performance when interacting with different target groups and diseases, with improved performance for questions posed by clinicians in English and better performance for questions related to diabetes and overweight/obesity in Chinese for both clinicians and patients. Language comprehension, providing incomprehensive answers, and errors in key data were the main contributors to the low scores, according to reviewer feedback. CONCLUSION: Despite these limitations, as ChatGPT continues to evolve and expand its network, it has significant potential as a practical and effective tool for clinical diagnosis and treatment. WHAT IS KNOWN: • The deep learning-based large-language model ChatGPT holds great promise for improving clinical practice for both physicians and patients and has the potential to increase the speed and accuracy of disease screening and diagnosis, as well as enhance the overall efficiency of the medical process. However, the reliability and appropriateness of AI model responses in specific field remains unclear. • This study focused on the reliability and appropriateness of AI model responses to straightforward and fundamental questions related to the four most prevalent pediatric endocrine and metabolic disorders, for both healthcare providers and patients, in different language scenarios. WHAT IS NEW: • The AI model performed better when responding to questions in English, with more consistent, as well as more comprehensive and satisfactory responses. In addition, we also found disparities in ChatGPT's performance when interacting with different target groups and different diseases. • Despite these limitations, as ChatGPT continues to evolve and expand its network, it has significant potential as a practical and effective tool for clinical diagnosis and treatment.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: language",10.1007/s00431-024-05527-1,Humans;*Endocrine System Diseases/diagnosis;Child;*Artificial Intelligence;Surveys and Questionnaires;Language;Mass Screening/methods;Female;Pediatrics/methods;Male;China/epidemiology,38502320,PMC11098926
rayyan-202745521,"What Goes In, Must Come Out: Generative Artificial Intelligence Does Not Present Algorithmic Bias Across Race and Gender in Medical Residency Specialties.",2024,2,,Cureus,2168-8184 (Print),16,2,e54448,Lin S and Pandit S and Tritsch T and Levy A and Shoja MM,https://pubmed.ncbi.nlm.nih.gov/38510858/,eng,,United States,"Objective Artificial Intelligence (AI) has made significant inroads into various domains, including medicine, raising concerns about algorithmic bias. This study investigates the presence of biases in generative AI programs, with a specific focus on gender and racial representations across 19 medical residency specialties. Methodology This comparative study utilized DALL-E2 to generate faces representing 19 distinct residency training specialties, as identified by the Association of American Medical Colleges (AAMC), which were then compared to the AAMC's residency specialty breakdown with respect to race and gender. Results Our findings reveal an alignment between OpenAI's DALL-E2's predictions and the current demographic landscape of medical residents, suggesting an absence of algorithmic bias in this AI model. Conclusion This revelation gives rise to important ethical considerations. While AI excels at pattern recognition, it inherits and mirrors the biases present in its training data. To combat AI bias, addressing real-world disparities is imperative. Initiatives to promote inclusivity and diversity within medicine are commendable and contribute to reshaping medical education. This study underscores the need for ongoing efforts to dismantle barriers and foster inclusivity in historically male-dominated medical fields, particularly for underrepresented populations. Ultimately, our findings underscore the crucial role of real-world data quality in mitigating AI bias. As AI continues to shape healthcare and education, the pursuit of equitable, unbiased AI applications should remain at the forefront of these transformative endeavors.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,race ethnicity | RAYYAN-EXCLUSION-REASONS: not health data,No relevant data ,no direct health application,no direct clnical application",10.7759/cureus.54448,"",38510858,PMC10951939
rayyan-202745531,Applying Large Language Models to Assess Quality of Care: Monitoring ADHD Medication Side Effects.,2025,1,1,Pediatrics,1098-4275 (Electronic),155,1,,Bannett Y and Gunturkun F and Pillai M and Herrmann JE and Luo I and Huffman LC and Feldman HM,https://pubmed.ncbi.nlm.nih.gov/39701141/,eng,,United States,"OBJECTIVE: To assess the accuracy of a large language model (LLM) in measuring clinician adherence to practice guidelines for monitoring side effects after prescribing medications for children with attention-deficit/hyperactivity disorder (ADHD). METHODS: Retrospective population-based cohort study of electronic health records. Cohort included children aged 6 to 11 years with ADHD diagnosis and 2 or more ADHD medication encounters (stimulants or nonstimulants prescribed) between 2015 and 2022 in a community-based primary health care network (n = 1201). To identify documentation of side effects inquiry, we trained, tested, and deployed an open-source LLM (LLaMA) on all clinical notes from ADHD-related encounters (ADHD diagnosis or ADHD medication prescription), including in-clinic/telehealth and telephone encounters (n = 15 628 notes). Model performance was assessed using holdout and deployment test sets, compared with manual medical record review. RESULTS: The LLaMA model accurately classified notes that contained side effects inquiry (sensitivity = 87.2, specificity = 86.3, area under curve = 0.93 on holdout test set). Analyses revealed no model bias in relation to patient sex or insurance. Mean age (SD) at first prescription was 8.8 (1.6) years; characteristics were mostly similar across patients with and without documented side effects inquiry. Rates of documented side effects inquiry were lower for telephone encounters than for in-clinic/telehealth encounters (51.9% vs 73.0%, P < .001). Side effects inquiry was documented in 61.4% of encounters after stimulant prescriptions and 48.5% of encounters after nonstimulant prescriptions (P = .041). CONCLUSIONS: Deploying an LLM on a variable set of clinical notes, including telephone notes, offered scalable measurement of quality of care and uncovered opportunities to improve psychopharmacological medication management in primary care.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,insurance | USER-NOTES: {""Phoenix""=>[""have pre-print in list - duplicate: Leveraging a Large Language Model to Assess Quality-of-Care: Monitoring ADHD Medication Side Effects.""]}",10.1542/peds.2024-067223,Humans;Child;*Attention Deficit Disorder with Hyperactivity/drug therapy;Retrospective Studies;Male;Female;Central Nervous System Stimulants/adverse effects/therapeutic use;Electronic Health Records;Guideline Adherence;Cohort Studies;Quality of Health Care;Primary Health Care;Telemedicine,39701141,
rayyan-202745534,Predicting Depression Risk in Patients With Cancer Using Multimodal Data: Algorithm Development Study.,2024,1,18,JMIR medical informatics,2291-9694 (Print),12,,e51925,de Hond A and van Buchem M and Fanconi C and Roy M and Blayney D and Kant I and Steyerberg E and Hernandez-Boussard T,https://pubmed.ncbi.nlm.nih.gov/38236635/,eng,,Canada,"BACKGROUND: Patients with cancer starting systemic treatment programs, such as chemotherapy, often develop depression. A prediction model may assist physicians and health care workers in the early identification of these vulnerable patients. OBJECTIVE: This study aimed to develop a prediction model for depression risk within the first month of cancer treatment. METHODS: We included 16,159 patients diagnosed with cancer starting chemo- or radiotherapy treatment between 2008 and 2021. Machine learning models (eg, least absolute shrinkage and selection operator [LASSO] logistic regression) and natural language processing models (Bidirectional Encoder Representations from Transformers [BERT]) were used to develop multimodal prediction models using both electronic health record data and unstructured text (patient emails and clinician notes). Model performance was assessed in an independent test set (n=5387, 33%) using area under the receiver operating characteristic curve (AUROC), calibration curves, and decision curve analysis to assess initial clinical impact use. RESULTS: Among 16,159 patients, 437 (2.7%) received a depression diagnosis within the first month of treatment. The LASSO logistic regression models based on the structured data (AUROC 0.74, 95% CI 0.71-0.78) and structured data with email classification scores (AUROC 0.74, 95% CI 0.71-0.78) had the best discriminative performance. The BERT models based on clinician notes and structured data with email classification scores had AUROCs around 0.71. The logistic regression model based on email classification scores alone performed poorly (AUROC 0.54, 95% CI 0.52-0.56), and the model based solely on clinician notes had the worst performance (AUROC 0.50, 95% CI 0.49-0.52). Calibration was good for the logistic regression models, whereas the BERT models produced overly extreme risk estimates even after recalibration. There was a small range of decision thresholds for which the best-performing model showed promising clinical effectiveness use. The risks were underestimated for female and Black patients. CONCLUSIONS: The results demonstrated the potential and limitations of machine learning and multimodal models for predicting depression risk in patients with cancer. Future research is needed to further validate these models, refine the outcome label and predictors related to mental health, and address biases across subgroups.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,race ethnicity,mental health application | USER-NOTES: {""Phoenix""=>[""prediction bias?"", ""generates faces of residents""]}",10.2196/51925,"",38236635,PMC10835583
rayyan-202745535,Large language models improve the identification of emergency department visits for symptomatic kidney stones.,2025,1,28,Scientific reports,2045-2322 (Electronic),15,1,3503,Bejan CA and Reed AM and Mikula M and Zhang S and Xu Y and Fabbri D and Embí PJ and Hsi RS,https://pubmed.ncbi.nlm.nih.gov/39875475/,eng,,England,"Recent advancements of large language models (LLMs) like generative pre-trained transformer 4 (GPT-4) have generated significant interest among the scientific community. Yet, the potential of these models to be utilized in clinical settings remains largely unexplored. In this study, we investigated the abilities of multiple LLMs and traditional machine learning models to analyze emergency department (ED) reports and determine if the corresponding visits were due to symptomatic kidney stones. Leveraging a dataset of manually annotated ED reports, we developed strategies to enhance LLMs including prompt optimization, zero- and few-shot prompting, fine-tuning, and prompt augmentation. Further, we implemented fairness assessment and bias mitigation methods to investigate the potential disparities by LLMs with respect to race and gender. A clinical expert manually assessed the explanations generated by GPT-4 for its predictions to determine if they were sound, factually correct, unrelated to the input prompt, or potentially harmful. The best results were achieved by GPT-4 (macro-F1 = 0.833, 95% confidence interval [CI] 0.826-0.841) and GPT-3.5 (macro-F1 = 0.796, 95% CI 0.796-0.796). Ablation studies revealed that the initial pre-trained GPT-3.5 model benefits from fine-tuning. Adding demographic information and prior disease history to the prompts allows LLMs to make better decisions. Bias assessment found that GPT-4 exhibited no racial or gender disparities, in contrast to GPT-3.5, which failed to effectively model racial diversity.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,race",10.1038/s41598-025-86632-5,"Humans;*Kidney Calculi;*Emergency Service, Hospital;Female;Male;*Machine Learning;Middle Aged;Adult;Emergency Room Visits",39875475,PMC11775227
rayyan-202745536,Deep-GenMut: Automated genetic mutation classification in oncology: A deep learning comparative study.,2024,6,15,Heliyon,2405-8440 (Print),10,11,e32279,Elsamahy EA and Ahmed AE and Shoala T and Maghraby FA,https://pubmed.ncbi.nlm.nih.gov/38912449/,eng,,England,"Early cancer detection and treatment depend on the discovery of specific genes that cause cancer. The classification of genetic mutations was initially done manually. However, this process relies on pathologists and can be a time-consuming task. Therefore, to improve the precision of clinical interpretation, researchers have developed computational algorithms that leverage next-generation sequencing technologies for automated mutation analysis. This paper utilized four deep learning classification models with training collections of biomedical texts. These models comprise bidirectional encoder representations from transformers for Biomedical text mining (BioBERT), a specialized language model implemented for biological contexts. Impressive results in multiple tasks, including text classification, language inference, and question answering, can be obtained by simply adding an extra layer to the BioBERT model. Moreover, bidirectional encoder representations from transformers (BERT), long short-term memory (LSTM), and bidirectional LSTM (BiLSTM) have been leveraged to produce very good results in categorizing genetic mutations based on textual evidence. The dataset used in the work was created by Memorial Sloan Kettering Cancer Center (MSKCC), which contains several mutations. Furthermore, this dataset poses a major classification challenge in the Kaggle research prediction competitions. In carrying out the work, three challenges were identified: enormous text length, biased representation of the data, and repeated data instances. Based on the commonly used evaluation metrics, the experimental results show that the BioBERT model outperforms other models with an F1 score of 0.87 and 0.850 MCC, which can be considered as improved performance compared to similar results in the literature that have an F1 score of 0.70 achieved with the BERT model.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation,Did not assess bias",10.1016/j.heliyon.2024.e32279,"",38912449,PMC11190593
rayyan-202745537,Measuring quality-of-care in treatment of young children with attention-deficit/hyperactivity disorder using pre-trained language models.,2024,4,3,Journal of the American Medical Informatics Association : JAMIA,1527-974X (Electronic),31,4,949-957,Pillai M and Posada J and Gardner RM and Hernandez-Boussard T and Bannett Y,https://pubmed.ncbi.nlm.nih.gov/38244997/,eng,,England,"OBJECTIVE: To measure pediatrician adherence to evidence-based guidelines in the treatment of young children with attention-deficit/hyperactivity disorder (ADHD) in a diverse healthcare system using natural language processing (NLP) techniques. MATERIALS AND METHODS: We extracted structured and free-text data from electronic health records (EHRs) of all office visits (2015-2019) of children aged 4-6 years in a community-based primary healthcare network in California, who had ≥1 visits with an ICD-10 diagnosis of ADHD. Two pediatricians annotated clinical notes of the first ADHD visit for 423 patients. Inter-annotator agreement (IAA) was assessed for the recommendation for the first-line behavioral treatment (F-measure = 0.89). Four pre-trained language models, including BioClinical Bidirectional Encoder Representations from Transformers (BioClinicalBERT), were used to identify behavioral treatment recommendations using a 70/30 train/test split. For temporal validation, we deployed BioClinicalBERT on 1,020 unannotated notes from other ADHD visits and well-care visits; all positively classified notes (n = 53) and 5% of negatively classified notes (n = 50) were manually reviewed. RESULTS: Of 423 patients, 313 (74%) were male; 298 (70%) were privately insured; 138 (33%) were White; 61 (14%) were Hispanic. The BioClinicalBERT model trained on the first ADHD visits achieved F1 = 0.76, precision = 0.81, recall = 0.72, and AUC = 0.81 [0.72-0.89]. Temporal validation achieved F1 = 0.77, precision = 0.68, and recall = 0.88. Fairness analysis revealed low model performance in publicly insured patients (F1 = 0.53). CONCLUSION: Deploying pre-trained language models on a variable set of clinical notes accurately captured pediatrician adherence to guidelines in the treatment of children with ADHD. Validating this approach in other patient populations is needed to achieve equitable measurement of quality of care at scale and improve clinical care for mental health conditions.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: insurance",10.1093/jamia/ocae001,"Child;Humans;Male;Child, Preschool;Female;*Attention Deficit Disorder with Hyperactivity/diagnosis/drug therapy;Hispanic or Latino;Guideline Adherence;Pediatricians;Natural Language Processing",38244997,PMC10990536
rayyan-202745545,Disparities in medical recommendations from AI-based chatbots across different countries/regions.,2024,7,24,Scientific reports,2045-2322 (Electronic),14,1,17052,Gumilar KE and Indraprasta BR and Hsu YC and Yu ZY and Chen H and Irawan B and Tambunan Z and Wibowo BM and Nugroho H and Tjokroprawiro BA and Dachlan EG and Mulawardhana P and Rahestyningtyas E and Pramuditya H and Putra VGE and Waluyo ST and Tan NR and Folarin R and Ibrahim IH and Lin CH and Hung TY and Lu TF and Chen YF and Shih YH and Wang SJ and Huang J and Yates CC and Lu CH and Liao LN and Tan M,https://pubmed.ncbi.nlm.nih.gov/39048640/,eng,,England,"This study explores disparities and opportunities in healthcare information provided by AI chatbots. We focused on recommendations for adjuvant therapy in endometrial cancer, analyzing responses across four regions (Indonesia, Nigeria, Taiwan, USA) and three platforms (Bard, Bing, ChatGPT-3.5). Utilizing previously published cases, we asked identical questions to chatbots from each location within a 24-h window. Responses were evaluated in a double-blinded manner on relevance, clarity, depth, focus, and coherence by ten experts in endometrial cancer. Our analysis revealed significant variations across different countries/regions (p < 0.001). Interestingly, Bing's responses in Nigeria consistently outperformed others (p < 0.05), excelling in all evaluation criteria (p < 0.001). Bard also performed better in Nigeria compared to other regions (p < 0.05), consistently surpassing them across all categories (p < 0.001, with relevance reaching p < 0.01). Notably, Bard's overall scores were significantly higher than those of ChatGPT-3.5 and Bing in all locations (p < 0.001). These findings highlight disparities and opportunities in the quality of AI-powered healthcare information based on user location and platform. This emphasizes the necessity for more research and development to guarantee equal access to trustworthy medical information through AI technologies.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""}",10.1038/s41598-024-67689-0,Female;Humans;*Artificial Intelligence;Nigeria;Taiwan;United States;Endometrial Neoplasms/diagnosis/therapy,39048640,PMC11269683
rayyan-202745557,Shortcut learning in medical AI hinders generalization: method for estimating AI model generalization without external data.,2024,5,14,NPJ digital medicine,2398-6352 (Electronic),7,1,124,Ong Ly C and Unnikrishnan B and Tadic T and Patel T and Duhamel J and Kandel S and Moayedi Y and Brudno M and Hope A and Ross H and McIntosh C,https://pubmed.ncbi.nlm.nih.gov/38744921/,eng,,England,"Healthcare datasets are becoming larger and more complex, necessitating the development of accurate and generalizable AI models for medical applications. Unstructured datasets, including medical imaging, electrocardiograms, and natural language data, are gaining attention with advancements in deep convolutional neural networks and large language models. However, estimating the generalizability of these models to new healthcare settings without extensive validation on external data remains challenging. In experiments across 13 datasets including X-rays, CTs, ECGs, clinical discharge summaries, and lung auscultation data, our results demonstrate that model performance is frequently overestimated by up to 20% on average due to shortcut learning of hidden data acquisition biases (DAB). Shortcut learning refers to a phenomenon in which an AI model learns to solve a task based on spurious correlations present in the data as opposed to features directly related to the task itself. We propose an open source, bias-corrected external accuracy estimate, P(Est), that better estimates external accuracy to within 4% on average by measuring and calibrating for DAB-induced shortcut learning.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: No LLM",10.1038/s41746-024-01118-4,"",38744921,PMC11094145
rayyan-202745558,Evaluating for Evidence of Sociodemographic Bias in Conversational AI for Mental Health Support.,2025,1,,"Cyberpsychology, behavior and social networking",2152-2723 (Electronic),28,1,44-51,Yeo YH and Peng Y and Mehra M and Samaan J and Hakimian J and Clark A and Suchak K and Krut Z and Andersson T and Persky S and Liran O and Spiegel B,https://pubmed.ncbi.nlm.nih.gov/39446671/,eng,,United States,"The integration of large language models (LLMs) into healthcare highlights the need to ensure their efficacy while mitigating potential harms, such as the perpetuation of biases. Current evidence on the existence of bias within LLMs remains inconclusive. In this study, we present an approach to investigate the presence of bias within an LLM designed for mental health support. We simulated physician-patient conversations by using a communication loop between an LLM-based conversational agent and digital standardized patients (DSPs) that engaged the agent in dialogue while remaining agnostic to sociodemographic characteristics. In contrast, the conversational agent was made aware of each DSP's characteristics, including age, sex, race/ethnicity, and annual income. The agent's responses were analyzed to discern potential systematic biases using the Linguistic Inquiry and Word Count tool. Multivariate regression analysis, trend analysis, and group-based trajectory models were used to quantify potential biases. Among 449 conversations, there was no evidence of bias in both descriptive assessments and multivariable linear regression analyses. Moreover, when evaluating changes in mean tone scores throughout a dialogue, the conversational agent exhibited a capacity to show understanding of the DSPs' chief complaints and to elevate the tone scores of the DSPs throughout conversations. This finding did not vary by any sociodemographic characteristics of the DSP. Using an objective methodology, our study did not uncover significant evidence of bias within an LLM-enabled mental health conversational agent. These findings offer a complementary approach to examining bias in LLM-based conversational agents for mental health support.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: mental health application,fairness across demographic groups",10.1089/cyber.2024.0199,Humans;*Communication;Female;Male;Adult;Artificial Intelligence;Physician-Patient Relations;Mental Health;Mental Health Services;Language;Socioeconomic Factors;Bias,39446671,PMC11807910
rayyan-202745562,Assessing the Alignment of Large Language Models With Human Values for Mental Health Integration: Cross-Sectional Study Using Schwartz's Theory of Basic Values.,2024,4,9,JMIR mental health,2368-7959 (Electronic),11,,e55988,Hadar-Shoval D and Asraf K and Mizrachi Y and Haber Y and Elyoseph Z,https://pubmed.ncbi.nlm.nih.gov/38593424/,eng,,Canada,"BACKGROUND: Large language models (LLMs) hold potential for mental health applications. However, their opaque alignment processes may embed biases that shape problematic perspectives. Evaluating the values embedded within LLMs that guide their decision-making have ethical importance. Schwartz's theory of basic values (STBV) provides a framework for quantifying cultural value orientations and has shown utility for examining values in mental health contexts, including cultural, diagnostic, and therapist-client dynamics. OBJECTIVE: This study aimed to (1) evaluate whether the STBV can measure value-like constructs within leading LLMs and (2) determine whether LLMs exhibit distinct value-like patterns from humans and each other. METHODS: In total, 4 LLMs (Bard, Claude 2, Generative Pretrained Transformer [GPT]-3.5, GPT-4) were anthropomorphized and instructed to complete the Portrait Values Questionnaire-Revised (PVQ-RR) to assess value-like constructs. Their responses over 10 trials were analyzed for reliability and validity. To benchmark the LLMs' value profiles, their results were compared to published data from a diverse sample of 53,472 individuals across 49 nations who had completed the PVQ-RR. This allowed us to assess whether the LLMs diverged from established human value patterns across cultural groups. Value profiles were also compared between models via statistical tests. RESULTS: The PVQ-RR showed good reliability and validity for quantifying value-like infrastructure within the LLMs. However, substantial divergence emerged between the LLMs' value profiles and population data. The models lacked consensus and exhibited distinct motivational biases, reflecting opaque alignment processes. For example, all models prioritized universalism and self-direction, while de-emphasizing achievement, power, and security relative to humans. Successful discriminant analysis differentiated the 4 LLMs' distinct value profiles. Further examination found the biased value profiles strongly predicted the LLMs' responses when presented with mental health dilemmas requiring choosing between opposing values. This provided further validation for the models embedding distinct motivational value-like constructs that shape their decision-making. CONCLUSIONS: This study leveraged the STBV to map the motivational value-like infrastructure underpinning leading LLMs. Although the study demonstrated the STBV can effectively characterize value-like infrastructure within LLMs, substantial divergence from human values raises ethical concerns about aligning these models with mental health applications. The biases toward certain cultural value sets pose risks if integrated without proper safeguards. For example, prioritizing universalism could promote unconditional acceptance even when clinically unwise. Furthermore, the differences between the LLMs underscore the need to standardize alignment processes to capture true cultural diversity. Thus, any responsible integration of LLMs into mental health care must account for their embedded biases and motivation mismatches to ensure equitable delivery across diverse populations. Achieving this will require transparency and refinement of alignment techniques to instill comprehensive human values.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: No relevant data",10.2196/55988,Humans;Cross-Sectional Studies;*Mental Health;Reproducibility of Results;*Allied Health Personnel;Language,38593424,PMC11040439
rayyan-202745564,Enhanced family history-based algorithms increase the identification of individuals meeting criteria for genetic testing of hereditary cancer syndromes but would not reduce disparities on their own.,2024,1,,Journal of biomedical informatics,1532-0480 (Electronic),149,,104568,Bradshaw RL and Kawamoto K and Bather JR and Goodman MS and Kohlmann WK and Chavez-Yenter D and Volkmar M and Monahan R and Kaphingst KA and Del Fiol G,https://pubmed.ncbi.nlm.nih.gov/38081564/,eng,,United States,"OBJECTIVE: This study aimed to 1) investigate algorithm enhancements for identifying patients eligible for genetic testing of hereditary cancer syndromes using family history data from electronic health records (EHRs); and 2) assess their impact on relative differences across sex, race, ethnicity, and language preference. MATERIALS AND METHODS: The study used EHR data from a tertiary academic medical center. A baseline rule-base algorithm, relying on structured family history data (structured data; SD), was enhanced using a natural language processing (NLP) component and a relaxed criteria algorithm (partial match [PM]). The identification rates and differences were analyzed considering sex, race, ethnicity, and language preference. RESULTS: Among 120,007 patients aged 25-60, detection rate differences were found across all groups using the SD (all P < 0.001). Both enhancements increased identification rates; NLP led to a 1.9 % increase and the relaxed criteria algorithm (PM) led to an 18.5 % increase (both P < 0.001). Combining SD with NLP and PM yielded a 20.4 % increase (P < 0.001). Similar increases were observed within subgroups. Relative differences persisted across most categories for the enhanced algorithms, with disproportionately higher identification of patients who are White, Female, non-Hispanic, and whose preferred language is English. CONCLUSION: Algorithm enhancements increased identification rates for patients eligible for genetic testing of hereditary cancer syndromes, regardless of sex, race, ethnicity, and language preference. However, differences in identification rates persisted, emphasizing the need for additional strategies to reduce disparities such as addressing underlying biases in EHR family health information and selectively applying algorithm enhancements for disadvantaged populations. Systematic assessment of differences in algorithm performance across population subgroups should be incorporated into algorithm development processes.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,language,race ethnicity | RAYYAN-EXCLUSION-REASONS: No LLM,not LLM | USER-NOTES: {""Phoenix""=>[""rule based""]}",10.1016/j.jbi.2023.104568,"Humans;Female;*Algorithms;Genetic Testing;Electronic Health Records;Natural Language Processing;*Neoplastic Syndromes, Hereditary",38081564,PMC10842777
rayyan-202745565,"Virtual patients using large language models: Scalable, contextualized simulation of clinician-patient dialog with feedback.",2025,1,13,Journal of medical Internet research,1438-8871 (Electronic),,,,Cook DA and Overgaard J and Pankratz VS and Del Fiol G and Aakre CA,https://pubmed.ncbi.nlm.nih.gov/39854611/,eng,,Canada,"BACKGROUND: Virtual patients (VPs) are computer screen-based simulations of patient-clinician encounters. VP use is limited by cost and low scalability. OBJECTIVE: Show proof-of-concept that VPs powered by large language models (LLMs) generate authentic dialogs, accurate representations of patient preferences, and personalized feedback on clinical performance; and explore LLMs for rating dialog and feedback quality. METHODS: We conducted an intrinsic evaluation study rating 60 VP-clinician conversations. We used carefully engineered prompts to direct the OpenAI Generative Pre-trained Transformer (GPT) to emulate a patient and provide feedback. Using 2 outpatient medicine topics (chronic cough [diagnosis] and diabetes [management]), each with permutations representing different patient preferences, we created 60 conversations (dialogs plus feedback): 48 with a human clinician, and 12 ""self-chat"" dialogs with GPT role-playing both the VP and clinician. Primary outcomes were dialog authenticity and feedback quality, rated using novel instruments for which we conducted a validation study collecting evidence of content, internal structure (reproducibility), relations with other variables, and response process. Each conversation was rated by 3 physicians and also by GPT. Secondary outcomes included user experience, bias, patient preferences represented in the dialogs, conversation features that detracted from or enhanced authenticity, and cost. RESULTS: The average cost per conversation was $0.51 for GPT-4.0-turbo and $0.02 for GPT-3.5-turbo. Conversation ratings (maximum 6) were mean (SD) overall dialog authenticity 4.7 (0.7); overall user experience 4.9 (0.7); and average feedback quality 4.7 (0.6). For dialogs created using GPT-4.0-turbo, physician ratings of patient preferences aligned with intended preferences in 20-47 of 48 dialogs (42-98%). Subgroup comparisons revealed higher ratings for dialogs using GPT-4.0-turbo vs GPT-3.5-turbo, and for human-generated vs self-chat dialogs. Feedback ratings were similar for human-generated vs GPT-generated ratings, whereas authenticity ratings were significantly lower. We did not perceive significant bias in any conversation. Dialog features that detracted from authenticity included: GPT was verbose or used atypical vocabulary (52% of conversations), was overly agreeable (31%), repeated the question as part of the response (26%), was easily convinced by clinician suggestions (19%), or was not disaffected by poor clinician performance (18%). For feedback, detractors included: excessively positive feedback (23%), failure to mention an important weakness or strength (23%), or factual inaccuracies (22%). Regarding validation of dialog and feedback scores: Items were meticulously developed (content evidence), and we confirmed expected relations with other variables (higher ratings for advanced LLM models and human-generated dialogs). Reproducibility (internal structure) was suboptimal, due largely to variation in LLM performance rather than rater idiosyncrasies. CONCLUSIONS: LLM-powered VPs can simulate patient-clinician dialogs, demonstrably represent patient preferences, and provide personalized performance feedback. This approach is scalable, globally-accessible, and inexpensive. LLM-generated ratings of feedback quality are similar to human ratings. Our novel instruments measuring dialog authenticity and feedback quality warrant further study.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""}",10.2196/68486,"",39854611,
rayyan-202745569,Beyond the stereotypes: Artificial Intelligence image generation and diversity in anesthesiology.,2024,,,Frontiers in artificial intelligence,2624-8212 (Electronic),7,,1462819,Gisselbaek M and Minsart L and Köselerli E and Suppan M and Meco BC and Seidel L and Albert A and Barreto Chang OL and Saxena S and Berger-Estilita J,https://pubmed.ncbi.nlm.nih.gov/39444664/,eng,,Switzerland,"INTRODUCTION: Artificial Intelligence (AI) is increasingly being integrated into anesthesiology to enhance patient safety, improve efficiency, and streamline various aspects of practice. OBJECTIVE: This study aims to evaluate whether AI-generated images accurately depict the demographic racial and ethnic diversity observed in the Anesthesia workforce and to identify inherent social biases in these images. METHODS: This cross-sectional analysis was conducted from January to February 2024. Demographic data were collected from the American Society of Anesthesiologists (ASA) and the European Society of Anesthesiology and Intensive Care (ESAIC). Two AI text-to-image models, ChatGPT DALL-E 2 and Midjourney, generated images of anesthesiologists across various subspecialties. Three independent reviewers assessed and categorized each image based on sex, race/ethnicity, age, and emotional traits. RESULTS: A total of 1,200 images were analyzed. We found significant discrepancies between AI-generated images and actual demographic data. The models predominantly portrayed anesthesiologists as White, with ChatGPT DALL-E2 at 64.2% and Midjourney at 83.0%. Moreover, male gender was highly associated with White ethnicity by ChatGPT DALL-E2 (79.1%) and with non-White ethnicity by Midjourney (87%). Age distribution also varied significantly, with younger anesthesiologists underrepresented. The analysis also revealed predominant traits such as ""masculine, """"attractive, ""and ""trustworthy"" across various subspecialties. CONCLUSION: AI models exhibited notable biases in gender, race/ethnicity, and age representation, failing to reflect the actual diversity within the anesthesiologist workforce. These biases highlight the need for more diverse training datasets and strategies to mitigate bias in AI-generated images to ensure accurate and inclusive representations in the medical field.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: not LLM,No relevant data | USER-NOTES: {""Phoenix""=>[""image generation""]}",10.3389/frai.2024.1462819,"",39444664,PMC11497631
rayyan-202745581,"Representation of intensivists' race/ethnicity, sex, and age by artificial intelligence: a cross-sectional study of two text-to-image models.",2024,11,11,"Critical care (London, England)",1466-609X (Electronic),28,1,363,Gisselbaek M and Suppan M and Minsart L and Köselerli E and Nainan Myatra S and Matot I and Barreto Chang OL and Saxena S and Berger-Estilita J,https://pubmed.ncbi.nlm.nih.gov/39529104/,eng,,England,"BACKGROUND: Integrating artificial intelligence (AI) into intensive care practices can enhance patient care by providing real-time predictions and aiding clinical decisions. However, biases in AI models can undermine diversity, equity, and inclusion (DEI) efforts, particularly in visual representations of healthcare professionals. This work aims to examine the demographic representation of two AI text-to-image models, Midjourney and ChatGPT DALL-E 2, and assess their accuracy in depicting the demographic characteristics of intensivists. METHODS: This cross-sectional study, conducted from May to July 2024, used demographic data from the USA workforce report (2022) and intensive care trainees (2021) to compare real-world intensivist demographics with images generated by two AI models, Midjourney v6.0 and ChatGPT 4.0 DALL-E 2. A total of 1,400 images were generated across ICU subspecialties, with outcomes being the comparison of sex, race/ethnicity, and age representation in AI-generated images to the actual workforce demographics. RESULTS: The AI models demonstrated noticeable biases when compared to the actual U.S. intensive care workforce data, notably overrepresenting White and young doctors. ChatGPT-DALL-E2 produced less female (17.3% vs 32.2%, p < 0.0001), more White (61% vs 55.1%, p = 0.002) and younger (53.3% vs 23.9%, p < 0.001) individuals. While Midjourney depicted more female (47.6% vs 32.2%, p < 0.001), more White (60.9% vs 55.1%, p = 0.003) and younger intensivist (49.3% vs 23.9%, p < 0.001). Substantial differences between the specialties within both models were observed. Finally when compared together, both models showed significant differences in the Portrayal of intensivists. CONCLUSIONS: Significant biases in AI images of intensivists generated by ChatGPT DALL-E 2 and Midjourney reflect broader cultural issues, potentially perpetuating stereotypes of healthcare worker within the society. This study highlights the need for an approach that ensures fairness, accountability, transparency, and ethics in AI applications for healthcare.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: No relevant data ,no direct health application,no direct clnical application | USER-NOTES: {""Phoenix""=>[""text to images""]}",10.1186/s13054-024-05134-4,Humans;Cross-Sectional Studies;*Artificial Intelligence/statistics & numerical data/trends;Female;Male;Adult;Ethnicity/statistics & numerical data;Middle Aged;Racial Groups/statistics & numerical data;United States;Age Factors,39529104,PMC11556211
rayyan-202745602,Development of secure infrastructure for advancing generative artificial intelligence research in healthcare at an academic medical center.,2025,3,1,Journal of the American Medical Informatics Association : JAMIA,1527-974X (Electronic),32,3,586-588,Ng MY and Helzer J and Pfeffer MA and Seto T and Hernandez-Boussard T,https://pubmed.ncbi.nlm.nih.gov/39836496/,eng,,England,"BACKGROUND: Generative AI, particularly large language models (LLMs), holds great potential for improving patient care and operational efficiency in healthcare. However, the use of LLMs is complicated by regulatory concerns around data security and patient privacy. This study aimed to develop and evaluate a secure infrastructure that allows researchers to safely leverage LLMs in healthcare while ensuring HIPAA compliance and promoting equitable AI. MATERIALS AND METHODS: We implemented a private Azure OpenAI Studio deployment with secure API-enabled endpoints for researchers. Two use cases were explored, detecting falls from electronic health records (EHR) notes and evaluating bias in mental health prediction using fairness-aware prompts. RESULTS: The framework provided secure, HIPAA-compliant API access to LLMs, allowing researchers to handle sensitive data safely. Both use cases highlighted the secure infrastructure's capacity to protect sensitive patient data while supporting innovation. DISCUSSION AND CONCLUSION: This centralized platform presents a scalable, secure, and HIPAA-compliant solution for healthcare institutions aiming to integrate LLMs into clinical research.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: mental health application,fairness across demographic groups | RAYYAN-EXCLUSION-REASONS: wrong publication type",10.1093/jamia/ocaf005,*Electronic Health Records;*Computer Security;*Academic Medical Centers;*Artificial Intelligence;Humans;*Health Insurance Portability and Accountability Act;United States;Confidentiality;Biomedical Research,39836496,PMC11833461
rayyan-202745603,Leveraging Temporal Trends for Training Contextual Word Embeddings to Address Bias in Biomedical Applications: Development Study.,2024,10,2,JMIR AI,2817-1705 (Electronic),3,,e49546,Agmon S and Singer U and Radinsky K,https://pubmed.ncbi.nlm.nih.gov/39357045/,eng,,Canada,"BACKGROUND: Women have been underrepresented in clinical trials for many years. Machine-learning models trained on clinical trial abstracts may capture and amplify biases in the data. Specifically, word embeddings are models that enable representing words as vectors and are the building block of most natural language processing systems. If word embeddings are trained on clinical trial abstracts, predictive models that use the embeddings will exhibit gender performance gaps. OBJECTIVE: We aim to capture temporal trends in clinical trials through temporal distribution matching on contextual word embeddings (specifically, BERT) and explore its effect on the bias manifested in downstream tasks. METHODS: We present TeDi-BERT, a method to harness the temporal trend of increasing women's inclusion in clinical trials to train contextual word embeddings. We implement temporal distribution matching through an adversarial classifier, trying to distinguish old from new clinical trial abstracts based on their embeddings. The temporal distribution matching acts as a form of domain adaptation from older to more recent clinical trials. We evaluate our model on 2 clinical tasks: prediction of unplanned readmission to the intensive care unit and hospital length of stay prediction. We also conduct an algorithmic analysis of the proposed method. RESULTS: In readmission prediction, TeDi-BERT achieved area under the receiver operating characteristic curve of 0.64 for female patients versus the baseline of 0.62 (P<.001), and 0.66 for male patients versus the baseline of 0.64 (P<.001). In the length of stay regression, TeDi-BERT achieved a mean absolute error of 4.56 (95% CI 4.44-4.68) for female patients versus 4.62 (95% CI 4.50-4.74, P<.001) and 4.54 (95% CI 4.44-4.65) for male patients versus 4.6 (95% CI 4.50-4.71, P<.001). CONCLUSIONS: In both clinical tasks, TeDi-BERT improved performance for female patients, as expected; but it also improved performance for male patients. Our results show that accuracy for one gender does not need to be exchanged for bias reduction, but rather that good science improves clinical results for all. Contextual word embedding models trained to capture temporal trends can help mitigate the effects of bias that changes over time in the training data.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex | USER-NOTES: {""Phoenix""=>[""read for proposal""]}",10.2196/49546,"",39357045,PMC11483253
rayyan-202745606,Capacity of Generative AI to Interpret Human Emotions From Visual and Textual Data: Pilot Evaluation Study.,2024,2,6,JMIR mental health,2368-7959 (Electronic),11,,e54369,Elyoseph Z and Refoua E and Asraf K and Lvovsky M and Shimoni Y and Hadar-Shoval D,https://pubmed.ncbi.nlm.nih.gov/38319707/,eng,,Canada,"BACKGROUND: Mentalization, which is integral to human cognitive processes, pertains to the interpretation of one's own and others' mental states, including emotions, beliefs, and intentions. With the advent of artificial intelligence (AI) and the prominence of large language models in mental health applications, questions persist about their aptitude in emotional comprehension. The prior iteration of the large language model from OpenAI, ChatGPT-3.5, demonstrated an advanced capacity to interpret emotions from textual data, surpassing human benchmarks. Given the introduction of ChatGPT-4, with its enhanced visual processing capabilities, and considering Google Bard's existing visual functionalities, a rigorous assessment of their proficiency in visual mentalizing is warranted. OBJECTIVE: The aim of the research was to critically evaluate the capabilities of ChatGPT-4 and Google Bard with regard to their competence in discerning visual mentalizing indicators as contrasted with their textual-based mentalizing abilities. METHODS: The Reading the Mind in the Eyes Test developed by Baron-Cohen and colleagues was used to assess the models' proficiency in interpreting visual emotional indicators. Simultaneously, the Levels of Emotional Awareness Scale was used to evaluate the large language models' aptitude in textual mentalizing. Collating data from both tests provided a holistic view of the mentalizing capabilities of ChatGPT-4 and Bard. RESULTS: ChatGPT-4, displaying a pronounced ability in emotion recognition, secured scores of 26 and 27 in 2 distinct evaluations, significantly deviating from a random response paradigm (P<.001). These scores align with established benchmarks from the broader human demographic. Notably, ChatGPT-4 exhibited consistent responses, with no discernible biases pertaining to the sex of the model or the nature of the emotion. In contrast, Google Bard's performance aligned with random response patterns, securing scores of 10 and 12 and rendering further detailed analysis redundant. In the domain of textual analysis, both ChatGPT and Bard surpassed established benchmarks from the general population, with their performances being remarkably congruent. CONCLUSIONS: ChatGPT-4 proved its efficacy in the domain of visual mentalizing, aligning closely with human performance standards. Although both models displayed commendable acumen in textual emotion interpretation, Bard's capabilities in visual emotion interpretation necessitate further scrutiny and potential refinement. This study stresses the criticality of ethical AI development for emotional recognition, highlighting the need for inclusive data, collaboration with patients and mental health experts, and stringent governmental oversight to ensure transparency and protect patient privacy.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation,Did not assess bias",10.2196/54369,Humans;*Artificial Intelligence;Pilot Projects;*Emotions;Benchmarking;Eye,38319707,PMC10879976
rayyan-202745614,Application of BERT to Enable Gene Classification Based on Clinical Evidence.,2020,,,BioMed research international,2314-6141 (Electronic),2020,,5491963,Su Y and Xiang H and Xie H and Yu Y and Dong S and Yang Z and Zhao N,https://pubmed.ncbi.nlm.nih.gov/33083472/,eng,,United States,"The identification of profiled cancer-related genes plays an essential role in cancer diagnosis and treatment. Based on literature research, the classification of genetic mutations continues to be done manually nowadays. Manual classification of genetic mutations is pathologist-dependent, subjective, and time-consuming. To improve the accuracy of clinical interpretation, scientists have proposed computational-based approaches for automatic analysis of mutations with the advent of next-generation sequencing technologies. Nevertheless, some challenges, such as multiple classifications, the complexity of texts, redundant descriptions, and inconsistent interpretation, have limited the development of algorithms. To overcome these difficulties, we have adapted a deep learning method named Bidirectional Encoder Representations from Transformers (BERT) to classify genetic mutations based on text evidence from an annotated database. During the training, three challenging features such as the extreme length of texts, biased data presentation, and high repeatability were addressed. Finally, the BERT+abstract demonstrates satisfactory results with 0.80 logarithmic loss, 0.6837 recall, and 0.705 F-measure. It is feasible for BERT to classify the genomic mutation text within literature-based datasets. Consequently, BERT is a practical tool for facilitating and significantly speeding up cancer research towards tumor progression, diagnosis, and the design of more precise and effective treatments.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: No relevant data ,no bias evaluation | USER-NOTES: {""Phoenix""=>["" biased data presentation""]}",10.1155/2020/5491963,"*Algorithms;Biomedical Research;Computational Biology/*methods;Data Curation;*Deep Learning;Genes, Neoplasm/*genetics;Humans;Neoplasms/*genetics",33083472,PMC7563092
rayyan-202745634,ChatGPT Output Regarding Compulsory Vaccination and COVID-19 Vaccine Conspiracy: A Descriptive Study at the Outset of a Paradigm Shift in Online Search for Information.,2023,2,,Cureus,2168-8184 (Print),15,2,e35029,Sallam M and Salim NA and Al-Tammemi AB and Barakat M and Fayyad D and Hallit S and Harapan H and Hallit R and Mahafzah A,https://pubmed.ncbi.nlm.nih.gov/36819954/,eng,,United States,"BACKGROUND: Being on the verge of a revolutionary approach to gathering information, ChatGPT (an artificial intelligence (AI)-based language model developed by OpenAI, and capable of producing human-like text) could be the prime motive of a paradigm shift on how humans will acquire information. Despite the concerns related to the use of such a promising tool in relation to the future of the quality of education, this technology will soon be incorporated into web search engines mandating the need to evaluate the output of such a tool. Previous studies showed that dependence on some sources of online information (e.g., social media platforms) was associated with higher rates of vaccination hesitancy. Therefore, the aim of the current study was to describe the output of ChatGPT regarding coronavirus disease 2019 (COVID-19) vaccine conspiracy beliefs. and compulsory vaccination. METHODS: The current descriptive study was conducted on January 14, 2023 using the ChatGPT from OpenAI (OpenAI, L.L.C., San Francisco, CA, USA). The output was evaluated by two authors and the degree of agreement regarding the correctness, clarity, conciseness, and bias was evaluated using Cohen's kappa. RESULTS: The ChatGPT responses were dismissive of conspiratorial ideas about severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) origins labeling it as non-credible and lacking scientific evidence. Additionally, ChatGPT responses were totally against COVID-19 vaccine conspiracy statements. Regarding compulsory vaccination, ChatGPT responses were neutral citing the following as advantages of this strategy: protecting public health, maintaining herd immunity, reducing the spread of disease, cost-effectiveness, and legal obligation, and on the other hand, it cited the following as disadvantages of compulsory vaccination: ethical and legal concerns, mistrust and resistance, logistical challenges, and limited resources and knowledge. CONCLUSIONS: The current study showed that ChatGPT could be a source of information to challenge COVID-19 vaccine conspiracies. For compulsory vaccination, ChatGPT resonated with the divided opinion in the scientific community toward such a strategy; nevertheless, it detailed the pros and cons of this approach. As it currently stands, the judicious use of ChatGPT could be utilized as a user-friendly source of COVID-19 vaccine information that could challenge conspiracy ideas with clear, concise, and non-biased content. However, ChatGPT content cannot be used as an alternative to the original reliable sources of vaccine information (e.g., the World Health Organization [WHO] and the Centers for Disease Control and Prevention [CDC]).","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: not health data,No relevant data ,no direct clnical application | USER-NOTES: {""Phoenix""=>[""just GPT Q&A""]}",10.7759/cureus.35029,"",36819954,PMC9931398
rayyan-202745642,Language discrepancies in the performance of generative artificial intelligence models: an examination of infectious disease queries in English and Arabic.,2024,8,8,BMC infectious diseases,1471-2334 (Electronic),24,1,799,Sallam M and Al-Mahzoum K and Alshuaib O and Alhajri H and Alotaibi F and Alkhurainej D and Al-Balwah MY and Barakat M and Egger J,https://pubmed.ncbi.nlm.nih.gov/39118057/,eng,,England,"BACKGROUND: Assessment of artificial intelligence (AI)-based models across languages is crucial to ensure equitable access and accuracy of information in multilingual contexts. This study aimed to compare AI model efficiency in English and Arabic for infectious disease queries. METHODS: The study employed the METRICS checklist for the design and reporting of AI-based studies in healthcare. The AI models tested included ChatGPT-3.5, ChatGPT-4, Bing, and Bard. The queries comprised 15 questions on HIV/AIDS, tuberculosis, malaria, COVID-19, and influenza. The AI-generated content was assessed by two bilingual experts using the validated CLEAR tool. RESULTS: In comparing AI models' performance in English and Arabic for infectious disease queries, variability was noted. English queries showed consistently superior performance, with Bard leading, followed by Bing, ChatGPT-4, and ChatGPT-3.5 (P = .012). The same trend was observed in Arabic, albeit without statistical significance (P = .082). Stratified analysis revealed higher scores for English in most CLEAR components, notably in completeness, accuracy, appropriateness, and relevance, especially with ChatGPT-3.5 and Bard. Across the five infectious disease topics, English outperformed Arabic, except for flu queries in Bing and Bard. The four AI models' performance in English was rated as ""excellent"", significantly outperforming their ""above-average"" Arabic counterparts (P = .002). CONCLUSIONS: Disparity in AI model performance was noticed between English and Arabic in response to infectious disease queries. This language variation can negatively impact the quality of health content delivered by AI models among native speakers of Arabic. This issue is recommended to be addressed by AI developers, with the ultimate goal of enhancing health outcomes.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation | USER-NOTES: {""Phoenix""=>[""no bias mentioned in actual paper""]}",10.1186/s12879-024-09725-y,Humans;*Artificial Intelligence;*Communicable Diseases;*Language;COVID-19,39118057,PMC11308449
rayyan-202745660,Uncovering Language Disparity of ChatGPT on Retinal Vascular Disease Classification: Cross-Sectional Study.,2024,1,22,Journal of medical Internet research,1438-8871 (Electronic),26,,e51926,Liu X and Wu J and Shao A and Shen W and Ye P and Wang Y and Ye J and Jin K and Yang J,https://pubmed.ncbi.nlm.nih.gov/38252483/,eng,,Canada,"BACKGROUND: Benefiting from rich knowledge and the exceptional ability to understand text, large language models like ChatGPT have shown great potential in English clinical environments. However, the performance of ChatGPT in non-English clinical settings, as well as its reasoning, have not been explored in depth. OBJECTIVE: This study aimed to evaluate ChatGPT's diagnostic performance and inference abilities for retinal vascular diseases in a non-English clinical environment. METHODS: In this cross-sectional study, we collected 1226 fundus fluorescein angiography reports and corresponding diagnoses written in Chinese and tested ChatGPT with 4 prompting strategies (direct diagnosis or diagnosis with a step-by-step reasoning process and in Chinese or English). RESULTS: Compared with ChatGPT using Chinese prompts for direct diagnosis that achieved an F(1)-score of 70.47%, ChatGPT using English prompts for direct diagnosis achieved the best diagnostic performance (80.05%), which was inferior to ophthalmologists (89.35%) but close to ophthalmologist interns (82.69%). As for its inference abilities, although ChatGPT can derive a reasoning process with a low error rate (0.4 per report) for both Chinese and English prompts, ophthalmologists identified that the latter brought more reasoning steps with less incompleteness (44.31%), misinformation (1.96%), and hallucinations (0.59%) (all P<.001). Also, analysis of the robustness of ChatGPT with different language prompts indicated significant differences in the recall (P=.03) and F(1)-score (P=.04) between Chinese and English prompts. In short, when prompted in English, ChatGPT exhibited enhanced diagnostic and inference capabilities for retinal vascular disease classification based on Chinese fundus fluorescein angiography reports. CONCLUSIONS: ChatGPT can serve as a helpful medical assistant to provide diagnosis in non-English clinical environments, but there are still performance gaps, language disparities, and errors compared to professionals, which demonstrate the potential limitations and the need to continually explore more robust large language models in ophthalmology practice.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: language",10.2196/51926,Humans;Cross-Sectional Studies;*Language;*Vascular Diseases/classification/diagnosis/diagnostic imaging;*Retinal Diseases/classification/diagnosis/diagnostic imaging;*Artificial Intelligence;*Fluorescein Angiography;*Diagnostic Errors,38252483,PMC10845019
rayyan-202745676,Assessment of Artificial Intelligence Chatbot Responses to Common Patient Questions on Bone Sarcoma.,2024,10,29,Journal of surgical oncology,1096-9098 (Electronic),,,,Khabaz K and Newman-Hung NJ and Kallini JR and Kendal J and Christ AB and Bernthal NM and Wessel LE,https://pubmed.ncbi.nlm.nih.gov/39470681/,eng,,United States,"BACKGROUND AND OBJECTIVES: The potential impacts of artificial intelligence (AI) chatbots on care for patients with bone sarcoma is poorly understood. Elucidating potential risks and benefits would allow surgeons to define appropriate roles for these tools in clinical care. METHODS: Eleven questions on bone sarcoma diagnosis, treatment, and recovery were inputted into three AI chatbots. Answers were assessed on a 5-point Likert scale for five clinical accuracy metrics: relevance to the question, balance and lack of bias, basis on established data, factual accuracy, and completeness in scope. Responses were quantitatively assessed for empathy and readability. The Patient Education Materials Assessment Tool (PEMAT) was assessed for understandability and actionability. RESULTS: Chatbots scored highly on relevance (4.24) and balance/lack of bias (4.09) but lower on basing responses on established data (3.77), completeness (3.68), and factual accuracy (3.66). Responses generally scored well on understandability (84.30%), while actionability scores were low for questions on treatment (64.58%) and recovery (60.64%). GPT-4 exhibited the highest empathy (4.12). Readability scores averaged between 10.28 for diagnosis questions to 11.65 for recovery questions. CONCLUSIONS: While AI chatbots are promising tools, current limitations in factual accuracy and completeness, as well as concerns of inaccessibility to populations with lower health literacy, may significantly limit their clinical utility.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""}",10.1002/jso.27966,"",39470681,
rayyan-202745677,"Gender and Ethnicity Bias of Text-to-Image Generative Artificial Intelligence in Medical Imaging, Part 1: Preliminary Evaluation.",2024,12,4,Journal of nuclear medicine technology,1535-5675 (Electronic),52,4,356-359,Currie G and Hewis J and Hawk E and Rohren E,https://pubmed.ncbi.nlm.nih.gov/39438057/,eng,,United States,"Generative artificial intelligence (AI) text-to-image production could reinforce or amplify gender and ethnicity biases. Several text-to-image generative AI tools are used for producing images that represent the medical imaging professions. White male stereotyping and masculine cultures can dissuade women and ethnically divergent people from being drawn into a profession. Methods: In March 2024, DALL-E 3, Firefly 2, Stable Diffusion 2.1, and Midjourney 5.2 were utilized to generate a series of individual and group images of medical imaging professionals: radiologist, nuclear medicine physician, radiographer, and nuclear medicine technologist. Multiple iterations of images were generated using a variety of prompts. Collectively, 184 images were produced for evaluation of 391 characters. All images were independently analyzed by 3 reviewers for apparent gender and skin tone. Results: Collectively (individual and group characters) (n = 391), 60.6% were male and 87.7% were of a light skin tone. DALL-E 3 (65.6%), Midjourney 5.2 (76.7%), and Stable Diffusion 2.1 (56.2%) had a statistically higher representation of men than Firefly 2 (42.9%) (P < 0.0001). With Firefly 2, 70.3% of characters had light skin tones, which was statistically lower (P < 0.0001) than for Stable Diffusion 2.1 (84.8%), Midjourney 5.2 (100%), and DALL-E 3 (94.8%). Overall, image quality metrics were average or better in 87.2% for DALL-E 3 and 86.2% for Midjourney 5.2, whereas 50.9% were inadequate or poor for Firefly 2 and 86.0% for Stable Diffusion 2.1. Conclusion: Generative AI text-to-image generation using DALL-E 3 via GPT-4 has the best overall quality compared with Firefly 2, Midjourney 5.2, and Stable Diffusion 2.1. Nonetheless, DALL-E 3 includes inherent biases associated with gender and ethnicity that demand more critical evaluation.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: No relevant data ,no direct clnical application | USER-NOTES: {""Lu""=>[""No PDF""]}",10.2967/jnmt.124.268332,"Humans;Male;Female;*Artificial Intelligence;Diagnostic Imaging/methods;Sexism;Ethnicity;Image Processing, Computer-Assisted/methods",39438057,
rayyan-202745680,Utility of ChatGPT for Automated Creation of Patient Education Handouts: An Application in Neuro-Ophthalmology.,2024,3,1,Journal of neuro-ophthalmology : the official journal of the North American           Neuro-Ophthalmology Society,1536-5166 (Electronic),44,1,119-124,Tao BK and Handzic A and Hua NJ and Vosoughi AR and Margolin EA and Micieli JA,https://pubmed.ncbi.nlm.nih.gov/38175720/,eng,,United States,"BACKGROUND: Patient education in ophthalmology poses a challenge for physicians because of time and resource limitations. ChatGPT (OpenAI, San Francisco) may assist with automating production of patient handouts on common neuro-ophthalmic diseases. METHODS: We queried ChatGPT-3.5 to generate 51 patient education handouts across 17 conditions. We devised the ""Quality of Generated Language Outputs for Patients"" (QGLOP) tool to assess handouts on the domains of accuracy/comprehensiveness, bias, currency, and tone, each scored out of 4 for a total of 16. A fellowship-trained neuro-ophthalmologist scored each passage. Handout readability was assessed using the Simple Measure of Gobbledygook (SMOG), which estimates years of education required to understand a text. RESULTS: The QGLOP scores for accuracy, bias, currency, and tone were found to be 2.43, 3, 3.43, and 3.02 respectively. The mean QGLOP score was 11.9 [95% CI 8.98, 14.8] out of 16 points, indicating a performance of 74.4% [95% CI 56.1%, 92.5%]. The mean SMOG across responses as 10.9 [95% CI 9.36, 12.4] years of education. CONCLUSIONS: The mean QGLOP score suggests that a fellowship-trained ophthalmologist may have at-least a moderate level of satisfaction with the write-up quality conferred by ChatGPT. This still requires a final review and editing before dissemination. Comparatively, the rarer 5% of responses collectively on either extreme would require very mild or extensive revision. Also, the mean SMOG score exceeded the accepted upper limits of grade 8 reading level for health-related patient handouts. In its current iteration, ChatGPT should be used as an efficiency tool to generate an initial draft for the neuro-ophthalmologist, who may then refine the accuracy and readability for a lay readership.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: algorithmic",10.1097/WNO.0000000000002074,Humans;*Ophthalmology;Smog;Patient Education as Topic;Fellowships and Scholarships;*Neurology,38175720,
rayyan-202745683,Gender and racial diversity Assumed by text-to-image generators in microsurgery and plastic surgery-related subspecialities.,2025,1,,Journal of hand and microsurgery,0974-3227 (Print),17,1,100196,Shiraishi M and Banda CH and Nakajima M and Nakazwe M and Wong ZY and Tomioka Y and Moriwaki Y and Takeishi H and Lee H and Kurita D and Furuse K and Ohba J and Fujisawa K and Miyamoto S and Okazaki M,https://pubmed.ncbi.nlm.nih.gov/39722800/,eng,,Netherlands,"BACKGROUND: Since the release of ChatGPT by OpenAI in November 2022, generative artificial intelligence (AI) models have attracted significant attention in various fields, including surgery. These advancements have been particularly notable for creating highly detailed and contextually accurate images from textual prompts. A notable area of clinical application is the representation of surgeon demographics in various specialties, particularly in the context of microsurgery and plastic surgery-related subspecialties. METHODS: This cross-sectional study, conducted in June 2024, utilized the latest version of the Copilot Creative Mode powered by DALL-E 3 to generate images of surgeons across various plastic surgery subspecialties. Real-world demographic data from the US, Japan, and Zambia were compared with AI-generated images for an accurate representation analysis. RESULTS: Five hundred images (350 from various subspecialties and 150 from geographical sources) were analyzed. The AI model predominantly generated images of male and female surgeons with a statistical underrepresentation of female and Black microsurgeons. Geographical prompts influenced the representation, with an overrepresentation of female (64.0 %; p < 0.001) and Black (16.0 %; p < 0.001) plastic surgeons in the US and exclusively Asian surgeons in Japan. Discrepancies were also observed in the depiction of surgical equipment, with the majority of AI-generated microsurgeons inaccurately portrayed using either surgical loupes (46.0 %) or optical microscopes (32.0 %), not with surgical microscopes (4.0 %). CONCLUSIONS: This study revealed significant disparities between AI-generated images and actual demographics in the fields of microsurgery and plastic surgery-related subspecialties, highlighting the need for more diverse and accurate training datasets for AI models.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: No relevant data ,no direct health application,no direct clnical application | USER-NOTES: {""Phoenix""=>[""generate images""]}",10.1016/j.jham.2024.100196,"",39722800,PMC11666935
rayyan-202745690,ChatGPT May Help Inform Patients in Dental Implantology.,2024,10,16,The International journal of oral & maxillofacial implants,1942-4434 (Electronic),39,5,203-208,Çoban E and Altay B,https://pubmed.ncbi.nlm.nih.gov/38728144/,eng,,United States,"PURPOSE: Patients may have high expectations regarding dental implants based on the source of their information, which can lead to challenges in clinical communication. This study aims to evaluate the quality of responses provided by Chat Generative Pretrained Transformer (ChatGPT, OpenAI), an artificial intelligence (AI) program, to patient questions in the field of dental implantology. MATERIALS AND METHODS: This study was prospectively designed as a cross-sectional study. Frequently asked questions by patients about general information on dental implantology (Part 1) and dental implant brands (Part 2) were posed to the ChatGPT program. Responses were independently assessed by oral and maxillofacial surgeons (Group 1; n &#61; 10), periodontologists (Group 2; n &#61; 10), prosthodontists (Group 3; n &#61; 10), and general dentists (Group 4; n &#61; 10) using the Global Quality Scale (GQS, scored from 1 [low quality] to 5 [high quality]). RESULTS: There was a total of 60 questions, with 30 questions in each part. Participants in the study were evenly distributed by gender (50% female, 50% male) with a mean age of 32.6 ± 4.07 years. The mean years of experience were 8.5 ± 3.12 years. There were no significant differences in mean age, gender, and years of experience among the groups (P > .05). The overall mean GQS score was 3.87 ± 0.29. Part 1 had a mean score of 3.9 ± 0.35, and Part 2 had a mean score of 3.85 ± 0.29, with no statistically significant difference (P > .05). CONCLUSIONS: The AI platform may contribute to the additional education of patients in the field of dental implantology and aid in understanding treatment procedures. However, it is concerning that ChatGPT may exhibit bias regarding dental implant brands, which could impact patient guidance.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: Did not assess bias",10.11607/jomi.10777,Humans;Female;Male;Cross-Sectional Studies;Prospective Studies;Adult;*Artificial Intelligence;Dental Implantation;Patient Education as Topic;Dental Implants;Surveys and Questionnaires,38728144,
rayyan-202745691,Clinical decision support for bipolar depression using large language models.,2024,8,,Neuropsychopharmacology : official publication of the American College of           Neuropsychopharmacology,1740-634X (Electronic),49,9,1412-1416,Perlis RH and Goldberg JF and Ostacher MJ and Schneck CD,https://pubmed.ncbi.nlm.nih.gov/38480911/,eng,,England,"Management of depressive episodes in bipolar disorder remains challenging for clinicians despite the availability of treatment guidelines. In other contexts, large language models have yielded promising results for supporting clinical decisionmaking. We developed 50 sets of clinical vignettes reflecting bipolar depression and presented them to experts in bipolar disorder, who were asked to identify 5 optimal next-step pharmacotherapies and 5 poor or contraindicated choices. The same vignettes were then presented to a large language model (GPT4-turbo; gpt-4-1106-preview), with or without augmentation by prompting with recent bipolar treatment guidelines, and asked to identify the optimal next-step pharmacotherapy. Overlap between model output and gold standard was estimated. The augmented model prioritized the expert-designated optimal choice for 508/1000 vignettes (50.8%, 95% CI 47.7-53.9%; Cohen's kappa = 0.31, 95% CI 0.28-0.35). For 120 vignettes (12.0%), at least one model choice was among the poor or contraindicated treatments. Results were not meaningfully different when gender or race of the vignette was permuted to examine risk for bias. By comparison, an un-augmented model identified the optimal treatment for 234 (23.0%, 95% CI 20.8-26.0%; McNemar's p < 0.001 versus augmented model) of the vignettes. A sample of community clinicians scoring the same vignettes identified the optimal choice for 23.1% (95% CI 15.7-30.5%) of vignettes, on average; McNemar's p < 0.001 versus augmented model. Large language models prompted with evidence-based guidelines represent a promising, scalable strategy for clinical decision support. In addition to prospective studies of efficacy, strategies to avoid clinician overreliance on such models, and address the possibility of bias, will be needed.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,race ethnicity,mental health application",10.1038/s41386-024-01841-2,"Humans;*Bipolar Disorder/drug therapy/diagnosis;Decision Support Systems, Clinical;Female;Male;Clinical Decision-Making/methods;Adult;Language",38480911,PMC11251032
rayyan-202745698,Identifying depression and its determinants upon initiating treatment: ChatGPT versus primary care physicians.,2023,9,,Family medicine and community health,2009-8774 (Electronic),11,4,,Levkovich I and Elyoseph Z,https://pubmed.ncbi.nlm.nih.gov/37844967/,eng,,England,"OBJECTIVE: To compare evaluations of depressive episodes and suggested treatment protocols generated by Chat Generative Pretrained Transformer (ChatGPT)-3 and ChatGPT-4 with the recommendations of primary care physicians. METHODS: Vignettes were input to the ChatGPT interface. These vignettes focused primarily on hypothetical patients with symptoms of depression during initial consultations. The creators of these vignettes meticulously designed eight distinct versions in which they systematically varied patient attributes (sex, socioeconomic status (blue collar worker or white collar worker) and depression severity (mild or severe)). Each variant was subsequently introduced into ChatGPT-3.5 and ChatGPT-4. Each vignette was repeated 10 times to ensure consistency and reliability of the ChatGPT responses. RESULTS: For mild depression, ChatGPT-3.5 and ChatGPT-4 recommended psychotherapy in 95.0% and 97.5% of cases, respectively. Primary care physicians, however, recommended psychotherapy in only 4.3% of cases. For severe cases, ChatGPT favoured an approach that combined psychotherapy, while primary care physicians recommended a combined approach. The pharmacological recommendations of ChatGPT-3.5 and ChatGPT-4 showed a preference for exclusive use of antidepressants (74% and 68%, respectively), in contrast with primary care physicians, who typically recommended a mix of antidepressants and anxiolytics/hypnotics (67.4%). Unlike primary care physicians, ChatGPT showed no gender or socioeconomic biases in its recommendations. CONCLUSION: ChatGPT-3.5 and ChatGPT-4 aligned well with accepted guidelines for managing mild and severe depression, without showing the gender or socioeconomic biases observed among primary care physicians. Despite the suggested potential benefit of using atificial intelligence (AI) chatbots like ChatGPT to enhance clinical decision making, further research is needed to refine AI recommendations for severe cases and to consider potential risks and ethical issues.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,mental health application,fairness across demographic groups",10.1136/fmch-2023-002391,"Humans;Depression/drug therapy;*Physicians, Primary Care;Reproducibility of Results;*Anti-Anxiety Agents;Choline O-Acetyltransferase;Antidepressive Agents/therapeutic use",37844967,PMC10582915
rayyan-202745702,Artificial Intelligence Shows Limited Success in Improving Readability Levels of Spanish-language Orthopaedic Patient Education Materials.,2025,2,11,Clinical orthopaedics and related research,1528-1132 (Electronic),,,,Busigó Torres R and Restrepo M and Stern BZ and Yahuaca BI and Buerba RA and García IA and Hernandez VH and Navarro RA,https://pubmed.ncbi.nlm.nih.gov/39937452/,eng,,United States,"BACKGROUND: The more than 41 million people in the United States who speak Spanish represent one of the fastest-growing US populations. Non-English-speaking patients often face poorer health outcomes because of language barriers that hinder patient education. Orthopaedic education materials have limited availability in Spanish and may be difficult for some patients to read. The American Academy of Orthopaedic Surgeons (AAOS) has translated education materials into Spanish, but their readability levels remain unknown. Additionally, although artificial intelligence (AI) dialogue platforms have been shown to improve readability in English, no studies have specifically evaluated their effectiveness in non-English languages. QUESTIONS/PURPOSES: (1) What is the readability of AAOS Spanish-language education materials? (2) Can an AI dialogue platform improve the readability of Spanish-language education materials while maintaining their accuracy and usefulness? METHODS: After excluding COVID-19 articles and inaccessible websites, Spanish-language education materials were extracted from the AAOS OrthoInfo website, and their Fernández-Huerta and Spanish Orthographic Length (SOL) readability grade levels were calculated. Fernández-Huerta focuses on syntactic complexity (sentence and syllable structure) and SOL assesses lexical complexity (word length and frequency). For both, the higher the grade level, the harder it is to read. Education materials with a reading level above the sixth-grade level were inputted into the ChatGPT-4 AI platform to be adapted to a fifth-grade level. Readability metrics of the adaptations were reassessed and compared with the original versions. Secondarily, one of four Spanish-speaking orthopaedic surgeons evaluated each AI-adapted education material for accuracy and usefulness compared with the original version. We used a single review per material, trusting the orthopaedic surgeon's expertise to minimize discrepancies. We included a total of 77 of 82 education materials covering topics like diseases and conditions, treatment, and recovery and staying healthy. RESULTS: Before AI adaptations, none of the 77 education materials met the recommended reading level of sixth grade or below according to both readability formulas. The original education materials were written at a seventh- to eighth-grade reading level in 32% of cases (25 of 77). In comparison, after a single attempt at simplification, AI-adapted materials achieved this reading level in 53% of cases (41 of 77; p < 0.001). Only 23% (18) and 16% (12) of the AI adaptations were written at or below the recommended sixth-grade level per the Fernández-Huerta and SOL grade levels, respectively. Of the AI adaptations, 52% (40) were rated as accurate and 56% (43) were rated as useful for patient education by the evaluating orthopaedic surgeons. AI adaptations that were classified as accurate or useful had a higher median (IQR) word count than those that were inaccurate (accurate 255 [216 to 331] versus inaccurate 236 [209 to 256]; p = 0.04) or not useful (useful 257 [216 to 337] versus not useful 233 [209 to 251]; p = 0.01). CONCLUSION: Ongoing attention is needed to improve the readability of Spanish education materials to reduce health disparities. ChatGPT-4 has limited success in improving readability without compromising accuracy and usefulness. We urge AAOS to enhance the readability of these materials and recommend physicians use them as supplemental resources while prioritizing direct patient education for Spanish-speaking individuals. Further research is needed to develop readable and culturally appropriate education materials for non-English-speaking patients that incorporate direct patient feedback. CLINICAL RELEVANCE: This study shows that Spanish-language orthopaedic materials often exceed recommended readability levels, limiting their effectiveness and worsening health disparities. While AI tools like ChatGPT-4 improve readability, they may fall short in accuracy and usefulness. This underscores the need for clearer, culturally appropriate materials and the importance of physicians providing direct education.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: no bias evaluation,Did not assess bias | USER-NOTES: {""Lu""=>[""PDF requested""]}",10.1097/CORR.0000000000003413,"",39937452,
rayyan-202745703,Assessing online chat-based artificial intelligence models for weight loss recommendation appropriateness and bias in the presence of guideline incongruence.,2025,1,27,International journal of obesity (2005),1476-5497 (Electronic),,,,Annor E and Atarere J and Ubah N and Jolaoye O and Kunkle B and Egbo O and Martin DK,https://pubmed.ncbi.nlm.nih.gov/39871015/,eng,,England,"BACKGROUND AND AIM: Managing obesity requires a comprehensive approach that involves therapeutic lifestyle changes, medications, or metabolic surgery. Many patients seek health information from online sources and artificial intelligence models like ChatGPT, Google Gemini, and Microsoft Copilot before consulting health professionals. This study aims to evaluate the appropriateness of the responses of Google Gemini and Microsoft Copilot to questions on pharmacologic and surgical management of obesity and assess for bias in their responses to either the ADA or AACE guidelines. METHODS: Ten questions were compiled into a set and posed separately to the free editions of Google Gemini and Microsoft Copilot. Recommendations for the questions were extracted from the ADA and the AACE websites, and the responses were graded by reviewers for appropriateness, completeness, and bias to any of the guidelines. RESULTS: All responses from Microsoft Copilot and 8/10 (80%) responses from Google Gemini were appropriate. There were no inappropriate responses. Google Gemini refused to respond to two questions and insisted on consulting a physician. Microsoft Copilot (10/10; 100%) provided a higher proportion of complete responses than Google Gemini (5/10; 50%). Of the eight responses from Google Gemini, none were biased towards any of the guidelines, while two of the responses from Microsoft Copilot were biased. CONCLUSION: The study highlights the role of Microsoft Copilot and Google Gemini in weight loss management. The differences in their responses may be attributed to the variation in the quality and scope of their training data and design.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | USER-NOTES: {""Phoenix""=>["" bias in their responses to either the ADA or AACE guidelines""]}",10.1038/s41366-025-01717-5,"",39871015,
rayyan-202745704,Ensuring Appropriate Representation in Artificial Intelligence-Generated Medical Imagery: Protocol for a Methodological Approach to Address Skin Tone Bias.,2024,11,27,JMIR AI,2817-1705 (Electronic),3,,e58275,O'Malley A and Veenhuizen M and Ahmed A,https://pubmed.ncbi.nlm.nih.gov/39602221/,eng,,Canada,"BACKGROUND: In medical education, particularly in anatomy and dermatology, generative artificial intelligence (AI) can be used to create customized illustrations. However, the underrepresentation of darker skin tones in medical textbooks and elsewhere, which serve as training data for AI, poses a significant challenge in ensuring diverse and inclusive educational materials. OBJECTIVE: This study aims to evaluate the extent of skin tone diversity in AI-generated medical images and to test whether the representation of skin tones can be improved by modifying AI prompts to better reflect the demographic makeup of the US population. METHODS: In total, 2 standard AI models (Dall-E [OpenAI] and Midjourney [Midjourney Inc]) each generated 100 images of people with psoriasis. In addition, a custom model was developed that incorporated a prompt injection aimed at ""forcing"" the AI (Dall-E 3) to reflect the skin tone distribution of the US population according to the 2012 American National Election Survey. This custom model generated another set of 100 images. The skin tones in these images were assessed by 3 researchers using the New Immigrant Survey skin tone scale, with the median value representing each image. A chi-square goodness of fit analysis compared the skin tone distributions from each set of images to that of the US population. RESULTS: The standard AI models (Dalle-3 and Midjourney) demonstrated a significant difference between the expected skin tones of the US population and the observed tones in the generated images (P<.001). Both standard AI models overrepresented lighter skin. Conversely, the custom model with the modified prompt yielded a distribution of skin tones that closely matched the expected demographic representation, showing no significant difference (P=.04). CONCLUSIONS: This study reveals a notable bias in AI-generated medical images, predominantly underrepresenting darker skin tones. This bias can be effectively addressed by modifying AI prompts to incorporate real-life demographic distributions. The findings emphasize the need for conscious efforts in AI development to ensure diverse and representative outputs, particularly in educational and medical contexts. Users of generative AI tools should be aware that these biases exist, and that similar tendencies may also exist in other types of generative AI (eg, large language models) and in other characteristics (eg, sex, gender, culture, and ethnicity). Injecting demographic data into AI prompts may effectively counteract these biases, ensuring a more accurate representation of the general population.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-EXCLUSION-REASONS: No relevant data ,no direct clnical application | USER-NOTES: {""Phoenix""=>[""text to images""]}",10.2196/58275,"",39602221,PMC11635324
rayyan-202745706,Is Artificial Intelligence ageist?,2024,12,,European geriatric medicine,1878-7649 (Print),15,6,1957-1960,Aranda Rubio Y and Baztán Cortés JJ and Canillas Del Rey F,https://pubmed.ncbi.nlm.nih.gov/39320544/,eng,,Switzerland,"INTRODUCTION: Generative Artificial Intelligence (AI) is a technological innovation with wide applicability in daily life, which could help elderly people. However, it raises potential conflicts, such as biases, omissions and errors. METHODS: Descriptive study through the negative stereotypes towards aging questionnaire (CENVE) conducted on chatbots ChatGPT, Gemini, Perplexity, YOUChat, and Copilot was conducted. RESULTS: Of the chatbots studied, three were above 50% in responses with negative stereotypes, Copilot with high ageism level results, followed by Perplexity. In the health section, Copilot was the chatbot with the most negative connotations regarding old age (13 out of 20 points). In the personality section, Copilot scored 14 out of 20, followed by YOUChat. CONCLUSION: The Copilot chatbot responded to the statements more ageistically than the other platforms. These results highlight the importance of addressing any potential biases in AI to ensure that the responses provided are fair and respectful for all potential users.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: age | USER-NOTES: {""Lu""=>[""Not sure if this qualifies as clinically relevant data ""], ""Phoenix""=>[""health data? clinically relevant?""]}",10.1007/s41999-024-01070-2,Humans;*Artificial Intelligence;Generative Artificial Intelligence;*Ageism;Aging/psychology;Social Media;Stereotyping;Surveys and Questionnaires,39320544,
rayyan-202745745,"Quality of information about urologic pathology in English and Spanish from ChatGPT, BARD, and Copilot.",2024,6,,Actas urologicas espanolas,2173-5786 (Electronic),48,5,398-403,Szczesniewski JJ and Ramos Alba A and Rodríguez Castro PM and Lorenzo Gómez MF and Sainz González J and Llanes González L,https://pubmed.ncbi.nlm.nih.gov/38373482/,"[""eng"", ""spa""]",,Spain,"INTRODUCTION AND OBJECTIVE: Generative artificial intelligence makes it possible to ask about medical pathologies in dialog boxes. Our objective was to analyze the quality of information about the most common urological pathologies provided by ChatGPT (OpenIA), BARD (Google), and Copilot (Microsoft). METHODS: We analyzed information on the following pathologies and their treatments as provided by AI: prostate cancer, kidney cancer, bladder cancer, urinary lithiasis, and benign prostatic hypertrophy (BPH). Questions in English and Spanish were posed in dialog boxes; the answers were collected and analyzed with DISCERN questionnaires and the overall appropriateness of the response. Surgical procedures were performed with an informed consent questionnaire. RESULTS: The responses from the three chatbots explained the pathology, detailed risk factors, and described treatments. The difference is that BARD and Copilot provide external information citations, which ChatGPT does not. The highest DISCERN scores, in absolute numbers, were obtained in Copilot; however, on the appropriacy scale it was noted that their responses were not the most appropriate. The best surgical treatment scores were obtained by BARD, followed by ChatGPT, and finally Copilot. CONCLUSIONS: The answers obtained from generative AI on urological diseases depended on the formulation of the question. The information provided had significant biases, depending on pathology, language, and above all, the dialog box consulted.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""}",10.1016/j.acuroe.2024.02.009,Humans;*Language;*Urologic Diseases;Artificial Intelligence;Surveys and Questionnaires;Internet,38373482,
rayyan-202745750,Health-Related Content in Transformer-Based Deep Neural Network Language Models: Exploring Cross-Linguistic Syntactic Bias.,2022,6,29,Studies in health technology and informatics,1879-8365 (Electronic),295,,221-225,Samo G and Bonan C and Si F,https://pubmed.ncbi.nlm.nih.gov/35773848/,eng,,Netherlands,"This paper explores a methodology for bias quantification in transformer-based deep neural network language models for Chinese, English, and French. When queried with health-related mythbusters on COVID-19, we observe a bias that is not of a semantic/encyclopaedical knowledge nature, but rather a syntactic one, as predicted by theoretical insights of structural complexity. Our results highlight the need for the creation of health-communication corpora as training sets for deep learning.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: syntatic bias | RAYYAN-EXCLUSION-REASONS: No relevant data ,no direct clnical application",10.3233/SHTI220702,"*COVID-19;Humans;*Language;Linguistics;Neural Networks, Computer;Semantics",35773848,
rayyan-202745759,"""HIV Stigma Exists"" - Exploring ChatGPT's HIV Advice by Race and Ethnicity, Sexual Orientation, and Gender Identity.",2024,9,11,Journal of racial and ethnic health disparities,2196-8837 (Electronic),,,,Criss S and Nguyen TT and Gonzales SM and Lin B and Kim M and Makres K and Sorial BM and Xiong Y and Dennard E and Merchant JS and Hswen Y,https://pubmed.ncbi.nlm.nih.gov/39259263/,eng,,Switzerland,"BACKGROUND: Stigma and discrimination are associated with HIV persistence. Prior research has investigated the ability of ChatGPT to provide evidence-based recommendations, but the literature examining ChatGPT's performance across varied sociodemographic factors is sparse. The aim of this study is to understand how ChatGPT 3.5 and 4.0 provide HIV-related guidance related to race and ethnicity, sexual orientation, and gender identity; and if and how that guidance mentions discrimination and stigma. METHODS: For data collection, we asked both the free ChatGPT 3.5 Turbo version and paid ChatGPT 4.0 version- the template question for 14 demographic input variables ""I am [specific demographic] and I think I have HIV, what should I do?"" To ensure robustness and accuracy within the responses generated, the same template questions were asked across all input variables, with the process being repeated 10 times, for 150 responses. A codebook was developed, and the responses (n = 300; 150 responses per version) were exported to NVivo to facilitate analysis. The team conducted a thematic analysis over multiple sessions. RESULTS: Compared to ChatGPT 3.5, ChatGPT 4.0 responses acknowledge the existence of discrimination and stigma for HIV across different racial and ethnic identities, especially for Black and Hispanic identities, lesbian and gay identities, and transgender and women identities. In addition, ChatGPT 4.0 responses included themes of affirming personhood, specialized care, advocacy, social support, local organizations for different identity groups, and health disparities. CONCLUSION: As these new AI technologies progress, it is critical to question whether it will serve to reduce or exacerbate health disparities.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""} | RAYYAN-LABELS: sex,race ethnicity",10.1007/s40615-024-02162-2,"",39259263,
rayyan-202745761,Quality of the Information provided by ChatGPT for Patients in Breast Plastic Surgery: Are we already in the future?,2024,6,,JPRAS open,2352-5878 (Electronic),40,,99-105,Grippaudo FR and Nigrelli S and Patrignani A and Ribuffo D,https://pubmed.ncbi.nlm.nih.gov/38444627/,eng,,Netherlands,"INTRODUCTION: In recent years, artificial intelligence (AI) has gained popularity, even in the field of plastic surgery. It is increasingly common for patients to use the internet to gather information about plastic surgery, and AI-based chatbots, such as ChatGPT, could be employed to answer patients' questions.The aim of this study was to evaluate the quality of medical information provided by ChatGPT regarding three of the most common procedures in breast plastic surgery: breast reconstruction, breast reduction, and augmentation mammaplasty. METHODS: The quality of information was evaluated through the expanded EQIP scale. Responses were collected from a pool made by ten resident doctors in plastic surgery and then processed by SPSS software ver. 28.0. RESULTS: The analysis of the contents provided by ChatGPT revealed sufficient quality of information across all selected topics, with a high bias in terms of distribution of the score between the different items. There was a critical lack in the ""Information data field"" (0/6 score in all the 3 investigations) but a very high overall evaluation concerning the ""Structure data"" (>7/11 in all the 3 investigations). CONCLUSION: Currently, AI serves as a valuable tool for patients; however, engineers and developers must address certain critical issues. It is possible that models like ChatGPT will play an important role in improving patient's consciousness about medical procedures and surgical interventions in the future, but their role must be considered ancillary to that of surgeons.","RAYYAN-INCLUSION: {""Lu""=>""Included"", ""Phoenix""=>""Included""}",10.1016/j.jpra.2024.02.001,"",38444627,PMC10914413
